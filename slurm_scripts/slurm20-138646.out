Sun Aug 11 23:28:35 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.183.01             Driver Version: 535.183.01   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  Tesla T4                       Off | 00000000:00:07.0 Off |                    0 |
| N/A   30C    P8               9W /  70W |      2MiB / 15360MiB |      0%      Default |
|                                         |                      |                  N/A |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
|  No running processes found                                                           |
+---------------------------------------------------------------------------------------+
 23:28:35 up 40 days, 13:20,  2 users,  load average: 0.07, 0.37, 0.74
cuda:0
Training epoch 1 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.2727581262588501
Training batch 2 / 32
Total batch reconstruction loss: 0.24840295314788818
Training batch 3 / 32
Total batch reconstruction loss: 0.21576666831970215
Training batch 4 / 32
Total batch reconstruction loss: 0.1965242177248001
Training batch 5 / 32
Total batch reconstruction loss: 0.17872527241706848
Training batch 6 / 32
Total batch reconstruction loss: 0.17078344523906708
Training batch 7 / 32
Total batch reconstruction loss: 0.15064692497253418
Training batch 8 / 32
Total batch reconstruction loss: 0.14432762563228607
Training batch 9 / 32
Total batch reconstruction loss: 0.1383674591779709
Training batch 10 / 32
Total batch reconstruction loss: 0.13468649983406067
Training batch 11 / 32
Total batch reconstruction loss: 0.13398416340351105
Training batch 12 / 32
Total batch reconstruction loss: 0.12363053858280182
Training batch 13 / 32
Total batch reconstruction loss: 0.11096827685832977
Training batch 14 / 32
Total batch reconstruction loss: 0.10253413021564484
Training batch 15 / 32
Total batch reconstruction loss: 0.09816024452447891
Training batch 16 / 32
Total batch reconstruction loss: 0.09906727075576782
Training batch 17 / 32
Total batch reconstruction loss: 0.09923237562179565
Training batch 18 / 32
Total batch reconstruction loss: 0.09881038963794708
Training batch 19 / 32
Total batch reconstruction loss: 0.09583484381437302
Training batch 20 / 32
Total batch reconstruction loss: 0.09637295454740524
Training batch 21 / 32
Total batch reconstruction loss: 0.09376239776611328
Training batch 22 / 32
Total batch reconstruction loss: 0.09504144638776779
Training batch 23 / 32
Total batch reconstruction loss: 0.09147904068231583
Training batch 24 / 32
Total batch reconstruction loss: 0.0930372029542923
Training batch 25 / 32
Total batch reconstruction loss: 0.09074676036834717
Training batch 26 / 32
Total batch reconstruction loss: 0.08873432874679565
Training batch 27 / 32
Total batch reconstruction loss: 0.08636511117219925
Training batch 28 / 32
Total batch reconstruction loss: 0.08701974153518677
Training batch 29 / 32
Total batch reconstruction loss: 0.08336776494979858
Training batch 30 / 32
Total batch reconstruction loss: 0.08897386491298676
Training batch 31 / 32
Total batch reconstruction loss: 0.08440788835287094
Training batch 32 / 32
Total batch reconstruction loss: 0.07842618227005005
Epoch [1/500], Train Loss: 0.1257, Validation Loss: 0.0999, Generator Loss: 25.7974, Discriminator Loss: 0.4967
Training epoch 2 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.08334602415561676
Training batch 2 / 32
Total batch reconstruction loss: 0.08432973921298981
Training batch 3 / 32
Total batch reconstruction loss: 0.08393001556396484
Training batch 4 / 32
Total batch reconstruction loss: 0.08817259967327118
Training batch 5 / 32
Total batch reconstruction loss: 0.08779744803905487
Training batch 6 / 32
Total batch reconstruction loss: 0.08258689939975739
Training batch 7 / 32
Total batch reconstruction loss: 0.08674775063991547
Training batch 8 / 32
Total batch reconstruction loss: 0.08283425867557526
Training batch 9 / 32
Total batch reconstruction loss: 0.08615721017122269
Training batch 10 / 32
Total batch reconstruction loss: 0.08679872751235962
Training batch 11 / 32
Total batch reconstruction loss: 0.08650495111942291
Training batch 12 / 32
Total batch reconstruction loss: 0.08230872452259064
Training batch 13 / 32
Total batch reconstruction loss: 0.08455505967140198
Training batch 14 / 32
Total batch reconstruction loss: 0.08020120114088058
Training batch 15 / 32
Total batch reconstruction loss: 0.07967982441186905
Training batch 16 / 32
Total batch reconstruction loss: 0.0830586850643158
Training batch 17 / 32
Total batch reconstruction loss: 0.07984332740306854
Training batch 18 / 32
Total batch reconstruction loss: 0.07824528962373734
Training batch 19 / 32
Total batch reconstruction loss: 0.08373896777629852
Training batch 20 / 32
Total batch reconstruction loss: 0.08229872584342957
Training batch 21 / 32
Total batch reconstruction loss: 0.08685287833213806
Training batch 22 / 32
Total batch reconstruction loss: 0.07848025858402252
Training batch 23 / 32
Total batch reconstruction loss: 0.08436398208141327
Training batch 24 / 32
Total batch reconstruction loss: 0.08316656947135925
Training batch 25 / 32
Total batch reconstruction loss: 0.08083866536617279
Training batch 26 / 32
Total batch reconstruction loss: 0.08111518621444702
Training batch 27 / 32
Total batch reconstruction loss: 0.08040400594472885
Training batch 28 / 32
Total batch reconstruction loss: 0.07768045365810394
Training batch 29 / 32
Total batch reconstruction loss: 0.07771451026201248
Training batch 30 / 32
Total batch reconstruction loss: 0.08007748425006866
Training batch 31 / 32
Total batch reconstruction loss: 0.0824539065361023
Training batch 32 / 32
Total batch reconstruction loss: 0.07748105376958847
Epoch [2/500], Train Loss: 0.0981, Validation Loss: 0.0996, Generator Loss: 17.4566, Discriminator Loss: 0.4843
Training epoch 3 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.08519959449768066
Training batch 2 / 32
Total batch reconstruction loss: 0.08480975031852722
Training batch 3 / 32
Total batch reconstruction loss: 0.08390089869499207
Training batch 4 / 32
Total batch reconstruction loss: 0.08277525007724762
Training batch 5 / 32
Total batch reconstruction loss: 0.0855431854724884
Training batch 6 / 32
Total batch reconstruction loss: 0.080638587474823
Training batch 7 / 32
Total batch reconstruction loss: 0.0839705616235733
Training batch 8 / 32
Total batch reconstruction loss: 0.07874825596809387
Training batch 9 / 32
Total batch reconstruction loss: 0.08458212018013
Training batch 10 / 32
Total batch reconstruction loss: 0.08604008704423904
Training batch 11 / 32
Total batch reconstruction loss: 0.08419854938983917
Training batch 12 / 32
Total batch reconstruction loss: 0.07953444123268127
Training batch 13 / 32
Total batch reconstruction loss: 0.08727708458900452
Training batch 14 / 32
Total batch reconstruction loss: 0.08366866409778595
Training batch 15 / 32
Total batch reconstruction loss: 0.0794859454035759
Training batch 16 / 32
Total batch reconstruction loss: 0.08169381320476532
Training batch 17 / 32
Total batch reconstruction loss: 0.08060389757156372
Training batch 18 / 32
Total batch reconstruction loss: 0.08325695991516113
Training batch 19 / 32
Total batch reconstruction loss: 0.07738810777664185
Training batch 20 / 32
Total batch reconstruction loss: 0.07920357584953308
Training batch 21 / 32
Total batch reconstruction loss: 0.07591573148965836
Training batch 22 / 32
Total batch reconstruction loss: 0.08058290183544159
Training batch 23 / 32
Total batch reconstruction loss: 0.08339560031890869
Training batch 24 / 32
Total batch reconstruction loss: 0.08019653707742691
Training batch 25 / 32
Total batch reconstruction loss: 0.07701995968818665
Training batch 26 / 32
Total batch reconstruction loss: 0.08227365463972092
Training batch 27 / 32
Total batch reconstruction loss: 0.08135112375020981
Training batch 28 / 32
Total batch reconstruction loss: 0.07986629009246826
Training batch 29 / 32
Total batch reconstruction loss: 0.07831338047981262
Training batch 30 / 32
Total batch reconstruction loss: 0.0797853171825409
Training batch 31 / 32
Total batch reconstruction loss: 0.07952466607093811
Training batch 32 / 32
Total batch reconstruction loss: 0.06980274617671967
Epoch [3/500], Train Loss: 0.0966, Validation Loss: 0.0953, Generator Loss: 17.1582, Discriminator Loss: 0.4497
Training epoch 4 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.0826585665345192
Training batch 2 / 32
Total batch reconstruction loss: 0.08125634491443634
Training batch 3 / 32
Total batch reconstruction loss: 0.07777832448482513
Training batch 4 / 32
Total batch reconstruction loss: 0.08134035021066666
Training batch 5 / 32
Total batch reconstruction loss: 0.08162461221218109
Training batch 6 / 32
Total batch reconstruction loss: 0.08425597846508026
Training batch 7 / 32
Total batch reconstruction loss: 0.07772301137447357
Training batch 8 / 32
Total batch reconstruction loss: 0.08015292882919312
Training batch 9 / 32
Total batch reconstruction loss: 0.08069384098052979
Training batch 10 / 32
Total batch reconstruction loss: 0.0817592665553093
Training batch 11 / 32
Total batch reconstruction loss: 0.08268193900585175
Training batch 12 / 32
Total batch reconstruction loss: 0.08270157128572464
Training batch 13 / 32
Total batch reconstruction loss: 0.0810660868883133
Training batch 14 / 32
Total batch reconstruction loss: 0.0770086795091629
Training batch 15 / 32
Total batch reconstruction loss: 0.07565631717443466
Training batch 16 / 32
Total batch reconstruction loss: 0.07501263916492462
Training batch 17 / 32
Total batch reconstruction loss: 0.07766956090927124
Training batch 18 / 32
Total batch reconstruction loss: 0.08616679906845093
Training batch 19 / 32
Total batch reconstruction loss: 0.07684382796287537
Training batch 20 / 32
Total batch reconstruction loss: 0.07858364284038544
Training batch 21 / 32
Total batch reconstruction loss: 0.07834240794181824
Training batch 22 / 32
Total batch reconstruction loss: 0.07984421402215958
Training batch 23 / 32
Total batch reconstruction loss: 0.07685041427612305
Training batch 24 / 32
Total batch reconstruction loss: 0.07654798030853271
Training batch 25 / 32
Total batch reconstruction loss: 0.07396753877401352
Training batch 26 / 32
Total batch reconstruction loss: 0.07671341300010681
Training batch 27 / 32
Total batch reconstruction loss: 0.07682066410779953
Training batch 28 / 32
Total batch reconstruction loss: 0.07780568301677704
Training batch 29 / 32
Total batch reconstruction loss: 0.076972097158432
Training batch 30 / 32
Total batch reconstruction loss: 0.07356036454439163
Training batch 31 / 32
Total batch reconstruction loss: 0.07501401752233505
Training batch 32 / 32
Total batch reconstruction loss: 0.07668668031692505
Epoch [4/500], Train Loss: 0.0934, Validation Loss: 0.0928, Generator Loss: 16.6285, Discriminator Loss: 0.4294
Training epoch 5 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.07992340624332428
Training batch 2 / 32
Total batch reconstruction loss: 0.07739439606666565
Training batch 3 / 32
Total batch reconstruction loss: 0.07905786484479904
Training batch 4 / 32
Total batch reconstruction loss: 0.07639680802822113
Training batch 5 / 32
Total batch reconstruction loss: 0.07646841555833817
Training batch 6 / 32
Total batch reconstruction loss: 0.07582488656044006
Training batch 7 / 32
Total batch reconstruction loss: 0.07753133773803711
Training batch 8 / 32
Total batch reconstruction loss: 0.07667025923728943
Training batch 9 / 32
Total batch reconstruction loss: 0.0760267972946167
Training batch 10 / 32
Total batch reconstruction loss: 0.07708188891410828
Training batch 11 / 32
Total batch reconstruction loss: 0.08088914304971695
Training batch 12 / 32
Total batch reconstruction loss: 0.07995770871639252
Training batch 13 / 32
Total batch reconstruction loss: 0.0768606960773468
Training batch 14 / 32
Total batch reconstruction loss: 0.07379858195781708
Training batch 15 / 32
Total batch reconstruction loss: 0.0793144702911377
Training batch 16 / 32
Total batch reconstruction loss: 0.07810978591442108
Training batch 17 / 32
Total batch reconstruction loss: 0.08229131251573563
Training batch 18 / 32
Total batch reconstruction loss: 0.0736345425248146
Training batch 19 / 32
Total batch reconstruction loss: 0.07716663926839828
Training batch 20 / 32
Total batch reconstruction loss: 0.07487516850233078
Training batch 21 / 32
Total batch reconstruction loss: 0.07819102704524994
Training batch 22 / 32
Total batch reconstruction loss: 0.07921382039785385
Training batch 23 / 32
Total batch reconstruction loss: 0.074099600315094
Training batch 24 / 32
Total batch reconstruction loss: 0.07990126311779022
Training batch 25 / 32
Total batch reconstruction loss: 0.07787179201841354
Training batch 26 / 32
Total batch reconstruction loss: 0.07848161458969116
Training batch 27 / 32
Total batch reconstruction loss: 0.08106128871440887
Training batch 28 / 32
Total batch reconstruction loss: 0.07534833252429962
Training batch 29 / 32
Total batch reconstruction loss: 0.0784573182463646
Training batch 30 / 32
Total batch reconstruction loss: 0.0783148780465126
Training batch 31 / 32
Total batch reconstruction loss: 0.07728530466556549
Training batch 32 / 32
Total batch reconstruction loss: 0.08080661296844482
Epoch [5/500], Train Loss: 0.0914, Validation Loss: 0.0906, Generator Loss: 16.3558, Discriminator Loss: 0.4051
Training epoch 6 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.07827900350093842
Training batch 2 / 32
Total batch reconstruction loss: 0.08184285461902618
Training batch 3 / 32
Total batch reconstruction loss: 0.07517558336257935
Training batch 4 / 32
Total batch reconstruction loss: 0.07649986445903778
Training batch 5 / 32
Total batch reconstruction loss: 0.07949905097484589
Training batch 6 / 32
Total batch reconstruction loss: 0.07904846966266632
Training batch 7 / 32
Total batch reconstruction loss: 0.07225634902715683
Training batch 8 / 32
Total batch reconstruction loss: 0.08039969205856323
Training batch 9 / 32
Total batch reconstruction loss: 0.075162872672081
Training batch 10 / 32
Total batch reconstruction loss: 0.07403453439474106
Training batch 11 / 32
Total batch reconstruction loss: 0.07850527763366699
Training batch 12 / 32
Total batch reconstruction loss: 0.0737302154302597
Training batch 13 / 32
Total batch reconstruction loss: 0.07466010004281998
Training batch 14 / 32
Total batch reconstruction loss: 0.07777607440948486
Training batch 15 / 32
Total batch reconstruction loss: 0.0753764659166336
Training batch 16 / 32
Total batch reconstruction loss: 0.07477350533008575
Training batch 17 / 32
Total batch reconstruction loss: 0.07622791081666946
Training batch 18 / 32
Total batch reconstruction loss: 0.07565408200025558
Training batch 19 / 32
Total batch reconstruction loss: 0.07189153879880905
Training batch 20 / 32
Total batch reconstruction loss: 0.07804712653160095
Training batch 21 / 32
Total batch reconstruction loss: 0.07648121565580368
Training batch 22 / 32
Total batch reconstruction loss: 0.07863302528858185
Training batch 23 / 32
Total batch reconstruction loss: 0.0760263204574585
Training batch 24 / 32
Total batch reconstruction loss: 0.07863922417163849
Training batch 25 / 32
Total batch reconstruction loss: 0.07376142591238022
Training batch 26 / 32
Total batch reconstruction loss: 0.0745307207107544
Training batch 27 / 32
Total batch reconstruction loss: 0.07403428852558136
Training batch 28 / 32
Total batch reconstruction loss: 0.07577250897884369
Training batch 29 / 32
Total batch reconstruction loss: 0.07557903975248337
Training batch 30 / 32
Total batch reconstruction loss: 0.07157976180315018
Training batch 31 / 32
Total batch reconstruction loss: 0.07594481110572815
Training batch 32 / 32
Total batch reconstruction loss: 0.08620759844779968
Epoch [6/500], Train Loss: 0.0882, Validation Loss: 0.0872, Generator Loss: 16.0338, Discriminator Loss: 0.3961
Training epoch 7 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.07678206264972687
Training batch 2 / 32
Total batch reconstruction loss: 0.07756392657756805
Training batch 3 / 32
Total batch reconstruction loss: 0.08316056430339813
Training batch 4 / 32
Total batch reconstruction loss: 0.08195453882217407
Training batch 5 / 32
Total batch reconstruction loss: 0.0739433765411377
Training batch 6 / 32
Total batch reconstruction loss: 0.076169952750206
Training batch 7 / 32
Total batch reconstruction loss: 0.07575549185276031
Training batch 8 / 32
Total batch reconstruction loss: 0.07420041412115097
Training batch 9 / 32
Total batch reconstruction loss: 0.07880949229001999
Training batch 10 / 32
Total batch reconstruction loss: 0.07341843843460083
Training batch 11 / 32
Total batch reconstruction loss: 0.07730984687805176
Training batch 12 / 32
Total batch reconstruction loss: 0.07753190398216248
Training batch 13 / 32
Total batch reconstruction loss: 0.07709842920303345
Training batch 14 / 32
Total batch reconstruction loss: 0.07748007029294968
Training batch 15 / 32
Total batch reconstruction loss: 0.07438132166862488
Training batch 16 / 32
Total batch reconstruction loss: 0.07782309502363205
Training batch 17 / 32
Total batch reconstruction loss: 0.07920974493026733
Training batch 18 / 32
Total batch reconstruction loss: 0.07481493800878525
Training batch 19 / 32
Total batch reconstruction loss: 0.07020264863967896
Training batch 20 / 32
Total batch reconstruction loss: 0.07916621118783951
Training batch 21 / 32
Total batch reconstruction loss: 0.07637953758239746
Training batch 22 / 32
Total batch reconstruction loss: 0.08072210848331451
Training batch 23 / 32
Total batch reconstruction loss: 0.07234649360179901
Training batch 24 / 32
Total batch reconstruction loss: 0.07447248697280884
Training batch 25 / 32
Total batch reconstruction loss: 0.07576797157526016
Training batch 26 / 32
Total batch reconstruction loss: 0.0730237364768982
Training batch 27 / 32
Total batch reconstruction loss: 0.07203010469675064
Training batch 28 / 32
Total batch reconstruction loss: 0.0762217789888382
Training batch 29 / 32
Total batch reconstruction loss: 0.07599584013223648
Training batch 30 / 32
Total batch reconstruction loss: 0.0798904299736023
Training batch 31 / 32
Total batch reconstruction loss: 0.07135456055402756
Training batch 32 / 32
Total batch reconstruction loss: 0.07800869643688202
Epoch [7/500], Train Loss: 0.0882, Validation Loss: 0.0869, Generator Loss: 16.0049, Discriminator Loss: 0.3862
Training epoch 8 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.07457239180803299
Training batch 2 / 32
Total batch reconstruction loss: 0.07486036419868469
Training batch 3 / 32
Total batch reconstruction loss: 0.07279576361179352
Training batch 4 / 32
Total batch reconstruction loss: 0.07167310267686844
Training batch 5 / 32
Total batch reconstruction loss: 0.07401716709136963
Training batch 6 / 32
Total batch reconstruction loss: 0.07886083424091339
Training batch 7 / 32
Total batch reconstruction loss: 0.07340305298566818
Training batch 8 / 32
Total batch reconstruction loss: 0.07853396236896515
Training batch 9 / 32
Total batch reconstruction loss: 0.07653069496154785
Training batch 10 / 32
Total batch reconstruction loss: 0.07082724571228027
Training batch 11 / 32
Total batch reconstruction loss: 0.07071304321289062
Training batch 12 / 32
Total batch reconstruction loss: 0.08069413900375366
Training batch 13 / 32
Total batch reconstruction loss: 0.07701632380485535
Training batch 14 / 32
Total batch reconstruction loss: 0.07002178579568863
Training batch 15 / 32
Total batch reconstruction loss: 0.07812714576721191
Training batch 16 / 32
Total batch reconstruction loss: 0.07087276130914688
Training batch 17 / 32
Total batch reconstruction loss: 0.0733138769865036
Training batch 18 / 32
Total batch reconstruction loss: 0.07480080425739288
Training batch 19 / 32
Total batch reconstruction loss: 0.0772344172000885
Training batch 20 / 32
Total batch reconstruction loss: 0.07477033883333206
Training batch 21 / 32
Total batch reconstruction loss: 0.07599584758281708
Training batch 22 / 32
Total batch reconstruction loss: 0.07317675650119781
Training batch 23 / 32
Total batch reconstruction loss: 0.0724896490573883
Training batch 24 / 32
Total batch reconstruction loss: 0.07271790504455566
Training batch 25 / 32
Total batch reconstruction loss: 0.08074015378952026
Training batch 26 / 32
Total batch reconstruction loss: 0.0731109082698822
Training batch 27 / 32
Total batch reconstruction loss: 0.07327564060688019
Training batch 28 / 32
Total batch reconstruction loss: 0.07394934445619583
Training batch 29 / 32
Total batch reconstruction loss: 0.07820592075586319
Training batch 30 / 32
Total batch reconstruction loss: 0.07499802112579346
Training batch 31 / 32
Total batch reconstruction loss: 0.07419324666261673
Training batch 32 / 32
Total batch reconstruction loss: 0.07265761494636536
Epoch [8/500], Train Loss: 0.0853, Validation Loss: 0.0850, Generator Loss: 15.6107, Discriminator Loss: 0.3679
Training epoch 9 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.07283993065357208
Training batch 2 / 32
Total batch reconstruction loss: 0.07145839184522629
Training batch 3 / 32
Total batch reconstruction loss: 0.07404269278049469
Training batch 4 / 32
Total batch reconstruction loss: 0.07521345466375351
Training batch 5 / 32
Total batch reconstruction loss: 0.0747145414352417
Training batch 6 / 32
Total batch reconstruction loss: 0.07459363341331482
Training batch 7 / 32
Total batch reconstruction loss: 0.07220920920372009
Training batch 8 / 32
Total batch reconstruction loss: 0.07052211463451385
Training batch 9 / 32
Total batch reconstruction loss: 0.0743095725774765
Training batch 10 / 32
Total batch reconstruction loss: 0.07128432393074036
Training batch 11 / 32
Total batch reconstruction loss: 0.0764164999127388
Training batch 12 / 32
Total batch reconstruction loss: 0.07533290982246399
Training batch 13 / 32
Total batch reconstruction loss: 0.07649451494216919
Training batch 14 / 32
Total batch reconstruction loss: 0.07368505001068115
Training batch 15 / 32
Total batch reconstruction loss: 0.07526518404483795
Training batch 16 / 32
Total batch reconstruction loss: 0.07113084197044373
Training batch 17 / 32
Total batch reconstruction loss: 0.07857656478881836
Training batch 18 / 32
Total batch reconstruction loss: 0.07605022192001343
Training batch 19 / 32
Total batch reconstruction loss: 0.07509808242321014
Training batch 20 / 32
Total batch reconstruction loss: 0.07518647611141205
Training batch 21 / 32
Total batch reconstruction loss: 0.07507677376270294
Training batch 22 / 32
Total batch reconstruction loss: 0.07714644819498062
Training batch 23 / 32
Total batch reconstruction loss: 0.07332150638103485
Training batch 24 / 32
Total batch reconstruction loss: 0.07336801290512085
Training batch 25 / 32
Total batch reconstruction loss: 0.07149888575077057
Training batch 26 / 32
Total batch reconstruction loss: 0.06850366294384003
Training batch 27 / 32
Total batch reconstruction loss: 0.07994769513607025
Training batch 28 / 32
Total batch reconstruction loss: 0.07489046454429626
Training batch 29 / 32
Total batch reconstruction loss: 0.07232989370822906
Training batch 30 / 32
Total batch reconstruction loss: 0.0716630294919014
Training batch 31 / 32
Total batch reconstruction loss: 0.07284684479236603
Training batch 32 / 32
Total batch reconstruction loss: 0.11234375089406967
Epoch [9/500], Train Loss: 0.0844, Validation Loss: 0.0806, Generator Loss: 15.7381, Discriminator Loss: 0.3451
Training epoch 10 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.0753665566444397
Training batch 2 / 32
Total batch reconstruction loss: 0.07840578258037567
Training batch 3 / 32
Total batch reconstruction loss: 0.07963714003562927
Training batch 4 / 32
Total batch reconstruction loss: 0.074623703956604
Training batch 5 / 32
Total batch reconstruction loss: 0.07664044946432114
Training batch 6 / 32
Total batch reconstruction loss: 0.07466353476047516
Training batch 7 / 32
Total batch reconstruction loss: 0.080622099339962
Training batch 8 / 32
Total batch reconstruction loss: 0.07930003106594086
Training batch 9 / 32
Total batch reconstruction loss: 0.07129236310720444
Training batch 10 / 32
Total batch reconstruction loss: 0.07670695334672928
Training batch 11 / 32
Total batch reconstruction loss: 0.07574979215860367
Training batch 12 / 32
Total batch reconstruction loss: 0.0726378858089447
Training batch 13 / 32
Total batch reconstruction loss: 0.07335260510444641
Training batch 14 / 32
Total batch reconstruction loss: 0.07188527286052704
Training batch 15 / 32
Total batch reconstruction loss: 0.07587223500013351
Training batch 16 / 32
Total batch reconstruction loss: 0.07943582534790039
Training batch 17 / 32
Total batch reconstruction loss: 0.07357168942689896
Training batch 18 / 32
Total batch reconstruction loss: 0.0715455487370491
Training batch 19 / 32
Total batch reconstruction loss: 0.07347215712070465
Training batch 20 / 32
Total batch reconstruction loss: 0.07278275489807129
Training batch 21 / 32
Total batch reconstruction loss: 0.07527182251214981
Training batch 22 / 32
Total batch reconstruction loss: 0.0730225220322609
Training batch 23 / 32
Total batch reconstruction loss: 0.07284112274646759
Training batch 24 / 32
Total batch reconstruction loss: 0.07200578600168228
Training batch 25 / 32
Total batch reconstruction loss: 0.069486103951931
Training batch 26 / 32
Total batch reconstruction loss: 0.07464563846588135
Training batch 27 / 32
Total batch reconstruction loss: 0.07499076426029205
Training batch 28 / 32
Total batch reconstruction loss: 0.07293237745761871
Training batch 29 / 32
Total batch reconstruction loss: 0.07168570905923843
Training batch 30 / 32
Total batch reconstruction loss: 0.07220348715782166
Training batch 31 / 32
Total batch reconstruction loss: 0.0729178935289383
Training batch 32 / 32
Total batch reconstruction loss: 0.0683223158121109
Epoch [10/500], Train Loss: 0.0818, Validation Loss: 0.0867, Generator Loss: 15.4446, Discriminator Loss: 0.3289
Training epoch 11 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.07164642214775085
Training batch 2 / 32
Total batch reconstruction loss: 0.07684576511383057
Training batch 3 / 32
Total batch reconstruction loss: 0.07911767065525055
Training batch 4 / 32
Total batch reconstruction loss: 0.07337343692779541
Training batch 5 / 32
Total batch reconstruction loss: 0.07170271128416061
Training batch 6 / 32
Total batch reconstruction loss: 0.074506476521492
Training batch 7 / 32
Total batch reconstruction loss: 0.0760229155421257
Training batch 8 / 32
Total batch reconstruction loss: 0.0791541188955307
Training batch 9 / 32
Total batch reconstruction loss: 0.0739799290895462
Training batch 10 / 32
Total batch reconstruction loss: 0.07272671163082123
Training batch 11 / 32
Total batch reconstruction loss: 0.07615213096141815
Training batch 12 / 32
Total batch reconstruction loss: 0.07143185287714005
Training batch 13 / 32
Total batch reconstruction loss: 0.07100564241409302
Training batch 14 / 32
Total batch reconstruction loss: 0.07083780318498611
Training batch 15 / 32
Total batch reconstruction loss: 0.0767151266336441
Training batch 16 / 32
Total batch reconstruction loss: 0.07671982049942017
Training batch 17 / 32
Total batch reconstruction loss: 0.07441698759794235
Training batch 18 / 32
Total batch reconstruction loss: 0.0739029198884964
Training batch 19 / 32
Total batch reconstruction loss: 0.0699334666132927
Training batch 20 / 32
Total batch reconstruction loss: 0.06928762793540955
Training batch 21 / 32
Total batch reconstruction loss: 0.07254689186811447
Training batch 22 / 32
Total batch reconstruction loss: 0.06931720674037933
Training batch 23 / 32
Total batch reconstruction loss: 0.06973933428525925
Training batch 24 / 32
Total batch reconstruction loss: 0.07734259963035583
Training batch 25 / 32
Total batch reconstruction loss: 0.0740320086479187
Training batch 26 / 32
Total batch reconstruction loss: 0.07226058840751648
Training batch 27 / 32
Total batch reconstruction loss: 0.07246696203947067
Training batch 28 / 32
Total batch reconstruction loss: 0.07005539536476135
Training batch 29 / 32
Total batch reconstruction loss: 0.07078184932470322
Training batch 30 / 32
Total batch reconstruction loss: 0.0767998918890953
Training batch 31 / 32
Total batch reconstruction loss: 0.07900022715330124
Training batch 32 / 32
Total batch reconstruction loss: 0.06990823894739151
Epoch [11/500], Train Loss: 0.0821, Validation Loss: 0.0815, Generator Loss: 15.2928, Discriminator Loss: 0.3238
Training epoch 12 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.07368755340576172
Training batch 2 / 32
Total batch reconstruction loss: 0.07167766988277435
Training batch 3 / 32
Total batch reconstruction loss: 0.07155434787273407
Training batch 4 / 32
Total batch reconstruction loss: 0.07360692322254181
Training batch 5 / 32
Total batch reconstruction loss: 0.07003454118967056
Training batch 6 / 32
Total batch reconstruction loss: 0.0782594382762909
Training batch 7 / 32
Total batch reconstruction loss: 0.07418517023324966
Training batch 8 / 32
Total batch reconstruction loss: 0.0746985599398613
Training batch 9 / 32
Total batch reconstruction loss: 0.07422243803739548
Training batch 10 / 32
Total batch reconstruction loss: 0.07685817033052444
Training batch 11 / 32
Total batch reconstruction loss: 0.07372498512268066
Training batch 12 / 32
Total batch reconstruction loss: 0.0758083239197731
Training batch 13 / 32
Total batch reconstruction loss: 0.070133276283741
Training batch 14 / 32
Total batch reconstruction loss: 0.07670177519321442
Training batch 15 / 32
Total batch reconstruction loss: 0.07450936734676361
Training batch 16 / 32
Total batch reconstruction loss: 0.07288378477096558
Training batch 17 / 32
Total batch reconstruction loss: 0.07192956656217575
Training batch 18 / 32
Total batch reconstruction loss: 0.06892706453800201
Training batch 19 / 32
Total batch reconstruction loss: 0.0678250789642334
Training batch 20 / 32
Total batch reconstruction loss: 0.07566750794649124
Training batch 21 / 32
Total batch reconstruction loss: 0.0734923854470253
Training batch 22 / 32
Total batch reconstruction loss: 0.07252967357635498
Training batch 23 / 32
Total batch reconstruction loss: 0.07237821817398071
Training batch 24 / 32
Total batch reconstruction loss: 0.07302728295326233
Training batch 25 / 32
Total batch reconstruction loss: 0.07558286190032959
Training batch 26 / 32
Total batch reconstruction loss: 0.07137326896190643
Training batch 27 / 32
Total batch reconstruction loss: 0.07650767266750336
Training batch 28 / 32
Total batch reconstruction loss: 0.07597832381725311
Training batch 29 / 32
Total batch reconstruction loss: 0.07017196714878082
Training batch 30 / 32
Total batch reconstruction loss: 0.07028000056743622
Training batch 31 / 32
Total batch reconstruction loss: 0.06929545849561691
Training batch 32 / 32
Total batch reconstruction loss: 0.06816907227039337
Epoch [12/500], Train Loss: 0.0806, Validation Loss: 0.0857, Generator Loss: 15.1398, Discriminator Loss: 0.3124
Training epoch 13 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.07521584630012512
Training batch 2 / 32
Total batch reconstruction loss: 0.07500706613063812
Training batch 3 / 32
Total batch reconstruction loss: 0.0784393697977066
Training batch 4 / 32
Total batch reconstruction loss: 0.07421412318944931
Training batch 5 / 32
Total batch reconstruction loss: 0.07022421061992645
Training batch 6 / 32
Total batch reconstruction loss: 0.07340379059314728
Training batch 7 / 32
Total batch reconstruction loss: 0.07069475203752518
Training batch 8 / 32
Total batch reconstruction loss: 0.07270906865596771
Training batch 9 / 32
Total batch reconstruction loss: 0.07483088970184326
Training batch 10 / 32
Total batch reconstruction loss: 0.07154054939746857
Training batch 11 / 32
Total batch reconstruction loss: 0.07703500986099243
Training batch 12 / 32
Total batch reconstruction loss: 0.07280153036117554
Training batch 13 / 32
Total batch reconstruction loss: 0.06894022226333618
Training batch 14 / 32
Total batch reconstruction loss: 0.07189661264419556
Training batch 15 / 32
Total batch reconstruction loss: 0.07063087821006775
Training batch 16 / 32
Total batch reconstruction loss: 0.07361976057291031
Training batch 17 / 32
Total batch reconstruction loss: 0.07182793319225311
Training batch 18 / 32
Total batch reconstruction loss: 0.07250902056694031
Training batch 19 / 32
Total batch reconstruction loss: 0.06959140300750732
Training batch 20 / 32
Total batch reconstruction loss: 0.06959674507379532
Training batch 21 / 32
Total batch reconstruction loss: 0.07783708721399307
Training batch 22 / 32
Total batch reconstruction loss: 0.07256276905536652
Training batch 23 / 32
Total batch reconstruction loss: 0.07105296850204468
Training batch 24 / 32
Total batch reconstruction loss: 0.07171806693077087
Training batch 25 / 32
Total batch reconstruction loss: 0.07244662195444107
Training batch 26 / 32
Total batch reconstruction loss: 0.07595853507518768
Training batch 27 / 32
Total batch reconstruction loss: 0.07267458736896515
Training batch 28 / 32
Total batch reconstruction loss: 0.07640737295150757
Training batch 29 / 32
Total batch reconstruction loss: 0.07433556020259857
Training batch 30 / 32
Total batch reconstruction loss: 0.06850139051675797
Training batch 31 / 32
Total batch reconstruction loss: 0.06961752474308014
Training batch 32 / 32
Total batch reconstruction loss: 0.06917376816272736
Epoch [13/500], Train Loss: 0.0805, Validation Loss: 0.0821, Generator Loss: 15.0716, Discriminator Loss: 0.2965
Training epoch 14 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.07675690948963165
Training batch 2 / 32
Total batch reconstruction loss: 0.0730246901512146
Training batch 3 / 32
Total batch reconstruction loss: 0.07184424251317978
Training batch 4 / 32
Total batch reconstruction loss: 0.07381589710712433
Training batch 5 / 32
Total batch reconstruction loss: 0.07262876629829407
Training batch 6 / 32
Total batch reconstruction loss: 0.07699628919363022
Training batch 7 / 32
Total batch reconstruction loss: 0.07075067609548569
Training batch 8 / 32
Total batch reconstruction loss: 0.06728643923997879
Training batch 9 / 32
Total batch reconstruction loss: 0.07160742580890656
Training batch 10 / 32
Total batch reconstruction loss: 0.07906465232372284
Training batch 11 / 32
Total batch reconstruction loss: 0.07633949816226959
Training batch 12 / 32
Total batch reconstruction loss: 0.07033127546310425
Training batch 13 / 32
Total batch reconstruction loss: 0.07517807185649872
Training batch 14 / 32
Total batch reconstruction loss: 0.07261107861995697
Training batch 15 / 32
Total batch reconstruction loss: 0.0722181499004364
Training batch 16 / 32
Total batch reconstruction loss: 0.07572844624519348
Training batch 17 / 32
Total batch reconstruction loss: 0.07218275219202042
Training batch 18 / 32
Total batch reconstruction loss: 0.07652179896831512
Training batch 19 / 32
Total batch reconstruction loss: 0.07446972280740738
Training batch 20 / 32
Total batch reconstruction loss: 0.07674290239810944
Training batch 21 / 32
Total batch reconstruction loss: 0.07393892854452133
Training batch 22 / 32
Total batch reconstruction loss: 0.07296544313430786
Training batch 23 / 32
Total batch reconstruction loss: 0.0759926289319992
Training batch 24 / 32
Total batch reconstruction loss: 0.07547461241483688
Training batch 25 / 32
Total batch reconstruction loss: 0.07217192649841309
Training batch 26 / 32
Total batch reconstruction loss: 0.07588258385658264
Training batch 27 / 32
Total batch reconstruction loss: 0.07701060175895691
Training batch 28 / 32
Total batch reconstruction loss: 0.0742700845003128
Training batch 29 / 32
Total batch reconstruction loss: 0.06892672181129456
Training batch 30 / 32
Total batch reconstruction loss: 0.07174570858478546
Training batch 31 / 32
Total batch reconstruction loss: 0.07480155676603317
Training batch 32 / 32
Total batch reconstruction loss: 0.07091931253671646
Epoch [14/500], Train Loss: 0.0824, Validation Loss: 0.0823, Generator Loss: 15.2389, Discriminator Loss: 0.2924
Training epoch 15 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.07533694803714752
Training batch 2 / 32
Total batch reconstruction loss: 0.07172175496816635
Training batch 3 / 32
Total batch reconstruction loss: 0.07155951857566833
Training batch 4 / 32
Total batch reconstruction loss: 0.07339838892221451
Training batch 5 / 32
Total batch reconstruction loss: 0.07015180587768555
Training batch 6 / 32
Total batch reconstruction loss: 0.07024120539426804
Training batch 7 / 32
Total batch reconstruction loss: 0.07304245233535767
Training batch 8 / 32
Total batch reconstruction loss: 0.07192780077457428
Training batch 9 / 32
Total batch reconstruction loss: 0.07976419478654861
Training batch 10 / 32
Total batch reconstruction loss: 0.07468792796134949
Training batch 11 / 32
Total batch reconstruction loss: 0.07265617698431015
Training batch 12 / 32
Total batch reconstruction loss: 0.07263955473899841
Training batch 13 / 32
Total batch reconstruction loss: 0.07480309903621674
Training batch 14 / 32
Total batch reconstruction loss: 0.0779561847448349
Training batch 15 / 32
Total batch reconstruction loss: 0.06953258067369461
Training batch 16 / 32
Total batch reconstruction loss: 0.07040183246135712
Training batch 17 / 32
Total batch reconstruction loss: 0.07210800796747208
Training batch 18 / 32
Total batch reconstruction loss: 0.07251930236816406
Training batch 19 / 32
Total batch reconstruction loss: 0.07012996822595596
Training batch 20 / 32
Total batch reconstruction loss: 0.0746290534734726
Training batch 21 / 32
Total batch reconstruction loss: 0.07031993567943573
Training batch 22 / 32
Total batch reconstruction loss: 0.07452496141195297
Training batch 23 / 32
Total batch reconstruction loss: 0.07350125163793564
Training batch 24 / 32
Total batch reconstruction loss: 0.07290495187044144
Training batch 25 / 32
Total batch reconstruction loss: 0.0782051756978035
Training batch 26 / 32
Total batch reconstruction loss: 0.07540493458509445
Training batch 27 / 32
Total batch reconstruction loss: 0.06967703998088837
Training batch 28 / 32
Total batch reconstruction loss: 0.0715475082397461
Training batch 29 / 32
Total batch reconstruction loss: 0.07485005259513855
Training batch 30 / 32
Total batch reconstruction loss: 0.07450458407402039
Training batch 31 / 32
Total batch reconstruction loss: 0.06984160840511322
Training batch 32 / 32
Total batch reconstruction loss: 0.09028206765651703
Epoch [15/500], Train Loss: 0.0819, Validation Loss: 0.0777, Generator Loss: 15.1780, Discriminator Loss: 0.2870
Training epoch 16 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.0721292719244957
Training batch 2 / 32
Total batch reconstruction loss: 0.07871325314044952
Training batch 3 / 32
Total batch reconstruction loss: 0.07218903303146362
Training batch 4 / 32
Total batch reconstruction loss: 0.07377855479717255
Training batch 5 / 32
Total batch reconstruction loss: 0.07057302445173264
Training batch 6 / 32
Total batch reconstruction loss: 0.07331841439008713
Training batch 7 / 32
Total batch reconstruction loss: 0.0781526267528534
Training batch 8 / 32
Total batch reconstruction loss: 0.0755675807595253
Training batch 9 / 32
Total batch reconstruction loss: 0.07900716364383698
Training batch 10 / 32
Total batch reconstruction loss: 0.07463722676038742
Training batch 11 / 32
Total batch reconstruction loss: 0.07925404608249664
Training batch 12 / 32
Total batch reconstruction loss: 0.07820364087820053
Training batch 13 / 32
Total batch reconstruction loss: 0.07427288591861725
Training batch 14 / 32
Total batch reconstruction loss: 0.07076014578342438
Training batch 15 / 32
Total batch reconstruction loss: 0.07194401323795319
Training batch 16 / 32
Total batch reconstruction loss: 0.08073815703392029
Training batch 17 / 32
Total batch reconstruction loss: 0.07907278835773468
Training batch 18 / 32
Total batch reconstruction loss: 0.06888241320848465
Training batch 19 / 32
Total batch reconstruction loss: 0.07152749598026276
Training batch 20 / 32
Total batch reconstruction loss: 0.07585166394710541
Training batch 21 / 32
Total batch reconstruction loss: 0.07347223162651062
Training batch 22 / 32
Total batch reconstruction loss: 0.072657510638237
Training batch 23 / 32
Total batch reconstruction loss: 0.07109791040420532
Training batch 24 / 32
Total batch reconstruction loss: 0.06990693509578705
Training batch 25 / 32
Total batch reconstruction loss: 0.07984242588281631
Training batch 26 / 32
Total batch reconstruction loss: 0.0716477781534195
Training batch 27 / 32
Total batch reconstruction loss: 0.07437428832054138
Training batch 28 / 32
Total batch reconstruction loss: 0.07304906100034714
Training batch 29 / 32
Total batch reconstruction loss: 0.06808660924434662
Training batch 30 / 32
Total batch reconstruction loss: 0.07416246831417084
Training batch 31 / 32
Total batch reconstruction loss: 0.07524140179157257
Training batch 32 / 32
Total batch reconstruction loss: 0.0738975778222084
Epoch [16/500], Train Loss: 0.0809, Validation Loss: 0.0823, Generator Loss: 15.2892, Discriminator Loss: 0.2761
Training epoch 17 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.07270467281341553
Training batch 2 / 32
Total batch reconstruction loss: 0.07293125987052917
Training batch 3 / 32
Total batch reconstruction loss: 0.07564150542020798
Training batch 4 / 32
Total batch reconstruction loss: 0.0730905681848526
Training batch 5 / 32
Total batch reconstruction loss: 0.07155510783195496
Training batch 6 / 32
Total batch reconstruction loss: 0.07686587423086166
Training batch 7 / 32
Total batch reconstruction loss: 0.07264983654022217
Training batch 8 / 32
Total batch reconstruction loss: 0.07400470227003098
Training batch 9 / 32
Total batch reconstruction loss: 0.07300619781017303
Training batch 10 / 32
Total batch reconstruction loss: 0.07696838676929474
Training batch 11 / 32
Total batch reconstruction loss: 0.07511284947395325
Training batch 12 / 32
Total batch reconstruction loss: 0.07831547409296036
Training batch 13 / 32
Total batch reconstruction loss: 0.069334015250206
Training batch 14 / 32
Total batch reconstruction loss: 0.0743243470788002
Training batch 15 / 32
Total batch reconstruction loss: 0.0697944238781929
Training batch 16 / 32
Total batch reconstruction loss: 0.07315392792224884
Training batch 17 / 32
Total batch reconstruction loss: 0.06973209977149963
Training batch 18 / 32
Total batch reconstruction loss: 0.07326273620128632
Training batch 19 / 32
Total batch reconstruction loss: 0.07741648703813553
Training batch 20 / 32
Total batch reconstruction loss: 0.07391579449176788
Training batch 21 / 32
Total batch reconstruction loss: 0.07278445363044739
Training batch 22 / 32
Total batch reconstruction loss: 0.07312552630901337
Training batch 23 / 32
Total batch reconstruction loss: 0.07130193710327148
Training batch 24 / 32
Total batch reconstruction loss: 0.0716957375407219
Training batch 25 / 32
Total batch reconstruction loss: 0.07459971308708191
Training batch 26 / 32
Total batch reconstruction loss: 0.07013391703367233
Training batch 27 / 32
Total batch reconstruction loss: 0.07255490124225616
Training batch 28 / 32
Total batch reconstruction loss: 0.07234710454940796
Training batch 29 / 32
Total batch reconstruction loss: 0.07419539242982864
Training batch 30 / 32
Total batch reconstruction loss: 0.06941284984350204
Training batch 31 / 32
Total batch reconstruction loss: 0.0708775594830513
Training batch 32 / 32
Total batch reconstruction loss: 0.06874505430459976
Epoch [17/500], Train Loss: 0.0809, Validation Loss: 0.0826, Generator Loss: 15.0048, Discriminator Loss: 0.2838
Training epoch 18 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.07127363979816437
Training batch 2 / 32
Total batch reconstruction loss: 0.07472934573888779
Training batch 3 / 32
Total batch reconstruction loss: 0.06751402467489243
Training batch 4 / 32
Total batch reconstruction loss: 0.07207541167736053
Training batch 5 / 32
Total batch reconstruction loss: 0.07255490124225616
Training batch 6 / 32
Total batch reconstruction loss: 0.07544207572937012
Training batch 7 / 32
Total batch reconstruction loss: 0.0725613683462143
Training batch 8 / 32
Total batch reconstruction loss: 0.07109087705612183
Training batch 9 / 32
Total batch reconstruction loss: 0.07154867798089981
Training batch 10 / 32
Total batch reconstruction loss: 0.07324530184268951
Training batch 11 / 32
Total batch reconstruction loss: 0.06952813267707825
Training batch 12 / 32
Total batch reconstruction loss: 0.07329627871513367
Training batch 13 / 32
Total batch reconstruction loss: 0.06922551989555359
Training batch 14 / 32
Total batch reconstruction loss: 0.07225839048624039
Training batch 15 / 32
Total batch reconstruction loss: 0.07038971781730652
Training batch 16 / 32
Total batch reconstruction loss: 0.07183735817670822
Training batch 17 / 32
Total batch reconstruction loss: 0.07118377089500427
Training batch 18 / 32
Total batch reconstruction loss: 0.07539007067680359
Training batch 19 / 32
Total batch reconstruction loss: 0.07175099849700928
Training batch 20 / 32
Total batch reconstruction loss: 0.0691303163766861
Training batch 21 / 32
Total batch reconstruction loss: 0.07225482910871506
Training batch 22 / 32
Total batch reconstruction loss: 0.06956100463867188
Training batch 23 / 32
Total batch reconstruction loss: 0.07401193678379059
Training batch 24 / 32
Total batch reconstruction loss: 0.06917065382003784
Training batch 25 / 32
Total batch reconstruction loss: 0.0691039189696312
Training batch 26 / 32
Total batch reconstruction loss: 0.06850317120552063
Training batch 27 / 32
Total batch reconstruction loss: 0.06959088891744614
Training batch 28 / 32
Total batch reconstruction loss: 0.07222497463226318
Training batch 29 / 32
Total batch reconstruction loss: 0.07278873026371002
Training batch 30 / 32
Total batch reconstruction loss: 0.07366050779819489
Training batch 31 / 32
Total batch reconstruction loss: 0.07101394981145859
Training batch 32 / 32
Total batch reconstruction loss: 0.0745655819773674
Epoch [18/500], Train Loss: 0.0789, Validation Loss: 0.0771, Generator Loss: 14.7195, Discriminator Loss: 0.2797
Training epoch 19 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.07502495497465134
Training batch 2 / 32
Total batch reconstruction loss: 0.07237361371517181
Training batch 3 / 32
Total batch reconstruction loss: 0.07134705036878586
Training batch 4 / 32
Total batch reconstruction loss: 0.07320332527160645
Training batch 5 / 32
Total batch reconstruction loss: 0.07668568193912506
Training batch 6 / 32
Total batch reconstruction loss: 0.07555456459522247
Training batch 7 / 32
Total batch reconstruction loss: 0.07449689507484436
Training batch 8 / 32
Total batch reconstruction loss: 0.07044971734285355
Training batch 9 / 32
Total batch reconstruction loss: 0.07450518012046814
Training batch 10 / 32
Total batch reconstruction loss: 0.06987056136131287
Training batch 11 / 32
Total batch reconstruction loss: 0.07565486431121826
Training batch 12 / 32
Total batch reconstruction loss: 0.0720287412405014
Training batch 13 / 32
Total batch reconstruction loss: 0.07598970830440521
Training batch 14 / 32
Total batch reconstruction loss: 0.07035447657108307
Training batch 15 / 32
Total batch reconstruction loss: 0.06907936930656433
Training batch 16 / 32
Total batch reconstruction loss: 0.07985325902700424
Training batch 17 / 32
Total batch reconstruction loss: 0.07026476413011551
Training batch 18 / 32
Total batch reconstruction loss: 0.07797633111476898
Training batch 19 / 32
Total batch reconstruction loss: 0.07657274603843689
Training batch 20 / 32
Total batch reconstruction loss: 0.07238677144050598
Training batch 21 / 32
Total batch reconstruction loss: 0.07120231539011002
Training batch 22 / 32
Total batch reconstruction loss: 0.06932538002729416
Training batch 23 / 32
Total batch reconstruction loss: 0.07038898020982742
Training batch 24 / 32
Total batch reconstruction loss: 0.07128385454416275
Training batch 25 / 32
Total batch reconstruction loss: 0.07338948547840118
Training batch 26 / 32
Total batch reconstruction loss: 0.07299868762493134
Training batch 27 / 32
Total batch reconstruction loss: 0.07432036846876144
Training batch 28 / 32
Total batch reconstruction loss: 0.06898917257785797
Training batch 29 / 32
Total batch reconstruction loss: 0.07026262581348419
Training batch 30 / 32
Total batch reconstruction loss: 0.0725845992565155
Training batch 31 / 32
Total batch reconstruction loss: 0.07213536649942398
Training batch 32 / 32
Total batch reconstruction loss: 0.07525520026683807
Epoch [19/500], Train Loss: 0.0805, Validation Loss: 0.0822, Generator Loss: 14.9692, Discriminator Loss: 0.2641
Training epoch 20 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.07450170814990997
Training batch 2 / 32
Total batch reconstruction loss: 0.07048749923706055
Training batch 3 / 32
Total batch reconstruction loss: 0.07451523840427399
Training batch 4 / 32
Total batch reconstruction loss: 0.07111556828022003
Training batch 5 / 32
Total batch reconstruction loss: 0.06955567002296448
Training batch 6 / 32
Total batch reconstruction loss: 0.0706830620765686
Training batch 7 / 32
Total batch reconstruction loss: 0.07450749725103378
Training batch 8 / 32
Total batch reconstruction loss: 0.07344010472297668
Training batch 9 / 32
Total batch reconstruction loss: 0.07472564280033112
Training batch 10 / 32
Total batch reconstruction loss: 0.06969644129276276
Training batch 11 / 32
Total batch reconstruction loss: 0.07028298079967499
Training batch 12 / 32
Total batch reconstruction loss: 0.07171595096588135
Training batch 13 / 32
Total batch reconstruction loss: 0.07204380631446838
Training batch 14 / 32
Total batch reconstruction loss: 0.07176239043474197
Training batch 15 / 32
Total batch reconstruction loss: 0.07254824042320251
Training batch 16 / 32
Total batch reconstruction loss: 0.07404907792806625
Training batch 17 / 32
Total batch reconstruction loss: 0.07256854325532913
Training batch 18 / 32
Total batch reconstruction loss: 0.06780623644590378
Training batch 19 / 32
Total batch reconstruction loss: 0.06819100677967072
Training batch 20 / 32
Total batch reconstruction loss: 0.07439474761486053
Training batch 21 / 32
Total batch reconstruction loss: 0.07139827311038971
Training batch 22 / 32
Total batch reconstruction loss: 0.07225586473941803
Training batch 23 / 32
Total batch reconstruction loss: 0.07056062668561935
Training batch 24 / 32
Total batch reconstruction loss: 0.07204782962799072
Training batch 25 / 32
Total batch reconstruction loss: 0.06972949206829071
Training batch 26 / 32
Total batch reconstruction loss: 0.07178202271461487
Training batch 27 / 32
Total batch reconstruction loss: 0.06941863894462585
Training batch 28 / 32
Total batch reconstruction loss: 0.06866376847028732
Training batch 29 / 32
Total batch reconstruction loss: 0.07117374986410141
Training batch 30 / 32
Total batch reconstruction loss: 0.07124543190002441
Training batch 31 / 32
Total batch reconstruction loss: 0.07089556753635406
Training batch 32 / 32
Total batch reconstruction loss: 0.07661192119121552
Epoch [20/500], Train Loss: 0.0787, Validation Loss: 0.0767, Generator Loss: 14.6836, Discriminator Loss: 0.2699
Training epoch 21 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.07062584906816483
Training batch 2 / 32
Total batch reconstruction loss: 0.07269016653299332
Training batch 3 / 32
Total batch reconstruction loss: 0.07570032775402069
Training batch 4 / 32
Total batch reconstruction loss: 0.07040432095527649
Training batch 5 / 32
Total batch reconstruction loss: 0.07288110256195068
Training batch 6 / 32
Total batch reconstruction loss: 0.06781434267759323
Training batch 7 / 32
Total batch reconstruction loss: 0.07158060371875763
Training batch 8 / 32
Total batch reconstruction loss: 0.07051369547843933
Training batch 9 / 32
Total batch reconstruction loss: 0.07699770480394363
Training batch 10 / 32
Total batch reconstruction loss: 0.06950259953737259
Training batch 11 / 32
Total batch reconstruction loss: 0.07578225433826447
Training batch 12 / 32
Total batch reconstruction loss: 0.07134780287742615
Training batch 13 / 32
Total batch reconstruction loss: 0.07075441628694534
Training batch 14 / 32
Total batch reconstruction loss: 0.07194583117961884
Training batch 15 / 32
Total batch reconstruction loss: 0.06920066475868225
Training batch 16 / 32
Total batch reconstruction loss: 0.07225032150745392
Training batch 17 / 32
Total batch reconstruction loss: 0.07226857542991638
Training batch 18 / 32
Total batch reconstruction loss: 0.07178881764411926
Training batch 19 / 32
Total batch reconstruction loss: 0.06925550103187561
Training batch 20 / 32
Total batch reconstruction loss: 0.0685405507683754
Training batch 21 / 32
Total batch reconstruction loss: 0.06946303695440292
Training batch 22 / 32
Total batch reconstruction loss: 0.06688088178634644
Training batch 23 / 32
Total batch reconstruction loss: 0.06901945918798447
Training batch 24 / 32
Total batch reconstruction loss: 0.0705208033323288
Training batch 25 / 32
Total batch reconstruction loss: 0.06684659421443939
Training batch 26 / 32
Total batch reconstruction loss: 0.07435543090105057
Training batch 27 / 32
Total batch reconstruction loss: 0.07009600847959518
Training batch 28 / 32
Total batch reconstruction loss: 0.06983260810375214
Training batch 29 / 32
Total batch reconstruction loss: 0.07022498548030853
Training batch 30 / 32
Total batch reconstruction loss: 0.07164895534515381
Training batch 31 / 32
Total batch reconstruction loss: 0.0702182799577713
Training batch 32 / 32
Total batch reconstruction loss: 0.06909993290901184
Epoch [21/500], Train Loss: 0.0776, Validation Loss: 0.0755, Generator Loss: 14.5095, Discriminator Loss: 0.2621
Training epoch 22 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.07168777287006378
Training batch 2 / 32
Total batch reconstruction loss: 0.07172126322984695
Training batch 3 / 32
Total batch reconstruction loss: 0.0688048005104065
Training batch 4 / 32
Total batch reconstruction loss: 0.06841379404067993
Training batch 5 / 32
Total batch reconstruction loss: 0.06964011490345001
Training batch 6 / 32
Total batch reconstruction loss: 0.0778023973107338
Training batch 7 / 32
Total batch reconstruction loss: 0.07293260097503662
Training batch 8 / 32
Total batch reconstruction loss: 0.06773824989795685
Training batch 9 / 32
Total batch reconstruction loss: 0.07155634462833405
Training batch 10 / 32
Total batch reconstruction loss: 0.0713641345500946
Training batch 11 / 32
Total batch reconstruction loss: 0.07161499559879303
Training batch 12 / 32
Total batch reconstruction loss: 0.07117670774459839
Training batch 13 / 32
Total batch reconstruction loss: 0.0714825689792633
Training batch 14 / 32
Total batch reconstruction loss: 0.0707819014787674
Training batch 15 / 32
Total batch reconstruction loss: 0.07600118219852448
Training batch 16 / 32
Total batch reconstruction loss: 0.06954728066921234
Training batch 17 / 32
Total batch reconstruction loss: 0.07060617208480835
Training batch 18 / 32
Total batch reconstruction loss: 0.07242709398269653
Training batch 19 / 32
Total batch reconstruction loss: 0.07260096818208694
Training batch 20 / 32
Total batch reconstruction loss: 0.07291927188634872
Training batch 21 / 32
Total batch reconstruction loss: 0.07488956302404404
Training batch 22 / 32
Total batch reconstruction loss: 0.0722547098994255
Training batch 23 / 32
Total batch reconstruction loss: 0.07152120023965836
Training batch 24 / 32
Total batch reconstruction loss: 0.07416225969791412
Training batch 25 / 32
Total batch reconstruction loss: 0.07501386851072311
Training batch 26 / 32
Total batch reconstruction loss: 0.07051729410886765
Training batch 27 / 32
Total batch reconstruction loss: 0.07425089925527573
Training batch 28 / 32
Total batch reconstruction loss: 0.07058985531330109
Training batch 29 / 32
Total batch reconstruction loss: 0.06756749749183655
Training batch 30 / 32
Total batch reconstruction loss: 0.07037538290023804
Training batch 31 / 32
Total batch reconstruction loss: 0.07357963174581528
Training batch 32 / 32
Total batch reconstruction loss: 0.06724822521209717
Epoch [22/500], Train Loss: 0.0783, Validation Loss: 0.0839, Generator Loss: 14.6622, Discriminator Loss: 0.2603
Training epoch 23 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.07421544939279556
Training batch 2 / 32
Total batch reconstruction loss: 0.06896831840276718
Training batch 3 / 32
Total batch reconstruction loss: 0.0729861706495285
Training batch 4 / 32
Total batch reconstruction loss: 0.0766419917345047
Training batch 5 / 32
Total batch reconstruction loss: 0.07037593424320221
Training batch 6 / 32
Total batch reconstruction loss: 0.07327334582805634
Training batch 7 / 32
Total batch reconstruction loss: 0.07271908223628998
Training batch 8 / 32
Total batch reconstruction loss: 0.07629063725471497
Training batch 9 / 32
Total batch reconstruction loss: 0.0696008950471878
Training batch 10 / 32
Total batch reconstruction loss: 0.07470719516277313
Training batch 11 / 32
Total batch reconstruction loss: 0.06856181472539902
Training batch 12 / 32
Total batch reconstruction loss: 0.06886882334947586
Training batch 13 / 32
Total batch reconstruction loss: 0.07197660952806473
Training batch 14 / 32
Total batch reconstruction loss: 0.07152450084686279
Training batch 15 / 32
Total batch reconstruction loss: 0.06934709846973419
Training batch 16 / 32
Total batch reconstruction loss: 0.06973928958177567
Training batch 17 / 32
Total batch reconstruction loss: 0.07090188562870026
Training batch 18 / 32
Total batch reconstruction loss: 0.06965744495391846
Training batch 19 / 32
Total batch reconstruction loss: 0.07035288214683533
Training batch 20 / 32
Total batch reconstruction loss: 0.06978433579206467
Training batch 21 / 32
Total batch reconstruction loss: 0.07140950858592987
Training batch 22 / 32
Total batch reconstruction loss: 0.07211720943450928
Training batch 23 / 32
Total batch reconstruction loss: 0.06845133006572723
Training batch 24 / 32
Total batch reconstruction loss: 0.07085013389587402
Training batch 25 / 32
Total batch reconstruction loss: 0.07174678146839142
Training batch 26 / 32
Total batch reconstruction loss: 0.07256053388118744
Training batch 27 / 32
Total batch reconstruction loss: 0.07213431596755981
Training batch 28 / 32
Total batch reconstruction loss: 0.07446886599063873
Training batch 29 / 32
Total batch reconstruction loss: 0.0714612603187561
Training batch 30 / 32
Total batch reconstruction loss: 0.07376731932163239
Training batch 31 / 32
Total batch reconstruction loss: 0.07077217102050781
Training batch 32 / 32
Total batch reconstruction loss: 0.07549906522035599
Epoch [23/500], Train Loss: 0.0787, Validation Loss: 0.0770, Generator Loss: 14.6343, Discriminator Loss: 0.2656
Training epoch 24 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.07357470691204071
Training batch 2 / 32
Total batch reconstruction loss: 0.07120490074157715
Training batch 3 / 32
Total batch reconstruction loss: 0.06800106167793274
Training batch 4 / 32
Total batch reconstruction loss: 0.07146859169006348
Training batch 5 / 32
Total batch reconstruction loss: 0.0756630003452301
Training batch 6 / 32
Total batch reconstruction loss: 0.07433882355690002
Training batch 7 / 32
Total batch reconstruction loss: 0.07003158330917358
Training batch 8 / 32
Total batch reconstruction loss: 0.06963424384593964
Training batch 9 / 32
Total batch reconstruction loss: 0.07219229638576508
Training batch 10 / 32
Total batch reconstruction loss: 0.07336019724607468
Training batch 11 / 32
Total batch reconstruction loss: 0.07276490330696106
Training batch 12 / 32
Total batch reconstruction loss: 0.06921236217021942
Training batch 13 / 32
Total batch reconstruction loss: 0.06731855124235153
Training batch 14 / 32
Total batch reconstruction loss: 0.06764894723892212
Training batch 15 / 32
Total batch reconstruction loss: 0.07082168012857437
Training batch 16 / 32
Total batch reconstruction loss: 0.06716234982013702
Training batch 17 / 32
Total batch reconstruction loss: 0.07137851417064667
Training batch 18 / 32
Total batch reconstruction loss: 0.06901755928993225
Training batch 19 / 32
Total batch reconstruction loss: 0.07057799398899078
Training batch 20 / 32
Total batch reconstruction loss: 0.07198259234428406
Training batch 21 / 32
Total batch reconstruction loss: 0.07371538877487183
Training batch 22 / 32
Total batch reconstruction loss: 0.06863013654947281
Training batch 23 / 32
Total batch reconstruction loss: 0.0694747269153595
Training batch 24 / 32
Total batch reconstruction loss: 0.06783036142587662
Training batch 25 / 32
Total batch reconstruction loss: 0.07004117965698242
Training batch 26 / 32
Total batch reconstruction loss: 0.06942488253116608
Training batch 27 / 32
Total batch reconstruction loss: 0.07209718227386475
Training batch 28 / 32
Total batch reconstruction loss: 0.06810048967599869
Training batch 29 / 32
Total batch reconstruction loss: 0.06893247365951538
Training batch 30 / 32
Total batch reconstruction loss: 0.07051222026348114
Training batch 31 / 32
Total batch reconstruction loss: 0.06890109926462173
Training batch 32 / 32
Total batch reconstruction loss: 0.07889390736818314
Epoch [24/500], Train Loss: 0.0767, Validation Loss: 0.0776, Generator Loss: 14.4612, Discriminator Loss: 0.2420
Training epoch 25 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.06959901005029678
Training batch 2 / 32
Total batch reconstruction loss: 0.07025502622127533
Training batch 3 / 32
Total batch reconstruction loss: 0.07380732893943787
Training batch 4 / 32
Total batch reconstruction loss: 0.07415718585252762
Training batch 5 / 32
Total batch reconstruction loss: 0.07055722177028656
Training batch 6 / 32
Total batch reconstruction loss: 0.06735925376415253
Training batch 7 / 32
Total batch reconstruction loss: 0.06786992400884628
Training batch 8 / 32
Total batch reconstruction loss: 0.07308827340602875
Training batch 9 / 32
Total batch reconstruction loss: 0.06924957036972046
Training batch 10 / 32
Total batch reconstruction loss: 0.06852100789546967
Training batch 11 / 32
Total batch reconstruction loss: 0.06761747598648071
Training batch 12 / 32
Total batch reconstruction loss: 0.066490039229393
Training batch 13 / 32
Total batch reconstruction loss: 0.07300135493278503
Training batch 14 / 32
Total batch reconstruction loss: 0.06662875413894653
Training batch 15 / 32
Total batch reconstruction loss: 0.06913680583238602
Training batch 16 / 32
Total batch reconstruction loss: 0.07217152416706085
Training batch 17 / 32
Total batch reconstruction loss: 0.06996066868305206
Training batch 18 / 32
Total batch reconstruction loss: 0.07379922270774841
Training batch 19 / 32
Total batch reconstruction loss: 0.06758803129196167
Training batch 20 / 32
Total batch reconstruction loss: 0.06879174709320068
Training batch 21 / 32
Total batch reconstruction loss: 0.07279050350189209
Training batch 22 / 32
Total batch reconstruction loss: 0.07125277817249298
Training batch 23 / 32
Total batch reconstruction loss: 0.07157297432422638
Training batch 24 / 32
Total batch reconstruction loss: 0.06792877614498138
Training batch 25 / 32
Total batch reconstruction loss: 0.06709431111812592
Training batch 26 / 32
Total batch reconstruction loss: 0.07321739196777344
Training batch 27 / 32
Total batch reconstruction loss: 0.06836217641830444
Training batch 28 / 32
Total batch reconstruction loss: 0.06936004757881165
Training batch 29 / 32
Total batch reconstruction loss: 0.07197564095258713
Training batch 30 / 32
Total batch reconstruction loss: 0.0708145722746849
Training batch 31 / 32
Total batch reconstruction loss: 0.06883685290813446
Training batch 32 / 32
Total batch reconstruction loss: 0.06572969257831573
Epoch [25/500], Train Loss: 0.0756, Validation Loss: 0.0812, Generator Loss: 14.2659, Discriminator Loss: 0.2462
Training epoch 26 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.07381927967071533
Training batch 2 / 32
Total batch reconstruction loss: 0.0686427429318428
Training batch 3 / 32
Total batch reconstruction loss: 0.07342568039894104
Training batch 4 / 32
Total batch reconstruction loss: 0.07663916051387787
Training batch 5 / 32
Total batch reconstruction loss: 0.06992510706186295
Training batch 6 / 32
Total batch reconstruction loss: 0.072903111577034
Training batch 7 / 32
Total batch reconstruction loss: 0.06832567602396011
Training batch 8 / 32
Total batch reconstruction loss: 0.06985191255807877
Training batch 9 / 32
Total batch reconstruction loss: 0.07078974694013596
Training batch 10 / 32
Total batch reconstruction loss: 0.07318244874477386
Training batch 11 / 32
Total batch reconstruction loss: 0.07345142960548401
Training batch 12 / 32
Total batch reconstruction loss: 0.07023315131664276
Training batch 13 / 32
Total batch reconstruction loss: 0.07291306555271149
Training batch 14 / 32
Total batch reconstruction loss: 0.07381683588027954
Training batch 15 / 32
Total batch reconstruction loss: 0.07046961784362793
Training batch 16 / 32
Total batch reconstruction loss: 0.07030290365219116
Training batch 17 / 32
Total batch reconstruction loss: 0.07044511288404465
Training batch 18 / 32
Total batch reconstruction loss: 0.07236059755086899
Training batch 19 / 32
Total batch reconstruction loss: 0.06843069195747375
Training batch 20 / 32
Total batch reconstruction loss: 0.06776454299688339
Training batch 21 / 32
Total batch reconstruction loss: 0.06697572767734528
Training batch 22 / 32
Total batch reconstruction loss: 0.06779925525188446
Training batch 23 / 32
Total batch reconstruction loss: 0.07027925550937653
Training batch 24 / 32
Total batch reconstruction loss: 0.07295966148376465
Training batch 25 / 32
Total batch reconstruction loss: 0.07262039929628372
Training batch 26 / 32
Total batch reconstruction loss: 0.07686188817024231
Training batch 27 / 32
Total batch reconstruction loss: 0.06843620538711548
Training batch 28 / 32
Total batch reconstruction loss: 0.07101896405220032
Training batch 29 / 32
Total batch reconstruction loss: 0.07346183806657791
Training batch 30 / 32
Total batch reconstruction loss: 0.06717348843812943
Training batch 31 / 32
Total batch reconstruction loss: 0.07306133955717087
Training batch 32 / 32
Total batch reconstruction loss: 0.06281155347824097
Epoch [26/500], Train Loss: 0.0775, Validation Loss: 0.0735, Generator Loss: 14.4519, Discriminator Loss: 0.2515
Training epoch 27 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.07023467123508453
Training batch 2 / 32
Total batch reconstruction loss: 0.06928520649671555
Training batch 3 / 32
Total batch reconstruction loss: 0.06917605549097061
Training batch 4 / 32
Total batch reconstruction loss: 0.07016383111476898
Training batch 5 / 32
Total batch reconstruction loss: 0.07316363602876663
Training batch 6 / 32
Total batch reconstruction loss: 0.06905991584062576
Training batch 7 / 32
Total batch reconstruction loss: 0.06921164691448212
Training batch 8 / 32
Total batch reconstruction loss: 0.07017213851213455
Training batch 9 / 32
Total batch reconstruction loss: 0.06931871175765991
Training batch 10 / 32
Total batch reconstruction loss: 0.06835290789604187
Training batch 11 / 32
Total batch reconstruction loss: 0.06674682348966599
Training batch 12 / 32
Total batch reconstruction loss: 0.06530863046646118
Training batch 13 / 32
Total batch reconstruction loss: 0.06826971471309662
Training batch 14 / 32
Total batch reconstruction loss: 0.06595507264137268
Training batch 15 / 32
Total batch reconstruction loss: 0.06962074339389801
Training batch 16 / 32
Total batch reconstruction loss: 0.06650544703006744
Training batch 17 / 32
Total batch reconstruction loss: 0.07297048717737198
Training batch 18 / 32
Total batch reconstruction loss: 0.07163918763399124
Training batch 19 / 32
Total batch reconstruction loss: 0.07009053230285645
Training batch 20 / 32
Total batch reconstruction loss: 0.06901098787784576
Training batch 21 / 32
Total batch reconstruction loss: 0.07362037897109985
Training batch 22 / 32
Total batch reconstruction loss: 0.07154986262321472
Training batch 23 / 32
Total batch reconstruction loss: 0.07170245051383972
Training batch 24 / 32
Total batch reconstruction loss: 0.07003751397132874
Training batch 25 / 32
Total batch reconstruction loss: 0.0701962485909462
Training batch 26 / 32
Total batch reconstruction loss: 0.06955161690711975
Training batch 27 / 32
Total batch reconstruction loss: 0.06802129745483398
Training batch 28 / 32
Total batch reconstruction loss: 0.07458850741386414
Training batch 29 / 32
Total batch reconstruction loss: 0.06741228699684143
Training batch 30 / 32
Total batch reconstruction loss: 0.06807342171669006
Training batch 31 / 32
Total batch reconstruction loss: 0.07002541422843933
Training batch 32 / 32
Total batch reconstruction loss: 0.07639072835445404
Epoch [27/500], Train Loss: 0.0753, Validation Loss: 0.0740, Generator Loss: 14.2097, Discriminator Loss: 0.2631
Training epoch 28 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.06754161417484283
Training batch 2 / 32
Total batch reconstruction loss: 0.07475633919239044
Training batch 3 / 32
Total batch reconstruction loss: 0.07017213851213455
Training batch 4 / 32
Total batch reconstruction loss: 0.07019412517547607
Training batch 5 / 32
Total batch reconstruction loss: 0.07128661870956421
Training batch 6 / 32
Total batch reconstruction loss: 0.06584213674068451
Training batch 7 / 32
Total batch reconstruction loss: 0.06979040801525116
Training batch 8 / 32
Total batch reconstruction loss: 0.07467484474182129
Training batch 9 / 32
Total batch reconstruction loss: 0.0675823763012886
Training batch 10 / 32
Total batch reconstruction loss: 0.07138899713754654
Training batch 11 / 32
Total batch reconstruction loss: 0.06718655675649643
Training batch 12 / 32
Total batch reconstruction loss: 0.06929702311754227
Training batch 13 / 32
Total batch reconstruction loss: 0.06950787454843521
Training batch 14 / 32
Total batch reconstruction loss: 0.07244344055652618
Training batch 15 / 32
Total batch reconstruction loss: 0.06704626977443695
Training batch 16 / 32
Total batch reconstruction loss: 0.06790977716445923
Training batch 17 / 32
Total batch reconstruction loss: 0.06996506452560425
Training batch 18 / 32
Total batch reconstruction loss: 0.07304203510284424
Training batch 19 / 32
Total batch reconstruction loss: 0.06798609346151352
Training batch 20 / 32
Total batch reconstruction loss: 0.06663782894611359
Training batch 21 / 32
Total batch reconstruction loss: 0.06977532804012299
Training batch 22 / 32
Total batch reconstruction loss: 0.07216519117355347
Training batch 23 / 32
Total batch reconstruction loss: 0.06788396090269089
Training batch 24 / 32
Total batch reconstruction loss: 0.06781592965126038
Training batch 25 / 32
Total batch reconstruction loss: 0.06989212334156036
Training batch 26 / 32
Total batch reconstruction loss: 0.06606799364089966
Training batch 27 / 32
Total batch reconstruction loss: 0.06726056337356567
Training batch 28 / 32
Total batch reconstruction loss: 0.07345454394817352
Training batch 29 / 32
Total batch reconstruction loss: 0.06436696648597717
Training batch 30 / 32
Total batch reconstruction loss: 0.065897136926651
Training batch 31 / 32
Total batch reconstruction loss: 0.07077363133430481
Training batch 32 / 32
Total batch reconstruction loss: 0.059761893004179
Epoch [28/500], Train Loss: 0.0745, Validation Loss: 0.0743, Generator Loss: 14.0292, Discriminator Loss: 0.2631
Training epoch 29 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.07049262523651123
Training batch 2 / 32
Total batch reconstruction loss: 0.07286561280488968
Training batch 3 / 32
Total batch reconstruction loss: 0.06947365403175354
Training batch 4 / 32
Total batch reconstruction loss: 0.06757497042417526
Training batch 5 / 32
Total batch reconstruction loss: 0.06996671855449677
Training batch 6 / 32
Total batch reconstruction loss: 0.06708674132823944
Training batch 7 / 32
Total batch reconstruction loss: 0.06929917633533478
Training batch 8 / 32
Total batch reconstruction loss: 0.0704711526632309
Training batch 9 / 32
Total batch reconstruction loss: 0.0704437866806984
Training batch 10 / 32
Total batch reconstruction loss: 0.06931853294372559
Training batch 11 / 32
Total batch reconstruction loss: 0.07150885462760925
Training batch 12 / 32
Total batch reconstruction loss: 0.06911777704954147
Training batch 13 / 32
Total batch reconstruction loss: 0.06882889568805695
Training batch 14 / 32
Total batch reconstruction loss: 0.06382597982883453
Training batch 15 / 32
Total batch reconstruction loss: 0.06639246642589569
Training batch 16 / 32
Total batch reconstruction loss: 0.07171718776226044
Training batch 17 / 32
Total batch reconstruction loss: 0.06900113821029663
Training batch 18 / 32
Total batch reconstruction loss: 0.06517785787582397
Training batch 19 / 32
Total batch reconstruction loss: 0.07095364481210709
Training batch 20 / 32
Total batch reconstruction loss: 0.07021668553352356
Training batch 21 / 32
Total batch reconstruction loss: 0.06503593176603317
Training batch 22 / 32
Total batch reconstruction loss: 0.06798794865608215
Training batch 23 / 32
Total batch reconstruction loss: 0.06522613018751144
Training batch 24 / 32
Total batch reconstruction loss: 0.0732986330986023
Training batch 25 / 32
Total batch reconstruction loss: 0.06918968260288239
Training batch 26 / 32
Total batch reconstruction loss: 0.0698099434375763
Training batch 27 / 32
Total batch reconstruction loss: 0.06956435739994049
Training batch 28 / 32
Total batch reconstruction loss: 0.06688696146011353
Training batch 29 / 32
Total batch reconstruction loss: 0.06920909881591797
Training batch 30 / 32
Total batch reconstruction loss: 0.07339435815811157
Training batch 31 / 32
Total batch reconstruction loss: 0.06981948018074036
Training batch 32 / 32
Total batch reconstruction loss: 0.08190201222896576
Epoch [29/500], Train Loss: 0.0742, Validation Loss: 0.0749, Generator Loss: 14.1235, Discriminator Loss: 0.2603
Training epoch 30 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.06792368739843369
Training batch 2 / 32
Total batch reconstruction loss: 0.07309791445732117
Training batch 3 / 32
Total batch reconstruction loss: 0.06847579777240753
Training batch 4 / 32
Total batch reconstruction loss: 0.07039294391870499
Training batch 5 / 32
Total batch reconstruction loss: 0.06719854474067688
Training batch 6 / 32
Total batch reconstruction loss: 0.06718230247497559
Training batch 7 / 32
Total batch reconstruction loss: 0.06607428193092346
Training batch 8 / 32
Total batch reconstruction loss: 0.0700034499168396
Training batch 9 / 32
Total batch reconstruction loss: 0.07312564551830292
Training batch 10 / 32
Total batch reconstruction loss: 0.06594672799110413
Training batch 11 / 32
Total batch reconstruction loss: 0.06731951981782913
Training batch 12 / 32
Total batch reconstruction loss: 0.0722021609544754
Training batch 13 / 32
Total batch reconstruction loss: 0.06791342794895172
Training batch 14 / 32
Total batch reconstruction loss: 0.07103931903839111
Training batch 15 / 32
Total batch reconstruction loss: 0.0687737762928009
Training batch 16 / 32
Total batch reconstruction loss: 0.07577618956565857
Training batch 17 / 32
Total batch reconstruction loss: 0.06910673528909683
Training batch 18 / 32
Total batch reconstruction loss: 0.06959233433008194
Training batch 19 / 32
Total batch reconstruction loss: 0.0747520923614502
Training batch 20 / 32
Total batch reconstruction loss: 0.07372656464576721
Training batch 21 / 32
Total batch reconstruction loss: 0.06820370256900787
Training batch 22 / 32
Total batch reconstruction loss: 0.06706613302230835
Training batch 23 / 32
Total batch reconstruction loss: 0.06874164938926697
Training batch 24 / 32
Total batch reconstruction loss: 0.06981101632118225
Training batch 25 / 32
Total batch reconstruction loss: 0.0652836263179779
Training batch 26 / 32
Total batch reconstruction loss: 0.06741839647293091
Training batch 27 / 32
Total batch reconstruction loss: 0.06683636456727982
Training batch 28 / 32
Total batch reconstruction loss: 0.06784215569496155
Training batch 29 / 32
Total batch reconstruction loss: 0.06889348477125168
Training batch 30 / 32
Total batch reconstruction loss: 0.06541246920824051
Training batch 31 / 32
Total batch reconstruction loss: 0.06446525454521179
Training batch 32 / 32
Total batch reconstruction loss: 0.062355268746614456
Epoch [30/500], Train Loss: 0.0738, Validation Loss: 0.0770, Generator Loss: 13.9632, Discriminator Loss: 0.2482
Training epoch 31 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.0647987574338913
Training batch 2 / 32
Total batch reconstruction loss: 0.07160025835037231
Training batch 3 / 32
Total batch reconstruction loss: 0.0692584216594696
Training batch 4 / 32
Total batch reconstruction loss: 0.0721522867679596
Training batch 5 / 32
Total batch reconstruction loss: 0.07206545770168304
Training batch 6 / 32
Total batch reconstruction loss: 0.07263766974210739
Training batch 7 / 32
Total batch reconstruction loss: 0.07630392909049988
Training batch 8 / 32
Total batch reconstruction loss: 0.06858062744140625
Training batch 9 / 32
Total batch reconstruction loss: 0.06581415235996246
Training batch 10 / 32
Total batch reconstruction loss: 0.06720716506242752
Training batch 11 / 32
Total batch reconstruction loss: 0.07130776345729828
Training batch 12 / 32
Total batch reconstruction loss: 0.06897889822721481
Training batch 13 / 32
Total batch reconstruction loss: 0.06923330575227737
Training batch 14 / 32
Total batch reconstruction loss: 0.06943091750144958
Training batch 15 / 32
Total batch reconstruction loss: 0.06673581898212433
Training batch 16 / 32
Total batch reconstruction loss: 0.06843528151512146
Training batch 17 / 32
Total batch reconstruction loss: 0.06538990139961243
Training batch 18 / 32
Total batch reconstruction loss: 0.06338857114315033
Training batch 19 / 32
Total batch reconstruction loss: 0.06770607829093933
Training batch 20 / 32
Total batch reconstruction loss: 0.07089602947235107
Training batch 21 / 32
Total batch reconstruction loss: 0.06681525707244873
Training batch 22 / 32
Total batch reconstruction loss: 0.06866754591464996
Training batch 23 / 32
Total batch reconstruction loss: 0.06554073095321655
Training batch 24 / 32
Total batch reconstruction loss: 0.06925865262746811
Training batch 25 / 32
Total batch reconstruction loss: 0.0663856789469719
Training batch 26 / 32
Total batch reconstruction loss: 0.06889282166957855
Training batch 27 / 32
Total batch reconstruction loss: 0.06749691069126129
Training batch 28 / 32
Total batch reconstruction loss: 0.07562358677387238
Training batch 29 / 32
Total batch reconstruction loss: 0.06876581162214279
Training batch 30 / 32
Total batch reconstruction loss: 0.0719200074672699
Training batch 31 / 32
Total batch reconstruction loss: 0.06458412110805511
Training batch 32 / 32
Total batch reconstruction loss: 0.0692339837551117
Epoch [31/500], Train Loss: 0.0737, Validation Loss: 0.0770, Generator Loss: 13.9774, Discriminator Loss: 0.2555
Training epoch 32 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.07343520224094391
Training batch 2 / 32
Total batch reconstruction loss: 0.06986090540885925
Training batch 3 / 32
Total batch reconstruction loss: 0.07383465766906738
Training batch 4 / 32
Total batch reconstruction loss: 0.06592758744955063
Training batch 5 / 32
Total batch reconstruction loss: 0.06628742069005966
Training batch 6 / 32
Total batch reconstruction loss: 0.06854817271232605
Training batch 7 / 32
Total batch reconstruction loss: 0.07182561606168747
Training batch 8 / 32
Total batch reconstruction loss: 0.0681883692741394
Training batch 9 / 32
Total batch reconstruction loss: 0.06611521542072296
Training batch 10 / 32
Total batch reconstruction loss: 0.06751193106174469
Training batch 11 / 32
Total batch reconstruction loss: 0.06632861495018005
Training batch 12 / 32
Total batch reconstruction loss: 0.06514842808246613
Training batch 13 / 32
Total batch reconstruction loss: 0.06651332974433899
Training batch 14 / 32
Total batch reconstruction loss: 0.07040009647607803
Training batch 15 / 32
Total batch reconstruction loss: 0.06525664031505585
Training batch 16 / 32
Total batch reconstruction loss: 0.06768840551376343
Training batch 17 / 32
Total batch reconstruction loss: 0.06688840687274933
Training batch 18 / 32
Total batch reconstruction loss: 0.06687095761299133
Training batch 19 / 32
Total batch reconstruction loss: 0.06728726625442505
Training batch 20 / 32
Total batch reconstruction loss: 0.06902255117893219
Training batch 21 / 32
Total batch reconstruction loss: 0.06494466215372086
Training batch 22 / 32
Total batch reconstruction loss: 0.07169196009635925
Training batch 23 / 32
Total batch reconstruction loss: 0.06832793354988098
Training batch 24 / 32
Total batch reconstruction loss: 0.06615299731492996
Training batch 25 / 32
Total batch reconstruction loss: 0.06957632303237915
Training batch 26 / 32
Total batch reconstruction loss: 0.07509678602218628
Training batch 27 / 32
Total batch reconstruction loss: 0.06836465001106262
Training batch 28 / 32
Total batch reconstruction loss: 0.07048635929822922
Training batch 29 / 32
Total batch reconstruction loss: 0.07054643332958221
Training batch 30 / 32
Total batch reconstruction loss: 0.06797175109386444
Training batch 31 / 32
Total batch reconstruction loss: 0.0676971897482872
Training batch 32 / 32
Total batch reconstruction loss: 0.06640276312828064
Epoch [32/500], Train Loss: 0.0734, Validation Loss: 0.0760, Generator Loss: 13.8699, Discriminator Loss: 0.2664
Training epoch 33 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.06783138960599899
Training batch 2 / 32
Total batch reconstruction loss: 0.0670696496963501
Training batch 3 / 32
Total batch reconstruction loss: 0.06842191517353058
Training batch 4 / 32
Total batch reconstruction loss: 0.06585925817489624
Training batch 5 / 32
Total batch reconstruction loss: 0.06747377663850784
Training batch 6 / 32
Total batch reconstruction loss: 0.06731535494327545
Training batch 7 / 32
Total batch reconstruction loss: 0.0665121078491211
Training batch 8 / 32
Total batch reconstruction loss: 0.06904232501983643
Training batch 9 / 32
Total batch reconstruction loss: 0.06896268576383591
Training batch 10 / 32
Total batch reconstruction loss: 0.06475294381380081
Training batch 11 / 32
Total batch reconstruction loss: 0.07400708645582199
Training batch 12 / 32
Total batch reconstruction loss: 0.06323137879371643
Training batch 13 / 32
Total batch reconstruction loss: 0.06598104536533356
Training batch 14 / 32
Total batch reconstruction loss: 0.07216396927833557
Training batch 15 / 32
Total batch reconstruction loss: 0.07130831480026245
Training batch 16 / 32
Total batch reconstruction loss: 0.06953335553407669
Training batch 17 / 32
Total batch reconstruction loss: 0.07124000787734985
Training batch 18 / 32
Total batch reconstruction loss: 0.07136303186416626
Training batch 19 / 32
Total batch reconstruction loss: 0.06634607911109924
Training batch 20 / 32
Total batch reconstruction loss: 0.07269890606403351
Training batch 21 / 32
Total batch reconstruction loss: 0.07270071655511856
Training batch 22 / 32
Total batch reconstruction loss: 0.06478039920330048
Training batch 23 / 32
Total batch reconstruction loss: 0.0668841078877449
Training batch 24 / 32
Total batch reconstruction loss: 0.07088109850883484
Training batch 25 / 32
Total batch reconstruction loss: 0.07032778859138489
Training batch 26 / 32
Total batch reconstruction loss: 0.0727379322052002
Training batch 27 / 32
Total batch reconstruction loss: 0.06717236340045929
Training batch 28 / 32
Total batch reconstruction loss: 0.06917178630828857
Training batch 29 / 32
Total batch reconstruction loss: 0.06620952486991882
Training batch 30 / 32
Total batch reconstruction loss: 0.060842085629701614
Training batch 31 / 32
Total batch reconstruction loss: 0.06793320178985596
Training batch 32 / 32
Total batch reconstruction loss: 0.06628720462322235
Epoch [33/500], Train Loss: 0.0731, Validation Loss: 0.0715, Generator Loss: 13.8439, Discriminator Loss: 0.2753
Training epoch 34 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.07049021124839783
Training batch 2 / 32
Total batch reconstruction loss: 0.0666554644703865
Training batch 3 / 32
Total batch reconstruction loss: 0.06416568905115128
Training batch 4 / 32
Total batch reconstruction loss: 0.07010634243488312
Training batch 5 / 32
Total batch reconstruction loss: 0.06959819793701172
Training batch 6 / 32
Total batch reconstruction loss: 0.06637626886367798
Training batch 7 / 32
Total batch reconstruction loss: 0.06615674495697021
Training batch 8 / 32
Total batch reconstruction loss: 0.06900060176849365
Training batch 9 / 32
Total batch reconstruction loss: 0.06687207520008087
Training batch 10 / 32
Total batch reconstruction loss: 0.06705163419246674
Training batch 11 / 32
Total batch reconstruction loss: 0.07453575730323792
Training batch 12 / 32
Total batch reconstruction loss: 0.06599417328834534
Training batch 13 / 32
Total batch reconstruction loss: 0.07404740154743195
Training batch 14 / 32
Total batch reconstruction loss: 0.06471021473407745
Training batch 15 / 32
Total batch reconstruction loss: 0.06505406647920609
Training batch 16 / 32
Total batch reconstruction loss: 0.06390759348869324
Training batch 17 / 32
Total batch reconstruction loss: 0.06641490012407303
Training batch 18 / 32
Total batch reconstruction loss: 0.06552771478891373
Training batch 19 / 32
Total batch reconstruction loss: 0.06618352979421616
Training batch 20 / 32
Total batch reconstruction loss: 0.06983751058578491
Training batch 21 / 32
Total batch reconstruction loss: 0.06791619956493378
Training batch 22 / 32
Total batch reconstruction loss: 0.06812677532434464
Training batch 23 / 32
Total batch reconstruction loss: 0.069736048579216
Training batch 24 / 32
Total batch reconstruction loss: 0.0690169408917427
Training batch 25 / 32
Total batch reconstruction loss: 0.06994204968214035
Training batch 26 / 32
Total batch reconstruction loss: 0.0690096765756607
Training batch 27 / 32
Total batch reconstruction loss: 0.06886449456214905
Training batch 28 / 32
Total batch reconstruction loss: 0.06811493635177612
Training batch 29 / 32
Total batch reconstruction loss: 0.06574299186468124
Training batch 30 / 32
Total batch reconstruction loss: 0.06485764682292938
Training batch 31 / 32
Total batch reconstruction loss: 0.06492013484239578
Training batch 32 / 32
Total batch reconstruction loss: 0.06263619661331177
Epoch [34/500], Train Loss: 0.0713, Validation Loss: 0.0760, Generator Loss: 13.6872, Discriminator Loss: 0.2597
Training epoch 35 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.07204288989305496
Training batch 2 / 32
Total batch reconstruction loss: 0.06519974023103714
Training batch 3 / 32
Total batch reconstruction loss: 0.0694204717874527
Training batch 4 / 32
Total batch reconstruction loss: 0.06745456904172897
Training batch 5 / 32
Total batch reconstruction loss: 0.06716691702604294
Training batch 6 / 32
Total batch reconstruction loss: 0.07402956485748291
Training batch 7 / 32
Total batch reconstruction loss: 0.0746791735291481
Training batch 8 / 32
Total batch reconstruction loss: 0.07274146378040314
Training batch 9 / 32
Total batch reconstruction loss: 0.07047908008098602
Training batch 10 / 32
Total batch reconstruction loss: 0.06587443500757217
Training batch 11 / 32
Total batch reconstruction loss: 0.06737035512924194
Training batch 12 / 32
Total batch reconstruction loss: 0.06952223181724548
Training batch 13 / 32
Total batch reconstruction loss: 0.07640528678894043
Training batch 14 / 32
Total batch reconstruction loss: 0.07006745040416718
Training batch 15 / 32
Total batch reconstruction loss: 0.07264605164527893
Training batch 16 / 32
Total batch reconstruction loss: 0.06406547129154205
Training batch 17 / 32
Total batch reconstruction loss: 0.06559936702251434
Training batch 18 / 32
Total batch reconstruction loss: 0.06697161495685577
Training batch 19 / 32
Total batch reconstruction loss: 0.06750991195440292
Training batch 20 / 32
Total batch reconstruction loss: 0.06753036379814148
Training batch 21 / 32
Total batch reconstruction loss: 0.06562525033950806
Training batch 22 / 32
Total batch reconstruction loss: 0.07372720539569855
Training batch 23 / 32
Total batch reconstruction loss: 0.06716536730527878
Training batch 24 / 32
Total batch reconstruction loss: 0.07683704048395157
Training batch 25 / 32
Total batch reconstruction loss: 0.07671970129013062
Training batch 26 / 32
Total batch reconstruction loss: 0.06689608097076416
Training batch 27 / 32
Total batch reconstruction loss: 0.06905201077461243
Training batch 28 / 32
Total batch reconstruction loss: 0.07404204457998276
Training batch 29 / 32
Total batch reconstruction loss: 0.06724380701780319
Training batch 30 / 32
Total batch reconstruction loss: 0.06570278108119965
Training batch 31 / 32
Total batch reconstruction loss: 0.06512473523616791
Training batch 32 / 32
Total batch reconstruction loss: 0.06290784478187561
Epoch [35/500], Train Loss: 0.0750, Validation Loss: 0.0757, Generator Loss: 14.0303, Discriminator Loss: 0.2680
Training epoch 36 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.07042598724365234
Training batch 2 / 32
Total batch reconstruction loss: 0.06936632096767426
Training batch 3 / 32
Total batch reconstruction loss: 0.06753572821617126
Training batch 4 / 32
Total batch reconstruction loss: 0.06548486649990082
Training batch 5 / 32
Total batch reconstruction loss: 0.0707143247127533
Training batch 6 / 32
Total batch reconstruction loss: 0.06875751912593842
Training batch 7 / 32
Total batch reconstruction loss: 0.06665156781673431
Training batch 8 / 32
Total batch reconstruction loss: 0.07060620188713074
Training batch 9 / 32
Total batch reconstruction loss: 0.0661904439330101
Training batch 10 / 32
Total batch reconstruction loss: 0.0665942132472992
Training batch 11 / 32
Total batch reconstruction loss: 0.06526073813438416
Training batch 12 / 32
Total batch reconstruction loss: 0.06839770078659058
Training batch 13 / 32
Total batch reconstruction loss: 0.06860356777906418
Training batch 14 / 32
Total batch reconstruction loss: 0.06887044757604599
Training batch 15 / 32
Total batch reconstruction loss: 0.06433060020208359
Training batch 16 / 32
Total batch reconstruction loss: 0.06465589255094528
Training batch 17 / 32
Total batch reconstruction loss: 0.06948928534984589
Training batch 18 / 32
Total batch reconstruction loss: 0.06725528836250305
Training batch 19 / 32
Total batch reconstruction loss: 0.06619375199079514
Training batch 20 / 32
Total batch reconstruction loss: 0.06608384847640991
Training batch 21 / 32
Total batch reconstruction loss: 0.06965765357017517
Training batch 22 / 32
Total batch reconstruction loss: 0.07228842377662659
Training batch 23 / 32
Total batch reconstruction loss: 0.0641096979379654
Training batch 24 / 32
Total batch reconstruction loss: 0.06655745208263397
Training batch 25 / 32
Total batch reconstruction loss: 0.06748911738395691
Training batch 26 / 32
Total batch reconstruction loss: 0.0672982931137085
Training batch 27 / 32
Total batch reconstruction loss: 0.06670935451984406
Training batch 28 / 32
Total batch reconstruction loss: 0.06749292463064194
Training batch 29 / 32
Total batch reconstruction loss: 0.07390567660331726
Training batch 30 / 32
Total batch reconstruction loss: 0.06602196395397186
Training batch 31 / 32
Total batch reconstruction loss: 0.06405778229236603
Training batch 32 / 32
Total batch reconstruction loss: 0.06738242506980896
Epoch [36/500], Train Loss: 0.0719, Validation Loss: 0.0731, Generator Loss: 13.6608, Discriminator Loss: 0.2833
Training epoch 37 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.06457214802503586
Training batch 2 / 32
Total batch reconstruction loss: 0.0675038993358612
Training batch 3 / 32
Total batch reconstruction loss: 0.06673574447631836
Training batch 4 / 32
Total batch reconstruction loss: 0.06887117028236389
Training batch 5 / 32
Total batch reconstruction loss: 0.06430491805076599
Training batch 6 / 32
Total batch reconstruction loss: 0.06629738211631775
Training batch 7 / 32
Total batch reconstruction loss: 0.06679409742355347
Training batch 8 / 32
Total batch reconstruction loss: 0.0681411474943161
Training batch 9 / 32
Total batch reconstruction loss: 0.06686738133430481
Training batch 10 / 32
Total batch reconstruction loss: 0.06881542503833771
Training batch 11 / 32
Total batch reconstruction loss: 0.07145228236913681
Training batch 12 / 32
Total batch reconstruction loss: 0.06245020031929016
Training batch 13 / 32
Total batch reconstruction loss: 0.06818258762359619
Training batch 14 / 32
Total batch reconstruction loss: 0.06561799347400665
Training batch 15 / 32
Total batch reconstruction loss: 0.06848342716693878
Training batch 16 / 32
Total batch reconstruction loss: 0.06956647336483002
Training batch 17 / 32
Total batch reconstruction loss: 0.07190953195095062
Training batch 18 / 32
Total batch reconstruction loss: 0.07133515179157257
Training batch 19 / 32
Total batch reconstruction loss: 0.06852154433727264
Training batch 20 / 32
Total batch reconstruction loss: 0.06739793717861176
Training batch 21 / 32
Total batch reconstruction loss: 0.06338335573673248
Training batch 22 / 32
Total batch reconstruction loss: 0.0689057782292366
Training batch 23 / 32
Total batch reconstruction loss: 0.06713796406984329
Training batch 24 / 32
Total batch reconstruction loss: 0.07112725079059601
Training batch 25 / 32
Total batch reconstruction loss: 0.06418626010417938
Training batch 26 / 32
Total batch reconstruction loss: 0.06898573040962219
Training batch 27 / 32
Total batch reconstruction loss: 0.06800934672355652
Training batch 28 / 32
Total batch reconstruction loss: 0.07427610456943512
Training batch 29 / 32
Total batch reconstruction loss: 0.07038140296936035
Training batch 30 / 32
Total batch reconstruction loss: 0.06471417099237442
Training batch 31 / 32
Total batch reconstruction loss: 0.0699990764260292
Training batch 32 / 32
Total batch reconstruction loss: 0.0635371282696724
Epoch [37/500], Train Loss: 0.0725, Validation Loss: 0.0715, Generator Loss: 13.7107, Discriminator Loss: 0.2651
Training epoch 38 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.07137725502252579
Training batch 2 / 32
Total batch reconstruction loss: 0.07295279204845428
Training batch 3 / 32
Total batch reconstruction loss: 0.0706852599978447
Training batch 4 / 32
Total batch reconstruction loss: 0.06895473599433899
Training batch 5 / 32
Total batch reconstruction loss: 0.06957000494003296
Training batch 6 / 32
Total batch reconstruction loss: 0.0695895403623581
Training batch 7 / 32
Total batch reconstruction loss: 0.07133912295103073
Training batch 8 / 32
Total batch reconstruction loss: 0.07863616943359375
Training batch 9 / 32
Total batch reconstruction loss: 0.06929019093513489
Training batch 10 / 32
Total batch reconstruction loss: 0.06725482642650604
Training batch 11 / 32
Total batch reconstruction loss: 0.06292819231748581
Training batch 12 / 32
Total batch reconstruction loss: 0.07317173480987549
Training batch 13 / 32
Total batch reconstruction loss: 0.07462891936302185
Training batch 14 / 32
Total batch reconstruction loss: 0.07127499580383301
Training batch 15 / 32
Total batch reconstruction loss: 0.07130429893732071
Training batch 16 / 32
Total batch reconstruction loss: 0.06507465988397598
Training batch 17 / 32
Total batch reconstruction loss: 0.06679211556911469
Training batch 18 / 32
Total batch reconstruction loss: 0.06669096648693085
Training batch 19 / 32
Total batch reconstruction loss: 0.06777150928974152
Training batch 20 / 32
Total batch reconstruction loss: 0.07293789833784103
Training batch 21 / 32
Total batch reconstruction loss: 0.0696016177535057
Training batch 22 / 32
Total batch reconstruction loss: 0.06435750424861908
Training batch 23 / 32
Total batch reconstruction loss: 0.06677863746881485
Training batch 24 / 32
Total batch reconstruction loss: 0.06393852829933167
Training batch 25 / 32
Total batch reconstruction loss: 0.06175515055656433
Training batch 26 / 32
Total batch reconstruction loss: 0.06565088033676147
Training batch 27 / 32
Total batch reconstruction loss: 0.067156121134758
Training batch 28 / 32
Total batch reconstruction loss: 0.06605593115091324
Training batch 29 / 32
Total batch reconstruction loss: 0.06647685915231705
Training batch 30 / 32
Total batch reconstruction loss: 0.06683595478534698
Training batch 31 / 32
Total batch reconstruction loss: 0.06833881139755249
Training batch 32 / 32
Total batch reconstruction loss: 0.05835864692926407
Epoch [38/500], Train Loss: 0.0735, Validation Loss: 0.0714, Generator Loss: 13.8245, Discriminator Loss: 0.2689
Training epoch 39 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.06393467634916306
Training batch 2 / 32
Total batch reconstruction loss: 0.06882617622613907
Training batch 3 / 32
Total batch reconstruction loss: 0.07216621190309525
Training batch 4 / 32
Total batch reconstruction loss: 0.06850206851959229
Training batch 5 / 32
Total batch reconstruction loss: 0.06389673054218292
Training batch 6 / 32
Total batch reconstruction loss: 0.06542101502418518
Training batch 7 / 32
Total batch reconstruction loss: 0.06265010684728622
Training batch 8 / 32
Total batch reconstruction loss: 0.06932593882083893
Training batch 9 / 32
Total batch reconstruction loss: 0.06767990440130234
Training batch 10 / 32
Total batch reconstruction loss: 0.07079177349805832
Training batch 11 / 32
Total batch reconstruction loss: 0.07205313444137573
Training batch 12 / 32
Total batch reconstruction loss: 0.06946636736392975
Training batch 13 / 32
Total batch reconstruction loss: 0.06770327687263489
Training batch 14 / 32
Total batch reconstruction loss: 0.07218681275844574
Training batch 15 / 32
Total batch reconstruction loss: 0.06635846942663193
Training batch 16 / 32
Total batch reconstruction loss: 0.07381068915128708
Training batch 17 / 32
Total batch reconstruction loss: 0.06474598497152328
Training batch 18 / 32
Total batch reconstruction loss: 0.05980118364095688
Training batch 19 / 32
Total batch reconstruction loss: 0.06625629961490631
Training batch 20 / 32
Total batch reconstruction loss: 0.0680789202451706
Training batch 21 / 32
Total batch reconstruction loss: 0.06633993238210678
Training batch 22 / 32
Total batch reconstruction loss: 0.06510777771472931
Training batch 23 / 32
Total batch reconstruction loss: 0.07012808322906494
Training batch 24 / 32
Total batch reconstruction loss: 0.0664929524064064
Training batch 25 / 32
Total batch reconstruction loss: 0.0673813745379448
Training batch 26 / 32
Total batch reconstruction loss: 0.068242147564888
Training batch 27 / 32
Total batch reconstruction loss: 0.06461412459611893
Training batch 28 / 32
Total batch reconstruction loss: 0.06405217945575714
Training batch 29 / 32
Total batch reconstruction loss: 0.06625700742006302
Training batch 30 / 32
Total batch reconstruction loss: 0.06504571437835693
Training batch 31 / 32
Total batch reconstruction loss: 0.0660439059138298
Training batch 32 / 32
Total batch reconstruction loss: 0.059452757239341736
Epoch [39/500], Train Loss: 0.0710, Validation Loss: 0.0700, Generator Loss: 13.5323, Discriminator Loss: 0.2758
Training epoch 40 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.06473366171121597
Training batch 2 / 32
Total batch reconstruction loss: 0.07213231176137924
Training batch 3 / 32
Total batch reconstruction loss: 0.06422215700149536
Training batch 4 / 32
Total batch reconstruction loss: 0.067527174949646
Training batch 5 / 32
Total batch reconstruction loss: 0.07314908504486084
Training batch 6 / 32
Total batch reconstruction loss: 0.0671619400382042
Training batch 7 / 32
Total batch reconstruction loss: 0.0637829452753067
Training batch 8 / 32
Total batch reconstruction loss: 0.061983563005924225
Training batch 9 / 32
Total batch reconstruction loss: 0.07057252526283264
Training batch 10 / 32
Total batch reconstruction loss: 0.06877103447914124
Training batch 11 / 32
Total batch reconstruction loss: 0.07123693078756332
Training batch 12 / 32
Total batch reconstruction loss: 0.06688927114009857
Training batch 13 / 32
Total batch reconstruction loss: 0.07432682812213898
Training batch 14 / 32
Total batch reconstruction loss: 0.06242014467716217
Training batch 15 / 32
Total batch reconstruction loss: 0.06985725462436676
Training batch 16 / 32
Total batch reconstruction loss: 0.0659177303314209
Training batch 17 / 32
Total batch reconstruction loss: 0.06257379055023193
Training batch 18 / 32
Total batch reconstruction loss: 0.06674443185329437
Training batch 19 / 32
Total batch reconstruction loss: 0.06915634870529175
Training batch 20 / 32
Total batch reconstruction loss: 0.06660423427820206
Training batch 21 / 32
Total batch reconstruction loss: 0.06658820062875748
Training batch 22 / 32
Total batch reconstruction loss: 0.06569014489650726
Training batch 23 / 32
Total batch reconstruction loss: 0.06521814316511154
Training batch 24 / 32
Total batch reconstruction loss: 0.06225196644663811
Training batch 25 / 32
Total batch reconstruction loss: 0.07432320713996887
Training batch 26 / 32
Total batch reconstruction loss: 0.0663970410823822
Training batch 27 / 32
Total batch reconstruction loss: 0.06921277940273285
Training batch 28 / 32
Total batch reconstruction loss: 0.06512361764907837
Training batch 29 / 32
Total batch reconstruction loss: 0.06314601004123688
Training batch 30 / 32
Total batch reconstruction loss: 0.06609182804822922
Training batch 31 / 32
Total batch reconstruction loss: 0.06876026093959808
Training batch 32 / 32
Total batch reconstruction loss: 0.05761624872684479
Epoch [40/500], Train Loss: 0.0701, Validation Loss: 0.0717, Generator Loss: 13.4956, Discriminator Loss: 0.3001
Training epoch 41 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.06627742946147919
Training batch 2 / 32
Total batch reconstruction loss: 0.06533563137054443
Training batch 3 / 32
Total batch reconstruction loss: 0.06332553923130035
Training batch 4 / 32
Total batch reconstruction loss: 0.07184591889381409
Training batch 5 / 32
Total batch reconstruction loss: 0.06693843007087708
Training batch 6 / 32
Total batch reconstruction loss: 0.06800494343042374
Training batch 7 / 32
Total batch reconstruction loss: 0.07082383334636688
Training batch 8 / 32
Total batch reconstruction loss: 0.06841341406106949
Training batch 9 / 32
Total batch reconstruction loss: 0.06392655521631241
Training batch 10 / 32
Total batch reconstruction loss: 0.07130361348390579
Training batch 11 / 32
Total batch reconstruction loss: 0.0648915022611618
Training batch 12 / 32
Total batch reconstruction loss: 0.06533698737621307
Training batch 13 / 32
Total batch reconstruction loss: 0.06372623145580292
Training batch 14 / 32
Total batch reconstruction loss: 0.0646752268075943
Training batch 15 / 32
Total batch reconstruction loss: 0.06791368126869202
Training batch 16 / 32
Total batch reconstruction loss: 0.06705857813358307
Training batch 17 / 32
Total batch reconstruction loss: 0.06370784342288971
Training batch 18 / 32
Total batch reconstruction loss: 0.06390310078859329
Training batch 19 / 32
Total batch reconstruction loss: 0.06503380089998245
Training batch 20 / 32
Total batch reconstruction loss: 0.06669197976589203
Training batch 21 / 32
Total batch reconstruction loss: 0.06260073184967041
Training batch 22 / 32
Total batch reconstruction loss: 0.06427863240242004
Training batch 23 / 32
Total batch reconstruction loss: 0.0745386853814125
Training batch 24 / 32
Total batch reconstruction loss: 0.06734205782413483
Training batch 25 / 32
Total batch reconstruction loss: 0.06508608162403107
Training batch 26 / 32
Total batch reconstruction loss: 0.06573744118213654
Training batch 27 / 32
Total batch reconstruction loss: 0.06657096743583679
Training batch 28 / 32
Total batch reconstruction loss: 0.07039675116539001
Training batch 29 / 32
Total batch reconstruction loss: 0.065779909491539
Training batch 30 / 32
Total batch reconstruction loss: 0.0697956532239914
Training batch 31 / 32
Total batch reconstruction loss: 0.06642971932888031
Training batch 32 / 32
Total batch reconstruction loss: 0.07518859207630157
Epoch [41/500], Train Loss: 0.0701, Validation Loss: 0.0709, Generator Loss: 13.5052, Discriminator Loss: 0.2812
Training epoch 42 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.0632207915186882
Training batch 2 / 32
Total batch reconstruction loss: 0.06721112132072449
Training batch 3 / 32
Total batch reconstruction loss: 0.0638541728258133
Training batch 4 / 32
Total batch reconstruction loss: 0.06702129542827606
Training batch 5 / 32
Total batch reconstruction loss: 0.06697054207324982
Training batch 6 / 32
Total batch reconstruction loss: 0.06544975936412811
Training batch 7 / 32
Total batch reconstruction loss: 0.0651574581861496
Training batch 8 / 32
Total batch reconstruction loss: 0.06573458015918732
Training batch 9 / 32
Total batch reconstruction loss: 0.06228737533092499
Training batch 10 / 32
Total batch reconstruction loss: 0.07114720344543457
Training batch 11 / 32
Total batch reconstruction loss: 0.06907614320516586
Training batch 12 / 32
Total batch reconstruction loss: 0.07286916673183441
Training batch 13 / 32
Total batch reconstruction loss: 0.06620737165212631
Training batch 14 / 32
Total batch reconstruction loss: 0.06779491901397705
Training batch 15 / 32
Total batch reconstruction loss: 0.06816596537828445
Training batch 16 / 32
Total batch reconstruction loss: 0.06983952224254608
Training batch 17 / 32
Total batch reconstruction loss: 0.06793598085641861
Training batch 18 / 32
Total batch reconstruction loss: 0.06468403339385986
Training batch 19 / 32
Total batch reconstruction loss: 0.0687708705663681
Training batch 20 / 32
Total batch reconstruction loss: 0.06899003684520721
Training batch 21 / 32
Total batch reconstruction loss: 0.06891897320747375
Training batch 22 / 32
Total batch reconstruction loss: 0.06593933701515198
Training batch 23 / 32
Total batch reconstruction loss: 0.07043707370758057
Training batch 24 / 32
Total batch reconstruction loss: 0.06953735649585724
Training batch 25 / 32
Total batch reconstruction loss: 0.06306508183479309
Training batch 26 / 32
Total batch reconstruction loss: 0.0673765167593956
Training batch 27 / 32
Total batch reconstruction loss: 0.06344880908727646
Training batch 28 / 32
Total batch reconstruction loss: 0.06635958701372147
Training batch 29 / 32
Total batch reconstruction loss: 0.06704367697238922
Training batch 30 / 32
Total batch reconstruction loss: 0.06643446534872055
Training batch 31 / 32
Total batch reconstruction loss: 0.06497815251350403
Training batch 32 / 32
Total batch reconstruction loss: 0.06973835825920105
Epoch [42/500], Train Loss: 0.0700, Validation Loss: 0.0681, Generator Loss: 13.5383, Discriminator Loss: 0.2802
Training epoch 43 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.06626436859369278
Training batch 2 / 32
Total batch reconstruction loss: 0.06342286616563797
Training batch 3 / 32
Total batch reconstruction loss: 0.0648651123046875
Training batch 4 / 32
Total batch reconstruction loss: 0.06191958114504814
Training batch 5 / 32
Total batch reconstruction loss: 0.06755480170249939
Training batch 6 / 32
Total batch reconstruction loss: 0.066716268658638
Training batch 7 / 32
Total batch reconstruction loss: 0.0682608038187027
Training batch 8 / 32
Total batch reconstruction loss: 0.07176758348941803
Training batch 9 / 32
Total batch reconstruction loss: 0.06516502797603607
Training batch 10 / 32
Total batch reconstruction loss: 0.0640953779220581
Training batch 11 / 32
Total batch reconstruction loss: 0.062149763107299805
Training batch 12 / 32
Total batch reconstruction loss: 0.06648676097393036
Training batch 13 / 32
Total batch reconstruction loss: 0.06468653678894043
Training batch 14 / 32
Total batch reconstruction loss: 0.06657043099403381
Training batch 15 / 32
Total batch reconstruction loss: 0.06820274889469147
Training batch 16 / 32
Total batch reconstruction loss: 0.0666077584028244
Training batch 17 / 32
Total batch reconstruction loss: 0.06546225398778915
Training batch 18 / 32
Total batch reconstruction loss: 0.06585441529750824
Training batch 19 / 32
Total batch reconstruction loss: 0.061381928622722626
Training batch 20 / 32
Total batch reconstruction loss: 0.06703384220600128
Training batch 21 / 32
Total batch reconstruction loss: 0.07164956629276276
Training batch 22 / 32
Total batch reconstruction loss: 0.06308671832084656
Training batch 23 / 32
Total batch reconstruction loss: 0.07433883100748062
Training batch 24 / 32
Total batch reconstruction loss: 0.0663372129201889
Training batch 25 / 32
Total batch reconstruction loss: 0.06531810015439987
Training batch 26 / 32
Total batch reconstruction loss: 0.06382405757904053
Training batch 27 / 32
Total batch reconstruction loss: 0.06468214094638824
Training batch 28 / 32
Total batch reconstruction loss: 0.06693357974290848
Training batch 29 / 32
Total batch reconstruction loss: 0.07324571907520294
Training batch 30 / 32
Total batch reconstruction loss: 0.06432949006557465
Training batch 31 / 32
Total batch reconstruction loss: 0.0653967559337616
Training batch 32 / 32
Total batch reconstruction loss: 0.05503377318382263
Epoch [43/500], Train Loss: 0.0693, Validation Loss: 0.0672, Generator Loss: 13.2868, Discriminator Loss: 0.3005
Training epoch 44 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.06343766301870346
Training batch 2 / 32
Total batch reconstruction loss: 0.06095457449555397
Training batch 3 / 32
Total batch reconstruction loss: 0.0701170563697815
Training batch 4 / 32
Total batch reconstruction loss: 0.06628008186817169
Training batch 5 / 32
Total batch reconstruction loss: 0.06760391592979431
Training batch 6 / 32
Total batch reconstruction loss: 0.06438785791397095
Training batch 7 / 32
Total batch reconstruction loss: 0.06778232753276825
Training batch 8 / 32
Total batch reconstruction loss: 0.06845372915267944
Training batch 9 / 32
Total batch reconstruction loss: 0.06837807595729828
Training batch 10 / 32
Total batch reconstruction loss: 0.06459643691778183
Training batch 11 / 32
Total batch reconstruction loss: 0.06758586317300797
Training batch 12 / 32
Total batch reconstruction loss: 0.07351477444171906
Training batch 13 / 32
Total batch reconstruction loss: 0.06515567004680634
Training batch 14 / 32
Total batch reconstruction loss: 0.06715479493141174
Training batch 15 / 32
Total batch reconstruction loss: 0.06591744720935822
Training batch 16 / 32
Total batch reconstruction loss: 0.06514139473438263
Training batch 17 / 32
Total batch reconstruction loss: 0.06804630160331726
Training batch 18 / 32
Total batch reconstruction loss: 0.06475579738616943
Training batch 19 / 32
Total batch reconstruction loss: 0.06939730048179626
Training batch 20 / 32
Total batch reconstruction loss: 0.066450335085392
Training batch 21 / 32
Total batch reconstruction loss: 0.06510907411575317
Training batch 22 / 32
Total batch reconstruction loss: 0.06509292125701904
Training batch 23 / 32
Total batch reconstruction loss: 0.07031577825546265
Training batch 24 / 32
Total batch reconstruction loss: 0.0676487535238266
Training batch 25 / 32
Total batch reconstruction loss: 0.06734541058540344
Training batch 26 / 32
Total batch reconstruction loss: 0.06218710541725159
Training batch 27 / 32
Total batch reconstruction loss: 0.06958849728107452
Training batch 28 / 32
Total batch reconstruction loss: 0.06464435905218124
Training batch 29 / 32
Total batch reconstruction loss: 0.0642932876944542
Training batch 30 / 32
Total batch reconstruction loss: 0.06261355429887772
Training batch 31 / 32
Total batch reconstruction loss: 0.06557042896747589
Training batch 32 / 32
Total batch reconstruction loss: 0.06536725163459778
Epoch [44/500], Train Loss: 0.0699, Validation Loss: 0.0708, Generator Loss: 13.3921, Discriminator Loss: 0.2917
Training epoch 45 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.07410679012537003
Training batch 2 / 32
Total batch reconstruction loss: 0.0722426027059555
Training batch 3 / 32
Total batch reconstruction loss: 0.06679599732160568
Training batch 4 / 32
Total batch reconstruction loss: 0.06711187958717346
Training batch 5 / 32
Total batch reconstruction loss: 0.07302074134349823
Training batch 6 / 32
Total batch reconstruction loss: 0.06752165406942368
Training batch 7 / 32
Total batch reconstruction loss: 0.07624226808547974
Training batch 8 / 32
Total batch reconstruction loss: 0.07027676701545715
Training batch 9 / 32
Total batch reconstruction loss: 0.06809908151626587
Training batch 10 / 32
Total batch reconstruction loss: 0.06822586059570312
Training batch 11 / 32
Total batch reconstruction loss: 0.06721635162830353
Training batch 12 / 32
Total batch reconstruction loss: 0.06620977073907852
Training batch 13 / 32
Total batch reconstruction loss: 0.06316699087619781
Training batch 14 / 32
Total batch reconstruction loss: 0.06746974587440491
Training batch 15 / 32
Total batch reconstruction loss: 0.06966106593608856
Training batch 16 / 32
Total batch reconstruction loss: 0.06546293199062347
Training batch 17 / 32
Total batch reconstruction loss: 0.06264014542102814
Training batch 18 / 32
Total batch reconstruction loss: 0.06602373719215393
Training batch 19 / 32
Total batch reconstruction loss: 0.06831855326890945
Training batch 20 / 32
Total batch reconstruction loss: 0.06364502757787704
Training batch 21 / 32
Total batch reconstruction loss: 0.06553368270397186
Training batch 22 / 32
Total batch reconstruction loss: 0.06574327498674393
Training batch 23 / 32
Total batch reconstruction loss: 0.0681099221110344
Training batch 24 / 32
Total batch reconstruction loss: 0.06206708401441574
Training batch 25 / 32
Total batch reconstruction loss: 0.06546143442392349
Training batch 26 / 32
Total batch reconstruction loss: 0.06500206142663956
Training batch 27 / 32
Total batch reconstruction loss: 0.06487986445426941
Training batch 28 / 32
Total batch reconstruction loss: 0.0698881447315216
Training batch 29 / 32
Total batch reconstruction loss: 0.06201041489839554
Training batch 30 / 32
Total batch reconstruction loss: 0.06999761611223221
Training batch 31 / 32
Total batch reconstruction loss: 0.06196920573711395
Training batch 32 / 32
Total batch reconstruction loss: 0.07149289548397064
Epoch [45/500], Train Loss: 0.0700, Validation Loss: 0.0697, Generator Loss: 13.5861, Discriminator Loss: 0.2796
Training epoch 46 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.06698136031627655
Training batch 2 / 32
Total batch reconstruction loss: 0.06717436015605927
Training batch 3 / 32
Total batch reconstruction loss: 0.06791669130325317
Training batch 4 / 32
Total batch reconstruction loss: 0.07243926078081131
Training batch 5 / 32
Total batch reconstruction loss: 0.07029253989458084
Training batch 6 / 32
Total batch reconstruction loss: 0.06558165699243546
Training batch 7 / 32
Total batch reconstruction loss: 0.06331507861614227
Training batch 8 / 32
Total batch reconstruction loss: 0.06420253962278366
Training batch 9 / 32
Total batch reconstruction loss: 0.06200572848320007
Training batch 10 / 32
Total batch reconstruction loss: 0.06398022174835205
Training batch 11 / 32
Total batch reconstruction loss: 0.06254351139068604
Training batch 12 / 32
Total batch reconstruction loss: 0.06584276258945465
Training batch 13 / 32
Total batch reconstruction loss: 0.06573215872049332
Training batch 14 / 32
Total batch reconstruction loss: 0.06430285423994064
Training batch 15 / 32
Total batch reconstruction loss: 0.07014940679073334
Training batch 16 / 32
Total batch reconstruction loss: 0.06238047033548355
Training batch 17 / 32
Total batch reconstruction loss: 0.06375649571418762
Training batch 18 / 32
Total batch reconstruction loss: 0.06535279750823975
Training batch 19 / 32
Total batch reconstruction loss: 0.06398977339267731
Training batch 20 / 32
Total batch reconstruction loss: 0.06465940177440643
Training batch 21 / 32
Total batch reconstruction loss: 0.07256074994802475
Training batch 22 / 32
Total batch reconstruction loss: 0.06917460262775421
Training batch 23 / 32
Total batch reconstruction loss: 0.06782497465610504
Training batch 24 / 32
Total batch reconstruction loss: 0.061525728553533554
Training batch 25 / 32
Total batch reconstruction loss: 0.06288682669401169
Training batch 26 / 32
Total batch reconstruction loss: 0.07539869844913483
Training batch 27 / 32
Total batch reconstruction loss: 0.06225451081991196
Training batch 28 / 32
Total batch reconstruction loss: 0.07126378268003464
Training batch 29 / 32
Total batch reconstruction loss: 0.06554751098155975
Training batch 30 / 32
Total batch reconstruction loss: 0.06541576981544495
Training batch 31 / 32
Total batch reconstruction loss: 0.0650428757071495
Training batch 32 / 32
Total batch reconstruction loss: 0.10823875665664673
Epoch [46/500], Train Loss: 0.0704, Validation Loss: 0.0674, Generator Loss: 13.5969, Discriminator Loss: 0.2944
Training epoch 47 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.06844890862703323
Training batch 2 / 32
Total batch reconstruction loss: 0.06433228403329849
Training batch 3 / 32
Total batch reconstruction loss: 0.06701278686523438
Training batch 4 / 32
Total batch reconstruction loss: 0.06894777715206146
Training batch 5 / 32
Total batch reconstruction loss: 0.06870679557323456
Training batch 6 / 32
Total batch reconstruction loss: 0.0681757852435112
Training batch 7 / 32
Total batch reconstruction loss: 0.06527120620012283
Training batch 8 / 32
Total batch reconstruction loss: 0.06614966690540314
Training batch 9 / 32
Total batch reconstruction loss: 0.0628642737865448
Training batch 10 / 32
Total batch reconstruction loss: 0.06410680711269379
Training batch 11 / 32
Total batch reconstruction loss: 0.062278248369693756
Training batch 12 / 32
Total batch reconstruction loss: 0.0656508207321167
Training batch 13 / 32
Total batch reconstruction loss: 0.06577561050653458
Training batch 14 / 32
Total batch reconstruction loss: 0.06749400496482849
Training batch 15 / 32
Total batch reconstruction loss: 0.06482552736997604
Training batch 16 / 32
Total batch reconstruction loss: 0.065707728266716
Training batch 17 / 32
Total batch reconstruction loss: 0.07079273462295532
Training batch 18 / 32
Total batch reconstruction loss: 0.06018374115228653
Training batch 19 / 32
Total batch reconstruction loss: 0.06457288563251495
Training batch 20 / 32
Total batch reconstruction loss: 0.062051113694906235
Training batch 21 / 32
Total batch reconstruction loss: 0.06349869072437286
Training batch 22 / 32
Total batch reconstruction loss: 0.06494994461536407
Training batch 23 / 32
Total batch reconstruction loss: 0.06812714040279388
Training batch 24 / 32
Total batch reconstruction loss: 0.06449270248413086
Training batch 25 / 32
Total batch reconstruction loss: 0.0682428777217865
Training batch 26 / 32
Total batch reconstruction loss: 0.06470806896686554
Training batch 27 / 32
Total batch reconstruction loss: 0.066161148250103
Training batch 28 / 32
Total batch reconstruction loss: 0.06524033844470978
Training batch 29 / 32
Total batch reconstruction loss: 0.06863608211278915
Training batch 30 / 32
Total batch reconstruction loss: 0.0770476683974266
Training batch 31 / 32
Total batch reconstruction loss: 0.06404682993888855
Training batch 32 / 32
Total batch reconstruction loss: 0.05898112431168556
Epoch [47/500], Train Loss: 0.0688, Validation Loss: 0.0685, Generator Loss: 13.2784, Discriminator Loss: 0.2929
Training epoch 48 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.06091150641441345
Training batch 2 / 32
Total batch reconstruction loss: 0.06558282673358917
Training batch 3 / 32
Total batch reconstruction loss: 0.07166565209627151
Training batch 4 / 32
Total batch reconstruction loss: 0.06581303477287292
Training batch 5 / 32
Total batch reconstruction loss: 0.06365872919559479
Training batch 6 / 32
Total batch reconstruction loss: 0.06468735635280609
Training batch 7 / 32
Total batch reconstruction loss: 0.06523744016885757
Training batch 8 / 32
Total batch reconstruction loss: 0.06864874064922333
Training batch 9 / 32
Total batch reconstruction loss: 0.06567393243312836
Training batch 10 / 32
Total batch reconstruction loss: 0.060343191027641296
Training batch 11 / 32
Total batch reconstruction loss: 0.06948873400688171
Training batch 12 / 32
Total batch reconstruction loss: 0.07336583733558655
Training batch 13 / 32
Total batch reconstruction loss: 0.07316210865974426
Training batch 14 / 32
Total batch reconstruction loss: 0.06425308436155319
Training batch 15 / 32
Total batch reconstruction loss: 0.07301218807697296
Training batch 16 / 32
Total batch reconstruction loss: 0.06579993665218353
Training batch 17 / 32
Total batch reconstruction loss: 0.0717584639787674
Training batch 18 / 32
Total batch reconstruction loss: 0.06461623311042786
Training batch 19 / 32
Total batch reconstruction loss: 0.06630919128656387
Training batch 20 / 32
Total batch reconstruction loss: 0.06904186308383942
Training batch 21 / 32
Total batch reconstruction loss: 0.07157272100448608
Training batch 22 / 32
Total batch reconstruction loss: 0.06656846404075623
Training batch 23 / 32
Total batch reconstruction loss: 0.06943623721599579
Training batch 24 / 32
Total batch reconstruction loss: 0.06363435089588165
Training batch 25 / 32
Total batch reconstruction loss: 0.06353519856929779
Training batch 26 / 32
Total batch reconstruction loss: 0.06613702327013016
Training batch 27 / 32
Total batch reconstruction loss: 0.06863021850585938
Training batch 28 / 32
Total batch reconstruction loss: 0.06642736494541168
Training batch 29 / 32
Total batch reconstruction loss: 0.0639207512140274
Training batch 30 / 32
Total batch reconstruction loss: 0.06489379703998566
Training batch 31 / 32
Total batch reconstruction loss: 0.066457599401474
Training batch 32 / 32
Total batch reconstruction loss: 0.06587857007980347
Epoch [48/500], Train Loss: 0.0704, Validation Loss: 0.0687, Generator Loss: 13.4827, Discriminator Loss: 0.2827
Training epoch 49 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.06317631900310516
Training batch 2 / 32
Total batch reconstruction loss: 0.06261534988880157
Training batch 3 / 32
Total batch reconstruction loss: 0.06457258760929108
Training batch 4 / 32
Total batch reconstruction loss: 0.06455159187316895
Training batch 5 / 32
Total batch reconstruction loss: 0.06573688238859177
Training batch 6 / 32
Total batch reconstruction loss: 0.07259780913591385
Training batch 7 / 32
Total batch reconstruction loss: 0.07095630466938019
Training batch 8 / 32
Total batch reconstruction loss: 0.06263022124767303
Training batch 9 / 32
Total batch reconstruction loss: 0.07172590494155884
Training batch 10 / 32
Total batch reconstruction loss: 0.06101953983306885
Training batch 11 / 32
Total batch reconstruction loss: 0.060745969414711
Training batch 12 / 32
Total batch reconstruction loss: 0.06913762539625168
Training batch 13 / 32
Total batch reconstruction loss: 0.06929705291986465
Training batch 14 / 32
Total batch reconstruction loss: 0.0654110535979271
Training batch 15 / 32
Total batch reconstruction loss: 0.06870648264884949
Training batch 16 / 32
Total batch reconstruction loss: 0.06897478550672531
Training batch 17 / 32
Total batch reconstruction loss: 0.06724219024181366
Training batch 18 / 32
Total batch reconstruction loss: 0.05986178293824196
Training batch 19 / 32
Total batch reconstruction loss: 0.06445244699716568
Training batch 20 / 32
Total batch reconstruction loss: 0.07533678412437439
Training batch 21 / 32
Total batch reconstruction loss: 0.06572768092155457
Training batch 22 / 32
Total batch reconstruction loss: 0.06332050263881683
Training batch 23 / 32
Total batch reconstruction loss: 0.060504987835884094
Training batch 24 / 32
Total batch reconstruction loss: 0.07274474203586578
Training batch 25 / 32
Total batch reconstruction loss: 0.07364062964916229
Training batch 26 / 32
Total batch reconstruction loss: 0.06736092269420624
Training batch 27 / 32
Total batch reconstruction loss: 0.06596778333187103
Training batch 28 / 32
Total batch reconstruction loss: 0.07471515238285065
Training batch 29 / 32
Total batch reconstruction loss: 0.06274981051683426
Training batch 30 / 32
Total batch reconstruction loss: 0.06408923864364624
Training batch 31 / 32
Total batch reconstruction loss: 0.06329110264778137
Training batch 32 / 32
Total batch reconstruction loss: 0.07539267838001251
Epoch [49/500], Train Loss: 0.0700, Validation Loss: 0.0689, Generator Loss: 13.4581, Discriminator Loss: 0.2929
Training epoch 50 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.06574463844299316
Training batch 2 / 32
Total batch reconstruction loss: 0.061921823769807816
Training batch 3 / 32
Total batch reconstruction loss: 0.06954985857009888
Training batch 4 / 32
Total batch reconstruction loss: 0.06622631847858429
Training batch 5 / 32
Total batch reconstruction loss: 0.06681510806083679
Training batch 6 / 32
Total batch reconstruction loss: 0.06390024721622467
Training batch 7 / 32
Total batch reconstruction loss: 0.0686149001121521
Training batch 8 / 32
Total batch reconstruction loss: 0.06727777421474457
Training batch 9 / 32
Total batch reconstruction loss: 0.06201719492673874
Training batch 10 / 32
Total batch reconstruction loss: 0.0646166056394577
Training batch 11 / 32
Total batch reconstruction loss: 0.06554519385099411
Training batch 12 / 32
Total batch reconstruction loss: 0.06908395886421204
Training batch 13 / 32
Total batch reconstruction loss: 0.05895066633820534
Training batch 14 / 32
Total batch reconstruction loss: 0.06456056237220764
Training batch 15 / 32
Total batch reconstruction loss: 0.07340838015079498
Training batch 16 / 32
Total batch reconstruction loss: 0.061582066118717194
Training batch 17 / 32
Total batch reconstruction loss: 0.06404253840446472
Training batch 18 / 32
Total batch reconstruction loss: 0.06394857913255692
Training batch 19 / 32
Total batch reconstruction loss: 0.06265059113502502
Training batch 20 / 32
Total batch reconstruction loss: 0.06606768816709518
Training batch 21 / 32
Total batch reconstruction loss: 0.0735337883234024
Training batch 22 / 32
Total batch reconstruction loss: 0.06491070240736008
Training batch 23 / 32
Total batch reconstruction loss: 0.0658772885799408
Training batch 24 / 32
Total batch reconstruction loss: 0.060905277729034424
Training batch 25 / 32
Total batch reconstruction loss: 0.06750933825969696
Training batch 26 / 32
Total batch reconstruction loss: 0.063939169049263
Training batch 27 / 32
Total batch reconstruction loss: 0.060495227575302124
Training batch 28 / 32
Total batch reconstruction loss: 0.07084760069847107
Training batch 29 / 32
Total batch reconstruction loss: 0.06788714975118637
Training batch 30 / 32
Total batch reconstruction loss: 0.06465759873390198
Training batch 31 / 32
Total batch reconstruction loss: 0.06701713800430298
Training batch 32 / 32
Total batch reconstruction loss: 0.07737752795219421
Epoch [50/500], Train Loss: 0.0683, Validation Loss: 0.0718, Generator Loss: 13.2908, Discriminator Loss: 0.2986
Training epoch 51 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.07186391204595566
Training batch 2 / 32
Total batch reconstruction loss: 0.07561249285936356
Training batch 3 / 32
Total batch reconstruction loss: 0.06910502165555954
Training batch 4 / 32
Total batch reconstruction loss: 0.06719398498535156
Training batch 5 / 32
Total batch reconstruction loss: 0.06495065987110138
Training batch 6 / 32
Total batch reconstruction loss: 0.07149747014045715
Training batch 7 / 32
Total batch reconstruction loss: 0.07010272145271301
Training batch 8 / 32
Total batch reconstruction loss: 0.066705122590065
Training batch 9 / 32
Total batch reconstruction loss: 0.06153155490756035
Training batch 10 / 32
Total batch reconstruction loss: 0.06783588230609894
Training batch 11 / 32
Total batch reconstruction loss: 0.06644096225500107
Training batch 12 / 32
Total batch reconstruction loss: 0.0667751133441925
Training batch 13 / 32
Total batch reconstruction loss: 0.06571969389915466
Training batch 14 / 32
Total batch reconstruction loss: 0.06807860732078552
Training batch 15 / 32
Total batch reconstruction loss: 0.07413868606090546
Training batch 16 / 32
Total batch reconstruction loss: 0.07079951465129852
Training batch 17 / 32
Total batch reconstruction loss: 0.06487856805324554
Training batch 18 / 32
Total batch reconstruction loss: 0.0647672563791275
Training batch 19 / 32
Total batch reconstruction loss: 0.07035790383815765
Training batch 20 / 32
Total batch reconstruction loss: 0.06516203284263611
Training batch 21 / 32
Total batch reconstruction loss: 0.07109281420707703
Training batch 22 / 32
Total batch reconstruction loss: 0.06422524154186249
Training batch 23 / 32
Total batch reconstruction loss: 0.07540435343980789
Training batch 24 / 32
Total batch reconstruction loss: 0.06467381119728088
Training batch 25 / 32
Total batch reconstruction loss: 0.06546089798212051
Training batch 26 / 32
Total batch reconstruction loss: 0.06783601641654968
Training batch 27 / 32
Total batch reconstruction loss: 0.06579093635082245
Training batch 28 / 32
Total batch reconstruction loss: 0.06843234598636627
Training batch 29 / 32
Total batch reconstruction loss: 0.0663909912109375
Training batch 30 / 32
Total batch reconstruction loss: 0.06605961918830872
Training batch 31 / 32
Total batch reconstruction loss: 0.06546270847320557
Training batch 32 / 32
Total batch reconstruction loss: 0.06594637781381607
Epoch [51/500], Train Loss: 0.0697, Validation Loss: 0.0706, Generator Loss: 13.6660, Discriminator Loss: 0.2966
Training epoch 52 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.07492303848266602
Training batch 2 / 32
Total batch reconstruction loss: 0.06139694154262543
Training batch 3 / 32
Total batch reconstruction loss: 0.06611672043800354
Training batch 4 / 32
Total batch reconstruction loss: 0.06507725268602371
Training batch 5 / 32
Total batch reconstruction loss: 0.06412465125322342
Training batch 6 / 32
Total batch reconstruction loss: 0.06619307398796082
Training batch 7 / 32
Total batch reconstruction loss: 0.06689015030860901
Training batch 8 / 32
Total batch reconstruction loss: 0.06270481646060944
Training batch 9 / 32
Total batch reconstruction loss: 0.0635513961315155
Training batch 10 / 32
Total batch reconstruction loss: 0.06848815083503723
Training batch 11 / 32
Total batch reconstruction loss: 0.06605863571166992
Training batch 12 / 32
Total batch reconstruction loss: 0.06491642445325851
Training batch 13 / 32
Total batch reconstruction loss: 0.06549129635095596
Training batch 14 / 32
Total batch reconstruction loss: 0.06140851229429245
Training batch 15 / 32
Total batch reconstruction loss: 0.06333746016025543
Training batch 16 / 32
Total batch reconstruction loss: 0.07101771235466003
Training batch 17 / 32
Total batch reconstruction loss: 0.06932935118675232
Training batch 18 / 32
Total batch reconstruction loss: 0.06667286902666092
Training batch 19 / 32
Total batch reconstruction loss: 0.07087826728820801
Training batch 20 / 32
Total batch reconstruction loss: 0.06627481430768967
Training batch 21 / 32
Total batch reconstruction loss: 0.06564334779977798
Training batch 22 / 32
Total batch reconstruction loss: 0.06675983220338821
Training batch 23 / 32
Total batch reconstruction loss: 0.06976721435785294
Training batch 24 / 32
Total batch reconstruction loss: 0.06546550989151001
Training batch 25 / 32
Total batch reconstruction loss: 0.07018320262432098
Training batch 26 / 32
Total batch reconstruction loss: 0.0660717710852623
Training batch 27 / 32
Total batch reconstruction loss: 0.06640229374170303
Training batch 28 / 32
Total batch reconstruction loss: 0.06552496552467346
Training batch 29 / 32
Total batch reconstruction loss: 0.06273800134658813
Training batch 30 / 32
Total batch reconstruction loss: 0.0670158714056015
Training batch 31 / 32
Total batch reconstruction loss: 0.06985975801944733
Training batch 32 / 32
Total batch reconstruction loss: 0.0608915314078331
Epoch [52/500], Train Loss: 0.0678, Validation Loss: 0.0695, Generator Loss: 13.3458, Discriminator Loss: 0.3065
Training epoch 53 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.061865340918302536
Training batch 2 / 32
Total batch reconstruction loss: 0.06256350874900818
Training batch 3 / 32
Total batch reconstruction loss: 0.06218146160244942
Training batch 4 / 32
Total batch reconstruction loss: 0.06515303999185562
Training batch 5 / 32
Total batch reconstruction loss: 0.06504572927951813
Training batch 6 / 32
Total batch reconstruction loss: 0.06420476734638214
Training batch 7 / 32
Total batch reconstruction loss: 0.06905119866132736
Training batch 8 / 32
Total batch reconstruction loss: 0.06527872383594513
Training batch 9 / 32
Total batch reconstruction loss: 0.06317012012004852
Training batch 10 / 32
Total batch reconstruction loss: 0.0711425393819809
Training batch 11 / 32
Total batch reconstruction loss: 0.06873774528503418
Training batch 12 / 32
Total batch reconstruction loss: 0.06618445366621017
Training batch 13 / 32
Total batch reconstruction loss: 0.06596241891384125
Training batch 14 / 32
Total batch reconstruction loss: 0.07109463959932327
Training batch 15 / 32
Total batch reconstruction loss: 0.06475795805454254
Training batch 16 / 32
Total batch reconstruction loss: 0.06616316735744476
Training batch 17 / 32
Total batch reconstruction loss: 0.06805406510829926
Training batch 18 / 32
Total batch reconstruction loss: 0.06250932067632675
Training batch 19 / 32
Total batch reconstruction loss: 0.06419213861227036
Training batch 20 / 32
Total batch reconstruction loss: 0.07013317942619324
Training batch 21 / 32
Total batch reconstruction loss: 0.06680658459663391
Training batch 22 / 32
Total batch reconstruction loss: 0.06658436357975006
Training batch 23 / 32
Total batch reconstruction loss: 0.06479573249816895
Training batch 24 / 32
Total batch reconstruction loss: 0.07311058044433594
Training batch 25 / 32
Total batch reconstruction loss: 0.07036076486110687
Training batch 26 / 32
Total batch reconstruction loss: 0.06280885636806488
Training batch 27 / 32
Total batch reconstruction loss: 0.06218840926885605
Training batch 28 / 32
Total batch reconstruction loss: 0.06128685921430588
Training batch 29 / 32
Total batch reconstruction loss: 0.07146848738193512
Training batch 30 / 32
Total batch reconstruction loss: 0.0602797195315361
Training batch 31 / 32
Total batch reconstruction loss: 0.0713329091668129
Training batch 32 / 32
Total batch reconstruction loss: 0.06312620639801025
Epoch [53/500], Train Loss: 0.0684, Validation Loss: 0.0674, Generator Loss: 13.2871, Discriminator Loss: 0.3094
Training epoch 54 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.07264137268066406
Training batch 2 / 32
Total batch reconstruction loss: 0.06877540796995163
Training batch 3 / 32
Total batch reconstruction loss: 0.0680823028087616
Training batch 4 / 32
Total batch reconstruction loss: 0.06375372409820557
Training batch 5 / 32
Total batch reconstruction loss: 0.06551791727542877
Training batch 6 / 32
Total batch reconstruction loss: 0.06655144691467285
Training batch 7 / 32
Total batch reconstruction loss: 0.061164021492004395
Training batch 8 / 32
Total batch reconstruction loss: 0.06542542576789856
Training batch 9 / 32
Total batch reconstruction loss: 0.06740495562553406
Training batch 10 / 32
Total batch reconstruction loss: 0.06888195872306824
Training batch 11 / 32
Total batch reconstruction loss: 0.06490673124790192
Training batch 12 / 32
Total batch reconstruction loss: 0.06281787157058716
Training batch 13 / 32
Total batch reconstruction loss: 0.06215319782495499
Training batch 14 / 32
Total batch reconstruction loss: 0.06252172589302063
Training batch 15 / 32
Total batch reconstruction loss: 0.06498337537050247
Training batch 16 / 32
Total batch reconstruction loss: 0.06546767055988312
Training batch 17 / 32
Total batch reconstruction loss: 0.0645594596862793
Training batch 18 / 32
Total batch reconstruction loss: 0.06386011093854904
Training batch 19 / 32
Total batch reconstruction loss: 0.06404241919517517
Training batch 20 / 32
Total batch reconstruction loss: 0.06510703265666962
Training batch 21 / 32
Total batch reconstruction loss: 0.07028218358755112
Training batch 22 / 32
Total batch reconstruction loss: 0.06542874872684479
Training batch 23 / 32
Total batch reconstruction loss: 0.06505369395017624
Training batch 24 / 32
Total batch reconstruction loss: 0.05951102077960968
Training batch 25 / 32
Total batch reconstruction loss: 0.06646754592657089
Training batch 26 / 32
Total batch reconstruction loss: 0.06434229761362076
Training batch 27 / 32
Total batch reconstruction loss: 0.07079200446605682
Training batch 28 / 32
Total batch reconstruction loss: 0.06415248662233353
Training batch 29 / 32
Total batch reconstruction loss: 0.06036151945590973
Training batch 30 / 32
Total batch reconstruction loss: 0.06237080693244934
Training batch 31 / 32
Total batch reconstruction loss: 0.0658743679523468
Training batch 32 / 32
Total batch reconstruction loss: 0.06606291979551315
Epoch [54/500], Train Loss: 0.0677, Validation Loss: 0.0656, Generator Loss: 13.1530, Discriminator Loss: 0.2891
Training epoch 55 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.06305275857448578
Training batch 2 / 32
Total batch reconstruction loss: 0.06125635653734207
Training batch 3 / 32
Total batch reconstruction loss: 0.06524357199668884
Training batch 4 / 32
Total batch reconstruction loss: 0.06595566868782043
Training batch 5 / 32
Total batch reconstruction loss: 0.0655922070145607
Training batch 6 / 32
Total batch reconstruction loss: 0.06394708156585693
Training batch 7 / 32
Total batch reconstruction loss: 0.06480682641267776
Training batch 8 / 32
Total batch reconstruction loss: 0.06170996278524399
Training batch 9 / 32
Total batch reconstruction loss: 0.06202946975827217
Training batch 10 / 32
Total batch reconstruction loss: 0.06817895174026489
Training batch 11 / 32
Total batch reconstruction loss: 0.06363807618618011
Training batch 12 / 32
Total batch reconstruction loss: 0.07563149929046631
Training batch 13 / 32
Total batch reconstruction loss: 0.06228061020374298
Training batch 14 / 32
Total batch reconstruction loss: 0.06787167489528656
Training batch 15 / 32
Total batch reconstruction loss: 0.06696444004774094
Training batch 16 / 32
Total batch reconstruction loss: 0.0679859071969986
Training batch 17 / 32
Total batch reconstruction loss: 0.06572236120700836
Training batch 18 / 32
Total batch reconstruction loss: 0.06434165686368942
Training batch 19 / 32
Total batch reconstruction loss: 0.06229904294013977
Training batch 20 / 32
Total batch reconstruction loss: 0.06488046795129776
Training batch 21 / 32
Total batch reconstruction loss: 0.068763867020607
Training batch 22 / 32
Total batch reconstruction loss: 0.06630826741456985
Training batch 23 / 32
Total batch reconstruction loss: 0.0701877772808075
Training batch 24 / 32
Total batch reconstruction loss: 0.0694478452205658
Training batch 25 / 32
Total batch reconstruction loss: 0.06009458750486374
Training batch 26 / 32
Total batch reconstruction loss: 0.06581136584281921
Training batch 27 / 32
Total batch reconstruction loss: 0.06490757316350937
Training batch 28 / 32
Total batch reconstruction loss: 0.06457150727510452
Training batch 29 / 32
Total batch reconstruction loss: 0.06267386674880981
Training batch 30 / 32
Total batch reconstruction loss: 0.06330442428588867
Training batch 31 / 32
Total batch reconstruction loss: 0.06384475529193878
Training batch 32 / 32
Total batch reconstruction loss: 0.07617251574993134
Epoch [55/500], Train Loss: 0.0674, Validation Loss: 0.0669, Generator Loss: 13.1962, Discriminator Loss: 0.3205
Training epoch 56 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.0635048970580101
Training batch 2 / 32
Total batch reconstruction loss: 0.06954380869865417
Training batch 3 / 32
Total batch reconstruction loss: 0.06521490216255188
Training batch 4 / 32
Total batch reconstruction loss: 0.06285756826400757
Training batch 5 / 32
Total batch reconstruction loss: 0.06469926238059998
Training batch 6 / 32
Total batch reconstruction loss: 0.06241791322827339
Training batch 7 / 32
Total batch reconstruction loss: 0.062004756182432175
Training batch 8 / 32
Total batch reconstruction loss: 0.06725777685642242
Training batch 9 / 32
Total batch reconstruction loss: 0.06446896493434906
Training batch 10 / 32
Total batch reconstruction loss: 0.06622260808944702
Training batch 11 / 32
Total batch reconstruction loss: 0.06761951744556427
Training batch 12 / 32
Total batch reconstruction loss: 0.06233491003513336
Training batch 13 / 32
Total batch reconstruction loss: 0.0697813332080841
Training batch 14 / 32
Total batch reconstruction loss: 0.061478905379772186
Training batch 15 / 32
Total batch reconstruction loss: 0.06807699799537659
Training batch 16 / 32
Total batch reconstruction loss: 0.06663930416107178
Training batch 17 / 32
Total batch reconstruction loss: 0.06657401472330093
Training batch 18 / 32
Total batch reconstruction loss: 0.06252571195363998
Training batch 19 / 32
Total batch reconstruction loss: 0.0620214119553566
Training batch 20 / 32
Total batch reconstruction loss: 0.060505468398332596
Training batch 21 / 32
Total batch reconstruction loss: 0.06705981492996216
Training batch 22 / 32
Total batch reconstruction loss: 0.06345754116773605
Training batch 23 / 32
Total batch reconstruction loss: 0.06619301438331604
Training batch 24 / 32
Total batch reconstruction loss: 0.06253727525472641
Training batch 25 / 32
Total batch reconstruction loss: 0.06729954481124878
Training batch 26 / 32
Total batch reconstruction loss: 0.06025179475545883
Training batch 27 / 32
Total batch reconstruction loss: 0.06318742036819458
Training batch 28 / 32
Total batch reconstruction loss: 0.06217118352651596
Training batch 29 / 32
Total batch reconstruction loss: 0.06659534573554993
Training batch 30 / 32
Total batch reconstruction loss: 0.07330989092588425
Training batch 31 / 32
Total batch reconstruction loss: 0.06831641495227814
Training batch 32 / 32
Total batch reconstruction loss: 0.05513610690832138
Epoch [56/500], Train Loss: 0.0670, Validation Loss: 0.0664, Generator Loss: 13.0265, Discriminator Loss: 0.3094
Training epoch 57 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.0635775476694107
Training batch 2 / 32
Total batch reconstruction loss: 0.06669118255376816
Training batch 3 / 32
Total batch reconstruction loss: 0.06499381363391876
Training batch 4 / 32
Total batch reconstruction loss: 0.06281864643096924
Training batch 5 / 32
Total batch reconstruction loss: 0.06008434295654297
Training batch 6 / 32
Total batch reconstruction loss: 0.06256608664989471
Training batch 7 / 32
Total batch reconstruction loss: 0.06368346512317657
Training batch 8 / 32
Total batch reconstruction loss: 0.06617873907089233
Training batch 9 / 32
Total batch reconstruction loss: 0.06290575861930847
Training batch 10 / 32
Total batch reconstruction loss: 0.06128396838903427
Training batch 11 / 32
Total batch reconstruction loss: 0.06052280217409134
Training batch 12 / 32
Total batch reconstruction loss: 0.06353739649057388
Training batch 13 / 32
Total batch reconstruction loss: 0.06656350195407867
Training batch 14 / 32
Total batch reconstruction loss: 0.06286591291427612
Training batch 15 / 32
Total batch reconstruction loss: 0.06313804537057877
Training batch 16 / 32
Total batch reconstruction loss: 0.06567186117172241
Training batch 17 / 32
Total batch reconstruction loss: 0.06008794158697128
Training batch 18 / 32
Total batch reconstruction loss: 0.06542767584323883
Training batch 19 / 32
Total batch reconstruction loss: 0.06459860503673553
Training batch 20 / 32
Total batch reconstruction loss: 0.06348268687725067
Training batch 21 / 32
Total batch reconstruction loss: 0.06433258205652237
Training batch 22 / 32
Total batch reconstruction loss: 0.06429877132177353
Training batch 23 / 32
Total batch reconstruction loss: 0.062447428703308105
Training batch 24 / 32
Total batch reconstruction loss: 0.06857873499393463
Training batch 25 / 32
Total batch reconstruction loss: 0.06126964092254639
Training batch 26 / 32
Total batch reconstruction loss: 0.06586960703134537
Training batch 27 / 32
Total batch reconstruction loss: 0.06914401799440384
Training batch 28 / 32
Total batch reconstruction loss: 0.06751388311386108
Training batch 29 / 32
Total batch reconstruction loss: 0.064177006483078
Training batch 30 / 32
Total batch reconstruction loss: 0.0612029954791069
Training batch 31 / 32
Total batch reconstruction loss: 0.06311365962028503
Training batch 32 / 32
Total batch reconstruction loss: 0.06844277679920197
Epoch [57/500], Train Loss: 0.0655, Validation Loss: 0.0648, Generator Loss: 12.9022, Discriminator Loss: 0.3101
Training epoch 58 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.06225263327360153
Training batch 2 / 32
Total batch reconstruction loss: 0.06019663065671921
Training batch 3 / 32
Total batch reconstruction loss: 0.06231187656521797
Training batch 4 / 32
Total batch reconstruction loss: 0.06850039213895798
Training batch 5 / 32
Total batch reconstruction loss: 0.06068544462323189
Training batch 6 / 32
Total batch reconstruction loss: 0.05966516584157944
Training batch 7 / 32
Total batch reconstruction loss: 0.06326031684875488
Training batch 8 / 32
Total batch reconstruction loss: 0.06338830292224884
Training batch 9 / 32
Total batch reconstruction loss: 0.06235864758491516
Training batch 10 / 32
Total batch reconstruction loss: 0.06274154782295227
Training batch 11 / 32
Total batch reconstruction loss: 0.06461897492408752
Training batch 12 / 32
Total batch reconstruction loss: 0.06605193018913269
Training batch 13 / 32
Total batch reconstruction loss: 0.06458918750286102
Training batch 14 / 32
Total batch reconstruction loss: 0.06411466002464294
Training batch 15 / 32
Total batch reconstruction loss: 0.06525973975658417
Training batch 16 / 32
Total batch reconstruction loss: 0.06409910321235657
Training batch 17 / 32
Total batch reconstruction loss: 0.06676289439201355
Training batch 18 / 32
Total batch reconstruction loss: 0.06422911584377289
Training batch 19 / 32
Total batch reconstruction loss: 0.06757281720638275
Training batch 20 / 32
Total batch reconstruction loss: 0.06548609584569931
Training batch 21 / 32
Total batch reconstruction loss: 0.06577827036380768
Training batch 22 / 32
Total batch reconstruction loss: 0.06761203706264496
Training batch 23 / 32
Total batch reconstruction loss: 0.06082099676132202
Training batch 24 / 32
Total batch reconstruction loss: 0.06529061496257782
Training batch 25 / 32
Total batch reconstruction loss: 0.06399518996477127
Training batch 26 / 32
Total batch reconstruction loss: 0.06437338888645172
Training batch 27 / 32
Total batch reconstruction loss: 0.06908667087554932
Training batch 28 / 32
Total batch reconstruction loss: 0.06725418567657471
Training batch 29 / 32
Total batch reconstruction loss: 0.0692594051361084
Training batch 30 / 32
Total batch reconstruction loss: 0.06134498119354248
Training batch 31 / 32
Total batch reconstruction loss: 0.06616105884313583
Training batch 32 / 32
Total batch reconstruction loss: 0.06033196300268173
Epoch [58/500], Train Loss: 0.0660, Validation Loss: 0.0701, Generator Loss: 12.9473, Discriminator Loss: 0.3245
Training epoch 59 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.07082196325063705
Training batch 2 / 32
Total batch reconstruction loss: 0.061427343636751175
Training batch 3 / 32
Total batch reconstruction loss: 0.07226671278476715
Training batch 4 / 32
Total batch reconstruction loss: 0.0660930722951889
Training batch 5 / 32
Total batch reconstruction loss: 0.06557455658912659
Training batch 6 / 32
Total batch reconstruction loss: 0.06294485926628113
Training batch 7 / 32
Total batch reconstruction loss: 0.06306882947683334
Training batch 8 / 32
Total batch reconstruction loss: 0.07193876802921295
Training batch 9 / 32
Total batch reconstruction loss: 0.06640776991844177
Training batch 10 / 32
Total batch reconstruction loss: 0.06560929119586945
Training batch 11 / 32
Total batch reconstruction loss: 0.06750357896089554
Training batch 12 / 32
Total batch reconstruction loss: 0.06524237990379333
Training batch 13 / 32
Total batch reconstruction loss: 0.062471188604831696
Training batch 14 / 32
Total batch reconstruction loss: 0.06255373358726501
Training batch 15 / 32
Total batch reconstruction loss: 0.06300166994333267
Training batch 16 / 32
Total batch reconstruction loss: 0.06445519626140594
Training batch 17 / 32
Total batch reconstruction loss: 0.06535467505455017
Training batch 18 / 32
Total batch reconstruction loss: 0.068018838763237
Training batch 19 / 32
Total batch reconstruction loss: 0.06605315208435059
Training batch 20 / 32
Total batch reconstruction loss: 0.06356817483901978
Training batch 21 / 32
Total batch reconstruction loss: 0.0631566196680069
Training batch 22 / 32
Total batch reconstruction loss: 0.07100668549537659
Training batch 23 / 32
Total batch reconstruction loss: 0.06729252636432648
Training batch 24 / 32
Total batch reconstruction loss: 0.06358769536018372
Training batch 25 / 32
Total batch reconstruction loss: 0.06262628734111786
Training batch 26 / 32
Total batch reconstruction loss: 0.06569231301546097
Training batch 27 / 32
Total batch reconstruction loss: 0.06449441611766815
Training batch 28 / 32
Total batch reconstruction loss: 0.06623705476522446
Training batch 29 / 32
Total batch reconstruction loss: 0.06237783282995224
Training batch 30 / 32
Total batch reconstruction loss: 0.06576378643512726
Training batch 31 / 32
Total batch reconstruction loss: 0.06368573009967804
Training batch 32 / 32
Total batch reconstruction loss: 0.0643543004989624
Epoch [59/500], Train Loss: 0.0679, Validation Loss: 0.0693, Generator Loss: 13.1676, Discriminator Loss: 0.3138
Training epoch 60 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.06313027441501617
Training batch 2 / 32
Total batch reconstruction loss: 0.06665805727243423
Training batch 3 / 32
Total batch reconstruction loss: 0.06416258215904236
Training batch 4 / 32
Total batch reconstruction loss: 0.06465227901935577
Training batch 5 / 32
Total batch reconstruction loss: 0.0651283785700798
Training batch 6 / 32
Total batch reconstruction loss: 0.06454569101333618
Training batch 7 / 32
Total batch reconstruction loss: 0.06757364422082901
Training batch 8 / 32
Total batch reconstruction loss: 0.06162312254309654
Training batch 9 / 32
Total batch reconstruction loss: 0.06536121666431427
Training batch 10 / 32
Total batch reconstruction loss: 0.06913463771343231
Training batch 11 / 32
Total batch reconstruction loss: 0.064536914229393
Training batch 12 / 32
Total batch reconstruction loss: 0.06645017862319946
Training batch 13 / 32
Total batch reconstruction loss: 0.06516098976135254
Training batch 14 / 32
Total batch reconstruction loss: 0.07021569460630417
Training batch 15 / 32
Total batch reconstruction loss: 0.06488616019487381
Training batch 16 / 32
Total batch reconstruction loss: 0.07000239938497543
Training batch 17 / 32
Total batch reconstruction loss: 0.06391781568527222
Training batch 18 / 32
Total batch reconstruction loss: 0.06510913372039795
Training batch 19 / 32
Total batch reconstruction loss: 0.06702421605587006
Training batch 20 / 32
Total batch reconstruction loss: 0.06922443211078644
Training batch 21 / 32
Total batch reconstruction loss: 0.06413654237985611
Training batch 22 / 32
Total batch reconstruction loss: 0.06564327329397202
Training batch 23 / 32
Total batch reconstruction loss: 0.06322313845157623
Training batch 24 / 32
Total batch reconstruction loss: 0.06280535459518433
Training batch 25 / 32
Total batch reconstruction loss: 0.06206054612994194
Training batch 26 / 32
Total batch reconstruction loss: 0.0644332617521286
Training batch 27 / 32
Total batch reconstruction loss: 0.06050544232130051
Training batch 28 / 32
Total batch reconstruction loss: 0.062216684222221375
Training batch 29 / 32
Total batch reconstruction loss: 0.06321656703948975
Training batch 30 / 32
Total batch reconstruction loss: 0.06400758028030396
Training batch 31 / 32
Total batch reconstruction loss: 0.06862567365169525
Training batch 32 / 32
Total batch reconstruction loss: 0.06424262374639511
Epoch [60/500], Train Loss: 0.0666, Validation Loss: 0.0661, Generator Loss: 13.1132, Discriminator Loss: 0.2983
Training epoch 61 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.06713666766881943
Training batch 2 / 32
Total batch reconstruction loss: 0.07102625072002411
Training batch 3 / 32
Total batch reconstruction loss: 0.06757897138595581
Training batch 4 / 32
Total batch reconstruction loss: 0.06432580947875977
Training batch 5 / 32
Total batch reconstruction loss: 0.05958802253007889
Training batch 6 / 32
Total batch reconstruction loss: 0.06550605595111847
Training batch 7 / 32
Total batch reconstruction loss: 0.06698141992092133
Training batch 8 / 32
Total batch reconstruction loss: 0.07348203659057617
Training batch 9 / 32
Total batch reconstruction loss: 0.06251214444637299
Training batch 10 / 32
Total batch reconstruction loss: 0.06418581306934357
Training batch 11 / 32
Total batch reconstruction loss: 0.06682683527469635
Training batch 12 / 32
Total batch reconstruction loss: 0.06699547171592712
Training batch 13 / 32
Total batch reconstruction loss: 0.0617549791932106
Training batch 14 / 32
Total batch reconstruction loss: 0.06544716656208038
Training batch 15 / 32
Total batch reconstruction loss: 0.0662570595741272
Training batch 16 / 32
Total batch reconstruction loss: 0.06656605005264282
Training batch 17 / 32
Total batch reconstruction loss: 0.07096049189567566
Training batch 18 / 32
Total batch reconstruction loss: 0.06250906735658646
Training batch 19 / 32
Total batch reconstruction loss: 0.06338309496641159
Training batch 20 / 32
Total batch reconstruction loss: 0.06364458799362183
Training batch 21 / 32
Total batch reconstruction loss: 0.06583293527364731
Training batch 22 / 32
Total batch reconstruction loss: 0.06006980687379837
Training batch 23 / 32
Total batch reconstruction loss: 0.06035961955785751
Training batch 24 / 32
Total batch reconstruction loss: 0.06321270763874054
Training batch 25 / 32
Total batch reconstruction loss: 0.06489004194736481
Training batch 26 / 32
Total batch reconstruction loss: 0.06523144245147705
Training batch 27 / 32
Total batch reconstruction loss: 0.06374422460794449
Training batch 28 / 32
Total batch reconstruction loss: 0.06558726727962494
Training batch 29 / 32
Total batch reconstruction loss: 0.06413446366786957
Training batch 30 / 32
Total batch reconstruction loss: 0.06126439571380615
Training batch 31 / 32
Total batch reconstruction loss: 0.06386580318212509
Training batch 32 / 32
Total batch reconstruction loss: 0.052712541073560715
Epoch [61/500], Train Loss: 0.0653, Validation Loss: 0.0634, Generator Loss: 13.0073, Discriminator Loss: 0.3122
Training epoch 62 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.06421391665935516
Training batch 2 / 32
Total batch reconstruction loss: 0.06959876418113708
Training batch 3 / 32
Total batch reconstruction loss: 0.0606083907186985
Training batch 4 / 32
Total batch reconstruction loss: 0.05955146253108978
Training batch 5 / 32
Total batch reconstruction loss: 0.06546859443187714
Training batch 6 / 32
Total batch reconstruction loss: 0.06324446201324463
Training batch 7 / 32
Total batch reconstruction loss: 0.05763034522533417
Training batch 8 / 32
Total batch reconstruction loss: 0.06485496461391449
Training batch 9 / 32
Total batch reconstruction loss: 0.06350342929363251
Training batch 10 / 32
Total batch reconstruction loss: 0.06329745799303055
Training batch 11 / 32
Total batch reconstruction loss: 0.06477098166942596
Training batch 12 / 32
Total batch reconstruction loss: 0.06713631749153137
Training batch 13 / 32
Total batch reconstruction loss: 0.06500352919101715
Training batch 14 / 32
Total batch reconstruction loss: 0.06410856544971466
Training batch 15 / 32
Total batch reconstruction loss: 0.060942329466342926
Training batch 16 / 32
Total batch reconstruction loss: 0.0690564513206482
Training batch 17 / 32
Total batch reconstruction loss: 0.06584393233060837
Training batch 18 / 32
Total batch reconstruction loss: 0.06088903918862343
Training batch 19 / 32
Total batch reconstruction loss: 0.06769990175962448
Training batch 20 / 32
Total batch reconstruction loss: 0.06102238595485687
Training batch 21 / 32
Total batch reconstruction loss: 0.06466272473335266
Training batch 22 / 32
Total batch reconstruction loss: 0.06170222908258438
Training batch 23 / 32
Total batch reconstruction loss: 0.061507925391197205
Training batch 24 / 32
Total batch reconstruction loss: 0.07277347892522812
Training batch 25 / 32
Total batch reconstruction loss: 0.06288230419158936
Training batch 26 / 32
Total batch reconstruction loss: 0.061519138514995575
Training batch 27 / 32
Total batch reconstruction loss: 0.0615256167948246
Training batch 28 / 32
Total batch reconstruction loss: 0.0647943913936615
Training batch 29 / 32
Total batch reconstruction loss: 0.06890931725502014
Training batch 30 / 32
Total batch reconstruction loss: 0.06276315450668335
Training batch 31 / 32
Total batch reconstruction loss: 0.06741300225257874
Training batch 32 / 32
Total batch reconstruction loss: 0.07349839806556702
Epoch [62/500], Train Loss: 0.0659, Validation Loss: 0.0650, Generator Loss: 12.9787, Discriminator Loss: 0.2998
Training epoch 63 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.06361586600542068
Training batch 2 / 32
Total batch reconstruction loss: 0.06256010383367538
Training batch 3 / 32
Total batch reconstruction loss: 0.06122245639562607
Training batch 4 / 32
Total batch reconstruction loss: 0.06812611222267151
Training batch 5 / 32
Total batch reconstruction loss: 0.06476452946662903
Training batch 6 / 32
Total batch reconstruction loss: 0.06309273838996887
Training batch 7 / 32
Total batch reconstruction loss: 0.07258288562297821
Training batch 8 / 32
Total batch reconstruction loss: 0.06127186119556427
Training batch 9 / 32
Total batch reconstruction loss: 0.06826557219028473
Training batch 10 / 32
Total batch reconstruction loss: 0.05969327688217163
Training batch 11 / 32
Total batch reconstruction loss: 0.06299843639135361
Training batch 12 / 32
Total batch reconstruction loss: 0.0664762631058693
Training batch 13 / 32
Total batch reconstruction loss: 0.07608295977115631
Training batch 14 / 32
Total batch reconstruction loss: 0.06391256302595139
Training batch 15 / 32
Total batch reconstruction loss: 0.0707215815782547
Training batch 16 / 32
Total batch reconstruction loss: 0.06540460884571075
Training batch 17 / 32
Total batch reconstruction loss: 0.06925307214260101
Training batch 18 / 32
Total batch reconstruction loss: 0.06779700517654419
Training batch 19 / 32
Total batch reconstruction loss: 0.06181969866156578
Training batch 20 / 32
Total batch reconstruction loss: 0.06280983984470367
Training batch 21 / 32
Total batch reconstruction loss: 0.06582999229431152
Training batch 22 / 32
Total batch reconstruction loss: 0.061579249799251556
Training batch 23 / 32
Total batch reconstruction loss: 0.06258834898471832
Training batch 24 / 32
Total batch reconstruction loss: 0.06393882632255554
Training batch 25 / 32
Total batch reconstruction loss: 0.06385812908411026
Training batch 26 / 32
Total batch reconstruction loss: 0.06486202031373978
Training batch 27 / 32
Total batch reconstruction loss: 0.0612756609916687
Training batch 28 / 32
Total batch reconstruction loss: 0.06594263762235641
Training batch 29 / 32
Total batch reconstruction loss: 0.06800912320613861
Training batch 30 / 32
Total batch reconstruction loss: 0.0635460913181305
Training batch 31 / 32
Total batch reconstruction loss: 0.06603621691465378
Training batch 32 / 32
Total batch reconstruction loss: 0.061981201171875
Epoch [63/500], Train Loss: 0.0677, Validation Loss: 0.0664, Generator Loss: 13.1030, Discriminator Loss: 0.2973
Training epoch 64 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.06034044176340103
Training batch 2 / 32
Total batch reconstruction loss: 0.06132444366812706
Training batch 3 / 32
Total batch reconstruction loss: 0.0620373971760273
Training batch 4 / 32
Total batch reconstruction loss: 0.06588713824748993
Training batch 5 / 32
Total batch reconstruction loss: 0.0710313618183136
Training batch 6 / 32
Total batch reconstruction loss: 0.06223779916763306
Training batch 7 / 32
Total batch reconstruction loss: 0.06485027074813843
Training batch 8 / 32
Total batch reconstruction loss: 0.0647372230887413
Training batch 9 / 32
Total batch reconstruction loss: 0.06681260466575623
Training batch 10 / 32
Total batch reconstruction loss: 0.06792163848876953
Training batch 11 / 32
Total batch reconstruction loss: 0.06299455463886261
Training batch 12 / 32
Total batch reconstruction loss: 0.062208592891693115
Training batch 13 / 32
Total batch reconstruction loss: 0.0662553682923317
Training batch 14 / 32
Total batch reconstruction loss: 0.06406871229410172
Training batch 15 / 32
Total batch reconstruction loss: 0.06581464409828186
Training batch 16 / 32
Total batch reconstruction loss: 0.06347601860761642
Training batch 17 / 32
Total batch reconstruction loss: 0.06709659099578857
Training batch 18 / 32
Total batch reconstruction loss: 0.06478573381900787
Training batch 19 / 32
Total batch reconstruction loss: 0.06160108372569084
Training batch 20 / 32
Total batch reconstruction loss: 0.06374680250883102
Training batch 21 / 32
Total batch reconstruction loss: 0.0680348128080368
Training batch 22 / 32
Total batch reconstruction loss: 0.06368466466665268
Training batch 23 / 32
Total batch reconstruction loss: 0.06084807589650154
Training batch 24 / 32
Total batch reconstruction loss: 0.06176454573869705
Training batch 25 / 32
Total batch reconstruction loss: 0.061008840799331665
Training batch 26 / 32
Total batch reconstruction loss: 0.06008688360452652
Training batch 27 / 32
Total batch reconstruction loss: 0.06298108398914337
Training batch 28 / 32
Total batch reconstruction loss: 0.06805869936943054
Training batch 29 / 32
Total batch reconstruction loss: 0.059559378772974014
Training batch 30 / 32
Total batch reconstruction loss: 0.06143004447221756
Training batch 31 / 32
Total batch reconstruction loss: 0.06372947990894318
Training batch 32 / 32
Total batch reconstruction loss: 0.06367944180965424
Epoch [64/500], Train Loss: 0.0652, Validation Loss: 0.0673, Generator Loss: 12.8609, Discriminator Loss: 0.3090
Training epoch 65 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.06405524164438248
Training batch 2 / 32
Total batch reconstruction loss: 0.06286995857954025
Training batch 3 / 32
Total batch reconstruction loss: 0.06132882088422775
Training batch 4 / 32
Total batch reconstruction loss: 0.06674619019031525
Training batch 5 / 32
Total batch reconstruction loss: 0.062115877866744995
Training batch 6 / 32
Total batch reconstruction loss: 0.06568095088005066
Training batch 7 / 32
Total batch reconstruction loss: 0.06309521943330765
Training batch 8 / 32
Total batch reconstruction loss: 0.06441068649291992
Training batch 9 / 32
Total batch reconstruction loss: 0.06579779833555222
Training batch 10 / 32
Total batch reconstruction loss: 0.06163748726248741
Training batch 11 / 32
Total batch reconstruction loss: 0.06312014162540436
Training batch 12 / 32
Total batch reconstruction loss: 0.05868963897228241
Training batch 13 / 32
Total batch reconstruction loss: 0.06378338485956192
Training batch 14 / 32
Total batch reconstruction loss: 0.06506048142910004
Training batch 15 / 32
Total batch reconstruction loss: 0.06415628641843796
Training batch 16 / 32
Total batch reconstruction loss: 0.06565539538860321
Training batch 17 / 32
Total batch reconstruction loss: 0.06268072873353958
Training batch 18 / 32
Total batch reconstruction loss: 0.06646682322025299
Training batch 19 / 32
Total batch reconstruction loss: 0.06438601016998291
Training batch 20 / 32
Total batch reconstruction loss: 0.06472001224756241
Training batch 21 / 32
Total batch reconstruction loss: 0.06405052542686462
Training batch 22 / 32
Total batch reconstruction loss: 0.06890064477920532
Training batch 23 / 32
Total batch reconstruction loss: 0.06250157207250595
Training batch 24 / 32
Total batch reconstruction loss: 0.06448078155517578
Training batch 25 / 32
Total batch reconstruction loss: 0.059325993061065674
Training batch 26 / 32
Total batch reconstruction loss: 0.06006592512130737
Training batch 27 / 32
Total batch reconstruction loss: 0.06479845941066742
Training batch 28 / 32
Total batch reconstruction loss: 0.07052575796842575
Training batch 29 / 32
Total batch reconstruction loss: 0.061510808765888214
Training batch 30 / 32
Total batch reconstruction loss: 0.06794220954179764
Training batch 31 / 32
Total batch reconstruction loss: 0.0615314282476902
Training batch 32 / 32
Total batch reconstruction loss: 0.06485368311405182
Epoch [65/500], Train Loss: 0.0649, Validation Loss: 0.0669, Generator Loss: 12.8798, Discriminator Loss: 0.2918
Training epoch 66 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.06332289427518845
Training batch 2 / 32
Total batch reconstruction loss: 0.06317213177680969
Training batch 3 / 32
Total batch reconstruction loss: 0.07277707010507584
Training batch 4 / 32
Total batch reconstruction loss: 0.07246565818786621
Training batch 5 / 32
Total batch reconstruction loss: 0.06280889362096786
Training batch 6 / 32
Total batch reconstruction loss: 0.06719392538070679
Training batch 7 / 32
Total batch reconstruction loss: 0.06680543720722198
Training batch 8 / 32
Total batch reconstruction loss: 0.06379516422748566
Training batch 9 / 32
Total batch reconstruction loss: 0.06473435461521149
Training batch 10 / 32
Total batch reconstruction loss: 0.06428521126508713
Training batch 11 / 32
Total batch reconstruction loss: 0.06325583904981613
Training batch 12 / 32
Total batch reconstruction loss: 0.060080304741859436
Training batch 13 / 32
Total batch reconstruction loss: 0.06366793811321259
Training batch 14 / 32
Total batch reconstruction loss: 0.0648280680179596
Training batch 15 / 32
Total batch reconstruction loss: 0.059449829161167145
Training batch 16 / 32
Total batch reconstruction loss: 0.0619758777320385
Training batch 17 / 32
Total batch reconstruction loss: 0.0649067834019661
Training batch 18 / 32
Total batch reconstruction loss: 0.06745925545692444
Training batch 19 / 32
Total batch reconstruction loss: 0.06548912823200226
Training batch 20 / 32
Total batch reconstruction loss: 0.06072734296321869
Training batch 21 / 32
Total batch reconstruction loss: 0.0600217767059803
Training batch 22 / 32
Total batch reconstruction loss: 0.06541997194290161
Training batch 23 / 32
Total batch reconstruction loss: 0.06737713515758514
Training batch 24 / 32
Total batch reconstruction loss: 0.06497547030448914
Training batch 25 / 32
Total batch reconstruction loss: 0.06560567021369934
Training batch 26 / 32
Total batch reconstruction loss: 0.06342758983373642
Training batch 27 / 32
Total batch reconstruction loss: 0.06486715376377106
Training batch 28 / 32
Total batch reconstruction loss: 0.06104907765984535
Training batch 29 / 32
Total batch reconstruction loss: 0.059522587805986404
Training batch 30 / 32
Total batch reconstruction loss: 0.06332042813301086
Training batch 31 / 32
Total batch reconstruction loss: 0.06281118094921112
Training batch 32 / 32
Total batch reconstruction loss: 0.06476548314094543
Epoch [66/500], Train Loss: 0.0659, Validation Loss: 0.0670, Generator Loss: 12.9484, Discriminator Loss: 0.2854
Training epoch 67 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.06449628621339798
Training batch 2 / 32
Total batch reconstruction loss: 0.06988094747066498
Training batch 3 / 32
Total batch reconstruction loss: 0.05926699936389923
Training batch 4 / 32
Total batch reconstruction loss: 0.06561984121799469
Training batch 5 / 32
Total batch reconstruction loss: 0.06640210747718811
Training batch 6 / 32
Total batch reconstruction loss: 0.06465741991996765
Training batch 7 / 32
Total batch reconstruction loss: 0.059152744710445404
Training batch 8 / 32
Total batch reconstruction loss: 0.06254086643457413
Training batch 9 / 32
Total batch reconstruction loss: 0.06055223196744919
Training batch 10 / 32
Total batch reconstruction loss: 0.06317111104726791
Training batch 11 / 32
Total batch reconstruction loss: 0.06079801172018051
Training batch 12 / 32
Total batch reconstruction loss: 0.06675899028778076
Training batch 13 / 32
Total batch reconstruction loss: 0.06327787041664124
Training batch 14 / 32
Total batch reconstruction loss: 0.06803817301988602
Training batch 15 / 32
Total batch reconstruction loss: 0.0678550973534584
Training batch 16 / 32
Total batch reconstruction loss: 0.0639629140496254
Training batch 17 / 32
Total batch reconstruction loss: 0.058989010751247406
Training batch 18 / 32
Total batch reconstruction loss: 0.06167802959680557
Training batch 19 / 32
Total batch reconstruction loss: 0.06269718706607819
Training batch 20 / 32
Total batch reconstruction loss: 0.06270484626293182
Training batch 21 / 32
Total batch reconstruction loss: 0.060407910495996475
Training batch 22 / 32
Total batch reconstruction loss: 0.06616757810115814
Training batch 23 / 32
Total batch reconstruction loss: 0.06836649030447006
Training batch 24 / 32
Total batch reconstruction loss: 0.06233390420675278
Training batch 25 / 32
Total batch reconstruction loss: 0.06206511706113815
Training batch 26 / 32
Total batch reconstruction loss: 0.06340363621711731
Training batch 27 / 32
Total batch reconstruction loss: 0.06342373788356781
Training batch 28 / 32
Total batch reconstruction loss: 0.06156419217586517
Training batch 29 / 32
Total batch reconstruction loss: 0.0663200169801712
Training batch 30 / 32
Total batch reconstruction loss: 0.06673459708690643
Training batch 31 / 32
Total batch reconstruction loss: 0.06286526471376419
Training batch 32 / 32
Total batch reconstruction loss: 0.0558788925409317
Epoch [67/500], Train Loss: 0.0644, Validation Loss: 0.0641, Generator Loss: 12.7784, Discriminator Loss: 0.3131
Training epoch 68 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.06306426227092743
Training batch 2 / 32
Total batch reconstruction loss: 0.06688235700130463
Training batch 3 / 32
Total batch reconstruction loss: 0.06036144495010376
Training batch 4 / 32
Total batch reconstruction loss: 0.062105972319841385
Training batch 5 / 32
Total batch reconstruction loss: 0.06194004788994789
Training batch 6 / 32
Total batch reconstruction loss: 0.06365186721086502
Training batch 7 / 32
Total batch reconstruction loss: 0.0688944160938263
Training batch 8 / 32
Total batch reconstruction loss: 0.06330269575119019
Training batch 9 / 32
Total batch reconstruction loss: 0.06806467473506927
Training batch 10 / 32
Total batch reconstruction loss: 0.06527045369148254
Training batch 11 / 32
Total batch reconstruction loss: 0.06475423276424408
Training batch 12 / 32
Total batch reconstruction loss: 0.06247175112366676
Training batch 13 / 32
Total batch reconstruction loss: 0.06849277019500732
Training batch 14 / 32
Total batch reconstruction loss: 0.06336256116628647
Training batch 15 / 32
Total batch reconstruction loss: 0.06514565646648407
Training batch 16 / 32
Total batch reconstruction loss: 0.06028154492378235
Training batch 17 / 32
Total batch reconstruction loss: 0.05953017622232437
Training batch 18 / 32
Total batch reconstruction loss: 0.06204773113131523
Training batch 19 / 32
Total batch reconstruction loss: 0.0625610500574112
Training batch 20 / 32
Total batch reconstruction loss: 0.05710558965802193
Training batch 21 / 32
Total batch reconstruction loss: 0.06909611076116562
Training batch 22 / 32
Total batch reconstruction loss: 0.06206795200705528
Training batch 23 / 32
Total batch reconstruction loss: 0.06777610629796982
Training batch 24 / 32
Total batch reconstruction loss: 0.0641724020242691
Training batch 25 / 32
Total batch reconstruction loss: 0.06466282904148102
Training batch 26 / 32
Total batch reconstruction loss: 0.06590579450130463
Training batch 27 / 32
Total batch reconstruction loss: 0.06519600003957748
Training batch 28 / 32
Total batch reconstruction loss: 0.06523535400629044
Training batch 29 / 32
Total batch reconstruction loss: 0.0663086473941803
Training batch 30 / 32
Total batch reconstruction loss: 0.05748282000422478
Training batch 31 / 32
Total batch reconstruction loss: 0.06656357645988464
Training batch 32 / 32
Total batch reconstruction loss: 0.07717470079660416
Epoch [68/500], Train Loss: 0.0661, Validation Loss: 0.0678, Generator Loss: 12.9631, Discriminator Loss: 0.3001
Training epoch 69 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.0678318440914154
Training batch 2 / 32
Total batch reconstruction loss: 0.065786212682724
Training batch 3 / 32
Total batch reconstruction loss: 0.06439238041639328
Training batch 4 / 32
Total batch reconstruction loss: 0.06572926044464111
Training batch 5 / 32
Total batch reconstruction loss: 0.0606146976351738
Training batch 6 / 32
Total batch reconstruction loss: 0.05773920565843582
Training batch 7 / 32
Total batch reconstruction loss: 0.06298660486936569
Training batch 8 / 32
Total batch reconstruction loss: 0.07033181935548782
Training batch 9 / 32
Total batch reconstruction loss: 0.06216062232851982
Training batch 10 / 32
Total batch reconstruction loss: 0.07021655887365341
Training batch 11 / 32
Total batch reconstruction loss: 0.058743856847286224
Training batch 12 / 32
Total batch reconstruction loss: 0.06414531171321869
Training batch 13 / 32
Total batch reconstruction loss: 0.06117771193385124
Training batch 14 / 32
Total batch reconstruction loss: 0.06568649411201477
Training batch 15 / 32
Total batch reconstruction loss: 0.06537199765443802
Training batch 16 / 32
Total batch reconstruction loss: 0.06102649122476578
Training batch 17 / 32
Total batch reconstruction loss: 0.06336171925067902
Training batch 18 / 32
Total batch reconstruction loss: 0.06421839445829391
Training batch 19 / 32
Total batch reconstruction loss: 0.06079164892435074
Training batch 20 / 32
Total batch reconstruction loss: 0.06621406972408295
Training batch 21 / 32
Total batch reconstruction loss: 0.06983790546655655
Training batch 22 / 32
Total batch reconstruction loss: 0.06723878532648087
Training batch 23 / 32
Total batch reconstruction loss: 0.06005444750189781
Training batch 24 / 32
Total batch reconstruction loss: 0.06388553977012634
Training batch 25 / 32
Total batch reconstruction loss: 0.06637730449438095
Training batch 26 / 32
Total batch reconstruction loss: 0.06543704122304916
Training batch 27 / 32
Total batch reconstruction loss: 0.06753133237361908
Training batch 28 / 32
Total batch reconstruction loss: 0.06498043984174728
Training batch 29 / 32
Total batch reconstruction loss: 0.06349903345108032
Training batch 30 / 32
Total batch reconstruction loss: 0.06280625611543655
Training batch 31 / 32
Total batch reconstruction loss: 0.0632776990532875
Training batch 32 / 32
Total batch reconstruction loss: 0.06377952545881271
Epoch [69/500], Train Loss: 0.0662, Validation Loss: 0.0687, Generator Loss: 12.9352, Discriminator Loss: 0.3072
Training epoch 70 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.06102174520492554
Training batch 2 / 32
Total batch reconstruction loss: 0.06276164948940277
Training batch 3 / 32
Total batch reconstruction loss: 0.06398413330316544
Training batch 4 / 32
Total batch reconstruction loss: 0.06529857218265533
Training batch 5 / 32
Total batch reconstruction loss: 0.06211152672767639
Training batch 6 / 32
Total batch reconstruction loss: 0.05993729084730148
Training batch 7 / 32
Total batch reconstruction loss: 0.06128954887390137
Training batch 8 / 32
Total batch reconstruction loss: 0.06414783000946045
Training batch 9 / 32
Total batch reconstruction loss: 0.058903783559799194
Training batch 10 / 32
Total batch reconstruction loss: 0.0649232417345047
Training batch 11 / 32
Total batch reconstruction loss: 0.06150611490011215
Training batch 12 / 32
Total batch reconstruction loss: 0.06710085272789001
Training batch 13 / 32
Total batch reconstruction loss: 0.06289449334144592
Training batch 14 / 32
Total batch reconstruction loss: 0.06914734840393066
Training batch 15 / 32
Total batch reconstruction loss: 0.06273316591978073
Training batch 16 / 32
Total batch reconstruction loss: 0.062305666506290436
Training batch 17 / 32
Total batch reconstruction loss: 0.05899990350008011
Training batch 18 / 32
Total batch reconstruction loss: 0.06761011481285095
Training batch 19 / 32
Total batch reconstruction loss: 0.06336855888366699
Training batch 20 / 32
Total batch reconstruction loss: 0.06635330617427826
Training batch 21 / 32
Total batch reconstruction loss: 0.0656166821718216
Training batch 22 / 32
Total batch reconstruction loss: 0.06494134664535522
Training batch 23 / 32
Total batch reconstruction loss: 0.06744574010372162
Training batch 24 / 32
Total batch reconstruction loss: 0.06117016822099686
Training batch 25 / 32
Total batch reconstruction loss: 0.07844336330890656
Training batch 26 / 32
Total batch reconstruction loss: 0.06597909331321716
Training batch 27 / 32
Total batch reconstruction loss: 0.06386621296405792
Training batch 28 / 32
Total batch reconstruction loss: 0.07480096817016602
Training batch 29 / 32
Total batch reconstruction loss: 0.06957864761352539
Training batch 30 / 32
Total batch reconstruction loss: 0.06633120775222778
Training batch 31 / 32
Total batch reconstruction loss: 0.06833679974079132
Training batch 32 / 32
Total batch reconstruction loss: 0.08731214702129364
Epoch [70/500], Train Loss: 0.0670, Validation Loss: 0.0649, Generator Loss: 13.1997, Discriminator Loss: 0.3118
Training epoch 71 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.06690515577793121
Training batch 2 / 32
Total batch reconstruction loss: 0.07979968190193176
Training batch 3 / 32
Total batch reconstruction loss: 0.06954848766326904
Training batch 4 / 32
Total batch reconstruction loss: 0.06886507570743561
Training batch 5 / 32
Total batch reconstruction loss: 0.0648590698838234
Training batch 6 / 32
Total batch reconstruction loss: 0.07287947088479996
Training batch 7 / 32
Total batch reconstruction loss: 0.06859604269266129
Training batch 8 / 32
Total batch reconstruction loss: 0.06420673429965973
Training batch 9 / 32
Total batch reconstruction loss: 0.06694255024194717
Training batch 10 / 32
Total batch reconstruction loss: 0.06817612051963806
Training batch 11 / 32
Total batch reconstruction loss: 0.06768092513084412
Training batch 12 / 32
Total batch reconstruction loss: 0.07131008803844452
Training batch 13 / 32
Total batch reconstruction loss: 0.06435808539390564
Training batch 14 / 32
Total batch reconstruction loss: 0.06338777393102646
Training batch 15 / 32
Total batch reconstruction loss: 0.0660036951303482
Training batch 16 / 32
Total batch reconstruction loss: 0.06950153410434723
Training batch 17 / 32
Total batch reconstruction loss: 0.06476549804210663
Training batch 18 / 32
Total batch reconstruction loss: 0.0628218799829483
Training batch 19 / 32
Total batch reconstruction loss: 0.06316762417554855
Training batch 20 / 32
Total batch reconstruction loss: 0.0642469972372055
Training batch 21 / 32
Total batch reconstruction loss: 0.06462530791759491
Training batch 22 / 32
Total batch reconstruction loss: 0.06717909872531891
Training batch 23 / 32
Total batch reconstruction loss: 0.06099805235862732
Training batch 24 / 32
Total batch reconstruction loss: 0.06531580537557602
Training batch 25 / 32
Total batch reconstruction loss: 0.06283432245254517
Training batch 26 / 32
Total batch reconstruction loss: 0.06531979888677597
Training batch 27 / 32
Total batch reconstruction loss: 0.0603463314473629
Training batch 28 / 32
Total batch reconstruction loss: 0.06698904186487198
Training batch 29 / 32
Total batch reconstruction loss: 0.06400568783283234
Training batch 30 / 32
Total batch reconstruction loss: 0.06528189033269882
Training batch 31 / 32
Total batch reconstruction loss: 0.06473153829574585
Training batch 32 / 32
Total batch reconstruction loss: 0.05813668668270111
Epoch [71/500], Train Loss: 0.0662, Validation Loss: 0.0661, Generator Loss: 13.2903, Discriminator Loss: 0.3027
Training epoch 72 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.06405726820230484
Training batch 2 / 32
Total batch reconstruction loss: 0.06425414234399796
Training batch 3 / 32
Total batch reconstruction loss: 0.06828272342681885
Training batch 4 / 32
Total batch reconstruction loss: 0.06380876898765564
Training batch 5 / 32
Total batch reconstruction loss: 0.06331776082515717
Training batch 6 / 32
Total batch reconstruction loss: 0.06391444802284241
Training batch 7 / 32
Total batch reconstruction loss: 0.062109723687171936
Training batch 8 / 32
Total batch reconstruction loss: 0.06703829765319824
Training batch 9 / 32
Total batch reconstruction loss: 0.062434833496809006
Training batch 10 / 32
Total batch reconstruction loss: 0.06290282309055328
Training batch 11 / 32
Total batch reconstruction loss: 0.06869354844093323
Training batch 12 / 32
Total batch reconstruction loss: 0.06472451984882355
Training batch 13 / 32
Total batch reconstruction loss: 0.06468172371387482
Training batch 14 / 32
Total batch reconstruction loss: 0.062121689319610596
Training batch 15 / 32
Total batch reconstruction loss: 0.062320031225681305
Training batch 16 / 32
Total batch reconstruction loss: 0.0624692440032959
Training batch 17 / 32
Total batch reconstruction loss: 0.05985994637012482
Training batch 18 / 32
Total batch reconstruction loss: 0.06248033791780472
Training batch 19 / 32
Total batch reconstruction loss: 0.0634976252913475
Training batch 20 / 32
Total batch reconstruction loss: 0.05998823046684265
Training batch 21 / 32
Total batch reconstruction loss: 0.06591405719518661
Training batch 22 / 32
Total batch reconstruction loss: 0.06437688320875168
Training batch 23 / 32
Total batch reconstruction loss: 0.06695249676704407
Training batch 24 / 32
Total batch reconstruction loss: 0.06551600992679596
Training batch 25 / 32
Total batch reconstruction loss: 0.0647411048412323
Training batch 26 / 32
Total batch reconstruction loss: 0.05988253653049469
Training batch 27 / 32
Total batch reconstruction loss: 0.06736934185028076
Training batch 28 / 32
Total batch reconstruction loss: 0.06697691977024078
Training batch 29 / 32
Total batch reconstruction loss: 0.058803901076316833
Training batch 30 / 32
Total batch reconstruction loss: 0.06883949041366577
Training batch 31 / 32
Total batch reconstruction loss: 0.06563448905944824
Training batch 32 / 32
Total batch reconstruction loss: 0.07500650733709335
Epoch [72/500], Train Loss: 0.0657, Validation Loss: 0.0670, Generator Loss: 12.9781, Discriminator Loss: 0.3007
Training epoch 73 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.06847336888313293
Training batch 2 / 32
Total batch reconstruction loss: 0.07238463312387466
Training batch 3 / 32
Total batch reconstruction loss: 0.07197743654251099
Training batch 4 / 32
Total batch reconstruction loss: 0.06403559446334839
Training batch 5 / 32
Total batch reconstruction loss: 0.07001729309558868
Training batch 6 / 32
Total batch reconstruction loss: 0.0602724552154541
Training batch 7 / 32
Total batch reconstruction loss: 0.07099142670631409
Training batch 8 / 32
Total batch reconstruction loss: 0.06962132453918457
Training batch 9 / 32
Total batch reconstruction loss: 0.0652284324169159
Training batch 10 / 32
Total batch reconstruction loss: 0.06115438789129257
Training batch 11 / 32
Total batch reconstruction loss: 0.07013216614723206
Training batch 12 / 32
Total batch reconstruction loss: 0.06361144036054611
Training batch 13 / 32
Total batch reconstruction loss: 0.06608779728412628
Training batch 14 / 32
Total batch reconstruction loss: 0.06554063409566879
Training batch 15 / 32
Total batch reconstruction loss: 0.06120909005403519
Training batch 16 / 32
Total batch reconstruction loss: 0.059770725667476654
Training batch 17 / 32
Total batch reconstruction loss: 0.06275078654289246
Training batch 18 / 32
Total batch reconstruction loss: 0.06505994498729706
Training batch 19 / 32
Total batch reconstruction loss: 0.0639265924692154
Training batch 20 / 32
Total batch reconstruction loss: 0.06485852599143982
Training batch 21 / 32
Total batch reconstruction loss: 0.061715465039014816
Training batch 22 / 32
Total batch reconstruction loss: 0.06107625365257263
Training batch 23 / 32
Total batch reconstruction loss: 0.06720542907714844
Training batch 24 / 32
Total batch reconstruction loss: 0.059967607259750366
Training batch 25 / 32
Total batch reconstruction loss: 0.06210029125213623
Training batch 26 / 32
Total batch reconstruction loss: 0.0625273734331131
Training batch 27 / 32
Total batch reconstruction loss: 0.05936910957098007
Training batch 28 / 32
Total batch reconstruction loss: 0.06254333257675171
Training batch 29 / 32
Total batch reconstruction loss: 0.0668158233165741
Training batch 30 / 32
Total batch reconstruction loss: 0.05950973182916641
Training batch 31 / 32
Total batch reconstruction loss: 0.06108885630965233
Training batch 32 / 32
Total batch reconstruction loss: 0.06315053999423981
Epoch [73/500], Train Loss: 0.0649, Validation Loss: 0.0649, Generator Loss: 12.9609, Discriminator Loss: 0.3325
Training epoch 74 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.060860686004161835
Training batch 2 / 32
Total batch reconstruction loss: 0.06702719628810883
Training batch 3 / 32
Total batch reconstruction loss: 0.06227097287774086
Training batch 4 / 32
Total batch reconstruction loss: 0.061857908964157104
Training batch 5 / 32
Total batch reconstruction loss: 0.06749409437179565
Training batch 6 / 32
Total batch reconstruction loss: 0.06396742165088654
Training batch 7 / 32
Total batch reconstruction loss: 0.0633130818605423
Training batch 8 / 32
Total batch reconstruction loss: 0.07390309870243073
Training batch 9 / 32
Total batch reconstruction loss: 0.06536675989627838
Training batch 10 / 32
Total batch reconstruction loss: 0.07017357647418976
Training batch 11 / 32
Total batch reconstruction loss: 0.06264069676399231
Training batch 12 / 32
Total batch reconstruction loss: 0.06850327551364899
Training batch 13 / 32
Total batch reconstruction loss: 0.06287738680839539
Training batch 14 / 32
Total batch reconstruction loss: 0.06487157195806503
Training batch 15 / 32
Total batch reconstruction loss: 0.06558030098676682
Training batch 16 / 32
Total batch reconstruction loss: 0.0619698241353035
Training batch 17 / 32
Total batch reconstruction loss: 0.05765067785978317
Training batch 18 / 32
Total batch reconstruction loss: 0.062330350279808044
Training batch 19 / 32
Total batch reconstruction loss: 0.06614494323730469
Training batch 20 / 32
Total batch reconstruction loss: 0.06098783016204834
Training batch 21 / 32
Total batch reconstruction loss: 0.06448066979646683
Training batch 22 / 32
Total batch reconstruction loss: 0.06495213508605957
Training batch 23 / 32
Total batch reconstruction loss: 0.06271544098854065
Training batch 24 / 32
Total batch reconstruction loss: 0.06032294034957886
Training batch 25 / 32
Total batch reconstruction loss: 0.06479339301586151
Training batch 26 / 32
Total batch reconstruction loss: 0.06378574669361115
Training batch 27 / 32
Total batch reconstruction loss: 0.06224910169839859
Training batch 28 / 32
Total batch reconstruction loss: 0.06226830929517746
Training batch 29 / 32
Total batch reconstruction loss: 0.06506457924842834
Training batch 30 / 32
Total batch reconstruction loss: 0.06378614157438278
Training batch 31 / 32
Total batch reconstruction loss: 0.06302237510681152
Training batch 32 / 32
Total batch reconstruction loss: 0.05563642829656601
Epoch [74/500], Train Loss: 0.0651, Validation Loss: 0.0650, Generator Loss: 12.8461, Discriminator Loss: 0.3140
Training epoch 75 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.06419220566749573
Training batch 2 / 32
Total batch reconstruction loss: 0.06065259501338005
Training batch 3 / 32
Total batch reconstruction loss: 0.06450989842414856
Training batch 4 / 32
Total batch reconstruction loss: 0.06548722088336945
Training batch 5 / 32
Total batch reconstruction loss: 0.06075011193752289
Training batch 6 / 32
Total batch reconstruction loss: 0.06269018352031708
Training batch 7 / 32
Total batch reconstruction loss: 0.0656571239233017
Training batch 8 / 32
Total batch reconstruction loss: 0.06325830519199371
Training batch 9 / 32
Total batch reconstruction loss: 0.060582369565963745
Training batch 10 / 32
Total batch reconstruction loss: 0.06674538552761078
Training batch 11 / 32
Total batch reconstruction loss: 0.061029575765132904
Training batch 12 / 32
Total batch reconstruction loss: 0.062096066772937775
Training batch 13 / 32
Total batch reconstruction loss: 0.06339523196220398
Training batch 14 / 32
Total batch reconstruction loss: 0.06394147872924805
Training batch 15 / 32
Total batch reconstruction loss: 0.06649714708328247
Training batch 16 / 32
Total batch reconstruction loss: 0.06651241332292557
Training batch 17 / 32
Total batch reconstruction loss: 0.06078026443719864
Training batch 18 / 32
Total batch reconstruction loss: 0.07099109888076782
Training batch 19 / 32
Total batch reconstruction loss: 0.06134252995252609
Training batch 20 / 32
Total batch reconstruction loss: 0.05901443213224411
Training batch 21 / 32
Total batch reconstruction loss: 0.06520456075668335
Training batch 22 / 32
Total batch reconstruction loss: 0.06622011959552765
Training batch 23 / 32
Total batch reconstruction loss: 0.06419536471366882
Training batch 24 / 32
Total batch reconstruction loss: 0.06556381285190582
Training batch 25 / 32
Total batch reconstruction loss: 0.06505212187767029
Training batch 26 / 32
Total batch reconstruction loss: 0.06097177416086197
Training batch 27 / 32
Total batch reconstruction loss: 0.06178755685687065
Training batch 28 / 32
Total batch reconstruction loss: 0.06252734363079071
Training batch 29 / 32
Total batch reconstruction loss: 0.0610148087143898
Training batch 30 / 32
Total batch reconstruction loss: 0.062240585684776306
Training batch 31 / 32
Total batch reconstruction loss: 0.0643242597579956
Training batch 32 / 32
Total batch reconstruction loss: 0.05632206052541733
Epoch [75/500], Train Loss: 0.0645, Validation Loss: 0.0645, Generator Loss: 12.7169, Discriminator Loss: 0.3442
Training epoch 76 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.06041373312473297
Training batch 2 / 32
Total batch reconstruction loss: 0.06015349179506302
Training batch 3 / 32
Total batch reconstruction loss: 0.06609630584716797
Training batch 4 / 32
Total batch reconstruction loss: 0.06722589582204819
Training batch 5 / 32
Total batch reconstruction loss: 0.060805708169937134
Training batch 6 / 32
Total batch reconstruction loss: 0.06756681203842163
Training batch 7 / 32
Total batch reconstruction loss: 0.06284359842538834
Training batch 8 / 32
Total batch reconstruction loss: 0.06575006246566772
Training batch 9 / 32
Total batch reconstruction loss: 0.06537048518657684
Training batch 10 / 32
Total batch reconstruction loss: 0.0635528564453125
Training batch 11 / 32
Total batch reconstruction loss: 0.0641327053308487
Training batch 12 / 32
Total batch reconstruction loss: 0.0652599185705185
Training batch 13 / 32
Total batch reconstruction loss: 0.06610672175884247
Training batch 14 / 32
Total batch reconstruction loss: 0.0644533634185791
Training batch 15 / 32
Total batch reconstruction loss: 0.06282294541597366
Training batch 16 / 32
Total batch reconstruction loss: 0.06188139319419861
Training batch 17 / 32
Total batch reconstruction loss: 0.0689077377319336
Training batch 18 / 32
Total batch reconstruction loss: 0.061066072434186935
Training batch 19 / 32
Total batch reconstruction loss: 0.06518103927373886
Training batch 20 / 32
Total batch reconstruction loss: 0.07166659086942673
Training batch 21 / 32
Total batch reconstruction loss: 0.06127307564020157
Training batch 22 / 32
Total batch reconstruction loss: 0.06600497663021088
Training batch 23 / 32
Total batch reconstruction loss: 0.0623941533267498
Training batch 24 / 32
Total batch reconstruction loss: 0.06358508765697479
Training batch 25 / 32
Total batch reconstruction loss: 0.06278593838214874
Training batch 26 / 32
Total batch reconstruction loss: 0.06846906244754791
Training batch 27 / 32
Total batch reconstruction loss: 0.07042792439460754
Training batch 28 / 32
Total batch reconstruction loss: 0.0589597150683403
Training batch 29 / 32
Total batch reconstruction loss: 0.05784299969673157
Training batch 30 / 32
Total batch reconstruction loss: 0.06082365661859512
Training batch 31 / 32
Total batch reconstruction loss: 0.06086418777704239
Training batch 32 / 32
Total batch reconstruction loss: 0.05274081602692604
Epoch [76/500], Train Loss: 0.0650, Validation Loss: 0.0667, Generator Loss: 12.8100, Discriminator Loss: 0.3041
Training epoch 77 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.07061377912759781
Training batch 2 / 32
Total batch reconstruction loss: 0.06091119349002838
Training batch 3 / 32
Total batch reconstruction loss: 0.06801260262727737
Training batch 4 / 32
Total batch reconstruction loss: 0.06489751487970352
Training batch 5 / 32
Total batch reconstruction loss: 0.061457276344299316
Training batch 6 / 32
Total batch reconstruction loss: 0.06634129583835602
Training batch 7 / 32
Total batch reconstruction loss: 0.0672173798084259
Training batch 8 / 32
Total batch reconstruction loss: 0.06626726686954498
Training batch 9 / 32
Total batch reconstruction loss: 0.06604698300361633
Training batch 10 / 32
Total batch reconstruction loss: 0.06449447572231293
Training batch 11 / 32
Total batch reconstruction loss: 0.06607784330844879
Training batch 12 / 32
Total batch reconstruction loss: 0.06518033146858215
Training batch 13 / 32
Total batch reconstruction loss: 0.06228301674127579
Training batch 14 / 32
Total batch reconstruction loss: 0.0646338164806366
Training batch 15 / 32
Total batch reconstruction loss: 0.06174492463469505
Training batch 16 / 32
Total batch reconstruction loss: 0.06237874925136566
Training batch 17 / 32
Total batch reconstruction loss: 0.06234826147556305
Training batch 18 / 32
Total batch reconstruction loss: 0.0667191818356514
Training batch 19 / 32
Total batch reconstruction loss: 0.0603262223303318
Training batch 20 / 32
Total batch reconstruction loss: 0.05975857749581337
Training batch 21 / 32
Total batch reconstruction loss: 0.06551001965999603
Training batch 22 / 32
Total batch reconstruction loss: 0.06333144009113312
Training batch 23 / 32
Total batch reconstruction loss: 0.06430947035551071
Training batch 24 / 32
Total batch reconstruction loss: 0.06293035298585892
Training batch 25 / 32
Total batch reconstruction loss: 0.06519100815057755
Training batch 26 / 32
Total batch reconstruction loss: 0.06594496965408325
Training batch 27 / 32
Total batch reconstruction loss: 0.06268316507339478
Training batch 28 / 32
Total batch reconstruction loss: 0.062128253281116486
Training batch 29 / 32
Total batch reconstruction loss: 0.07277191430330276
Training batch 30 / 32
Total batch reconstruction loss: 0.06375076621770859
Training batch 31 / 32
Total batch reconstruction loss: 0.059742070734500885
Training batch 32 / 32
Total batch reconstruction loss: 0.053590789437294006
Epoch [77/500], Train Loss: 0.0651, Validation Loss: 0.0642, Generator Loss: 12.8758, Discriminator Loss: 0.3261
Training epoch 78 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.06792634725570679
Training batch 2 / 32
Total batch reconstruction loss: 0.06365831941366196
Training batch 3 / 32
Total batch reconstruction loss: 0.06141909956932068
Training batch 4 / 32
Total batch reconstruction loss: 0.06066753342747688
Training batch 5 / 32
Total batch reconstruction loss: 0.06353580951690674
Training batch 6 / 32
Total batch reconstruction loss: 0.06492643803358078
Training batch 7 / 32
Total batch reconstruction loss: 0.06094956398010254
Training batch 8 / 32
Total batch reconstruction loss: 0.05756668001413345
Training batch 9 / 32
Total batch reconstruction loss: 0.059108391404151917
Training batch 10 / 32
Total batch reconstruction loss: 0.06472066044807434
Training batch 11 / 32
Total batch reconstruction loss: 0.0612792894244194
Training batch 12 / 32
Total batch reconstruction loss: 0.06446416676044464
Training batch 13 / 32
Total batch reconstruction loss: 0.06369241327047348
Training batch 14 / 32
Total batch reconstruction loss: 0.06619425117969513
Training batch 15 / 32
Total batch reconstruction loss: 0.06634019315242767
Training batch 16 / 32
Total batch reconstruction loss: 0.06548064202070236
Training batch 17 / 32
Total batch reconstruction loss: 0.05929727852344513
Training batch 18 / 32
Total batch reconstruction loss: 0.061961643397808075
Training batch 19 / 32
Total batch reconstruction loss: 0.07022372633218765
Training batch 20 / 32
Total batch reconstruction loss: 0.06931133568286896
Training batch 21 / 32
Total batch reconstruction loss: 0.06548115611076355
Training batch 22 / 32
Total batch reconstruction loss: 0.06090270355343819
Training batch 23 / 32
Total batch reconstruction loss: 0.06779919564723969
Training batch 24 / 32
Total batch reconstruction loss: 0.06551367044448853
Training batch 25 / 32
Total batch reconstruction loss: 0.06625965237617493
Training batch 26 / 32
Total batch reconstruction loss: 0.06610114127397537
Training batch 27 / 32
Total batch reconstruction loss: 0.06347517669200897
Training batch 28 / 32
Total batch reconstruction loss: 0.06201540678739548
Training batch 29 / 32
Total batch reconstruction loss: 0.06216221675276756
Training batch 30 / 32
Total batch reconstruction loss: 0.06312128901481628
Training batch 31 / 32
Total batch reconstruction loss: 0.06462522596120834
Training batch 32 / 32
Total batch reconstruction loss: 0.057177118957042694
Epoch [78/500], Train Loss: 0.0648, Validation Loss: 0.0637, Generator Loss: 12.8155, Discriminator Loss: 0.3084
Training epoch 79 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.0663689449429512
Training batch 2 / 32
Total batch reconstruction loss: 0.06634284555912018
Training batch 3 / 32
Total batch reconstruction loss: 0.06408768892288208
Training batch 4 / 32
Total batch reconstruction loss: 0.060382187366485596
Training batch 5 / 32
Total batch reconstruction loss: 0.06552021950483322
Training batch 6 / 32
Total batch reconstruction loss: 0.0627688318490982
Training batch 7 / 32
Total batch reconstruction loss: 0.06531794369220734
Training batch 8 / 32
Total batch reconstruction loss: 0.06315891444683075
Training batch 9 / 32
Total batch reconstruction loss: 0.05950043722987175
Training batch 10 / 32
Total batch reconstruction loss: 0.06584890931844711
Training batch 11 / 32
Total batch reconstruction loss: 0.06440744549036026
Training batch 12 / 32
Total batch reconstruction loss: 0.06456916034221649
Training batch 13 / 32
Total batch reconstruction loss: 0.062029026448726654
Training batch 14 / 32
Total batch reconstruction loss: 0.0666084736585617
Training batch 15 / 32
Total batch reconstruction loss: 0.06220546364784241
Training batch 16 / 32
Total batch reconstruction loss: 0.06317746639251709
Training batch 17 / 32
Total batch reconstruction loss: 0.06500113010406494
Training batch 18 / 32
Total batch reconstruction loss: 0.058790188282728195
Training batch 19 / 32
Total batch reconstruction loss: 0.06342770904302597
Training batch 20 / 32
Total batch reconstruction loss: 0.06360383331775665
Training batch 21 / 32
Total batch reconstruction loss: 0.06396278738975525
Training batch 22 / 32
Total batch reconstruction loss: 0.060149725526571274
Training batch 23 / 32
Total batch reconstruction loss: 0.07005802541971207
Training batch 24 / 32
Total batch reconstruction loss: 0.059415608644485474
Training batch 25 / 32
Total batch reconstruction loss: 0.06633524596691132
Training batch 26 / 32
Total batch reconstruction loss: 0.06384994834661484
Training batch 27 / 32
Total batch reconstruction loss: 0.06424406170845032
Training batch 28 / 32
Total batch reconstruction loss: 0.06582221388816833
Training batch 29 / 32
Total batch reconstruction loss: 0.062584288418293
Training batch 30 / 32
Total batch reconstruction loss: 0.06444741785526276
Training batch 31 / 32
Total batch reconstruction loss: 0.06359048932790756
Training batch 32 / 32
Total batch reconstruction loss: 0.05911003798246384
Epoch [79/500], Train Loss: 0.0648, Validation Loss: 0.0640, Generator Loss: 12.7904, Discriminator Loss: 0.3311
Training epoch 80 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.06590691953897476
Training batch 2 / 32
Total batch reconstruction loss: 0.06104394048452377
Training batch 3 / 32
Total batch reconstruction loss: 0.059185177087783813
Training batch 4 / 32
Total batch reconstruction loss: 0.0620298907160759
Training batch 5 / 32
Total batch reconstruction loss: 0.06262248754501343
Training batch 6 / 32
Total batch reconstruction loss: 0.06444046646356583
Training batch 7 / 32
Total batch reconstruction loss: 0.06460074335336685
Training batch 8 / 32
Total batch reconstruction loss: 0.06888623535633087
Training batch 9 / 32
Total batch reconstruction loss: 0.06057063490152359
Training batch 10 / 32
Total batch reconstruction loss: 0.06716649234294891
Training batch 11 / 32
Total batch reconstruction loss: 0.06503884494304657
Training batch 12 / 32
Total batch reconstruction loss: 0.06351997703313828
Training batch 13 / 32
Total batch reconstruction loss: 0.05940713360905647
Training batch 14 / 32
Total batch reconstruction loss: 0.06368286907672882
Training batch 15 / 32
Total batch reconstruction loss: 0.06284572929143906
Training batch 16 / 32
Total batch reconstruction loss: 0.060036443173885345
Training batch 17 / 32
Total batch reconstruction loss: 0.06325630843639374
Training batch 18 / 32
Total batch reconstruction loss: 0.06424926221370697
Training batch 19 / 32
Total batch reconstruction loss: 0.05903487280011177
Training batch 20 / 32
Total batch reconstruction loss: 0.06306721270084381
Training batch 21 / 32
Total batch reconstruction loss: 0.05520665645599365
Training batch 22 / 32
Total batch reconstruction loss: 0.05912450700998306
Training batch 23 / 32
Total batch reconstruction loss: 0.06686177104711533
Training batch 24 / 32
Total batch reconstruction loss: 0.06316551566123962
Training batch 25 / 32
Total batch reconstruction loss: 0.06419410556554794
Training batch 26 / 32
Total batch reconstruction loss: 0.06499570608139038
Training batch 27 / 32
Total batch reconstruction loss: 0.06827366352081299
Training batch 28 / 32
Total batch reconstruction loss: 0.061437033116817474
Training batch 29 / 32
Total batch reconstruction loss: 0.06308955699205399
Training batch 30 / 32
Total batch reconstruction loss: 0.06576215475797653
Training batch 31 / 32
Total batch reconstruction loss: 0.05984894186258316
Training batch 32 / 32
Total batch reconstruction loss: 0.06288619339466095
Epoch [80/500], Train Loss: 0.0639, Validation Loss: 0.0637, Generator Loss: 12.6656, Discriminator Loss: 0.3347
Training epoch 81 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.06273844838142395
Training batch 2 / 32
Total batch reconstruction loss: 0.06549176573753357
Training batch 3 / 32
Total batch reconstruction loss: 0.06415368616580963
Training batch 4 / 32
Total batch reconstruction loss: 0.06328897923231125
Training batch 5 / 32
Total batch reconstruction loss: 0.06097441911697388
Training batch 6 / 32
Total batch reconstruction loss: 0.06286296993494034
Training batch 7 / 32
Total batch reconstruction loss: 0.06420571357011795
Training batch 8 / 32
Total batch reconstruction loss: 0.06294485181570053
Training batch 9 / 32
Total batch reconstruction loss: 0.06187238171696663
Training batch 10 / 32
Total batch reconstruction loss: 0.06423275172710419
Training batch 11 / 32
Total batch reconstruction loss: 0.0632377341389656
Training batch 12 / 32
Total batch reconstruction loss: 0.06104917824268341
Training batch 13 / 32
Total batch reconstruction loss: 0.06743286550045013
Training batch 14 / 32
Total batch reconstruction loss: 0.05917590111494064
Training batch 15 / 32
Total batch reconstruction loss: 0.06005719304084778
Training batch 16 / 32
Total batch reconstruction loss: 0.06486816704273224
Training batch 17 / 32
Total batch reconstruction loss: 0.06738399714231491
Training batch 18 / 32
Total batch reconstruction loss: 0.06258551776409149
Training batch 19 / 32
Total batch reconstruction loss: 0.06307274103164673
Training batch 20 / 32
Total batch reconstruction loss: 0.06082697957754135
Training batch 21 / 32
Total batch reconstruction loss: 0.059899311512708664
Training batch 22 / 32
Total batch reconstruction loss: 0.06286763399839401
Training batch 23 / 32
Total batch reconstruction loss: 0.060995910316705704
Training batch 24 / 32
Total batch reconstruction loss: 0.06462094187736511
Training batch 25 / 32
Total batch reconstruction loss: 0.0628855749964714
Training batch 26 / 32
Total batch reconstruction loss: 0.06244582682847977
Training batch 27 / 32
Total batch reconstruction loss: 0.06662704050540924
Training batch 28 / 32
Total batch reconstruction loss: 0.06075470149517059
Training batch 29 / 32
Total batch reconstruction loss: 0.059677205979824066
Training batch 30 / 32
Total batch reconstruction loss: 0.06155204772949219
Training batch 31 / 32
Total batch reconstruction loss: 0.060844793915748596
Training batch 32 / 32
Total batch reconstruction loss: 0.1035011038184166
Epoch [81/500], Train Loss: 0.0643, Validation Loss: 0.0618, Generator Loss: 12.8722, Discriminator Loss: 0.3239
Training epoch 82 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.06587096303701401
Training batch 2 / 32
Total batch reconstruction loss: 0.06875737756490707
Training batch 3 / 32
Total batch reconstruction loss: 0.06310713291168213
Training batch 4 / 32
Total batch reconstruction loss: 0.060726575553417206
Training batch 5 / 32
Total batch reconstruction loss: 0.06222546100616455
Training batch 6 / 32
Total batch reconstruction loss: 0.06200201064348221
Training batch 7 / 32
Total batch reconstruction loss: 0.0644138902425766
Training batch 8 / 32
Total batch reconstruction loss: 0.06035751849412918
Training batch 9 / 32
Total batch reconstruction loss: 0.0633917823433876
Training batch 10 / 32
Total batch reconstruction loss: 0.06339714676141739
Training batch 11 / 32
Total batch reconstruction loss: 0.06008565425872803
Training batch 12 / 32
Total batch reconstruction loss: 0.058557070791721344
Training batch 13 / 32
Total batch reconstruction loss: 0.06173576042056084
Training batch 14 / 32
Total batch reconstruction loss: 0.07808782160282135
Training batch 15 / 32
Total batch reconstruction loss: 0.061577387154102325
Training batch 16 / 32
Total batch reconstruction loss: 0.0631546899676323
Training batch 17 / 32
Total batch reconstruction loss: 0.06355498731136322
Training batch 18 / 32
Total batch reconstruction loss: 0.06566442549228668
Training batch 19 / 32
Total batch reconstruction loss: 0.06166906654834747
Training batch 20 / 32
Total batch reconstruction loss: 0.06719763576984406
Training batch 21 / 32
Total batch reconstruction loss: 0.06617649644613266
Training batch 22 / 32
Total batch reconstruction loss: 0.06803874671459198
Training batch 23 / 32
Total batch reconstruction loss: 0.06453484296798706
Training batch 24 / 32
Total batch reconstruction loss: 0.06318631768226624
Training batch 25 / 32
Total batch reconstruction loss: 0.06494826078414917
Training batch 26 / 32
Total batch reconstruction loss: 0.062195561826229095
Training batch 27 / 32
Total batch reconstruction loss: 0.060086920857429504
Training batch 28 / 32
Total batch reconstruction loss: 0.06332047283649445
Training batch 29 / 32
Total batch reconstruction loss: 0.06092669069766998
Training batch 30 / 32
Total batch reconstruction loss: 0.05946736037731171
Training batch 31 / 32
Total batch reconstruction loss: 0.06426678597927094
Training batch 32 / 32
Total batch reconstruction loss: 0.05487658828496933
Epoch [82/500], Train Loss: 0.0648, Validation Loss: 0.0654, Generator Loss: 12.7416, Discriminator Loss: 0.3226
Training epoch 83 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.05733099207282066
Training batch 2 / 32
Total batch reconstruction loss: 0.061049193143844604
Training batch 3 / 32
Total batch reconstruction loss: 0.05856950581073761
Training batch 4 / 32
Total batch reconstruction loss: 0.060772933065891266
Training batch 5 / 32
Total batch reconstruction loss: 0.06024817004799843
Training batch 6 / 32
Total batch reconstruction loss: 0.06355717033147812
Training batch 7 / 32
Total batch reconstruction loss: 0.061778172850608826
Training batch 8 / 32
Total batch reconstruction loss: 0.0655197724699974
Training batch 9 / 32
Total batch reconstruction loss: 0.05955694243311882
Training batch 10 / 32
Total batch reconstruction loss: 0.062169671058654785
Training batch 11 / 32
Total batch reconstruction loss: 0.06262259930372238
Training batch 12 / 32
Total batch reconstruction loss: 0.06335043162107468
Training batch 13 / 32
Total batch reconstruction loss: 0.06026574224233627
Training batch 14 / 32
Total batch reconstruction loss: 0.06465467810630798
Training batch 15 / 32
Total batch reconstruction loss: 0.06428499519824982
Training batch 16 / 32
Total batch reconstruction loss: 0.06757897883653641
Training batch 17 / 32
Total batch reconstruction loss: 0.06211594492197037
Training batch 18 / 32
Total batch reconstruction loss: 0.06628081947565079
Training batch 19 / 32
Total batch reconstruction loss: 0.06334573775529861
Training batch 20 / 32
Total batch reconstruction loss: 0.06473137438297272
Training batch 21 / 32
Total batch reconstruction loss: 0.06342209875583649
Training batch 22 / 32
Total batch reconstruction loss: 0.06609932333230972
Training batch 23 / 32
Total batch reconstruction loss: 0.0683869943022728
Training batch 24 / 32
Total batch reconstruction loss: 0.060875795781612396
Training batch 25 / 32
Total batch reconstruction loss: 0.06750977784395218
Training batch 26 / 32
Total batch reconstruction loss: 0.06475609540939331
Training batch 27 / 32
Total batch reconstruction loss: 0.06761417537927628
Training batch 28 / 32
Total batch reconstruction loss: 0.05990121141076088
Training batch 29 / 32
Total batch reconstruction loss: 0.06411422044038773
Training batch 30 / 32
Total batch reconstruction loss: 0.06882426887750626
Training batch 31 / 32
Total batch reconstruction loss: 0.06375935673713684
Training batch 32 / 32
Total batch reconstruction loss: 0.06257274001836777
Epoch [83/500], Train Loss: 0.0640, Validation Loss: 0.0639, Generator Loss: 12.7465, Discriminator Loss: 0.3121
Training epoch 84 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.05829627439379692
Training batch 2 / 32
Total batch reconstruction loss: 0.061694562435150146
Training batch 3 / 32
Total batch reconstruction loss: 0.0672881007194519
Training batch 4 / 32
Total batch reconstruction loss: 0.0674811378121376
Training batch 5 / 32
Total batch reconstruction loss: 0.06521822512149811
Training batch 6 / 32
Total batch reconstruction loss: 0.06684421747922897
Training batch 7 / 32
Total batch reconstruction loss: 0.06568166613578796
Training batch 8 / 32
Total batch reconstruction loss: 0.06431817263364792
Training batch 9 / 32
Total batch reconstruction loss: 0.06180606782436371
Training batch 10 / 32
Total batch reconstruction loss: 0.06401064991950989
Training batch 11 / 32
Total batch reconstruction loss: 0.061888642609119415
Training batch 12 / 32
Total batch reconstruction loss: 0.06187132000923157
Training batch 13 / 32
Total batch reconstruction loss: 0.062317315489053726
Training batch 14 / 32
Total batch reconstruction loss: 0.06309004127979279
Training batch 15 / 32
Total batch reconstruction loss: 0.06329348683357239
Training batch 16 / 32
Total batch reconstruction loss: 0.06144196540117264
Training batch 17 / 32
Total batch reconstruction loss: 0.06123970448970795
Training batch 18 / 32
Total batch reconstruction loss: 0.06368517875671387
Training batch 19 / 32
Total batch reconstruction loss: 0.06884237378835678
Training batch 20 / 32
Total batch reconstruction loss: 0.057231344282627106
Training batch 21 / 32
Total batch reconstruction loss: 0.06126314401626587
Training batch 22 / 32
Total batch reconstruction loss: 0.059741437435150146
Training batch 23 / 32
Total batch reconstruction loss: 0.06233188509941101
Training batch 24 / 32
Total batch reconstruction loss: 0.056325480341911316
Training batch 25 / 32
Total batch reconstruction loss: 0.06401009112596512
Training batch 26 / 32
Total batch reconstruction loss: 0.060162484645843506
Training batch 27 / 32
Total batch reconstruction loss: 0.06814108043909073
Training batch 28 / 32
Total batch reconstruction loss: 0.06788013875484467
Training batch 29 / 32
Total batch reconstruction loss: 0.05920005589723587
Training batch 30 / 32
Total batch reconstruction loss: 0.06102002412080765
Training batch 31 / 32
Total batch reconstruction loss: 0.06108875572681427
Training batch 32 / 32
Total batch reconstruction loss: 0.0670924261212349
Epoch [84/500], Train Loss: 0.0635, Validation Loss: 0.0622, Generator Loss: 12.6841, Discriminator Loss: 0.3032
Training epoch 85 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.059856947511434555
Training batch 2 / 32
Total batch reconstruction loss: 0.0607001930475235
Training batch 3 / 32
Total batch reconstruction loss: 0.062493473291397095
Training batch 4 / 32
Total batch reconstruction loss: 0.07029105722904205
Training batch 5 / 32
Total batch reconstruction loss: 0.05794136971235275
Training batch 6 / 32
Total batch reconstruction loss: 0.06190379709005356
Training batch 7 / 32
Total batch reconstruction loss: 0.060765646398067474
Training batch 8 / 32
Total batch reconstruction loss: 0.05919767916202545
Training batch 9 / 32
Total batch reconstruction loss: 0.0619061179459095
Training batch 10 / 32
Total batch reconstruction loss: 0.06498464941978455
Training batch 11 / 32
Total batch reconstruction loss: 0.0708237737417221
Training batch 12 / 32
Total batch reconstruction loss: 0.05977052450180054
Training batch 13 / 32
Total batch reconstruction loss: 0.06551045179367065
Training batch 14 / 32
Total batch reconstruction loss: 0.06392335891723633
Training batch 15 / 32
Total batch reconstruction loss: 0.06530927866697311
Training batch 16 / 32
Total batch reconstruction loss: 0.0637362003326416
Training batch 17 / 32
Total batch reconstruction loss: 0.0625978410243988
Training batch 18 / 32
Total batch reconstruction loss: 0.06454599648714066
Training batch 19 / 32
Total batch reconstruction loss: 0.06598754227161407
Training batch 20 / 32
Total batch reconstruction loss: 0.06516803801059723
Training batch 21 / 32
Total batch reconstruction loss: 0.06433280557394028
Training batch 22 / 32
Total batch reconstruction loss: 0.0673416405916214
Training batch 23 / 32
Total batch reconstruction loss: 0.05968800187110901
Training batch 24 / 32
Total batch reconstruction loss: 0.06243552267551422
Training batch 25 / 32
Total batch reconstruction loss: 0.06650255620479584
Training batch 26 / 32
Total batch reconstruction loss: 0.06042487546801567
Training batch 27 / 32
Total batch reconstruction loss: 0.06947004795074463
Training batch 28 / 32
Total batch reconstruction loss: 0.061768051236867905
Training batch 29 / 32
Total batch reconstruction loss: 0.06509219110012054
Training batch 30 / 32
Total batch reconstruction loss: 0.0620984248816967
Training batch 31 / 32
Total batch reconstruction loss: 0.05940590798854828
Training batch 32 / 32
Total batch reconstruction loss: 0.0749986320734024
Epoch [85/500], Train Loss: 0.0646, Validation Loss: 0.0667, Generator Loss: 12.8276, Discriminator Loss: 0.3169
Training epoch 86 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.060681119561195374
Training batch 2 / 32
Total batch reconstruction loss: 0.06941892206668854
Training batch 3 / 32
Total batch reconstruction loss: 0.06784874200820923
Training batch 4 / 32
Total batch reconstruction loss: 0.0654163807630539
Training batch 5 / 32
Total batch reconstruction loss: 0.06128149479627609
Training batch 6 / 32
Total batch reconstruction loss: 0.061788804829120636
Training batch 7 / 32
Total batch reconstruction loss: 0.06871926784515381
Training batch 8 / 32
Total batch reconstruction loss: 0.06178166717290878
Training batch 9 / 32
Total batch reconstruction loss: 0.06915657967329025
Training batch 10 / 32
Total batch reconstruction loss: 0.057477883994579315
Training batch 11 / 32
Total batch reconstruction loss: 0.0639757364988327
Training batch 12 / 32
Total batch reconstruction loss: 0.06230988726019859
Training batch 13 / 32
Total batch reconstruction loss: 0.06546451151371002
Training batch 14 / 32
Total batch reconstruction loss: 0.06615433096885681
Training batch 15 / 32
Total batch reconstruction loss: 0.0629323422908783
Training batch 16 / 32
Total batch reconstruction loss: 0.06143782660365105
Training batch 17 / 32
Total batch reconstruction loss: 0.06044619530439377
Training batch 18 / 32
Total batch reconstruction loss: 0.06255506724119186
Training batch 19 / 32
Total batch reconstruction loss: 0.06302192807197571
Training batch 20 / 32
Total batch reconstruction loss: 0.059740662574768066
Training batch 21 / 32
Total batch reconstruction loss: 0.06169554218649864
Training batch 22 / 32
Total batch reconstruction loss: 0.06931957602500916
Training batch 23 / 32
Total batch reconstruction loss: 0.0600777193903923
Training batch 24 / 32
Total batch reconstruction loss: 0.06047985702753067
Training batch 25 / 32
Total batch reconstruction loss: 0.06387399882078171
Training batch 26 / 32
Total batch reconstruction loss: 0.06825390458106995
Training batch 27 / 32
Total batch reconstruction loss: 0.0635451227426529
Training batch 28 / 32
Total batch reconstruction loss: 0.058740317821502686
Training batch 29 / 32
Total batch reconstruction loss: 0.0641060546040535
Training batch 30 / 32
Total batch reconstruction loss: 0.06473097950220108
Training batch 31 / 32
Total batch reconstruction loss: 0.06401633471250534
Training batch 32 / 32
Total batch reconstruction loss: 0.07259132713079453
Epoch [86/500], Train Loss: 0.0640, Validation Loss: 0.0616, Generator Loss: 12.8345, Discriminator Loss: 0.3262
Training epoch 87 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.05948453024029732
Training batch 2 / 32
Total batch reconstruction loss: 0.06534034013748169
Training batch 3 / 32
Total batch reconstruction loss: 0.06008138880133629
Training batch 4 / 32
Total batch reconstruction loss: 0.06294281035661697
Training batch 5 / 32
Total batch reconstruction loss: 0.06604970991611481
Training batch 6 / 32
Total batch reconstruction loss: 0.06199350208044052
Training batch 7 / 32
Total batch reconstruction loss: 0.06376626342535019
Training batch 8 / 32
Total batch reconstruction loss: 0.062173981219530106
Training batch 9 / 32
Total batch reconstruction loss: 0.06547673046588898
Training batch 10 / 32
Total batch reconstruction loss: 0.06411059200763702
Training batch 11 / 32
Total batch reconstruction loss: 0.06960803270339966
Training batch 12 / 32
Total batch reconstruction loss: 0.06423633545637131
Training batch 13 / 32
Total batch reconstruction loss: 0.058494552969932556
Training batch 14 / 32
Total batch reconstruction loss: 0.06855067610740662
Training batch 15 / 32
Total batch reconstruction loss: 0.06322339177131653
Training batch 16 / 32
Total batch reconstruction loss: 0.06009889394044876
Training batch 17 / 32
Total batch reconstruction loss: 0.059521377086639404
Training batch 18 / 32
Total batch reconstruction loss: 0.06302368640899658
Training batch 19 / 32
Total batch reconstruction loss: 0.06140173226594925
Training batch 20 / 32
Total batch reconstruction loss: 0.060508135706186295
Training batch 21 / 32
Total batch reconstruction loss: 0.06469665467739105
Training batch 22 / 32
Total batch reconstruction loss: 0.06089743971824646
Training batch 23 / 32
Total batch reconstruction loss: 0.061900027096271515
Training batch 24 / 32
Total batch reconstruction loss: 0.062067992985248566
Training batch 25 / 32
Total batch reconstruction loss: 0.062223225831985474
Training batch 26 / 32
Total batch reconstruction loss: 0.0624166876077652
Training batch 27 / 32
Total batch reconstruction loss: 0.06253006309270859
Training batch 28 / 32
Total batch reconstruction loss: 0.06060967221856117
Training batch 29 / 32
Total batch reconstruction loss: 0.06261822581291199
Training batch 30 / 32
Total batch reconstruction loss: 0.0631096214056015
Training batch 31 / 32
Total batch reconstruction loss: 0.06315331906080246
Training batch 32 / 32
Total batch reconstruction loss: 0.05588148906826973
Epoch [87/500], Train Loss: 0.0627, Validation Loss: 0.0672, Generator Loss: 12.5821, Discriminator Loss: 0.3190
Training epoch 88 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.06619235873222351
Training batch 2 / 32
Total batch reconstruction loss: 0.06336027383804321
Training batch 3 / 32
Total batch reconstruction loss: 0.0634869635105133
Training batch 4 / 32
Total batch reconstruction loss: 0.05729227140545845
Training batch 5 / 32
Total batch reconstruction loss: 0.062472209334373474
Training batch 6 / 32
Total batch reconstruction loss: 0.06420280784368515
Training batch 7 / 32
Total batch reconstruction loss: 0.05769713222980499
Training batch 8 / 32
Total batch reconstruction loss: 0.06130756437778473
Training batch 9 / 32
Total batch reconstruction loss: 0.05971827730536461
Training batch 10 / 32
Total batch reconstruction loss: 0.061098694801330566
Training batch 11 / 32
Total batch reconstruction loss: 0.057310499250888824
Training batch 12 / 32
Total batch reconstruction loss: 0.0734824538230896
Training batch 13 / 32
Total batch reconstruction loss: 0.06696929782629013
Training batch 14 / 32
Total batch reconstruction loss: 0.060832515358924866
Training batch 15 / 32
Total batch reconstruction loss: 0.06184110790491104
Training batch 16 / 32
Total batch reconstruction loss: 0.061152394860982895
Training batch 17 / 32
Total batch reconstruction loss: 0.0595245435833931
Training batch 18 / 32
Total batch reconstruction loss: 0.06496768444776535
Training batch 19 / 32
Total batch reconstruction loss: 0.061053317040205
Training batch 20 / 32
Total batch reconstruction loss: 0.06306964159011841
Training batch 21 / 32
Total batch reconstruction loss: 0.06098945438861847
Training batch 22 / 32
Total batch reconstruction loss: 0.059515081346035004
Training batch 23 / 32
Total batch reconstruction loss: 0.06135061755776405
Training batch 24 / 32
Total batch reconstruction loss: 0.0613572932779789
Training batch 25 / 32
Total batch reconstruction loss: 0.06233552470803261
Training batch 26 / 32
Total batch reconstruction loss: 0.06663700938224792
Training batch 27 / 32
Total batch reconstruction loss: 0.06507065892219543
Training batch 28 / 32
Total batch reconstruction loss: 0.06166897714138031
Training batch 29 / 32
Total batch reconstruction loss: 0.06180664151906967
Training batch 30 / 32
Total batch reconstruction loss: 0.06677170842885971
Training batch 31 / 32
Total batch reconstruction loss: 0.0634649246931076
Training batch 32 / 32
Total batch reconstruction loss: 0.06722234189510345
Epoch [88/500], Train Loss: 0.0634, Validation Loss: 0.0629, Generator Loss: 12.6040, Discriminator Loss: 0.3152
Training epoch 89 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.0632237046957016
Training batch 2 / 32
Total batch reconstruction loss: 0.06193816289305687
Training batch 3 / 32
Total batch reconstruction loss: 0.05779489129781723
Training batch 4 / 32
Total batch reconstruction loss: 0.06750297546386719
Training batch 5 / 32
Total batch reconstruction loss: 0.07277275621891022
Training batch 6 / 32
Total batch reconstruction loss: 0.05939437448978424
Training batch 7 / 32
Total batch reconstruction loss: 0.05948328971862793
Training batch 8 / 32
Total batch reconstruction loss: 0.06287413835525513
Training batch 9 / 32
Total batch reconstruction loss: 0.06190768629312515
Training batch 10 / 32
Total batch reconstruction loss: 0.06858300417661667
Training batch 11 / 32
Total batch reconstruction loss: 0.07338268309831619
Training batch 12 / 32
Total batch reconstruction loss: 0.05995500832796097
Training batch 13 / 32
Total batch reconstruction loss: 0.06833267211914062
Training batch 14 / 32
Total batch reconstruction loss: 0.0654444545507431
Training batch 15 / 32
Total batch reconstruction loss: 0.05949503928422928
Training batch 16 / 32
Total batch reconstruction loss: 0.06461752951145172
Training batch 17 / 32
Total batch reconstruction loss: 0.062599316239357
Training batch 18 / 32
Total batch reconstruction loss: 0.06146499142050743
Training batch 19 / 32
Total batch reconstruction loss: 0.06379584968090057
Training batch 20 / 32
Total batch reconstruction loss: 0.06388558447360992
Training batch 21 / 32
Total batch reconstruction loss: 0.06456759572029114
Training batch 22 / 32
Total batch reconstruction loss: 0.058842115104198456
Training batch 23 / 32
Total batch reconstruction loss: 0.05512230843305588
Training batch 24 / 32
Total batch reconstruction loss: 0.05920342355966568
Training batch 25 / 32
Total batch reconstruction loss: 0.0650053471326828
Training batch 26 / 32
Total batch reconstruction loss: 0.06261616945266724
Training batch 27 / 32
Total batch reconstruction loss: 0.06957580894231796
Training batch 28 / 32
Total batch reconstruction loss: 0.0649886429309845
Training batch 29 / 32
Total batch reconstruction loss: 0.06368447095155716
Training batch 30 / 32
Total batch reconstruction loss: 0.057233281433582306
Training batch 31 / 32
Total batch reconstruction loss: 0.06013030558824539
Training batch 32 / 32
Total batch reconstruction loss: 0.05148269236087799
Epoch [89/500], Train Loss: 0.0632, Validation Loss: 0.0624, Generator Loss: 12.6353, Discriminator Loss: 0.3216
Training epoch 90 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.059864215552806854
Training batch 2 / 32
Total batch reconstruction loss: 0.0566679909825325
Training batch 3 / 32
Total batch reconstruction loss: 0.06044851616024971
Training batch 4 / 32
Total batch reconstruction loss: 0.06616910547018051
Training batch 5 / 32
Total batch reconstruction loss: 0.061125144362449646
Training batch 6 / 32
Total batch reconstruction loss: 0.06433670222759247
Training batch 7 / 32
Total batch reconstruction loss: 0.06380027532577515
Training batch 8 / 32
Total batch reconstruction loss: 0.06371978670358658
Training batch 9 / 32
Total batch reconstruction loss: 0.05938265845179558
Training batch 10 / 32
Total batch reconstruction loss: 0.05921890214085579
Training batch 11 / 32
Total batch reconstruction loss: 0.06438358128070831
Training batch 12 / 32
Total batch reconstruction loss: 0.0626777783036232
Training batch 13 / 32
Total batch reconstruction loss: 0.06928864121437073
Training batch 14 / 32
Total batch reconstruction loss: 0.058374036103487015
Training batch 15 / 32
Total batch reconstruction loss: 0.06133745610713959
Training batch 16 / 32
Total batch reconstruction loss: 0.0651378259062767
Training batch 17 / 32
Total batch reconstruction loss: 0.060002800077199936
Training batch 18 / 32
Total batch reconstruction loss: 0.06169063597917557
Training batch 19 / 32
Total batch reconstruction loss: 0.063998281955719
Training batch 20 / 32
Total batch reconstruction loss: 0.06099880486726761
Training batch 21 / 32
Total batch reconstruction loss: 0.06071803346276283
Training batch 22 / 32
Total batch reconstruction loss: 0.061624690890312195
Training batch 23 / 32
Total batch reconstruction loss: 0.061445288360118866
Training batch 24 / 32
Total batch reconstruction loss: 0.06186961382627487
Training batch 25 / 32
Total batch reconstruction loss: 0.06596118211746216
Training batch 26 / 32
Total batch reconstruction loss: 0.06085401400923729
Training batch 27 / 32
Total batch reconstruction loss: 0.058328356593847275
Training batch 28 / 32
Total batch reconstruction loss: 0.065277099609375
Training batch 29 / 32
Total batch reconstruction loss: 0.06377388536930084
Training batch 30 / 32
Total batch reconstruction loss: 0.0642469972372055
Training batch 31 / 32
Total batch reconstruction loss: 0.062015630304813385
Training batch 32 / 32
Total batch reconstruction loss: 0.059127241373062134
Epoch [90/500], Train Loss: 0.0623, Validation Loss: 0.0610, Generator Loss: 12.5087, Discriminator Loss: 0.3046
Training epoch 91 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.0633961483836174
Training batch 2 / 32
Total batch reconstruction loss: 0.060533877462148666
Training batch 3 / 32
Total batch reconstruction loss: 0.06581009924411774
Training batch 4 / 32
Total batch reconstruction loss: 0.06259558349847794
Training batch 5 / 32
Total batch reconstruction loss: 0.05881848186254501
Training batch 6 / 32
Total batch reconstruction loss: 0.062030233442783356
Training batch 7 / 32
Total batch reconstruction loss: 0.06316466629505157
Training batch 8 / 32
Total batch reconstruction loss: 0.05897191911935806
Training batch 9 / 32
Total batch reconstruction loss: 0.07076172530651093
Training batch 10 / 32
Total batch reconstruction loss: 0.06794784963130951
Training batch 11 / 32
Total batch reconstruction loss: 0.0591425783932209
Training batch 12 / 32
Total batch reconstruction loss: 0.05751970410346985
Training batch 13 / 32
Total batch reconstruction loss: 0.06306087970733643
Training batch 14 / 32
Total batch reconstruction loss: 0.06254151463508606
Training batch 15 / 32
Total batch reconstruction loss: 0.06542623788118362
Training batch 16 / 32
Total batch reconstruction loss: 0.06443682312965393
Training batch 17 / 32
Total batch reconstruction loss: 0.060074806213378906
Training batch 18 / 32
Total batch reconstruction loss: 0.06006358563899994
Training batch 19 / 32
Total batch reconstruction loss: 0.061283282935619354
Training batch 20 / 32
Total batch reconstruction loss: 0.07093317806720734
Training batch 21 / 32
Total batch reconstruction loss: 0.06395670026540756
Training batch 22 / 32
Total batch reconstruction loss: 0.06058889627456665
Training batch 23 / 32
Total batch reconstruction loss: 0.06264951080083847
Training batch 24 / 32
Total batch reconstruction loss: 0.062000565230846405
Training batch 25 / 32
Total batch reconstruction loss: 0.060046903789043427
Training batch 26 / 32
Total batch reconstruction loss: 0.06207188963890076
Training batch 27 / 32
Total batch reconstruction loss: 0.06421694159507751
Training batch 28 / 32
Total batch reconstruction loss: 0.06902430951595306
Training batch 29 / 32
Total batch reconstruction loss: 0.06706332415342331
Training batch 30 / 32
Total batch reconstruction loss: 0.06600906699895859
Training batch 31 / 32
Total batch reconstruction loss: 0.057632699608802795
Training batch 32 / 32
Total batch reconstruction loss: 0.07718826085329056
Epoch [91/500], Train Loss: 0.0634, Validation Loss: 0.0635, Generator Loss: 12.7682, Discriminator Loss: 0.3167
Training epoch 92 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.06591734290122986
Training batch 2 / 32
Total batch reconstruction loss: 0.07066719233989716
Training batch 3 / 32
Total batch reconstruction loss: 0.06092283874750137
Training batch 4 / 32
Total batch reconstruction loss: 0.06388047337532043
Training batch 5 / 32
Total batch reconstruction loss: 0.06243298202753067
Training batch 6 / 32
Total batch reconstruction loss: 0.06793410331010818
Training batch 7 / 32
Total batch reconstruction loss: 0.0618160218000412
Training batch 8 / 32
Total batch reconstruction loss: 0.06495707482099533
Training batch 9 / 32
Total batch reconstruction loss: 0.05896971374750137
Training batch 10 / 32
Total batch reconstruction loss: 0.06093737110495567
Training batch 11 / 32
Total batch reconstruction loss: 0.06249178200960159
Training batch 12 / 32
Total batch reconstruction loss: 0.06368796527385712
Training batch 13 / 32
Total batch reconstruction loss: 0.0648670643568039
Training batch 14 / 32
Total batch reconstruction loss: 0.05986522138118744
Training batch 15 / 32
Total batch reconstruction loss: 0.06447149068117142
Training batch 16 / 32
Total batch reconstruction loss: 0.06625312566757202
Training batch 17 / 32
Total batch reconstruction loss: 0.06567664444446564
Training batch 18 / 32
Total batch reconstruction loss: 0.06406397372484207
Training batch 19 / 32
Total batch reconstruction loss: 0.061972036957740784
Training batch 20 / 32
Total batch reconstruction loss: 0.06943405419588089
Training batch 21 / 32
Total batch reconstruction loss: 0.06493797898292542
Training batch 22 / 32
Total batch reconstruction loss: 0.0635218694806099
Training batch 23 / 32
Total batch reconstruction loss: 0.06010536849498749
Training batch 24 / 32
Total batch reconstruction loss: 0.06603015959262848
Training batch 25 / 32
Total batch reconstruction loss: 0.06018169969320297
Training batch 26 / 32
Total batch reconstruction loss: 0.06011129170656204
Training batch 27 / 32
Total batch reconstruction loss: 0.06806705892086029
Training batch 28 / 32
Total batch reconstruction loss: 0.06331457942724228
Training batch 29 / 32
Total batch reconstruction loss: 0.06039790064096451
Training batch 30 / 32
Total batch reconstruction loss: 0.06187574937939644
Training batch 31 / 32
Total batch reconstruction loss: 0.06253156810998917
Training batch 32 / 32
Total batch reconstruction loss: 0.06463371217250824
Epoch [92/500], Train Loss: 0.0635, Validation Loss: 0.0636, Generator Loss: 12.8007, Discriminator Loss: 0.3178
Training epoch 93 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.06057057902216911
Training batch 2 / 32
Total batch reconstruction loss: 0.0611039400100708
Training batch 3 / 32
Total batch reconstruction loss: 0.06148820370435715
Training batch 4 / 32
Total batch reconstruction loss: 0.06626638025045395
Training batch 5 / 32
Total batch reconstruction loss: 0.06367920339107513
Training batch 6 / 32
Total batch reconstruction loss: 0.06905017793178558
Training batch 7 / 32
Total batch reconstruction loss: 0.059024930000305176
Training batch 8 / 32
Total batch reconstruction loss: 0.06574675440788269
Training batch 9 / 32
Total batch reconstruction loss: 0.0640358030796051
Training batch 10 / 32
Total batch reconstruction loss: 0.06622006744146347
Training batch 11 / 32
Total batch reconstruction loss: 0.06546005606651306
Training batch 12 / 32
Total batch reconstruction loss: 0.06690027564764023
Training batch 13 / 32
Total batch reconstruction loss: 0.060699209570884705
Training batch 14 / 32
Total batch reconstruction loss: 0.06154169887304306
Training batch 15 / 32
Total batch reconstruction loss: 0.060776568949222565
Training batch 16 / 32
Total batch reconstruction loss: 0.05997011810541153
Training batch 17 / 32
Total batch reconstruction loss: 0.06326942890882492
Training batch 18 / 32
Total batch reconstruction loss: 0.06109525263309479
Training batch 19 / 32
Total batch reconstruction loss: 0.06336712092161179
Training batch 20 / 32
Total batch reconstruction loss: 0.06414686143398285
Training batch 21 / 32
Total batch reconstruction loss: 0.06460317969322205
Training batch 22 / 32
Total batch reconstruction loss: 0.060495950281620026
Training batch 23 / 32
Total batch reconstruction loss: 0.059084489941596985
Training batch 24 / 32
Total batch reconstruction loss: 0.062385886907577515
Training batch 25 / 32
Total batch reconstruction loss: 0.0605384036898613
Training batch 26 / 32
Total batch reconstruction loss: 0.0609213151037693
Training batch 27 / 32
Total batch reconstruction loss: 0.06249071657657623
Training batch 28 / 32
Total batch reconstruction loss: 0.06680350005626678
Training batch 29 / 32
Total batch reconstruction loss: 0.06860992312431335
Training batch 30 / 32
Total batch reconstruction loss: 0.059606608003377914
Training batch 31 / 32
Total batch reconstruction loss: 0.06176750361919403
Training batch 32 / 32
Total batch reconstruction loss: 0.06692442297935486
Epoch [93/500], Train Loss: 0.0637, Validation Loss: 0.0654, Generator Loss: 12.6928, Discriminator Loss: 0.3128
Training epoch 94 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.06475864350795746
Training batch 2 / 32
Total batch reconstruction loss: 0.06770806014537811
Training batch 3 / 32
Total batch reconstruction loss: 0.06326296925544739
Training batch 4 / 32
Total batch reconstruction loss: 0.06654154509305954
Training batch 5 / 32
Total batch reconstruction loss: 0.05971907451748848
Training batch 6 / 32
Total batch reconstruction loss: 0.06538714468479156
Training batch 7 / 32
Total batch reconstruction loss: 0.06570816040039062
Training batch 8 / 32
Total batch reconstruction loss: 0.0629010945558548
Training batch 9 / 32
Total batch reconstruction loss: 0.06795062124729156
Training batch 10 / 32
Total batch reconstruction loss: 0.06110285595059395
Training batch 11 / 32
Total batch reconstruction loss: 0.0613967590034008
Training batch 12 / 32
Total batch reconstruction loss: 0.06261198967695236
Training batch 13 / 32
Total batch reconstruction loss: 0.059712693095207214
Training batch 14 / 32
Total batch reconstruction loss: 0.06285595148801804
Training batch 15 / 32
Total batch reconstruction loss: 0.05945528298616409
Training batch 16 / 32
Total batch reconstruction loss: 0.05971301347017288
Training batch 17 / 32
Total batch reconstruction loss: 0.057574957609176636
Training batch 18 / 32
Total batch reconstruction loss: 0.06212572380900383
Training batch 19 / 32
Total batch reconstruction loss: 0.06250277161598206
Training batch 20 / 32
Total batch reconstruction loss: 0.060033462941646576
Training batch 21 / 32
Total batch reconstruction loss: 0.060890305787324905
Training batch 22 / 32
Total batch reconstruction loss: 0.0627078264951706
Training batch 23 / 32
Total batch reconstruction loss: 0.06098112836480141
Training batch 24 / 32
Total batch reconstruction loss: 0.05813415348529816
Training batch 25 / 32
Total batch reconstruction loss: 0.06398062407970428
Training batch 26 / 32
Total batch reconstruction loss: 0.06791532039642334
Training batch 27 / 32
Total batch reconstruction loss: 0.05952925980091095
Training batch 28 / 32
Total batch reconstruction loss: 0.060992442071437836
Training batch 29 / 32
Total batch reconstruction loss: 0.06013733893632889
Training batch 30 / 32
Total batch reconstruction loss: 0.06788692623376846
Training batch 31 / 32
Total batch reconstruction loss: 0.06482627987861633
Training batch 32 / 32
Total batch reconstruction loss: 0.0719386637210846
Epoch [94/500], Train Loss: 0.0632, Validation Loss: 0.0645, Generator Loss: 12.6402, Discriminator Loss: 0.3380
Training epoch 95 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.06390722095966339
Training batch 2 / 32
Total batch reconstruction loss: 0.06384009122848511
Training batch 3 / 32
Total batch reconstruction loss: 0.06399150937795639
Training batch 4 / 32
Total batch reconstruction loss: 0.06414088606834412
Training batch 5 / 32
Total batch reconstruction loss: 0.06451237201690674
Training batch 6 / 32
Total batch reconstruction loss: 0.06522873789072037
Training batch 7 / 32
Total batch reconstruction loss: 0.06104227155447006
Training batch 8 / 32
Total batch reconstruction loss: 0.06144633889198303
Training batch 9 / 32
Total batch reconstruction loss: 0.06577050685882568
Training batch 10 / 32
Total batch reconstruction loss: 0.05904296040534973
Training batch 11 / 32
Total batch reconstruction loss: 0.06429433822631836
Training batch 12 / 32
Total batch reconstruction loss: 0.06669051200151443
Training batch 13 / 32
Total batch reconstruction loss: 0.0639352947473526
Training batch 14 / 32
Total batch reconstruction loss: 0.06159142404794693
Training batch 15 / 32
Total batch reconstruction loss: 0.061873212456703186
Training batch 16 / 32
Total batch reconstruction loss: 0.0647704154253006
Training batch 17 / 32
Total batch reconstruction loss: 0.06897088140249252
Training batch 18 / 32
Total batch reconstruction loss: 0.06711109727621078
Training batch 19 / 32
Total batch reconstruction loss: 0.06515605747699738
Training batch 20 / 32
Total batch reconstruction loss: 0.060998864471912384
Training batch 21 / 32
Total batch reconstruction loss: 0.06253236532211304
Training batch 22 / 32
Total batch reconstruction loss: 0.061996303498744965
Training batch 23 / 32
Total batch reconstruction loss: 0.058460354804992676
Training batch 24 / 32
Total batch reconstruction loss: 0.062468454241752625
Training batch 25 / 32
Total batch reconstruction loss: 0.06260281056165695
Training batch 26 / 32
Total batch reconstruction loss: 0.06693273782730103
Training batch 27 / 32
Total batch reconstruction loss: 0.061550453305244446
Training batch 28 / 32
Total batch reconstruction loss: 0.06832680106163025
Training batch 29 / 32
Total batch reconstruction loss: 0.06217490881681442
Training batch 30 / 32
Total batch reconstruction loss: 0.06374853104352951
Training batch 31 / 32
Total batch reconstruction loss: 0.06312527507543564
Training batch 32 / 32
Total batch reconstruction loss: 0.056746698915958405
Epoch [95/500], Train Loss: 0.0640, Validation Loss: 0.0638, Generator Loss: 12.7592, Discriminator Loss: 0.3138
Training epoch 96 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.06075678765773773
Training batch 2 / 32
Total batch reconstruction loss: 0.05853996425867081
Training batch 3 / 32
Total batch reconstruction loss: 0.06271035224199295
Training batch 4 / 32
Total batch reconstruction loss: 0.06747783720493317
Training batch 5 / 32
Total batch reconstruction loss: 0.06420541554689407
Training batch 6 / 32
Total batch reconstruction loss: 0.06047928333282471
Training batch 7 / 32
Total batch reconstruction loss: 0.05928521603345871
Training batch 8 / 32
Total batch reconstruction loss: 0.05895257741212845
Training batch 9 / 32
Total batch reconstruction loss: 0.060176245868206024
Training batch 10 / 32
Total batch reconstruction loss: 0.06396560370922089
Training batch 11 / 32
Total batch reconstruction loss: 0.061184193938970566
Training batch 12 / 32
Total batch reconstruction loss: 0.058793388307094574
Training batch 13 / 32
Total batch reconstruction loss: 0.05974885821342468
Training batch 14 / 32
Total batch reconstruction loss: 0.06736023724079132
Training batch 15 / 32
Total batch reconstruction loss: 0.06868910789489746
Training batch 16 / 32
Total batch reconstruction loss: 0.06510838121175766
Training batch 17 / 32
Total batch reconstruction loss: 0.06295587122440338
Training batch 18 / 32
Total batch reconstruction loss: 0.06327641010284424
Training batch 19 / 32
Total batch reconstruction loss: 0.0631314292550087
Training batch 20 / 32
Total batch reconstruction loss: 0.06434072554111481
Training batch 21 / 32
Total batch reconstruction loss: 0.06674510985612869
Training batch 22 / 32
Total batch reconstruction loss: 0.05926050245761871
Training batch 23 / 32
Total batch reconstruction loss: 0.06674522161483765
Training batch 24 / 32
Total batch reconstruction loss: 0.06155874952673912
Training batch 25 / 32
Total batch reconstruction loss: 0.05936342477798462
Training batch 26 / 32
Total batch reconstruction loss: 0.06086781620979309
Training batch 27 / 32
Total batch reconstruction loss: 0.06755518168210983
Training batch 28 / 32
Total batch reconstruction loss: 0.06601065397262573
Training batch 29 / 32
Total batch reconstruction loss: 0.05952958017587662
Training batch 30 / 32
Total batch reconstruction loss: 0.06239619851112366
Training batch 31 / 32
Total batch reconstruction loss: 0.06137256324291229
Training batch 32 / 32
Total batch reconstruction loss: 0.06870532780885696
Epoch [96/500], Train Loss: 0.0630, Validation Loss: 0.0641, Generator Loss: 12.6422, Discriminator Loss: 0.3185
Training epoch 97 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.06461179256439209
Training batch 2 / 32
Total batch reconstruction loss: 0.05882749706506729
Training batch 3 / 32
Total batch reconstruction loss: 0.05758011341094971
Training batch 4 / 32
Total batch reconstruction loss: 0.06408819556236267
Training batch 5 / 32
Total batch reconstruction loss: 0.05946581810712814
Training batch 6 / 32
Total batch reconstruction loss: 0.06033393368124962
Training batch 7 / 32
Total batch reconstruction loss: 0.059376057237386703
Training batch 8 / 32
Total batch reconstruction loss: 0.06228337436914444
Training batch 9 / 32
Total batch reconstruction loss: 0.06101272255182266
Training batch 10 / 32
Total batch reconstruction loss: 0.06590738892555237
Training batch 11 / 32
Total batch reconstruction loss: 0.06743717193603516
Training batch 12 / 32
Total batch reconstruction loss: 0.06173958629369736
Training batch 13 / 32
Total batch reconstruction loss: 0.06397019326686859
Training batch 14 / 32
Total batch reconstruction loss: 0.05762059986591339
Training batch 15 / 32
Total batch reconstruction loss: 0.06077149510383606
Training batch 16 / 32
Total batch reconstruction loss: 0.06423740088939667
Training batch 17 / 32
Total batch reconstruction loss: 0.06049986556172371
Training batch 18 / 32
Total batch reconstruction loss: 0.06214166060090065
Training batch 19 / 32
Total batch reconstruction loss: 0.0637374296784401
Training batch 20 / 32
Total batch reconstruction loss: 0.062278710305690765
Training batch 21 / 32
Total batch reconstruction loss: 0.06261211633682251
Training batch 22 / 32
Total batch reconstruction loss: 0.06392320245504379
Training batch 23 / 32
Total batch reconstruction loss: 0.06518840789794922
Training batch 24 / 32
Total batch reconstruction loss: 0.06221786141395569
Training batch 25 / 32
Total batch reconstruction loss: 0.06022477522492409
Training batch 26 / 32
Total batch reconstruction loss: 0.05904841423034668
Training batch 27 / 32
Total batch reconstruction loss: 0.06175949051976204
Training batch 28 / 32
Total batch reconstruction loss: 0.06616666913032532
Training batch 29 / 32
Total batch reconstruction loss: 0.06332994997501373
Training batch 30 / 32
Total batch reconstruction loss: 0.06544502079486847
Training batch 31 / 32
Total batch reconstruction loss: 0.06470449268817902
Training batch 32 / 32
Total batch reconstruction loss: 0.07397232949733734
Epoch [97/500], Train Loss: 0.0629, Validation Loss: 0.0621, Generator Loss: 12.6117, Discriminator Loss: 0.3174
Training epoch 98 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.06560175120830536
Training batch 2 / 32
Total batch reconstruction loss: 0.06762757897377014
Training batch 3 / 32
Total batch reconstruction loss: 0.06715701520442963
Training batch 4 / 32
Total batch reconstruction loss: 0.06228867173194885
Training batch 5 / 32
Total batch reconstruction loss: 0.062335655093193054
Training batch 6 / 32
Total batch reconstruction loss: 0.06725462526082993
Training batch 7 / 32
Total batch reconstruction loss: 0.05862892046570778
Training batch 8 / 32
Total batch reconstruction loss: 0.05705549940466881
Training batch 9 / 32
Total batch reconstruction loss: 0.06312502175569534
Training batch 10 / 32
Total batch reconstruction loss: 0.06053686887025833
Training batch 11 / 32
Total batch reconstruction loss: 0.06783362478017807
Training batch 12 / 32
Total batch reconstruction loss: 0.0670698881149292
Training batch 13 / 32
Total batch reconstruction loss: 0.06403382867574692
Training batch 14 / 32
Total batch reconstruction loss: 0.0590357631444931
Training batch 15 / 32
Total batch reconstruction loss: 0.05800370126962662
Training batch 16 / 32
Total batch reconstruction loss: 0.06486056745052338
Training batch 17 / 32
Total batch reconstruction loss: 0.06473998725414276
Training batch 18 / 32
Total batch reconstruction loss: 0.05996852368116379
Training batch 19 / 32
Total batch reconstruction loss: 0.060464270412921906
Training batch 20 / 32
Total batch reconstruction loss: 0.06009203940629959
Training batch 21 / 32
Total batch reconstruction loss: 0.05973614752292633
Training batch 22 / 32
Total batch reconstruction loss: 0.06617896258831024
Training batch 23 / 32
Total batch reconstruction loss: 0.06123829632997513
Training batch 24 / 32
Total batch reconstruction loss: 0.0602128729224205
Training batch 25 / 32
Total batch reconstruction loss: 0.059597283601760864
Training batch 26 / 32
Total batch reconstruction loss: 0.060868509113788605
Training batch 27 / 32
Total batch reconstruction loss: 0.06510429829359055
Training batch 28 / 32
Total batch reconstruction loss: 0.06932595372200012
Training batch 29 / 32
Total batch reconstruction loss: 0.07089515030384064
Training batch 30 / 32
Total batch reconstruction loss: 0.06016227602958679
Training batch 31 / 32
Total batch reconstruction loss: 0.06584403663873672
Training batch 32 / 32
Total batch reconstruction loss: 0.04683568328619003
Epoch [98/500], Train Loss: 0.0627, Validation Loss: 0.0641, Generator Loss: 12.5801, Discriminator Loss: 0.3399
Training epoch 99 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.06499631702899933
Training batch 2 / 32
Total batch reconstruction loss: 0.06632419675588608
Training batch 3 / 32
Total batch reconstruction loss: 0.060547638684511185
Training batch 4 / 32
Total batch reconstruction loss: 0.05854376405477524
Training batch 5 / 32
Total batch reconstruction loss: 0.06235659867525101
Training batch 6 / 32
Total batch reconstruction loss: 0.0696990042924881
Training batch 7 / 32
Total batch reconstruction loss: 0.062248263508081436
Training batch 8 / 32
Total batch reconstruction loss: 0.06344185769557953
Training batch 9 / 32
Total batch reconstruction loss: 0.0594622939825058
Training batch 10 / 32
Total batch reconstruction loss: 0.05971621349453926
Training batch 11 / 32
Total batch reconstruction loss: 0.06603455543518066
Training batch 12 / 32
Total batch reconstruction loss: 0.059809546917676926
Training batch 13 / 32
Total batch reconstruction loss: 0.057980261743068695
Training batch 14 / 32
Total batch reconstruction loss: 0.06547357141971588
Training batch 15 / 32
Total batch reconstruction loss: 0.06575630605220795
Training batch 16 / 32
Total batch reconstruction loss: 0.06650608777999878
Training batch 17 / 32
Total batch reconstruction loss: 0.05931419879198074
Training batch 18 / 32
Total batch reconstruction loss: 0.06253340095281601
Training batch 19 / 32
Total batch reconstruction loss: 0.060804858803749084
Training batch 20 / 32
Total batch reconstruction loss: 0.06439492106437683
Training batch 21 / 32
Total batch reconstruction loss: 0.06308053433895111
Training batch 22 / 32
Total batch reconstruction loss: 0.06876649707555771
Training batch 23 / 32
Total batch reconstruction loss: 0.061918094754219055
Training batch 24 / 32
Total batch reconstruction loss: 0.0628449097275734
Training batch 25 / 32
Total batch reconstruction loss: 0.0672852098941803
Training batch 26 / 32
Total batch reconstruction loss: 0.05945316702127457
Training batch 27 / 32
Total batch reconstruction loss: 0.06966312229633331
Training batch 28 / 32
Total batch reconstruction loss: 0.06563480943441391
Training batch 29 / 32
Total batch reconstruction loss: 0.06148834899067879
Training batch 30 / 32
Total batch reconstruction loss: 0.05982760339975357
Training batch 31 / 32
Total batch reconstruction loss: 0.059978753328323364
Training batch 32 / 32
Total batch reconstruction loss: 0.07229787111282349
Epoch [99/500], Train Loss: 0.0641, Validation Loss: 0.0642, Generator Loss: 12.7361, Discriminator Loss: 0.3298
Training epoch 100 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.060480229556560516
Training batch 2 / 32
Total batch reconstruction loss: 0.06717391312122345
Training batch 3 / 32
Total batch reconstruction loss: 0.06140296161174774
Training batch 4 / 32
Total batch reconstruction loss: 0.06172244995832443
Training batch 5 / 32
Total batch reconstruction loss: 0.05843745917081833
Training batch 6 / 32
Total batch reconstruction loss: 0.06182323396205902
Training batch 7 / 32
Total batch reconstruction loss: 0.06870891153812408
Training batch 8 / 32
Total batch reconstruction loss: 0.06877864897251129
Training batch 9 / 32
Total batch reconstruction loss: 0.06098980829119682
Training batch 10 / 32
Total batch reconstruction loss: 0.06201557070016861
Training batch 11 / 32
Total batch reconstruction loss: 0.06331183016300201
Training batch 12 / 32
Total batch reconstruction loss: 0.06632492691278458
Training batch 13 / 32
Total batch reconstruction loss: 0.06272044777870178
Training batch 14 / 32
Total batch reconstruction loss: 0.061605021357536316
Training batch 15 / 32
Total batch reconstruction loss: 0.06009382754564285
Training batch 16 / 32
Total batch reconstruction loss: 0.06271190196275711
Training batch 17 / 32
Total batch reconstruction loss: 0.062384285032749176
Training batch 18 / 32
Total batch reconstruction loss: 0.0628461241722107
Training batch 19 / 32
Total batch reconstruction loss: 0.06775657832622528
Training batch 20 / 32
Total batch reconstruction loss: 0.0645626038312912
Training batch 21 / 32
Total batch reconstruction loss: 0.058546338230371475
Training batch 22 / 32
Total batch reconstruction loss: 0.06014171242713928
Training batch 23 / 32
Total batch reconstruction loss: 0.0592053085565567
Training batch 24 / 32
Total batch reconstruction loss: 0.06189863756299019
Training batch 25 / 32
Total batch reconstruction loss: 0.06232602149248123
Training batch 26 / 32
Total batch reconstruction loss: 0.06380133330821991
Training batch 27 / 32
Total batch reconstruction loss: 0.06274962425231934
Training batch 28 / 32
Total batch reconstruction loss: 0.06332573294639587
Training batch 29 / 32
Total batch reconstruction loss: 0.06266149878501892
Training batch 30 / 32
Total batch reconstruction loss: 0.06246907263994217
Training batch 31 / 32
Total batch reconstruction loss: 0.06029955670237541
Training batch 32 / 32
Total batch reconstruction loss: 0.04973069578409195
Epoch [100/500], Train Loss: 0.0620, Validation Loss: 0.0632, Generator Loss: 12.5391, Discriminator Loss: 0.3080
Training epoch 101 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.0606301873922348
Training batch 2 / 32
Total batch reconstruction loss: 0.061754513531923294
Training batch 3 / 32
Total batch reconstruction loss: 0.06530356407165527
Training batch 4 / 32
Total batch reconstruction loss: 0.06067885085940361
Training batch 5 / 32
Total batch reconstruction loss: 0.06234978511929512
Training batch 6 / 32
Total batch reconstruction loss: 0.0634991005063057
Training batch 7 / 32
Total batch reconstruction loss: 0.06353914737701416
Training batch 8 / 32
Total batch reconstruction loss: 0.060668978840112686
Training batch 9 / 32
Total batch reconstruction loss: 0.0655156821012497
Training batch 10 / 32
Total batch reconstruction loss: 0.06149125099182129
Training batch 11 / 32
Total batch reconstruction loss: 0.06476052105426788
Training batch 12 / 32
Total batch reconstruction loss: 0.06026902794837952
Training batch 13 / 32
Total batch reconstruction loss: 0.061221711337566376
Training batch 14 / 32
Total batch reconstruction loss: 0.06454989314079285
Training batch 15 / 32
Total batch reconstruction loss: 0.05939425528049469
Training batch 16 / 32
Total batch reconstruction loss: 0.06414517760276794
Training batch 17 / 32
Total batch reconstruction loss: 0.05835916846990585
Training batch 18 / 32
Total batch reconstruction loss: 0.0691843181848526
Training batch 19 / 32
Total batch reconstruction loss: 0.05926787108182907
Training batch 20 / 32
Total batch reconstruction loss: 0.06393122673034668
Training batch 21 / 32
Total batch reconstruction loss: 0.06312884390354156
Training batch 22 / 32
Total batch reconstruction loss: 0.06528390944004059
Training batch 23 / 32
Total batch reconstruction loss: 0.06214553862810135
Training batch 24 / 32
Total batch reconstruction loss: 0.06278669834136963
Training batch 25 / 32
Total batch reconstruction loss: 0.05739907920360565
Training batch 26 / 32
Total batch reconstruction loss: 0.06663530319929123
Training batch 27 / 32
Total batch reconstruction loss: 0.06513597071170807
Training batch 28 / 32
Total batch reconstruction loss: 0.0635160505771637
Training batch 29 / 32
Total batch reconstruction loss: 0.0641556829214096
Training batch 30 / 32
Total batch reconstruction loss: 0.05925239622592926
Training batch 31 / 32
Total batch reconstruction loss: 0.06451544910669327
Training batch 32 / 32
Total batch reconstruction loss: 0.09278197586536407
Epoch [101/500], Train Loss: 0.0635, Validation Loss: 0.0629, Generator Loss: 12.8124, Discriminator Loss: 0.3168
Training epoch 102 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.06042974069714546
Training batch 2 / 32
Total batch reconstruction loss: 0.062310874462127686
Training batch 3 / 32
Total batch reconstruction loss: 0.060939207673072815
Training batch 4 / 32
Total batch reconstruction loss: 0.06241126358509064
Training batch 5 / 32
Total batch reconstruction loss: 0.0596577450633049
Training batch 6 / 32
Total batch reconstruction loss: 0.06161733716726303
Training batch 7 / 32
Total batch reconstruction loss: 0.06819599866867065
Training batch 8 / 32
Total batch reconstruction loss: 0.06129327416419983
Training batch 9 / 32
Total batch reconstruction loss: 0.06050688028335571
Training batch 10 / 32
Total batch reconstruction loss: 0.06588293612003326
Training batch 11 / 32
Total batch reconstruction loss: 0.0584295317530632
Training batch 12 / 32
Total batch reconstruction loss: 0.06249255686998367
Training batch 13 / 32
Total batch reconstruction loss: 0.06805314123630524
Training batch 14 / 32
Total batch reconstruction loss: 0.0680752769112587
Training batch 15 / 32
Total batch reconstruction loss: 0.06224780157208443
Training batch 16 / 32
Total batch reconstruction loss: 0.06118009239435196
Training batch 17 / 32
Total batch reconstruction loss: 0.05914764106273651
Training batch 18 / 32
Total batch reconstruction loss: 0.06429468840360641
Training batch 19 / 32
Total batch reconstruction loss: 0.06133626028895378
Training batch 20 / 32
Total batch reconstruction loss: 0.061014581471681595
Training batch 21 / 32
Total batch reconstruction loss: 0.06243869289755821
Training batch 22 / 32
Total batch reconstruction loss: 0.05912186950445175
Training batch 23 / 32
Total batch reconstruction loss: 0.06774371862411499
Training batch 24 / 32
Total batch reconstruction loss: 0.058254141360521317
Training batch 25 / 32
Total batch reconstruction loss: 0.06738200038671494
Training batch 26 / 32
Total batch reconstruction loss: 0.06400100886821747
Training batch 27 / 32
Total batch reconstruction loss: 0.06242046877741814
Training batch 28 / 32
Total batch reconstruction loss: 0.06429498642683029
Training batch 29 / 32
Total batch reconstruction loss: 0.06030110642313957
Training batch 30 / 32
Total batch reconstruction loss: 0.0641486644744873
Training batch 31 / 32
Total batch reconstruction loss: 0.06703929603099823
Training batch 32 / 32
Total batch reconstruction loss: 0.06302869319915771
Epoch [102/500], Train Loss: 0.0635, Validation Loss: 0.0631, Generator Loss: 12.6525, Discriminator Loss: 0.2899
Training epoch 103 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.062344443053007126
Training batch 2 / 32
Total batch reconstruction loss: 0.06253140419721603
Training batch 3 / 32
Total batch reconstruction loss: 0.05831773579120636
Training batch 4 / 32
Total batch reconstruction loss: 0.06153910607099533
Training batch 5 / 32
Total batch reconstruction loss: 0.06342992186546326
Training batch 6 / 32
Total batch reconstruction loss: 0.0635661855340004
Training batch 7 / 32
Total batch reconstruction loss: 0.06270474940538406
Training batch 8 / 32
Total batch reconstruction loss: 0.060020461678504944
Training batch 9 / 32
Total batch reconstruction loss: 0.059962160885334015
Training batch 10 / 32
Total batch reconstruction loss: 0.05961921811103821
Training batch 11 / 32
Total batch reconstruction loss: 0.06388790160417557
Training batch 12 / 32
Total batch reconstruction loss: 0.0615825355052948
Training batch 13 / 32
Total batch reconstruction loss: 0.06775859743356705
Training batch 14 / 32
Total batch reconstruction loss: 0.06522034108638763
Training batch 15 / 32
Total batch reconstruction loss: 0.057827189564704895
Training batch 16 / 32
Total batch reconstruction loss: 0.06331837922334671
Training batch 17 / 32
Total batch reconstruction loss: 0.05963660776615143
Training batch 18 / 32
Total batch reconstruction loss: 0.06582798063755035
Training batch 19 / 32
Total batch reconstruction loss: 0.06350447237491608
Training batch 20 / 32
Total batch reconstruction loss: 0.06469191610813141
Training batch 21 / 32
Total batch reconstruction loss: 0.06401507556438446
Training batch 22 / 32
Total batch reconstruction loss: 0.05931190028786659
Training batch 23 / 32
Total batch reconstruction loss: 0.05920499935746193
Training batch 24 / 32
Total batch reconstruction loss: 0.057955458760261536
Training batch 25 / 32
Total batch reconstruction loss: 0.06003813445568085
Training batch 26 / 32
Total batch reconstruction loss: 0.06332145631313324
Training batch 27 / 32
Total batch reconstruction loss: 0.06397950649261475
Training batch 28 / 32
Total batch reconstruction loss: 0.06302256137132645
Training batch 29 / 32
Total batch reconstruction loss: 0.060130342841148376
Training batch 30 / 32
Total batch reconstruction loss: 0.05873487889766693
Training batch 31 / 32
Total batch reconstruction loss: 0.058508116751909256
Training batch 32 / 32
Total batch reconstruction loss: 0.061904843896627426
Epoch [103/500], Train Loss: 0.0619, Validation Loss: 0.0626, Generator Loss: 12.4293, Discriminator Loss: 0.3187
Training epoch 104 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.06013406440615654
Training batch 2 / 32
Total batch reconstruction loss: 0.05996542051434517
Training batch 3 / 32
Total batch reconstruction loss: 0.059466395527124405
Training batch 4 / 32
Total batch reconstruction loss: 0.05935634672641754
Training batch 5 / 32
Total batch reconstruction loss: 0.06551229953765869
Training batch 6 / 32
Total batch reconstruction loss: 0.06458766758441925
Training batch 7 / 32
Total batch reconstruction loss: 0.06510710716247559
Training batch 8 / 32
Total batch reconstruction loss: 0.060558758676052094
Training batch 9 / 32
Total batch reconstruction loss: 0.059178128838539124
Training batch 10 / 32
Total batch reconstruction loss: 0.058361515402793884
Training batch 11 / 32
Total batch reconstruction loss: 0.06447820365428925
Training batch 12 / 32
Total batch reconstruction loss: 0.061625733971595764
Training batch 13 / 32
Total batch reconstruction loss: 0.0638403445482254
Training batch 14 / 32
Total batch reconstruction loss: 0.06740741431713104
Training batch 15 / 32
Total batch reconstruction loss: 0.05996204912662506
Training batch 16 / 32
Total batch reconstruction loss: 0.06074795126914978
Training batch 17 / 32
Total batch reconstruction loss: 0.06090211123228073
Training batch 18 / 32
Total batch reconstruction loss: 0.06624732911586761
Training batch 19 / 32
Total batch reconstruction loss: 0.061083149164915085
Training batch 20 / 32
Total batch reconstruction loss: 0.06112284958362579
Training batch 21 / 32
Total batch reconstruction loss: 0.06698793917894363
Training batch 22 / 32
Total batch reconstruction loss: 0.06709218770265579
Training batch 23 / 32
Total batch reconstruction loss: 0.06061409413814545
Training batch 24 / 32
Total batch reconstruction loss: 0.06423821300268173
Training batch 25 / 32
Total batch reconstruction loss: 0.06563709676265717
Training batch 26 / 32
Total batch reconstruction loss: 0.06434984505176544
Training batch 27 / 32
Total batch reconstruction loss: 0.06113635003566742
Training batch 28 / 32
Total batch reconstruction loss: 0.06105305626988411
Training batch 29 / 32
Total batch reconstruction loss: 0.06296996772289276
Training batch 30 / 32
Total batch reconstruction loss: 0.060450248420238495
Training batch 31 / 32
Total batch reconstruction loss: 0.063850536942482
Training batch 32 / 32
Total batch reconstruction loss: 0.054180778563022614
Epoch [104/500], Train Loss: 0.0625, Validation Loss: 0.0661, Generator Loss: 12.5149, Discriminator Loss: 0.3249
Training epoch 105 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.06720598042011261
Training batch 2 / 32
Total batch reconstruction loss: 0.06364403665065765
Training batch 3 / 32
Total batch reconstruction loss: 0.06061525270342827
Training batch 4 / 32
Total batch reconstruction loss: 0.0668993592262268
Training batch 5 / 32
Total batch reconstruction loss: 0.06236381083726883
Training batch 6 / 32
Total batch reconstruction loss: 0.06633366644382477
Training batch 7 / 32
Total batch reconstruction loss: 0.06223328784108162
Training batch 8 / 32
Total batch reconstruction loss: 0.06076078116893768
Training batch 9 / 32
Total batch reconstruction loss: 0.06291145086288452
Training batch 10 / 32
Total batch reconstruction loss: 0.06153326854109764
Training batch 11 / 32
Total batch reconstruction loss: 0.06538359075784683
Training batch 12 / 32
Total batch reconstruction loss: 0.06637652218341827
Training batch 13 / 32
Total batch reconstruction loss: 0.06190071627497673
Training batch 14 / 32
Total batch reconstruction loss: 0.06268005073070526
Training batch 15 / 32
Total batch reconstruction loss: 0.058607641607522964
Training batch 16 / 32
Total batch reconstruction loss: 0.06644059717655182
Training batch 17 / 32
Total batch reconstruction loss: 0.06159041076898575
Training batch 18 / 32
Total batch reconstruction loss: 0.06649819761514664
Training batch 19 / 32
Total batch reconstruction loss: 0.061505015939474106
Training batch 20 / 32
Total batch reconstruction loss: 0.06264857947826385
Training batch 21 / 32
Total batch reconstruction loss: 0.06426451355218887
Training batch 22 / 32
Total batch reconstruction loss: 0.06524854898452759
Training batch 23 / 32
Total batch reconstruction loss: 0.06588016450405121
Training batch 24 / 32
Total batch reconstruction loss: 0.058982256799936295
Training batch 25 / 32
Total batch reconstruction loss: 0.0661231130361557
Training batch 26 / 32
Total batch reconstruction loss: 0.06504853069782257
Training batch 27 / 32
Total batch reconstruction loss: 0.06448359787464142
Training batch 28 / 32
Total batch reconstruction loss: 0.05803181231021881
Training batch 29 / 32
Total batch reconstruction loss: 0.0638180747628212
Training batch 30 / 32
Total batch reconstruction loss: 0.06559034436941147
Training batch 31 / 32
Total batch reconstruction loss: 0.06389500200748444
Training batch 32 / 32
Total batch reconstruction loss: 0.05547705292701721
Epoch [105/500], Train Loss: 0.0638, Validation Loss: 0.0606, Generator Loss: 12.7225, Discriminator Loss: 0.3272
Training epoch 106 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.061640284955501556
Training batch 2 / 32
Total batch reconstruction loss: 0.059929877519607544
Training batch 3 / 32
Total batch reconstruction loss: 0.06964054703712463
Training batch 4 / 32
Total batch reconstruction loss: 0.05889134481549263
Training batch 5 / 32
Total batch reconstruction loss: 0.06823665648698807
Training batch 6 / 32
Total batch reconstruction loss: 0.06457148492336273
Training batch 7 / 32
Total batch reconstruction loss: 0.06124180182814598
Training batch 8 / 32
Total batch reconstruction loss: 0.06569883227348328
Training batch 9 / 32
Total batch reconstruction loss: 0.06653675436973572
Training batch 10 / 32
Total batch reconstruction loss: 0.06020303815603256
Training batch 11 / 32
Total batch reconstruction loss: 0.06248743087053299
Training batch 12 / 32
Total batch reconstruction loss: 0.06053229793906212
Training batch 13 / 32
Total batch reconstruction loss: 0.0641331672668457
Training batch 14 / 32
Total batch reconstruction loss: 0.0656580924987793
Training batch 15 / 32
Total batch reconstruction loss: 0.06414975970983505
Training batch 16 / 32
Total batch reconstruction loss: 0.0635155737400055
Training batch 17 / 32
Total batch reconstruction loss: 0.06374987959861755
Training batch 18 / 32
Total batch reconstruction loss: 0.06036387383937836
Training batch 19 / 32
Total batch reconstruction loss: 0.06518349051475525
Training batch 20 / 32
Total batch reconstruction loss: 0.06729397177696228
Training batch 21 / 32
Total batch reconstruction loss: 0.06106298416852951
Training batch 22 / 32
Total batch reconstruction loss: 0.05899902433156967
Training batch 23 / 32
Total batch reconstruction loss: 0.06393207609653473
Training batch 24 / 32
Total batch reconstruction loss: 0.06072036176919937
Training batch 25 / 32
Total batch reconstruction loss: 0.05969991534948349
Training batch 26 / 32
Total batch reconstruction loss: 0.06172081455588341
Training batch 27 / 32
Total batch reconstruction loss: 0.05668947100639343
Training batch 28 / 32
Total batch reconstruction loss: 0.05981099605560303
Training batch 29 / 32
Total batch reconstruction loss: 0.0596618577837944
Training batch 30 / 32
Total batch reconstruction loss: 0.058140628039836884
Training batch 31 / 32
Total batch reconstruction loss: 0.06083740293979645
Training batch 32 / 32
Total batch reconstruction loss: 0.08787211775779724
Epoch [106/500], Train Loss: 0.0636, Validation Loss: 0.0636, Generator Loss: 12.7064, Discriminator Loss: 0.3361
Training epoch 107 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.05870307981967926
Training batch 2 / 32
Total batch reconstruction loss: 0.06473955512046814
Training batch 3 / 32
Total batch reconstruction loss: 0.06771018356084824
Training batch 4 / 32
Total batch reconstruction loss: 0.06381721794605255
Training batch 5 / 32
Total batch reconstruction loss: 0.06408248841762543
Training batch 6 / 32
Total batch reconstruction loss: 0.06567598879337311
Training batch 7 / 32
Total batch reconstruction loss: 0.06124298274517059
Training batch 8 / 32
Total batch reconstruction loss: 0.06612338125705719
Training batch 9 / 32
Total batch reconstruction loss: 0.06057066470384598
Training batch 10 / 32
Total batch reconstruction loss: 0.06357195228338242
Training batch 11 / 32
Total batch reconstruction loss: 0.06453274935483932
Training batch 12 / 32
Total batch reconstruction loss: 0.06221332773566246
Training batch 13 / 32
Total batch reconstruction loss: 0.06057266891002655
Training batch 14 / 32
Total batch reconstruction loss: 0.06703969836235046
Training batch 15 / 32
Total batch reconstruction loss: 0.061454445123672485
Training batch 16 / 32
Total batch reconstruction loss: 0.059142909944057465
Training batch 17 / 32
Total batch reconstruction loss: 0.0591084286570549
Training batch 18 / 32
Total batch reconstruction loss: 0.05868444964289665
Training batch 19 / 32
Total batch reconstruction loss: 0.0632815882563591
Training batch 20 / 32
Total batch reconstruction loss: 0.061341121792793274
Training batch 21 / 32
Total batch reconstruction loss: 0.06534607708454132
Training batch 22 / 32
Total batch reconstruction loss: 0.05963824689388275
Training batch 23 / 32
Total batch reconstruction loss: 0.05786038190126419
Training batch 24 / 32
Total batch reconstruction loss: 0.06715643405914307
Training batch 25 / 32
Total batch reconstruction loss: 0.06139972805976868
Training batch 26 / 32
Total batch reconstruction loss: 0.06656523048877716
Training batch 27 / 32
Total batch reconstruction loss: 0.059071242809295654
Training batch 28 / 32
Total batch reconstruction loss: 0.06338857114315033
Training batch 29 / 32
Total batch reconstruction loss: 0.06114158034324646
Training batch 30 / 32
Total batch reconstruction loss: 0.060923803597688675
Training batch 31 / 32
Total batch reconstruction loss: 0.06106884405016899
Training batch 32 / 32
Total batch reconstruction loss: 0.06399178504943848
Epoch [107/500], Train Loss: 0.0627, Validation Loss: 0.0632, Generator Loss: 12.5725, Discriminator Loss: 0.3300
Training epoch 108 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.06344090402126312
Training batch 2 / 32
Total batch reconstruction loss: 0.06267114728689194
Training batch 3 / 32
Total batch reconstruction loss: 0.06290774047374725
Training batch 4 / 32
Total batch reconstruction loss: 0.05898592621088028
Training batch 5 / 32
Total batch reconstruction loss: 0.060043543577194214
Training batch 6 / 32
Total batch reconstruction loss: 0.06076192483305931
Training batch 7 / 32
Total batch reconstruction loss: 0.06252187490463257
Training batch 8 / 32
Total batch reconstruction loss: 0.059165775775909424
Training batch 9 / 32
Total batch reconstruction loss: 0.06376443803310394
Training batch 10 / 32
Total batch reconstruction loss: 0.06312870979309082
Training batch 11 / 32
Total batch reconstruction loss: 0.05739276111125946
Training batch 12 / 32
Total batch reconstruction loss: 0.0647164136171341
Training batch 13 / 32
Total batch reconstruction loss: 0.062477320432662964
Training batch 14 / 32
Total batch reconstruction loss: 0.0626729279756546
Training batch 15 / 32
Total batch reconstruction loss: 0.06527954339981079
Training batch 16 / 32
Total batch reconstruction loss: 0.06554915755987167
Training batch 17 / 32
Total batch reconstruction loss: 0.06564005464315414
Training batch 18 / 32
Total batch reconstruction loss: 0.06226707249879837
Training batch 19 / 32
Total batch reconstruction loss: 0.06379681825637817
Training batch 20 / 32
Total batch reconstruction loss: 0.062105223536491394
Training batch 21 / 32
Total batch reconstruction loss: 0.06008654087781906
Training batch 22 / 32
Total batch reconstruction loss: 0.06445283442735672
Training batch 23 / 32
Total batch reconstruction loss: 0.060562822967767715
Training batch 24 / 32
Total batch reconstruction loss: 0.061828333884477615
Training batch 25 / 32
Total batch reconstruction loss: 0.06297049671411514
Training batch 26 / 32
Total batch reconstruction loss: 0.06325861811637878
Training batch 27 / 32
Total batch reconstruction loss: 0.06562657654285431
Training batch 28 / 32
Total batch reconstruction loss: 0.06085304170846939
Training batch 29 / 32
Total batch reconstruction loss: 0.059523046016693115
Training batch 30 / 32
Total batch reconstruction loss: 0.06425538659095764
Training batch 31 / 32
Total batch reconstruction loss: 0.062192901968955994
Training batch 32 / 32
Total batch reconstruction loss: 0.057134196162223816
Epoch [108/500], Train Loss: 0.0620, Validation Loss: 0.0631, Generator Loss: 12.5177, Discriminator Loss: 0.3153
Training epoch 109 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.05796779692173004
Training batch 2 / 32
Total batch reconstruction loss: 0.059498030692338943
Training batch 3 / 32
Total batch reconstruction loss: 0.06481844186782837
Training batch 4 / 32
Total batch reconstruction loss: 0.06218184158205986
Training batch 5 / 32
Total batch reconstruction loss: 0.06237350404262543
Training batch 6 / 32
Total batch reconstruction loss: 0.06333580613136292
Training batch 7 / 32
Total batch reconstruction loss: 0.06323389708995819
Training batch 8 / 32
Total batch reconstruction loss: 0.05869745463132858
Training batch 9 / 32
Total batch reconstruction loss: 0.061819158494472504
Training batch 10 / 32
Total batch reconstruction loss: 0.06428224593400955
Training batch 11 / 32
Total batch reconstruction loss: 0.06074688583612442
Training batch 12 / 32
Total batch reconstruction loss: 0.06217047572135925
Training batch 13 / 32
Total batch reconstruction loss: 0.060701243579387665
Training batch 14 / 32
Total batch reconstruction loss: 0.0595339834690094
Training batch 15 / 32
Total batch reconstruction loss: 0.06168920919299126
Training batch 16 / 32
Total batch reconstruction loss: 0.06036052852869034
Training batch 17 / 32
Total batch reconstruction loss: 0.059808243066072464
Training batch 18 / 32
Total batch reconstruction loss: 0.06323973834514618
Training batch 19 / 32
Total batch reconstruction loss: 0.06890986859798431
Training batch 20 / 32
Total batch reconstruction loss: 0.059823937714099884
Training batch 21 / 32
Total batch reconstruction loss: 0.06038554012775421
Training batch 22 / 32
Total batch reconstruction loss: 0.059313029050827026
Training batch 23 / 32
Total batch reconstruction loss: 0.0635155588388443
Training batch 24 / 32
Total batch reconstruction loss: 0.06221271678805351
Training batch 25 / 32
Total batch reconstruction loss: 0.06236141920089722
Training batch 26 / 32
Total batch reconstruction loss: 0.05687379837036133
Training batch 27 / 32
Total batch reconstruction loss: 0.061440981924533844
Training batch 28 / 32
Total batch reconstruction loss: 0.061840906739234924
Training batch 29 / 32
Total batch reconstruction loss: 0.06166114658117294
Training batch 30 / 32
Total batch reconstruction loss: 0.06746993958950043
Training batch 31 / 32
Total batch reconstruction loss: 0.05958358943462372
Training batch 32 / 32
Total batch reconstruction loss: 0.05038519576191902
Epoch [109/500], Train Loss: 0.0612, Validation Loss: 0.0617, Generator Loss: 12.3330, Discriminator Loss: 0.3189
Training epoch 110 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.06309090554714203
Training batch 2 / 32
Total batch reconstruction loss: 0.05874059349298477
Training batch 3 / 32
Total batch reconstruction loss: 0.062119752168655396
Training batch 4 / 32
Total batch reconstruction loss: 0.05986004322767258
Training batch 5 / 32
Total batch reconstruction loss: 0.05721350014209747
Training batch 6 / 32
Total batch reconstruction loss: 0.06104196608066559
Training batch 7 / 32
Total batch reconstruction loss: 0.06319347023963928
Training batch 8 / 32
Total batch reconstruction loss: 0.06459952145814896
Training batch 9 / 32
Total batch reconstruction loss: 0.061677269637584686
Training batch 10 / 32
Total batch reconstruction loss: 0.06347957253456116
Training batch 11 / 32
Total batch reconstruction loss: 0.06665077060461044
Training batch 12 / 32
Total batch reconstruction loss: 0.0630730390548706
Training batch 13 / 32
Total batch reconstruction loss: 0.06078120321035385
Training batch 14 / 32
Total batch reconstruction loss: 0.06348392367362976
Training batch 15 / 32
Total batch reconstruction loss: 0.06309030950069427
Training batch 16 / 32
Total batch reconstruction loss: 0.06232954561710358
Training batch 17 / 32
Total batch reconstruction loss: 0.06256907433271408
Training batch 18 / 32
Total batch reconstruction loss: 0.06655018031597137
Training batch 19 / 32
Total batch reconstruction loss: 0.06207619607448578
Training batch 20 / 32
Total batch reconstruction loss: 0.0630803108215332
Training batch 21 / 32
Total batch reconstruction loss: 0.06272554397583008
Training batch 22 / 32
Total batch reconstruction loss: 0.05940072238445282
Training batch 23 / 32
Total batch reconstruction loss: 0.05931717902421951
Training batch 24 / 32
Total batch reconstruction loss: 0.07340501248836517
Training batch 25 / 32
Total batch reconstruction loss: 0.06351077556610107
Training batch 26 / 32
Total batch reconstruction loss: 0.06807659566402435
Training batch 27 / 32
Total batch reconstruction loss: 0.059447742998600006
Training batch 28 / 32
Total batch reconstruction loss: 0.061498839408159256
Training batch 29 / 32
Total batch reconstruction loss: 0.06536097079515457
Training batch 30 / 32
Total batch reconstruction loss: 0.05780303478240967
Training batch 31 / 32
Total batch reconstruction loss: 0.062279604375362396
Training batch 32 / 32
Total batch reconstruction loss: 0.0537889301776886
Epoch [110/500], Train Loss: 0.0630, Validation Loss: 0.0639, Generator Loss: 12.5377, Discriminator Loss: 0.3231
Training epoch 111 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.06208376958966255
Training batch 2 / 32
Total batch reconstruction loss: 0.06574268639087677
Training batch 3 / 32
Total batch reconstruction loss: 0.06162075325846672
Training batch 4 / 32
Total batch reconstruction loss: 0.06068185716867447
Training batch 5 / 32
Total batch reconstruction loss: 0.060381241142749786
Training batch 6 / 32
Total batch reconstruction loss: 0.06174108758568764
Training batch 7 / 32
Total batch reconstruction loss: 0.059315480291843414
Training batch 8 / 32
Total batch reconstruction loss: 0.06054755300283432
Training batch 9 / 32
Total batch reconstruction loss: 0.06520642340183258
Training batch 10 / 32
Total batch reconstruction loss: 0.06046977639198303
Training batch 11 / 32
Total batch reconstruction loss: 0.060606226325035095
Training batch 12 / 32
Total batch reconstruction loss: 0.06769850105047226
Training batch 13 / 32
Total batch reconstruction loss: 0.06490050256252289
Training batch 14 / 32
Total batch reconstruction loss: 0.06123300641775131
Training batch 15 / 32
Total batch reconstruction loss: 0.06330443918704987
Training batch 16 / 32
Total batch reconstruction loss: 0.06455877423286438
Training batch 17 / 32
Total batch reconstruction loss: 0.06017882376909256
Training batch 18 / 32
Total batch reconstruction loss: 0.06282758712768555
Training batch 19 / 32
Total batch reconstruction loss: 0.06155442073941231
Training batch 20 / 32
Total batch reconstruction loss: 0.06384089589118958
Training batch 21 / 32
Total batch reconstruction loss: 0.06053946912288666
Training batch 22 / 32
Total batch reconstruction loss: 0.06399495899677277
Training batch 23 / 32
Total batch reconstruction loss: 0.06144458428025246
Training batch 24 / 32
Total batch reconstruction loss: 0.06082673743367195
Training batch 25 / 32
Total batch reconstruction loss: 0.06234775483608246
Training batch 26 / 32
Total batch reconstruction loss: 0.061747968196868896
Training batch 27 / 32
Total batch reconstruction loss: 0.059136390686035156
Training batch 28 / 32
Total batch reconstruction loss: 0.06042102351784706
Training batch 29 / 32
Total batch reconstruction loss: 0.05957181379199028
Training batch 30 / 32
Total batch reconstruction loss: 0.058537207543849945
Training batch 31 / 32
Total batch reconstruction loss: 0.0635080635547638
Training batch 32 / 32
Total batch reconstruction loss: 0.05371389165520668
Epoch [111/500], Train Loss: 0.0617, Validation Loss: 0.0613, Generator Loss: 12.4161, Discriminator Loss: 0.3196
Training epoch 112 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.06611734628677368
Training batch 2 / 32
Total batch reconstruction loss: 0.05921565741300583
Training batch 3 / 32
Total batch reconstruction loss: 0.06336016207933426
Training batch 4 / 32
Total batch reconstruction loss: 0.05947616696357727
Training batch 5 / 32
Total batch reconstruction loss: 0.06431525945663452
Training batch 6 / 32
Total batch reconstruction loss: 0.05839717015624046
Training batch 7 / 32
Total batch reconstruction loss: 0.0653207004070282
Training batch 8 / 32
Total batch reconstruction loss: 0.06181594356894493
Training batch 9 / 32
Total batch reconstruction loss: 0.06348530203104019
Training batch 10 / 32
Total batch reconstruction loss: 0.0640869289636612
Training batch 11 / 32
Total batch reconstruction loss: 0.06328845769166946
Training batch 12 / 32
Total batch reconstruction loss: 0.0624597892165184
Training batch 13 / 32
Total batch reconstruction loss: 0.06301242113113403
Training batch 14 / 32
Total batch reconstruction loss: 0.06341829150915146
Training batch 15 / 32
Total batch reconstruction loss: 0.06275055557489395
Training batch 16 / 32
Total batch reconstruction loss: 0.05870727077126503
Training batch 17 / 32
Total batch reconstruction loss: 0.06546613574028015
Training batch 18 / 32
Total batch reconstruction loss: 0.06360888481140137
Training batch 19 / 32
Total batch reconstruction loss: 0.061946019530296326
Training batch 20 / 32
Total batch reconstruction loss: 0.06266839057207108
Training batch 21 / 32
Total batch reconstruction loss: 0.058144986629486084
Training batch 22 / 32
Total batch reconstruction loss: 0.07031456381082535
Training batch 23 / 32
Total batch reconstruction loss: 0.06179407238960266
Training batch 24 / 32
Total batch reconstruction loss: 0.056729551404714584
Training batch 25 / 32
Total batch reconstruction loss: 0.06457658857107162
Training batch 26 / 32
Total batch reconstruction loss: 0.06445367634296417
Training batch 27 / 32
Total batch reconstruction loss: 0.06428072601556778
Training batch 28 / 32
Total batch reconstruction loss: 0.06029218062758446
Training batch 29 / 32
Total batch reconstruction loss: 0.05928843840956688
Training batch 30 / 32
Total batch reconstruction loss: 0.06491601467132568
Training batch 31 / 32
Total batch reconstruction loss: 0.06203579902648926
Training batch 32 / 32
Total batch reconstruction loss: 0.059458374977111816
Epoch [112/500], Train Loss: 0.0632, Validation Loss: 0.0616, Generator Loss: 12.5756, Discriminator Loss: 0.3043
Training epoch 113 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.0626116693019867
Training batch 2 / 32
Total batch reconstruction loss: 0.058822453022003174
Training batch 3 / 32
Total batch reconstruction loss: 0.06531979143619537
Training batch 4 / 32
Total batch reconstruction loss: 0.06100616976618767
Training batch 5 / 32
Total batch reconstruction loss: 0.061796512454748154
Training batch 6 / 32
Total batch reconstruction loss: 0.06414060294628143
Training batch 7 / 32
Total batch reconstruction loss: 0.06096411123871803
Training batch 8 / 32
Total batch reconstruction loss: 0.06085016205906868
Training batch 9 / 32
Total batch reconstruction loss: 0.060335248708724976
Training batch 10 / 32
Total batch reconstruction loss: 0.05617116391658783
Training batch 11 / 32
Total batch reconstruction loss: 0.0634351372718811
Training batch 12 / 32
Total batch reconstruction loss: 0.06367754936218262
Training batch 13 / 32
Total batch reconstruction loss: 0.0671909898519516
Training batch 14 / 32
Total batch reconstruction loss: 0.058727990835905075
Training batch 15 / 32
Total batch reconstruction loss: 0.059104904532432556
Training batch 16 / 32
Total batch reconstruction loss: 0.06425586342811584
Training batch 17 / 32
Total batch reconstruction loss: 0.06491664797067642
Training batch 18 / 32
Total batch reconstruction loss: 0.06456442177295685
Training batch 19 / 32
Total batch reconstruction loss: 0.06252485513687134
Training batch 20 / 32
Total batch reconstruction loss: 0.062079042196273804
Training batch 21 / 32
Total batch reconstruction loss: 0.06612829864025116
Training batch 22 / 32
Total batch reconstruction loss: 0.06392128765583038
Training batch 23 / 32
Total batch reconstruction loss: 0.06593458354473114
Training batch 24 / 32
Total batch reconstruction loss: 0.05967039614915848
Training batch 25 / 32
Total batch reconstruction loss: 0.06051036715507507
Training batch 26 / 32
Total batch reconstruction loss: 0.05970446765422821
Training batch 27 / 32
Total batch reconstruction loss: 0.06278808414936066
Training batch 28 / 32
Total batch reconstruction loss: 0.06025121361017227
Training batch 29 / 32
Total batch reconstruction loss: 0.06434445083141327
Training batch 30 / 32
Total batch reconstruction loss: 0.06598265469074249
Training batch 31 / 32
Total batch reconstruction loss: 0.061197519302368164
Training batch 32 / 32
Total batch reconstruction loss: 0.04703753441572189
Epoch [113/500], Train Loss: 0.0621, Validation Loss: 0.0643, Generator Loss: 12.4323, Discriminator Loss: 0.3413
Training epoch 114 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.06356751918792725
Training batch 2 / 32
Total batch reconstruction loss: 0.05928311496973038
Training batch 3 / 32
Total batch reconstruction loss: 0.06144595891237259
Training batch 4 / 32
Total batch reconstruction loss: 0.05673889070749283
Training batch 5 / 32
Total batch reconstruction loss: 0.06325380504131317
Training batch 6 / 32
Total batch reconstruction loss: 0.062271229922771454
Training batch 7 / 32
Total batch reconstruction loss: 0.06068830192089081
Training batch 8 / 32
Total batch reconstruction loss: 0.06066616252064705
Training batch 9 / 32
Total batch reconstruction loss: 0.06184885650873184
Training batch 10 / 32
Total batch reconstruction loss: 0.062098950147628784
Training batch 11 / 32
Total batch reconstruction loss: 0.05712214112281799
Training batch 12 / 32
Total batch reconstruction loss: 0.06448110938072205
Training batch 13 / 32
Total batch reconstruction loss: 0.06008472293615341
Training batch 14 / 32
Total batch reconstruction loss: 0.07036940008401871
Training batch 15 / 32
Total batch reconstruction loss: 0.06276139616966248
Training batch 16 / 32
Total batch reconstruction loss: 0.05842304229736328
Training batch 17 / 32
Total batch reconstruction loss: 0.06114489585161209
Training batch 18 / 32
Total batch reconstruction loss: 0.060476381331682205
Training batch 19 / 32
Total batch reconstruction loss: 0.06323352456092834
Training batch 20 / 32
Total batch reconstruction loss: 0.06553848832845688
Training batch 21 / 32
Total batch reconstruction loss: 0.060615941882133484
Training batch 22 / 32
Total batch reconstruction loss: 0.06318096071481705
Training batch 23 / 32
Total batch reconstruction loss: 0.06787243485450745
Training batch 24 / 32
Total batch reconstruction loss: 0.06163518875837326
Training batch 25 / 32
Total batch reconstruction loss: 0.06223342567682266
Training batch 26 / 32
Total batch reconstruction loss: 0.06154971569776535
Training batch 27 / 32
Total batch reconstruction loss: 0.05846090614795685
Training batch 28 / 32
Total batch reconstruction loss: 0.06158863753080368
Training batch 29 / 32
Total batch reconstruction loss: 0.0682268887758255
Training batch 30 / 32
Total batch reconstruction loss: 0.06202495098114014
Training batch 31 / 32
Total batch reconstruction loss: 0.06582649797201157
Training batch 32 / 32
Total batch reconstruction loss: 0.06529437750577927
Epoch [114/500], Train Loss: 0.0621, Validation Loss: 0.0629, Generator Loss: 12.5398, Discriminator Loss: 0.3099
Training epoch 115 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.06069685518741608
Training batch 2 / 32
Total batch reconstruction loss: 0.060890115797519684
Training batch 3 / 32
Total batch reconstruction loss: 0.06567647308111191
Training batch 4 / 32
Total batch reconstruction loss: 0.07538136839866638
Training batch 5 / 32
Total batch reconstruction loss: 0.06653400510549545
Training batch 6 / 32
Total batch reconstruction loss: 0.06646787375211716
Training batch 7 / 32
Total batch reconstruction loss: 0.06103997677564621
Training batch 8 / 32
Total batch reconstruction loss: 0.06440665572881699
Training batch 9 / 32
Total batch reconstruction loss: 0.06427067518234253
Training batch 10 / 32
Total batch reconstruction loss: 0.06010199338197708
Training batch 11 / 32
Total batch reconstruction loss: 0.06271679699420929
Training batch 12 / 32
Total batch reconstruction loss: 0.05981196463108063
Training batch 13 / 32
Total batch reconstruction loss: 0.06101585924625397
Training batch 14 / 32
Total batch reconstruction loss: 0.05704118683934212
Training batch 15 / 32
Total batch reconstruction loss: 0.06744727492332458
Training batch 16 / 32
Total batch reconstruction loss: 0.06257150322198868
Training batch 17 / 32
Total batch reconstruction loss: 0.0655532255768776
Training batch 18 / 32
Total batch reconstruction loss: 0.065561942756176
Training batch 19 / 32
Total batch reconstruction loss: 0.05782521516084671
Training batch 20 / 32
Total batch reconstruction loss: 0.058218032121658325
Training batch 21 / 32
Total batch reconstruction loss: 0.06154143065214157
Training batch 22 / 32
Total batch reconstruction loss: 0.06215488165616989
Training batch 23 / 32
Total batch reconstruction loss: 0.06092068925499916
Training batch 24 / 32
Total batch reconstruction loss: 0.059779006987810135
Training batch 25 / 32
Total batch reconstruction loss: 0.058931171894073486
Training batch 26 / 32
Total batch reconstruction loss: 0.05872344598174095
Training batch 27 / 32
Total batch reconstruction loss: 0.0626649409532547
Training batch 28 / 32
Total batch reconstruction loss: 0.06136450916528702
Training batch 29 / 32
Total batch reconstruction loss: 0.06786639243364334
Training batch 30 / 32
Total batch reconstruction loss: 0.05936454236507416
Training batch 31 / 32
Total batch reconstruction loss: 0.060943037271499634
Training batch 32 / 32
Total batch reconstruction loss: 0.05948049575090408
Epoch [115/500], Train Loss: 0.0620, Validation Loss: 0.0617, Generator Loss: 12.5526, Discriminator Loss: 0.3196
Training epoch 116 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.0630093589425087
Training batch 2 / 32
Total batch reconstruction loss: 0.056260522454977036
Training batch 3 / 32
Total batch reconstruction loss: 0.05843004584312439
Training batch 4 / 32
Total batch reconstruction loss: 0.06343608349561691
Training batch 5 / 32
Total batch reconstruction loss: 0.06256598234176636
Training batch 6 / 32
Total batch reconstruction loss: 0.05795873701572418
Training batch 7 / 32
Total batch reconstruction loss: 0.05944804474711418
Training batch 8 / 32
Total batch reconstruction loss: 0.06181608885526657
Training batch 9 / 32
Total batch reconstruction loss: 0.062048912048339844
Training batch 10 / 32
Total batch reconstruction loss: 0.06243038550019264
Training batch 11 / 32
Total batch reconstruction loss: 0.06485290080308914
Training batch 12 / 32
Total batch reconstruction loss: 0.06120879203081131
Training batch 13 / 32
Total batch reconstruction loss: 0.0672745555639267
Training batch 14 / 32
Total batch reconstruction loss: 0.06412816792726517
Training batch 15 / 32
Total batch reconstruction loss: 0.06316889077425003
Training batch 16 / 32
Total batch reconstruction loss: 0.060706548392772675
Training batch 17 / 32
Total batch reconstruction loss: 0.06442603468894958
Training batch 18 / 32
Total batch reconstruction loss: 0.06592269986867905
Training batch 19 / 32
Total batch reconstruction loss: 0.062146857380867004
Training batch 20 / 32
Total batch reconstruction loss: 0.06479880213737488
Training batch 21 / 32
Total batch reconstruction loss: 0.06305883079767227
Training batch 22 / 32
Total batch reconstruction loss: 0.06478742510080338
Training batch 23 / 32
Total batch reconstruction loss: 0.05861604958772659
Training batch 24 / 32
Total batch reconstruction loss: 0.061496905982494354
Training batch 25 / 32
Total batch reconstruction loss: 0.061893779784440994
Training batch 26 / 32
Total batch reconstruction loss: 0.06699460744857788
Training batch 27 / 32
Total batch reconstruction loss: 0.05993622541427612
Training batch 28 / 32
Total batch reconstruction loss: 0.057858191430568695
Training batch 29 / 32
Total batch reconstruction loss: 0.062387097626924515
Training batch 30 / 32
Total batch reconstruction loss: 0.05876658111810684
Training batch 31 / 32
Total batch reconstruction loss: 0.06078725308179855
Training batch 32 / 32
Total batch reconstruction loss: 0.06082068383693695
Epoch [116/500], Train Loss: 0.0618, Validation Loss: 0.0625, Generator Loss: 12.4612, Discriminator Loss: 0.3309
Training epoch 117 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.06212712824344635
Training batch 2 / 32
Total batch reconstruction loss: 0.06284423172473907
Training batch 3 / 32
Total batch reconstruction loss: 0.061902519315481186
Training batch 4 / 32
Total batch reconstruction loss: 0.061612535268068314
Training batch 5 / 32
Total batch reconstruction loss: 0.06640107929706573
Training batch 6 / 32
Total batch reconstruction loss: 0.06035197526216507
Training batch 7 / 32
Total batch reconstruction loss: 0.05936200171709061
Training batch 8 / 32
Total batch reconstruction loss: 0.061283666640520096
Training batch 9 / 32
Total batch reconstruction loss: 0.060660555958747864
Training batch 10 / 32
Total batch reconstruction loss: 0.05870339274406433
Training batch 11 / 32
Total batch reconstruction loss: 0.06655310094356537
Training batch 12 / 32
Total batch reconstruction loss: 0.060740876942873
Training batch 13 / 32
Total batch reconstruction loss: 0.06737940013408661
Training batch 14 / 32
Total batch reconstruction loss: 0.061091091483831406
Training batch 15 / 32
Total batch reconstruction loss: 0.05989445373415947
Training batch 16 / 32
Total batch reconstruction loss: 0.06102335825562477
Training batch 17 / 32
Total batch reconstruction loss: 0.06166540086269379
Training batch 18 / 32
Total batch reconstruction loss: 0.06291702389717102
Training batch 19 / 32
Total batch reconstruction loss: 0.06117376685142517
Training batch 20 / 32
Total batch reconstruction loss: 0.061428140848875046
Training batch 21 / 32
Total batch reconstruction loss: 0.05812960863113403
Training batch 22 / 32
Total batch reconstruction loss: 0.05911336466670036
Training batch 23 / 32
Total batch reconstruction loss: 0.06704691052436829
Training batch 24 / 32
Total batch reconstruction loss: 0.06383858621120453
Training batch 25 / 32
Total batch reconstruction loss: 0.06852066516876221
Training batch 26 / 32
Total batch reconstruction loss: 0.05913611501455307
Training batch 27 / 32
Total batch reconstruction loss: 0.061953186988830566
Training batch 28 / 32
Total batch reconstruction loss: 0.06031806766986847
Training batch 29 / 32
Total batch reconstruction loss: 0.05987176299095154
Training batch 30 / 32
Total batch reconstruction loss: 0.062494441866874695
Training batch 31 / 32
Total batch reconstruction loss: 0.05709662288427353
Training batch 32 / 32
Total batch reconstruction loss: 0.057753462344408035
Epoch [117/500], Train Loss: 0.0611, Validation Loss: 0.0620, Generator Loss: 12.4187, Discriminator Loss: 0.3095
Training epoch 118 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.05784747004508972
Training batch 2 / 32
Total batch reconstruction loss: 0.06497245281934738
Training batch 3 / 32
Total batch reconstruction loss: 0.06064986437559128
Training batch 4 / 32
Total batch reconstruction loss: 0.0581708699464798
Training batch 5 / 32
Total batch reconstruction loss: 0.06284727156162262
Training batch 6 / 32
Total batch reconstruction loss: 0.06499910354614258
Training batch 7 / 32
Total batch reconstruction loss: 0.0608627051115036
Training batch 8 / 32
Total batch reconstruction loss: 0.06415785849094391
Training batch 9 / 32
Total batch reconstruction loss: 0.06131955236196518
Training batch 10 / 32
Total batch reconstruction loss: 0.06125681847333908
Training batch 11 / 32
Total batch reconstruction loss: 0.0670677199959755
Training batch 12 / 32
Total batch reconstruction loss: 0.06173774227499962
Training batch 13 / 32
Total batch reconstruction loss: 0.06118934601545334
Training batch 14 / 32
Total batch reconstruction loss: 0.06085854768753052
Training batch 15 / 32
Total batch reconstruction loss: 0.06323720514774323
Training batch 16 / 32
Total batch reconstruction loss: 0.0625142753124237
Training batch 17 / 32
Total batch reconstruction loss: 0.06296685338020325
Training batch 18 / 32
Total batch reconstruction loss: 0.06201305240392685
Training batch 19 / 32
Total batch reconstruction loss: 0.05509204417467117
Training batch 20 / 32
Total batch reconstruction loss: 0.0589352548122406
Training batch 21 / 32
Total batch reconstruction loss: 0.05801555514335632
Training batch 22 / 32
Total batch reconstruction loss: 0.05767468363046646
Training batch 23 / 32
Total batch reconstruction loss: 0.062284935265779495
Training batch 24 / 32
Total batch reconstruction loss: 0.06952977925539017
Training batch 25 / 32
Total batch reconstruction loss: 0.0631580501794815
Training batch 26 / 32
Total batch reconstruction loss: 0.06106267124414444
Training batch 27 / 32
Total batch reconstruction loss: 0.05879851058125496
Training batch 28 / 32
Total batch reconstruction loss: 0.06030412018299103
Training batch 29 / 32
Total batch reconstruction loss: 0.06293949484825134
Training batch 30 / 32
Total batch reconstruction loss: 0.06228411942720413
Training batch 31 / 32
Total batch reconstruction loss: 0.0590117946267128
Training batch 32 / 32
Total batch reconstruction loss: 0.06469976902008057
Epoch [118/500], Train Loss: 0.0614, Validation Loss: 0.0613, Generator Loss: 12.3886, Discriminator Loss: 0.3393
Training epoch 119 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.060220811516046524
Training batch 2 / 32
Total batch reconstruction loss: 0.06336812674999237
Training batch 3 / 32
Total batch reconstruction loss: 0.06085712090134621
Training batch 4 / 32
Total batch reconstruction loss: 0.06079266592860222
Training batch 5 / 32
Total batch reconstruction loss: 0.058525532484054565
Training batch 6 / 32
Total batch reconstruction loss: 0.05646418780088425
Training batch 7 / 32
Total batch reconstruction loss: 0.06516017019748688
Training batch 8 / 32
Total batch reconstruction loss: 0.05943823605775833
Training batch 9 / 32
Total batch reconstruction loss: 0.062215644866228104
Training batch 10 / 32
Total batch reconstruction loss: 0.06374403834342957
Training batch 11 / 32
Total batch reconstruction loss: 0.0591672882437706
Training batch 12 / 32
Total batch reconstruction loss: 0.06652523577213287
Training batch 13 / 32
Total batch reconstruction loss: 0.06147371605038643
Training batch 14 / 32
Total batch reconstruction loss: 0.06545139849185944
Training batch 15 / 32
Total batch reconstruction loss: 0.06185173988342285
Training batch 16 / 32
Total batch reconstruction loss: 0.07152818888425827
Training batch 17 / 32
Total batch reconstruction loss: 0.06542134284973145
Training batch 18 / 32
Total batch reconstruction loss: 0.05889295041561127
Training batch 19 / 32
Total batch reconstruction loss: 0.0639888197183609
Training batch 20 / 32
Total batch reconstruction loss: 0.0626484751701355
Training batch 21 / 32
Total batch reconstruction loss: 0.06685598194599152
Training batch 22 / 32
Total batch reconstruction loss: 0.060369815677404404
Training batch 23 / 32
Total batch reconstruction loss: 0.061484046280384064
Training batch 24 / 32
Total batch reconstruction loss: 0.06278908252716064
Training batch 25 / 32
Total batch reconstruction loss: 0.0637778490781784
Training batch 26 / 32
Total batch reconstruction loss: 0.06291908025741577
Training batch 27 / 32
Total batch reconstruction loss: 0.06038286164402962
Training batch 28 / 32
Total batch reconstruction loss: 0.0626034364104271
Training batch 29 / 32
Total batch reconstruction loss: 0.0641404390335083
Training batch 30 / 32
Total batch reconstruction loss: 0.06801855564117432
Training batch 31 / 32
Total batch reconstruction loss: 0.06006031483411789
Training batch 32 / 32
Total batch reconstruction loss: 0.0602567233145237
Epoch [119/500], Train Loss: 0.0624, Validation Loss: 0.0643, Generator Loss: 12.5837, Discriminator Loss: 0.3116
Training epoch 120 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.06005983054637909
Training batch 2 / 32
Total batch reconstruction loss: 0.061637863516807556
Training batch 3 / 32
Total batch reconstruction loss: 0.0635622963309288
Training batch 4 / 32
Total batch reconstruction loss: 0.06390228867530823
Training batch 5 / 32
Total batch reconstruction loss: 0.0633399486541748
Training batch 6 / 32
Total batch reconstruction loss: 0.06232624500989914
Training batch 7 / 32
Total batch reconstruction loss: 0.057948507368564606
Training batch 8 / 32
Total batch reconstruction loss: 0.06504328548908234
Training batch 9 / 32
Total batch reconstruction loss: 0.06644278764724731
Training batch 10 / 32
Total batch reconstruction loss: 0.05923746898770332
Training batch 11 / 32
Total batch reconstruction loss: 0.062270764261484146
Training batch 12 / 32
Total batch reconstruction loss: 0.06371712684631348
Training batch 13 / 32
Total batch reconstruction loss: 0.0606156624853611
Training batch 14 / 32
Total batch reconstruction loss: 0.06696668267250061
Training batch 15 / 32
Total batch reconstruction loss: 0.06325918436050415
Training batch 16 / 32
Total batch reconstruction loss: 0.061198968440294266
Training batch 17 / 32
Total batch reconstruction loss: 0.06417255103588104
Training batch 18 / 32
Total batch reconstruction loss: 0.06102601811289787
Training batch 19 / 32
Total batch reconstruction loss: 0.06145436689257622
Training batch 20 / 32
Total batch reconstruction loss: 0.06343945860862732
Training batch 21 / 32
Total batch reconstruction loss: 0.060071516782045364
Training batch 22 / 32
Total batch reconstruction loss: 0.058616116642951965
Training batch 23 / 32
Total batch reconstruction loss: 0.055359628051519394
Training batch 24 / 32
Total batch reconstruction loss: 0.058880023658275604
Training batch 25 / 32
Total batch reconstruction loss: 0.06352390348911285
Training batch 26 / 32
Total batch reconstruction loss: 0.06117790937423706
Training batch 27 / 32
Total batch reconstruction loss: 0.05898056551814079
Training batch 28 / 32
Total batch reconstruction loss: 0.061967939138412476
Training batch 29 / 32
Total batch reconstruction loss: 0.06254176050424576
Training batch 30 / 32
Total batch reconstruction loss: 0.06413016468286514
Training batch 31 / 32
Total batch reconstruction loss: 0.057957135140895844
Training batch 32 / 32
Total batch reconstruction loss: 0.05830885097384453
Epoch [120/500], Train Loss: 0.0611, Validation Loss: 0.0627, Generator Loss: 12.4086, Discriminator Loss: 0.3119
Training epoch 121 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.05825543403625488
Training batch 2 / 32
Total batch reconstruction loss: 0.05950653553009033
Training batch 3 / 32
Total batch reconstruction loss: 0.06966491043567657
Training batch 4 / 32
Total batch reconstruction loss: 0.06677251309156418
Training batch 5 / 32
Total batch reconstruction loss: 0.060317639261484146
Training batch 6 / 32
Total batch reconstruction loss: 0.06010257452726364
Training batch 7 / 32
Total batch reconstruction loss: 0.06312455236911774
Training batch 8 / 32
Total batch reconstruction loss: 0.06233593821525574
Training batch 9 / 32
Total batch reconstruction loss: 0.05797483026981354
Training batch 10 / 32
Total batch reconstruction loss: 0.06332455575466156
Training batch 11 / 32
Total batch reconstruction loss: 0.05924443155527115
Training batch 12 / 32
Total batch reconstruction loss: 0.06072262302041054
Training batch 13 / 32
Total batch reconstruction loss: 0.06428031623363495
Training batch 14 / 32
Total batch reconstruction loss: 0.06420038640499115
Training batch 15 / 32
Total batch reconstruction loss: 0.06334906816482544
Training batch 16 / 32
Total batch reconstruction loss: 0.05722305178642273
Training batch 17 / 32
Total batch reconstruction loss: 0.06367823481559753
Training batch 18 / 32
Total batch reconstruction loss: 0.06121699884533882
Training batch 19 / 32
Total batch reconstruction loss: 0.060894593596458435
Training batch 20 / 32
Total batch reconstruction loss: 0.06309740245342255
Training batch 21 / 32
Total batch reconstruction loss: 0.0603162981569767
Training batch 22 / 32
Total batch reconstruction loss: 0.059517476707696915
Training batch 23 / 32
Total batch reconstruction loss: 0.06048908084630966
Training batch 24 / 32
Total batch reconstruction loss: 0.06255513429641724
Training batch 25 / 32
Total batch reconstruction loss: 0.05981991067528725
Training batch 26 / 32
Total batch reconstruction loss: 0.057560473680496216
Training batch 27 / 32
Total batch reconstruction loss: 0.06177768111228943
Training batch 28 / 32
Total batch reconstruction loss: 0.0586305633187294
Training batch 29 / 32
Total batch reconstruction loss: 0.06130361557006836
Training batch 30 / 32
Total batch reconstruction loss: 0.0626109316945076
Training batch 31 / 32
Total batch reconstruction loss: 0.06617800891399384
Training batch 32 / 32
Total batch reconstruction loss: 0.05342113599181175
Epoch [121/500], Train Loss: 0.0610, Validation Loss: 0.0622, Generator Loss: 12.3539, Discriminator Loss: 0.3014
Training epoch 122 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.06031373515725136
Training batch 2 / 32
Total batch reconstruction loss: 0.05837719142436981
Training batch 3 / 32
Total batch reconstruction loss: 0.06714948266744614
Training batch 4 / 32
Total batch reconstruction loss: 0.060795657336711884
Training batch 5 / 32
Total batch reconstruction loss: 0.06361723691225052
Training batch 6 / 32
Total batch reconstruction loss: 0.06166083365678787
Training batch 7 / 32
Total batch reconstruction loss: 0.06366202235221863
Training batch 8 / 32
Total batch reconstruction loss: 0.06838209927082062
Training batch 9 / 32
Total batch reconstruction loss: 0.05785630643367767
Training batch 10 / 32
Total batch reconstruction loss: 0.05838563293218613
Training batch 11 / 32
Total batch reconstruction loss: 0.0656207799911499
Training batch 12 / 32
Total batch reconstruction loss: 0.06707549095153809
Training batch 13 / 32
Total batch reconstruction loss: 0.06060447543859482
Training batch 14 / 32
Total batch reconstruction loss: 0.06082477420568466
Training batch 15 / 32
Total batch reconstruction loss: 0.06267382204532623
Training batch 16 / 32
Total batch reconstruction loss: 0.0612478107213974
Training batch 17 / 32
Total batch reconstruction loss: 0.05819933861494064
Training batch 18 / 32
Total batch reconstruction loss: 0.06043269485235214
Training batch 19 / 32
Total batch reconstruction loss: 0.06275381147861481
Training batch 20 / 32
Total batch reconstruction loss: 0.06243851035833359
Training batch 21 / 32
Total batch reconstruction loss: 0.06172989308834076
Training batch 22 / 32
Total batch reconstruction loss: 0.0584736131131649
Training batch 23 / 32
Total batch reconstruction loss: 0.06052479147911072
Training batch 24 / 32
Total batch reconstruction loss: 0.059643782675266266
Training batch 25 / 32
Total batch reconstruction loss: 0.06345440447330475
Training batch 26 / 32
Total batch reconstruction loss: 0.06733854115009308
Training batch 27 / 32
Total batch reconstruction loss: 0.06163305789232254
Training batch 28 / 32
Total batch reconstruction loss: 0.06127207726240158
Training batch 29 / 32
Total batch reconstruction loss: 0.05984252318739891
Training batch 30 / 32
Total batch reconstruction loss: 0.05599133297801018
Training batch 31 / 32
Total batch reconstruction loss: 0.06084248796105385
Training batch 32 / 32
Total batch reconstruction loss: 0.0556127205491066
Epoch [122/500], Train Loss: 0.0613, Validation Loss: 0.0605, Generator Loss: 12.3731, Discriminator Loss: 0.3153
Training epoch 123 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.06031612306833267
Training batch 2 / 32
Total batch reconstruction loss: 0.06169024109840393
Training batch 3 / 32
Total batch reconstruction loss: 0.06106875464320183
Training batch 4 / 32
Total batch reconstruction loss: 0.06460300087928772
Training batch 5 / 32
Total batch reconstruction loss: 0.06491579115390778
Training batch 6 / 32
Total batch reconstruction loss: 0.0625767707824707
Training batch 7 / 32
Total batch reconstruction loss: 0.0607457160949707
Training batch 8 / 32
Total batch reconstruction loss: 0.059955235570669174
Training batch 9 / 32
Total batch reconstruction loss: 0.05976080149412155
Training batch 10 / 32
Total batch reconstruction loss: 0.06322085112333298
Training batch 11 / 32
Total batch reconstruction loss: 0.058696597814559937
Training batch 12 / 32
Total batch reconstruction loss: 0.056923456490039825
Training batch 13 / 32
Total batch reconstruction loss: 0.062440671026706696
Training batch 14 / 32
Total batch reconstruction loss: 0.0630979835987091
Training batch 15 / 32
Total batch reconstruction loss: 0.05827528238296509
Training batch 16 / 32
Total batch reconstruction loss: 0.061544161289930344
Training batch 17 / 32
Total batch reconstruction loss: 0.059864409267902374
Training batch 18 / 32
Total batch reconstruction loss: 0.05874937027692795
Training batch 19 / 32
Total batch reconstruction loss: 0.06360390037298203
Training batch 20 / 32
Total batch reconstruction loss: 0.06503454595804214
Training batch 21 / 32
Total batch reconstruction loss: 0.06139066815376282
Training batch 22 / 32
Total batch reconstruction loss: 0.05871449410915375
Training batch 23 / 32
Total batch reconstruction loss: 0.06411422789096832
Training batch 24 / 32
Total batch reconstruction loss: 0.05829690769314766
Training batch 25 / 32
Total batch reconstruction loss: 0.05717149376869202
Training batch 26 / 32
Total batch reconstruction loss: 0.05960202217102051
Training batch 27 / 32
Total batch reconstruction loss: 0.06691624224185944
Training batch 28 / 32
Total batch reconstruction loss: 0.05895053595304489
Training batch 29 / 32
Total batch reconstruction loss: 0.0593690425157547
Training batch 30 / 32
Total batch reconstruction loss: 0.06593143939971924
Training batch 31 / 32
Total batch reconstruction loss: 0.0677298828959465
Training batch 32 / 32
Total batch reconstruction loss: 0.061817020177841187
Epoch [123/500], Train Loss: 0.0611, Validation Loss: 0.0619, Generator Loss: 12.3585, Discriminator Loss: 0.3255
Training epoch 124 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.05768989771604538
Training batch 2 / 32
Total batch reconstruction loss: 0.0570264607667923
Training batch 3 / 32
Total batch reconstruction loss: 0.0629921704530716
Training batch 4 / 32
Total batch reconstruction loss: 0.06291943043470383
Training batch 5 / 32
Total batch reconstruction loss: 0.05820518732070923
Training batch 6 / 32
Total batch reconstruction loss: 0.06062847375869751
Training batch 7 / 32
Total batch reconstruction loss: 0.06180962547659874
Training batch 8 / 32
Total batch reconstruction loss: 0.06233092024922371
Training batch 9 / 32
Total batch reconstruction loss: 0.06299145519733429
Training batch 10 / 32
Total batch reconstruction loss: 0.05852071940898895
Training batch 11 / 32
Total batch reconstruction loss: 0.0654863566160202
Training batch 12 / 32
Total batch reconstruction loss: 0.06471936404705048
Training batch 13 / 32
Total batch reconstruction loss: 0.05761055275797844
Training batch 14 / 32
Total batch reconstruction loss: 0.0609702542424202
Training batch 15 / 32
Total batch reconstruction loss: 0.06629715859889984
Training batch 16 / 32
Total batch reconstruction loss: 0.0613248273730278
Training batch 17 / 32
Total batch reconstruction loss: 0.059839434921741486
Training batch 18 / 32
Total batch reconstruction loss: 0.05836723744869232
Training batch 19 / 32
Total batch reconstruction loss: 0.06432849168777466
Training batch 20 / 32
Total batch reconstruction loss: 0.06570148468017578
Training batch 21 / 32
Total batch reconstruction loss: 0.06225186958909035
Training batch 22 / 32
Total batch reconstruction loss: 0.06449452042579651
Training batch 23 / 32
Total batch reconstruction loss: 0.05952727049589157
Training batch 24 / 32
Total batch reconstruction loss: 0.06252484023571014
Training batch 25 / 32
Total batch reconstruction loss: 0.06437823921442032
Training batch 26 / 32
Total batch reconstruction loss: 0.06092124432325363
Training batch 27 / 32
Total batch reconstruction loss: 0.059230439364910126
Training batch 28 / 32
Total batch reconstruction loss: 0.05572720617055893
Training batch 29 / 32
Total batch reconstruction loss: 0.06521491706371307
Training batch 30 / 32
Total batch reconstruction loss: 0.06275340914726257
Training batch 31 / 32
Total batch reconstruction loss: 0.06512323766946793
Training batch 32 / 32
Total batch reconstruction loss: 0.06012691929936409
Epoch [124/500], Train Loss: 0.0612, Validation Loss: 0.0601, Generator Loss: 12.4044, Discriminator Loss: 0.3055
Training epoch 125 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.06115621700882912
Training batch 2 / 32
Total batch reconstruction loss: 0.06367595493793488
Training batch 3 / 32
Total batch reconstruction loss: 0.06160597503185272
Training batch 4 / 32
Total batch reconstruction loss: 0.05735067278146744
Training batch 5 / 32
Total batch reconstruction loss: 0.05942434072494507
Training batch 6 / 32
Total batch reconstruction loss: 0.06376933306455612
Training batch 7 / 32
Total batch reconstruction loss: 0.06074889749288559
Training batch 8 / 32
Total batch reconstruction loss: 0.06181417405605316
Training batch 9 / 32
Total batch reconstruction loss: 0.06009005010128021
Training batch 10 / 32
Total batch reconstruction loss: 0.06423899531364441
Training batch 11 / 32
Total batch reconstruction loss: 0.06388180702924728
Training batch 12 / 32
Total batch reconstruction loss: 0.06354908645153046
Training batch 13 / 32
Total batch reconstruction loss: 0.06354174017906189
Training batch 14 / 32
Total batch reconstruction loss: 0.06214047968387604
Training batch 15 / 32
Total batch reconstruction loss: 0.05908406525850296
Training batch 16 / 32
Total batch reconstruction loss: 0.06025121361017227
Training batch 17 / 32
Total batch reconstruction loss: 0.06557179242372513
Training batch 18 / 32
Total batch reconstruction loss: 0.06292474269866943
Training batch 19 / 32
Total batch reconstruction loss: 0.06120908260345459
Training batch 20 / 32
Total batch reconstruction loss: 0.0667375922203064
Training batch 21 / 32
Total batch reconstruction loss: 0.06152934953570366
Training batch 22 / 32
Total batch reconstruction loss: 0.05911753326654434
Training batch 23 / 32
Total batch reconstruction loss: 0.0566793754696846
Training batch 24 / 32
Total batch reconstruction loss: 0.060724422335624695
Training batch 25 / 32
Total batch reconstruction loss: 0.06222802400588989
Training batch 26 / 32
Total batch reconstruction loss: 0.058990299701690674
Training batch 27 / 32
Total batch reconstruction loss: 0.05957997590303421
Training batch 28 / 32
Total batch reconstruction loss: 0.060918595641851425
Training batch 29 / 32
Total batch reconstruction loss: 0.060773685574531555
Training batch 30 / 32
Total batch reconstruction loss: 0.06155684217810631
Training batch 31 / 32
Total batch reconstruction loss: 0.05961041897535324
Training batch 32 / 32
Total batch reconstruction loss: 0.05239518731832504
Epoch [125/500], Train Loss: 0.0610, Validation Loss: 0.0622, Generator Loss: 12.2987, Discriminator Loss: 0.3169
Training epoch 126 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.05887403339147568
Training batch 2 / 32
Total batch reconstruction loss: 0.0641930028796196
Training batch 3 / 32
Total batch reconstruction loss: 0.06951497495174408
Training batch 4 / 32
Total batch reconstruction loss: 0.06246396154165268
Training batch 5 / 32
Total batch reconstruction loss: 0.063942551612854
Training batch 6 / 32
Total batch reconstruction loss: 0.0645654946565628
Training batch 7 / 32
Total batch reconstruction loss: 0.06072738766670227
Training batch 8 / 32
Total batch reconstruction loss: 0.0626562237739563
Training batch 9 / 32
Total batch reconstruction loss: 0.06160854548215866
Training batch 10 / 32
Total batch reconstruction loss: 0.063079833984375
Training batch 11 / 32
Total batch reconstruction loss: 0.06126101315021515
Training batch 12 / 32
Total batch reconstruction loss: 0.06179920583963394
Training batch 13 / 32
Total batch reconstruction loss: 0.05980876833200455
Training batch 14 / 32
Total batch reconstruction loss: 0.060321416705846786
Training batch 15 / 32
Total batch reconstruction loss: 0.05840254947543144
Training batch 16 / 32
Total batch reconstruction loss: 0.06263896077871323
Training batch 17 / 32
Total batch reconstruction loss: 0.06098632141947746
Training batch 18 / 32
Total batch reconstruction loss: 0.06252077966928482
Training batch 19 / 32
Total batch reconstruction loss: 0.06485869735479355
Training batch 20 / 32
Total batch reconstruction loss: 0.06223830208182335
Training batch 21 / 32
Total batch reconstruction loss: 0.05940108746290207
Training batch 22 / 32
Total batch reconstruction loss: 0.061906859278678894
Training batch 23 / 32
Total batch reconstruction loss: 0.05974211171269417
Training batch 24 / 32
Total batch reconstruction loss: 0.05883675441145897
Training batch 25 / 32
Total batch reconstruction loss: 0.059583090245723724
Training batch 26 / 32
Total batch reconstruction loss: 0.06210552155971527
Training batch 27 / 32
Total batch reconstruction loss: 0.06173064559698105
Training batch 28 / 32
Total batch reconstruction loss: 0.05951323360204697
Training batch 29 / 32
Total batch reconstruction loss: 0.06006897985935211
Training batch 30 / 32
Total batch reconstruction loss: 0.05988703668117523
Training batch 31 / 32
Total batch reconstruction loss: 0.06399887055158615
Training batch 32 / 32
Total batch reconstruction loss: 0.0526505783200264
Epoch [126/500], Train Loss: 0.0612, Validation Loss: 0.0614, Generator Loss: 12.3593, Discriminator Loss: 0.3147
Training epoch 127 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.0561334490776062
Training batch 2 / 32
Total batch reconstruction loss: 0.05755535513162613
Training batch 3 / 32
Total batch reconstruction loss: 0.06250718235969543
Training batch 4 / 32
Total batch reconstruction loss: 0.05885958671569824
Training batch 5 / 32
Total batch reconstruction loss: 0.05890314280986786
Training batch 6 / 32
Total batch reconstruction loss: 0.0653945654630661
Training batch 7 / 32
Total batch reconstruction loss: 0.05874095857143402
Training batch 8 / 32
Total batch reconstruction loss: 0.061007410287857056
Training batch 9 / 32
Total batch reconstruction loss: 0.06137343496084213
Training batch 10 / 32
Total batch reconstruction loss: 0.06257519125938416
Training batch 11 / 32
Total batch reconstruction loss: 0.06137692928314209
Training batch 12 / 32
Total batch reconstruction loss: 0.06461775302886963
Training batch 13 / 32
Total batch reconstruction loss: 0.06225060671567917
Training batch 14 / 32
Total batch reconstruction loss: 0.06469917297363281
Training batch 15 / 32
Total batch reconstruction loss: 0.05563712120056152
Training batch 16 / 32
Total batch reconstruction loss: 0.0645214319229126
Training batch 17 / 32
Total batch reconstruction loss: 0.06196741759777069
Training batch 18 / 32
Total batch reconstruction loss: 0.05835764482617378
Training batch 19 / 32
Total batch reconstruction loss: 0.06291686743497849
Training batch 20 / 32
Total batch reconstruction loss: 0.06080302596092224
Training batch 21 / 32
Total batch reconstruction loss: 0.05713912844657898
Training batch 22 / 32
Total batch reconstruction loss: 0.06320397555828094
Training batch 23 / 32
Total batch reconstruction loss: 0.061265379190444946
Training batch 24 / 32
Total batch reconstruction loss: 0.0643761083483696
Training batch 25 / 32
Total batch reconstruction loss: 0.06016296520829201
Training batch 26 / 32
Total batch reconstruction loss: 0.06378580629825592
Training batch 27 / 32
Total batch reconstruction loss: 0.061779532581567764
Training batch 28 / 32
Total batch reconstruction loss: 0.06313753128051758
Training batch 29 / 32
Total batch reconstruction loss: 0.05604277923703194
Training batch 30 / 32
Total batch reconstruction loss: 0.061583079397678375
Training batch 31 / 32
Total batch reconstruction loss: 0.0639566034078598
Training batch 32 / 32
Total batch reconstruction loss: 0.049670424312353134
Epoch [127/500], Train Loss: 0.0605, Validation Loss: 0.0627, Generator Loss: 12.2385, Discriminator Loss: 0.3185
Training epoch 128 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.06080237030982971
Training batch 2 / 32
Total batch reconstruction loss: 0.062015119940042496
Training batch 3 / 32
Total batch reconstruction loss: 0.05963543429970741
Training batch 4 / 32
Total batch reconstruction loss: 0.0608673095703125
Training batch 5 / 32
Total batch reconstruction loss: 0.06017477437853813
Training batch 6 / 32
Total batch reconstruction loss: 0.060287512838840485
Training batch 7 / 32
Total batch reconstruction loss: 0.06705233454704285
Training batch 8 / 32
Total batch reconstruction loss: 0.06304175406694412
Training batch 9 / 32
Total batch reconstruction loss: 0.06005013361573219
Training batch 10 / 32
Total batch reconstruction loss: 0.05979876592755318
Training batch 11 / 32
Total batch reconstruction loss: 0.06854553520679474
Training batch 12 / 32
Total batch reconstruction loss: 0.05942033231258392
Training batch 13 / 32
Total batch reconstruction loss: 0.060136403888463974
Training batch 14 / 32
Total batch reconstruction loss: 0.059358809143304825
Training batch 15 / 32
Total batch reconstruction loss: 0.06395532935857773
Training batch 16 / 32
Total batch reconstruction loss: 0.05928214266896248
Training batch 17 / 32
Total batch reconstruction loss: 0.06479422003030777
Training batch 18 / 32
Total batch reconstruction loss: 0.05730627104640007
Training batch 19 / 32
Total batch reconstruction loss: 0.06270770728588104
Training batch 20 / 32
Total batch reconstruction loss: 0.05982789769768715
Training batch 21 / 32
Total batch reconstruction loss: 0.06019621342420578
Training batch 22 / 32
Total batch reconstruction loss: 0.058384135365486145
Training batch 23 / 32
Total batch reconstruction loss: 0.05978813022375107
Training batch 24 / 32
Total batch reconstruction loss: 0.06307615339756012
Training batch 25 / 32
Total batch reconstruction loss: 0.06278522312641144
Training batch 26 / 32
Total batch reconstruction loss: 0.06095409765839577
Training batch 27 / 32
Total batch reconstruction loss: 0.05748404562473297
Training batch 28 / 32
Total batch reconstruction loss: 0.0631062313914299
Training batch 29 / 32
Total batch reconstruction loss: 0.0608569011092186
Training batch 30 / 32
Total batch reconstruction loss: 0.06258067488670349
Training batch 31 / 32
Total batch reconstruction loss: 0.05847388878464699
Training batch 32 / 32
Total batch reconstruction loss: 0.05808287858963013
Epoch [128/500], Train Loss: 0.0609, Validation Loss: 0.0617, Generator Loss: 12.2807, Discriminator Loss: 0.3304
Training epoch 129 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.059384655207395554
Training batch 2 / 32
Total batch reconstruction loss: 0.06062138080596924
Training batch 3 / 32
Total batch reconstruction loss: 0.05896170437335968
Training batch 4 / 32
Total batch reconstruction loss: 0.06001169979572296
Training batch 5 / 32
Total batch reconstruction loss: 0.062432270497083664
Training batch 6 / 32
Total batch reconstruction loss: 0.06044430658221245
Training batch 7 / 32
Total batch reconstruction loss: 0.0637102946639061
Training batch 8 / 32
Total batch reconstruction loss: 0.06584775447845459
Training batch 9 / 32
Total batch reconstruction loss: 0.06516280770301819
Training batch 10 / 32
Total batch reconstruction loss: 0.06039193272590637
Training batch 11 / 32
Total batch reconstruction loss: 0.06010986119508743
Training batch 12 / 32
Total batch reconstruction loss: 0.060151439160108566
Training batch 13 / 32
Total batch reconstruction loss: 0.06626907736063004
Training batch 14 / 32
Total batch reconstruction loss: 0.07048093527555466
Training batch 15 / 32
Total batch reconstruction loss: 0.05630464106798172
Training batch 16 / 32
Total batch reconstruction loss: 0.06114516779780388
Training batch 17 / 32
Total batch reconstruction loss: 0.05934866517782211
Training batch 18 / 32
Total batch reconstruction loss: 0.06293587386608124
Training batch 19 / 32
Total batch reconstruction loss: 0.0622958168387413
Training batch 20 / 32
Total batch reconstruction loss: 0.05998390540480614
Training batch 21 / 32
Total batch reconstruction loss: 0.0609562061727047
Training batch 22 / 32
Total batch reconstruction loss: 0.06088263541460037
Training batch 23 / 32
Total batch reconstruction loss: 0.061211034655570984
Training batch 24 / 32
Total batch reconstruction loss: 0.06094639375805855
Training batch 25 / 32
Total batch reconstruction loss: 0.06058911606669426
Training batch 26 / 32
Total batch reconstruction loss: 0.06068088486790657
Training batch 27 / 32
Total batch reconstruction loss: 0.05991896986961365
Training batch 28 / 32
Total batch reconstruction loss: 0.05852854251861572
Training batch 29 / 32
Total batch reconstruction loss: 0.06106703355908394
Training batch 30 / 32
Total batch reconstruction loss: 0.062289390712976456
Training batch 31 / 32
Total batch reconstruction loss: 0.05720458924770355
Training batch 32 / 32
Total batch reconstruction loss: 0.056373633444309235
Epoch [129/500], Train Loss: 0.0607, Validation Loss: 0.0620, Generator Loss: 12.3091, Discriminator Loss: 0.3127
Training epoch 130 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.05913683399558067
Training batch 2 / 32
Total batch reconstruction loss: 0.06250149011611938
Training batch 3 / 32
Total batch reconstruction loss: 0.05640745535492897
Training batch 4 / 32
Total batch reconstruction loss: 0.06182533875107765
Training batch 5 / 32
Total batch reconstruction loss: 0.05746202543377876
Training batch 6 / 32
Total batch reconstruction loss: 0.05954689532518387
Training batch 7 / 32
Total batch reconstruction loss: 0.0645807757973671
Training batch 8 / 32
Total batch reconstruction loss: 0.05776901915669441
Training batch 9 / 32
Total batch reconstruction loss: 0.06334763020277023
Training batch 10 / 32
Total batch reconstruction loss: 0.06144118309020996
Training batch 11 / 32
Total batch reconstruction loss: 0.062045514583587646
Training batch 12 / 32
Total batch reconstruction loss: 0.058654509484767914
Training batch 13 / 32
Total batch reconstruction loss: 0.062247760593891144
Training batch 14 / 32
Total batch reconstruction loss: 0.06127511337399483
Training batch 15 / 32
Total batch reconstruction loss: 0.06242925673723221
Training batch 16 / 32
Total batch reconstruction loss: 0.06185057386755943
Training batch 17 / 32
Total batch reconstruction loss: 0.07566326856613159
Training batch 18 / 32
Total batch reconstruction loss: 0.05770155042409897
Training batch 19 / 32
Total batch reconstruction loss: 0.06181666627526283
Training batch 20 / 32
Total batch reconstruction loss: 0.06262629479169846
Training batch 21 / 32
Total batch reconstruction loss: 0.055212900042533875
Training batch 22 / 32
Total batch reconstruction loss: 0.06242084875702858
Training batch 23 / 32
Total batch reconstruction loss: 0.06247154623270035
Training batch 24 / 32
Total batch reconstruction loss: 0.06373284757137299
Training batch 25 / 32
Total batch reconstruction loss: 0.06262397021055222
Training batch 26 / 32
Total batch reconstruction loss: 0.06382927298545837
Training batch 27 / 32
Total batch reconstruction loss: 0.062651127576828
Training batch 28 / 32
Total batch reconstruction loss: 0.06075909361243248
Training batch 29 / 32
Total batch reconstruction loss: 0.06479139626026154
Training batch 30 / 32
Total batch reconstruction loss: 0.06302151083946228
Training batch 31 / 32
Total batch reconstruction loss: 0.059725597500801086
Training batch 32 / 32
Total batch reconstruction loss: 0.06706906855106354
Epoch [130/500], Train Loss: 0.0621, Validation Loss: 0.0620, Generator Loss: 12.4411, Discriminator Loss: 0.3139
Training epoch 131 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.06367737054824829
Training batch 2 / 32
Total batch reconstruction loss: 0.0614314042031765
Training batch 3 / 32
Total batch reconstruction loss: 0.06370461732149124
Training batch 4 / 32
Total batch reconstruction loss: 0.060809437185525894
Training batch 5 / 32
Total batch reconstruction loss: 0.06103668734431267
Training batch 6 / 32
Total batch reconstruction loss: 0.05835231393575668
Training batch 7 / 32
Total batch reconstruction loss: 0.06188756600022316
Training batch 8 / 32
Total batch reconstruction loss: 0.06185567378997803
Training batch 9 / 32
Total batch reconstruction loss: 0.058730922639369965
Training batch 10 / 32
Total batch reconstruction loss: 0.06363824754953384
Training batch 11 / 32
Total batch reconstruction loss: 0.06285233795642853
Training batch 12 / 32
Total batch reconstruction loss: 0.06607212126255035
Training batch 13 / 32
Total batch reconstruction loss: 0.06204071640968323
Training batch 14 / 32
Total batch reconstruction loss: 0.06219794228672981
Training batch 15 / 32
Total batch reconstruction loss: 0.06450149416923523
Training batch 16 / 32
Total batch reconstruction loss: 0.059272684156894684
Training batch 17 / 32
Total batch reconstruction loss: 0.06166527047753334
Training batch 18 / 32
Total batch reconstruction loss: 0.0629633441567421
Training batch 19 / 32
Total batch reconstruction loss: 0.06306933611631393
Training batch 20 / 32
Total batch reconstruction loss: 0.058667927980422974
Training batch 21 / 32
Total batch reconstruction loss: 0.05877884477376938
Training batch 22 / 32
Total batch reconstruction loss: 0.06410893052816391
Training batch 23 / 32
Total batch reconstruction loss: 0.07245036214590073
Training batch 24 / 32
Total batch reconstruction loss: 0.06076446548104286
Training batch 25 / 32
Total batch reconstruction loss: 0.05625053495168686
Training batch 26 / 32
Total batch reconstruction loss: 0.0558256059885025
Training batch 27 / 32
Total batch reconstruction loss: 0.06150899827480316
Training batch 28 / 32
Total batch reconstruction loss: 0.059606727212667465
Training batch 29 / 32
Total batch reconstruction loss: 0.061397332698106766
Training batch 30 / 32
Total batch reconstruction loss: 0.06693192571401596
Training batch 31 / 32
Total batch reconstruction loss: 0.06059213727712631
Training batch 32 / 32
Total batch reconstruction loss: 0.06183185428380966
Epoch [131/500], Train Loss: 0.0617, Validation Loss: 0.0614, Generator Loss: 12.4215, Discriminator Loss: 0.3347
Training epoch 132 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.06523658335208893
Training batch 2 / 32
Total batch reconstruction loss: 0.06977225095033646
Training batch 3 / 32
Total batch reconstruction loss: 0.05587761104106903
Training batch 4 / 32
Total batch reconstruction loss: 0.06234864518046379
Training batch 5 / 32
Total batch reconstruction loss: 0.0653952807188034
Training batch 6 / 32
Total batch reconstruction loss: 0.06288675218820572
Training batch 7 / 32
Total batch reconstruction loss: 0.05859716236591339
Training batch 8 / 32
Total batch reconstruction loss: 0.06892095506191254
Training batch 9 / 32
Total batch reconstruction loss: 0.05567578226327896
Training batch 10 / 32
Total batch reconstruction loss: 0.0565081462264061
Training batch 11 / 32
Total batch reconstruction loss: 0.05594282224774361
Training batch 12 / 32
Total batch reconstruction loss: 0.0611409917473793
Training batch 13 / 32
Total batch reconstruction loss: 0.06418497860431671
Training batch 14 / 32
Total batch reconstruction loss: 0.05734565854072571
Training batch 15 / 32
Total batch reconstruction loss: 0.06123283505439758
Training batch 16 / 32
Total batch reconstruction loss: 0.05896686390042305
Training batch 17 / 32
Total batch reconstruction loss: 0.06020890921354294
Training batch 18 / 32
Total batch reconstruction loss: 0.0582667775452137
Training batch 19 / 32
Total batch reconstruction loss: 0.061159394681453705
Training batch 20 / 32
Total batch reconstruction loss: 0.06123709678649902
Training batch 21 / 32
Total batch reconstruction loss: 0.06218566372990608
Training batch 22 / 32
Total batch reconstruction loss: 0.061763957142829895
Training batch 23 / 32
Total batch reconstruction loss: 0.061164382845163345
Training batch 24 / 32
Total batch reconstruction loss: 0.060556598007678986
Training batch 25 / 32
Total batch reconstruction loss: 0.06532928347587585
Training batch 26 / 32
Total batch reconstruction loss: 0.066502645611763
Training batch 27 / 32
Total batch reconstruction loss: 0.060053884983062744
Training batch 28 / 32
Total batch reconstruction loss: 0.06258292496204376
Training batch 29 / 32
Total batch reconstruction loss: 0.06222911179065704
Training batch 30 / 32
Total batch reconstruction loss: 0.06480272859334946
Training batch 31 / 32
Total batch reconstruction loss: 0.06361578404903412
Training batch 32 / 32
Total batch reconstruction loss: 0.05112527310848236
Epoch [132/500], Train Loss: 0.0614, Validation Loss: 0.0618, Generator Loss: 12.3292, Discriminator Loss: 0.3272
Training epoch 133 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.05867210030555725
Training batch 2 / 32
Total batch reconstruction loss: 0.06463804841041565
Training batch 3 / 32
Total batch reconstruction loss: 0.06694809347391129
Training batch 4 / 32
Total batch reconstruction loss: 0.06041146069765091
Training batch 5 / 32
Total batch reconstruction loss: 0.06185194477438927
Training batch 6 / 32
Total batch reconstruction loss: 0.0674716979265213
Training batch 7 / 32
Total batch reconstruction loss: 0.0710097998380661
Training batch 8 / 32
Total batch reconstruction loss: 0.05877532809972763
Training batch 9 / 32
Total batch reconstruction loss: 0.06298574060201645
Training batch 10 / 32
Total batch reconstruction loss: 0.06535795331001282
Training batch 11 / 32
Total batch reconstruction loss: 0.06128814443945885
Training batch 12 / 32
Total batch reconstruction loss: 0.06318571418523788
Training batch 13 / 32
Total batch reconstruction loss: 0.05835847184062004
Training batch 14 / 32
Total batch reconstruction loss: 0.060959141701459885
Training batch 15 / 32
Total batch reconstruction loss: 0.06766381114721298
Training batch 16 / 32
Total batch reconstruction loss: 0.05896195024251938
Training batch 17 / 32
Total batch reconstruction loss: 0.05694108456373215
Training batch 18 / 32
Total batch reconstruction loss: 0.06437849998474121
Training batch 19 / 32
Total batch reconstruction loss: 0.061233095824718475
Training batch 20 / 32
Total batch reconstruction loss: 0.06152525544166565
Training batch 21 / 32
Total batch reconstruction loss: 0.06669777631759644
Training batch 22 / 32
Total batch reconstruction loss: 0.06085605546832085
Training batch 23 / 32
Total batch reconstruction loss: 0.06374219805002213
Training batch 24 / 32
Total batch reconstruction loss: 0.056881967931985855
Training batch 25 / 32
Total batch reconstruction loss: 0.06063780188560486
Training batch 26 / 32
Total batch reconstruction loss: 0.059964947402477264
Training batch 27 / 32
Total batch reconstruction loss: 0.06161000579595566
Training batch 28 / 32
Total batch reconstruction loss: 0.06000085920095444
Training batch 29 / 32
Total batch reconstruction loss: 0.061162084341049194
Training batch 30 / 32
Total batch reconstruction loss: 0.05946490168571472
Training batch 31 / 32
Total batch reconstruction loss: 0.061147332191467285
Training batch 32 / 32
Total batch reconstruction loss: 0.06274200230836868
Epoch [133/500], Train Loss: 0.0614, Validation Loss: 0.0622, Generator Loss: 12.5026, Discriminator Loss: 0.3077
Training epoch 134 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.0605727881193161
Training batch 2 / 32
Total batch reconstruction loss: 0.0569562092423439
Training batch 3 / 32
Total batch reconstruction loss: 0.06096503883600235
Training batch 4 / 32
Total batch reconstruction loss: 0.05944199115037918
Training batch 5 / 32
Total batch reconstruction loss: 0.06067400053143501
Training batch 6 / 32
Total batch reconstruction loss: 0.06113557517528534
Training batch 7 / 32
Total batch reconstruction loss: 0.06083172187209129
Training batch 8 / 32
Total batch reconstruction loss: 0.06359830498695374
Training batch 9 / 32
Total batch reconstruction loss: 0.06279002130031586
Training batch 10 / 32
Total batch reconstruction loss: 0.0627957135438919
Training batch 11 / 32
Total batch reconstruction loss: 0.06341101229190826
Training batch 12 / 32
Total batch reconstruction loss: 0.06077469885349274
Training batch 13 / 32
Total batch reconstruction loss: 0.06450842320919037
Training batch 14 / 32
Total batch reconstruction loss: 0.062352389097213745
Training batch 15 / 32
Total batch reconstruction loss: 0.06164509057998657
Training batch 16 / 32
Total batch reconstruction loss: 0.060303617268800735
Training batch 17 / 32
Total batch reconstruction loss: 0.059580713510513306
Training batch 18 / 32
Total batch reconstruction loss: 0.06761427223682404
Training batch 19 / 32
Total batch reconstruction loss: 0.05708140879869461
Training batch 20 / 32
Total batch reconstruction loss: 0.0672634094953537
Training batch 21 / 32
Total batch reconstruction loss: 0.05768437683582306
Training batch 22 / 32
Total batch reconstruction loss: 0.059980813413858414
Training batch 23 / 32
Total batch reconstruction loss: 0.06036002188920975
Training batch 24 / 32
Total batch reconstruction loss: 0.06230906769633293
Training batch 25 / 32
Total batch reconstruction loss: 0.0606493279337883
Training batch 26 / 32
Total batch reconstruction loss: 0.06608887016773224
Training batch 27 / 32
Total batch reconstruction loss: 0.06039169430732727
Training batch 28 / 32
Total batch reconstruction loss: 0.05961337313055992
Training batch 29 / 32
Total batch reconstruction loss: 0.06642266362905502
Training batch 30 / 32
Total batch reconstruction loss: 0.05818134546279907
Training batch 31 / 32
Total batch reconstruction loss: 0.060238130390644073
Training batch 32 / 32
Total batch reconstruction loss: 0.09766288101673126
Epoch [134/500], Train Loss: 0.0618, Validation Loss: 0.0641, Generator Loss: 12.6076, Discriminator Loss: 0.3034
Training epoch 135 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.058977775275707245
Training batch 2 / 32
Total batch reconstruction loss: 0.0600079745054245
Training batch 3 / 32
Total batch reconstruction loss: 0.0652066022157669
Training batch 4 / 32
Total batch reconstruction loss: 0.060527361929416656
Training batch 5 / 32
Total batch reconstruction loss: 0.06543679535388947
Training batch 6 / 32
Total batch reconstruction loss: 0.059206392616033554
Training batch 7 / 32
Total batch reconstruction loss: 0.061283137649297714
Training batch 8 / 32
Total batch reconstruction loss: 0.06577032804489136
Training batch 9 / 32
Total batch reconstruction loss: 0.06463530659675598
Training batch 10 / 32
Total batch reconstruction loss: 0.06401924788951874
Training batch 11 / 32
Total batch reconstruction loss: 0.060487985610961914
Training batch 12 / 32
Total batch reconstruction loss: 0.06406746804714203
Training batch 13 / 32
Total batch reconstruction loss: 0.06147317960858345
Training batch 14 / 32
Total batch reconstruction loss: 0.05822093039751053
Training batch 15 / 32
Total batch reconstruction loss: 0.059311747550964355
Training batch 16 / 32
Total batch reconstruction loss: 0.060647837817668915
Training batch 17 / 32
Total batch reconstruction loss: 0.05981595814228058
Training batch 18 / 32
Total batch reconstruction loss: 0.06137574091553688
Training batch 19 / 32
Total batch reconstruction loss: 0.059412382543087006
Training batch 20 / 32
Total batch reconstruction loss: 0.0595974400639534
Training batch 21 / 32
Total batch reconstruction loss: 0.06471458077430725
Training batch 22 / 32
Total batch reconstruction loss: 0.060042090713977814
Training batch 23 / 32
Total batch reconstruction loss: 0.06397822499275208
Training batch 24 / 32
Total batch reconstruction loss: 0.0663437470793724
Training batch 25 / 32
Total batch reconstruction loss: 0.05608804523944855
Training batch 26 / 32
Total batch reconstruction loss: 0.05971333384513855
Training batch 27 / 32
Total batch reconstruction loss: 0.0602339506149292
Training batch 28 / 32
Total batch reconstruction loss: 0.06360577046871185
Training batch 29 / 32
Total batch reconstruction loss: 0.06052839756011963
Training batch 30 / 32
Total batch reconstruction loss: 0.064301036298275
Training batch 31 / 32
Total batch reconstruction loss: 0.0634804517030716
Training batch 32 / 32
Total batch reconstruction loss: 0.07025188207626343
Epoch [135/500], Train Loss: 0.0618, Validation Loss: 0.0609, Generator Loss: 12.4772, Discriminator Loss: 0.3066
Training epoch 136 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.06105997413396835
Training batch 2 / 32
Total batch reconstruction loss: 0.06289578974246979
Training batch 3 / 32
Total batch reconstruction loss: 0.06440829485654831
Training batch 4 / 32
Total batch reconstruction loss: 0.05907038599252701
Training batch 5 / 32
Total batch reconstruction loss: 0.061758317053318024
Training batch 6 / 32
Total batch reconstruction loss: 0.06108638271689415
Training batch 7 / 32
Total batch reconstruction loss: 0.06146614998579025
Training batch 8 / 32
Total batch reconstruction loss: 0.060548923909664154
Training batch 9 / 32
Total batch reconstruction loss: 0.06311356276273727
Training batch 10 / 32
Total batch reconstruction loss: 0.06266475468873978
Training batch 11 / 32
Total batch reconstruction loss: 0.0662630945444107
Training batch 12 / 32
Total batch reconstruction loss: 0.06298239529132843
Training batch 13 / 32
Total batch reconstruction loss: 0.0642968937754631
Training batch 14 / 32
Total batch reconstruction loss: 0.06631693243980408
Training batch 15 / 32
Total batch reconstruction loss: 0.06090497970581055
Training batch 16 / 32
Total batch reconstruction loss: 0.05850685387849808
Training batch 17 / 32
Total batch reconstruction loss: 0.056673966348171234
Training batch 18 / 32
Total batch reconstruction loss: 0.061712924391031265
Training batch 19 / 32
Total batch reconstruction loss: 0.05848260596394539
Training batch 20 / 32
Total batch reconstruction loss: 0.05946960300207138
Training batch 21 / 32
Total batch reconstruction loss: 0.06241000443696976
Training batch 22 / 32
Total batch reconstruction loss: 0.06694984436035156
Training batch 23 / 32
Total batch reconstruction loss: 0.05668406933546066
Training batch 24 / 32
Total batch reconstruction loss: 0.05981519818305969
Training batch 25 / 32
Total batch reconstruction loss: 0.0600491464138031
Training batch 26 / 32
Total batch reconstruction loss: 0.06465058028697968
Training batch 27 / 32
Total batch reconstruction loss: 0.06123840808868408
Training batch 28 / 32
Total batch reconstruction loss: 0.06072899326682091
Training batch 29 / 32
Total batch reconstruction loss: 0.06122884154319763
Training batch 30 / 32
Total batch reconstruction loss: 0.06341953575611115
Training batch 31 / 32
Total batch reconstruction loss: 0.06185028329491615
Training batch 32 / 32
Total batch reconstruction loss: 0.05955944582819939
Epoch [136/500], Train Loss: 0.0612, Validation Loss: 0.0604, Generator Loss: 12.3931, Discriminator Loss: 0.3181
Training epoch 137 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.05852493271231651
Training batch 2 / 32
Total batch reconstruction loss: 0.062113359570503235
Training batch 3 / 32
Total batch reconstruction loss: 0.058831967413425446
Training batch 4 / 32
Total batch reconstruction loss: 0.059675805270671844
Training batch 5 / 32
Total batch reconstruction loss: 0.05450454354286194
Training batch 6 / 32
Total batch reconstruction loss: 0.06570635735988617
Training batch 7 / 32
Total batch reconstruction loss: 0.05710924044251442
Training batch 8 / 32
Total batch reconstruction loss: 0.062070105224847794
Training batch 9 / 32
Total batch reconstruction loss: 0.06654022634029388
Training batch 10 / 32
Total batch reconstruction loss: 0.06261802464723587
Training batch 11 / 32
Total batch reconstruction loss: 0.06180986762046814
Training batch 12 / 32
Total batch reconstruction loss: 0.06762033700942993
Training batch 13 / 32
Total batch reconstruction loss: 0.0589410662651062
Training batch 14 / 32
Total batch reconstruction loss: 0.06111449375748634
Training batch 15 / 32
Total batch reconstruction loss: 0.05851186439394951
Training batch 16 / 32
Total batch reconstruction loss: 0.05905614048242569
Training batch 17 / 32
Total batch reconstruction loss: 0.06671492010354996
Training batch 18 / 32
Total batch reconstruction loss: 0.05970029532909393
Training batch 19 / 32
Total batch reconstruction loss: 0.06498247385025024
Training batch 20 / 32
Total batch reconstruction loss: 0.06511181592941284
Training batch 21 / 32
Total batch reconstruction loss: 0.058842312544584274
Training batch 22 / 32
Total batch reconstruction loss: 0.05711501091718674
Training batch 23 / 32
Total batch reconstruction loss: 0.060231178998947144
Training batch 24 / 32
Total batch reconstruction loss: 0.061389822512865067
Training batch 25 / 32
Total batch reconstruction loss: 0.05944783240556717
Training batch 26 / 32
Total batch reconstruction loss: 0.057381972670555115
Training batch 27 / 32
Total batch reconstruction loss: 0.06006484106183052
Training batch 28 / 32
Total batch reconstruction loss: 0.06314373761415482
Training batch 29 / 32
Total batch reconstruction loss: 0.06015893816947937
Training batch 30 / 32
Total batch reconstruction loss: 0.06101790443062782
Training batch 31 / 32
Total batch reconstruction loss: 0.06469322741031647
Training batch 32 / 32
Total batch reconstruction loss: 0.06724470853805542
Epoch [137/500], Train Loss: 0.0610, Validation Loss: 0.0659, Generator Loss: 12.3301, Discriminator Loss: 0.3199
Training epoch 138 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.059756092727184296
Training batch 2 / 32
Total batch reconstruction loss: 0.05949481204152107
Training batch 3 / 32
Total batch reconstruction loss: 0.06167884171009064
Training batch 4 / 32
Total batch reconstruction loss: 0.06012088432908058
Training batch 5 / 32
Total batch reconstruction loss: 0.058038026094436646
Training batch 6 / 32
Total batch reconstruction loss: 0.06832908093929291
Training batch 7 / 32
Total batch reconstruction loss: 0.06530739367008209
Training batch 8 / 32
Total batch reconstruction loss: 0.059530723839998245
Training batch 9 / 32
Total batch reconstruction loss: 0.06150581315159798
Training batch 10 / 32
Total batch reconstruction loss: 0.06283598393201828
Training batch 11 / 32
Total batch reconstruction loss: 0.06085888668894768
Training batch 12 / 32
Total batch reconstruction loss: 0.06310626864433289
Training batch 13 / 32
Total batch reconstruction loss: 0.06229749694466591
Training batch 14 / 32
Total batch reconstruction loss: 0.05896422266960144
Training batch 15 / 32
Total batch reconstruction loss: 0.06214267015457153
Training batch 16 / 32
Total batch reconstruction loss: 0.06597286462783813
Training batch 17 / 32
Total batch reconstruction loss: 0.06288604438304901
Training batch 18 / 32
Total batch reconstruction loss: 0.06730827689170837
Training batch 19 / 32
Total batch reconstruction loss: 0.06214762106537819
Training batch 20 / 32
Total batch reconstruction loss: 0.06033250689506531
Training batch 21 / 32
Total batch reconstruction loss: 0.06272727251052856
Training batch 22 / 32
Total batch reconstruction loss: 0.06352350860834122
Training batch 23 / 32
Total batch reconstruction loss: 0.062374718487262726
Training batch 24 / 32
Total batch reconstruction loss: 0.059974271804094315
Training batch 25 / 32
Total batch reconstruction loss: 0.06430236250162125
Training batch 26 / 32
Total batch reconstruction loss: 0.05840315669775009
Training batch 27 / 32
Total batch reconstruction loss: 0.06210891902446747
Training batch 28 / 32
Total batch reconstruction loss: 0.0638214647769928
Training batch 29 / 32
Total batch reconstruction loss: 0.05890906974673271
Training batch 30 / 32
Total batch reconstruction loss: 0.06437936425209045
Training batch 31 / 32
Total batch reconstruction loss: 0.0630679726600647
Training batch 32 / 32
Total batch reconstruction loss: 0.05402534455060959
Epoch [138/500], Train Loss: 0.0617, Validation Loss: 0.0616, Generator Loss: 12.4590, Discriminator Loss: 0.3038
Training epoch 139 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.060293857008218765
Training batch 2 / 32
Total batch reconstruction loss: 0.0632123351097107
Training batch 3 / 32
Total batch reconstruction loss: 0.0647306814789772
Training batch 4 / 32
Total batch reconstruction loss: 0.05838564410805702
Training batch 5 / 32
Total batch reconstruction loss: 0.06239859014749527
Training batch 6 / 32
Total batch reconstruction loss: 0.06624235212802887
Training batch 7 / 32
Total batch reconstruction loss: 0.06311984360218048
Training batch 8 / 32
Total batch reconstruction loss: 0.061633337289094925
Training batch 9 / 32
Total batch reconstruction loss: 0.05938464403152466
Training batch 10 / 32
Total batch reconstruction loss: 0.06011659651994705
Training batch 11 / 32
Total batch reconstruction loss: 0.060475725680589676
Training batch 12 / 32
Total batch reconstruction loss: 0.06308220326900482
Training batch 13 / 32
Total batch reconstruction loss: 0.06271146982908249
Training batch 14 / 32
Total batch reconstruction loss: 0.05949530750513077
Training batch 15 / 32
Total batch reconstruction loss: 0.06697259843349457
Training batch 16 / 32
Total batch reconstruction loss: 0.06331810355186462
Training batch 17 / 32
Total batch reconstruction loss: 0.06519846618175507
Training batch 18 / 32
Total batch reconstruction loss: 0.05814775079488754
Training batch 19 / 32
Total batch reconstruction loss: 0.05769236013293266
Training batch 20 / 32
Total batch reconstruction loss: 0.063057541847229
Training batch 21 / 32
Total batch reconstruction loss: 0.06102604418992996
Training batch 22 / 32
Total batch reconstruction loss: 0.05710253119468689
Training batch 23 / 32
Total batch reconstruction loss: 0.05693820118904114
Training batch 24 / 32
Total batch reconstruction loss: 0.061258748173713684
Training batch 25 / 32
Total batch reconstruction loss: 0.061409465968608856
Training batch 26 / 32
Total batch reconstruction loss: 0.0570562370121479
Training batch 27 / 32
Total batch reconstruction loss: 0.06425413489341736
Training batch 28 / 32
Total batch reconstruction loss: 0.059399887919425964
Training batch 29 / 32
Total batch reconstruction loss: 0.06318958103656769
Training batch 30 / 32
Total batch reconstruction loss: 0.0616927444934845
Training batch 31 / 32
Total batch reconstruction loss: 0.05833204835653305
Training batch 32 / 32
Total batch reconstruction loss: 0.06316721439361572
Epoch [139/500], Train Loss: 0.0611, Validation Loss: 0.0651, Generator Loss: 12.3449, Discriminator Loss: 0.3228
Training epoch 140 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.060669008642435074
Training batch 2 / 32
Total batch reconstruction loss: 0.060217007994651794
Training batch 3 / 32
Total batch reconstruction loss: 0.06246693804860115
Training batch 4 / 32
Total batch reconstruction loss: 0.06143540143966675
Training batch 5 / 32
Total batch reconstruction loss: 0.06624379009008408
Training batch 6 / 32
Total batch reconstruction loss: 0.05721370130777359
Training batch 7 / 32
Total batch reconstruction loss: 0.06456999480724335
Training batch 8 / 32
Total batch reconstruction loss: 0.06195440888404846
Training batch 9 / 32
Total batch reconstruction loss: 0.05871758982539177
Training batch 10 / 32
Total batch reconstruction loss: 0.05793256685137749
Training batch 11 / 32
Total batch reconstruction loss: 0.059732161462306976
Training batch 12 / 32
Total batch reconstruction loss: 0.06154734641313553
Training batch 13 / 32
Total batch reconstruction loss: 0.06779520958662033
Training batch 14 / 32
Total batch reconstruction loss: 0.06391285359859467
Training batch 15 / 32
Total batch reconstruction loss: 0.062459368258714676
Training batch 16 / 32
Total batch reconstruction loss: 0.059409309178590775
Training batch 17 / 32
Total batch reconstruction loss: 0.06054070219397545
Training batch 18 / 32
Total batch reconstruction loss: 0.06175209581851959
Training batch 19 / 32
Total batch reconstruction loss: 0.06436458230018616
Training batch 20 / 32
Total batch reconstruction loss: 0.06274200975894928
Training batch 21 / 32
Total batch reconstruction loss: 0.06165630742907524
Training batch 22 / 32
Total batch reconstruction loss: 0.06256446242332458
Training batch 23 / 32
Total batch reconstruction loss: 0.06024803966283798
Training batch 24 / 32
Total batch reconstruction loss: 0.06242353469133377
Training batch 25 / 32
Total batch reconstruction loss: 0.05510195717215538
Training batch 26 / 32
Total batch reconstruction loss: 0.062330327928066254
Training batch 27 / 32
Total batch reconstruction loss: 0.06143081933259964
Training batch 28 / 32
Total batch reconstruction loss: 0.061009690165519714
Training batch 29 / 32
Total batch reconstruction loss: 0.060334958136081696
Training batch 30 / 32
Total batch reconstruction loss: 0.06314663589000702
Training batch 31 / 32
Total batch reconstruction loss: 0.0652107298374176
Training batch 32 / 32
Total batch reconstruction loss: 0.06533282995223999
Epoch [140/500], Train Loss: 0.0618, Validation Loss: 0.0625, Generator Loss: 12.4298, Discriminator Loss: 0.3103
Training epoch 141 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.06142885982990265
Training batch 2 / 32
Total batch reconstruction loss: 0.058724917471408844
Training batch 3 / 32
Total batch reconstruction loss: 0.05648510158061981
Training batch 4 / 32
Total batch reconstruction loss: 0.060167089104652405
Training batch 5 / 32
Total batch reconstruction loss: 0.05634525790810585
Training batch 6 / 32
Total batch reconstruction loss: 0.059985145926475525
Training batch 7 / 32
Total batch reconstruction loss: 0.05790324509143829
Training batch 8 / 32
Total batch reconstruction loss: 0.06103627383708954
Training batch 9 / 32
Total batch reconstruction loss: 0.062443796545267105
Training batch 10 / 32
Total batch reconstruction loss: 0.06004204601049423
Training batch 11 / 32
Total batch reconstruction loss: 0.06715402007102966
Training batch 12 / 32
Total batch reconstruction loss: 0.06124287098646164
Training batch 13 / 32
Total batch reconstruction loss: 0.056622523814439774
Training batch 14 / 32
Total batch reconstruction loss: 0.061643149703741074
Training batch 15 / 32
Total batch reconstruction loss: 0.06085168570280075
Training batch 16 / 32
Total batch reconstruction loss: 0.05710884556174278
Training batch 17 / 32
Total batch reconstruction loss: 0.06703446805477142
Training batch 18 / 32
Total batch reconstruction loss: 0.06156034767627716
Training batch 19 / 32
Total batch reconstruction loss: 0.06943990290164948
Training batch 20 / 32
Total batch reconstruction loss: 0.061491698026657104
Training batch 21 / 32
Total batch reconstruction loss: 0.05792214721441269
Training batch 22 / 32
Total batch reconstruction loss: 0.06218819320201874
Training batch 23 / 32
Total batch reconstruction loss: 0.06047707796096802
Training batch 24 / 32
Total batch reconstruction loss: 0.05871783569455147
Training batch 25 / 32
Total batch reconstruction loss: 0.05831834673881531
Training batch 26 / 32
Total batch reconstruction loss: 0.06060967594385147
Training batch 27 / 32
Total batch reconstruction loss: 0.06798781454563141
Training batch 28 / 32
Total batch reconstruction loss: 0.062206484377384186
Training batch 29 / 32
Total batch reconstruction loss: 0.061090804636478424
Training batch 30 / 32
Total batch reconstruction loss: 0.06271536648273468
Training batch 31 / 32
Total batch reconstruction loss: 0.06259145587682724
Training batch 32 / 32
Total batch reconstruction loss: 0.07953468710184097
Epoch [141/500], Train Loss: 0.0611, Validation Loss: 0.0623, Generator Loss: 12.4114, Discriminator Loss: 0.3063
Training epoch 142 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.06149264797568321
Training batch 2 / 32
Total batch reconstruction loss: 0.06202152371406555
Training batch 3 / 32
Total batch reconstruction loss: 0.06836497783660889
Training batch 4 / 32
Total batch reconstruction loss: 0.06285491585731506
Training batch 5 / 32
Total batch reconstruction loss: 0.060142599046230316
Training batch 6 / 32
Total batch reconstruction loss: 0.06385964155197144
Training batch 7 / 32
Total batch reconstruction loss: 0.06201975792646408
Training batch 8 / 32
Total batch reconstruction loss: 0.06231696531176567
Training batch 9 / 32
Total batch reconstruction loss: 0.06259054690599442
Training batch 10 / 32
Total batch reconstruction loss: 0.06232297793030739
Training batch 11 / 32
Total batch reconstruction loss: 0.05894245207309723
Training batch 12 / 32
Total batch reconstruction loss: 0.060596317052841187
Training batch 13 / 32
Total batch reconstruction loss: 0.05887221917510033
Training batch 14 / 32
Total batch reconstruction loss: 0.0589279942214489
Training batch 15 / 32
Total batch reconstruction loss: 0.06172117963433266
Training batch 16 / 32
Total batch reconstruction loss: 0.058033205568790436
Training batch 17 / 32
Total batch reconstruction loss: 0.05907526612281799
Training batch 18 / 32
Total batch reconstruction loss: 0.05901266634464264
Training batch 19 / 32
Total batch reconstruction loss: 0.0682840645313263
Training batch 20 / 32
Total batch reconstruction loss: 0.06507585942745209
Training batch 21 / 32
Total batch reconstruction loss: 0.06577745079994202
Training batch 22 / 32
Total batch reconstruction loss: 0.06020008772611618
Training batch 23 / 32
Total batch reconstruction loss: 0.06305589526891708
Training batch 24 / 32
Total batch reconstruction loss: 0.05740481615066528
Training batch 25 / 32
Total batch reconstruction loss: 0.05936424061655998
Training batch 26 / 32
Total batch reconstruction loss: 0.05874083191156387
Training batch 27 / 32
Total batch reconstruction loss: 0.06125842034816742
Training batch 28 / 32
Total batch reconstruction loss: 0.06214937940239906
Training batch 29 / 32
Total batch reconstruction loss: 0.0552218034863472
Training batch 30 / 32
Total batch reconstruction loss: 0.057767145335674286
Training batch 31 / 32
Total batch reconstruction loss: 0.06006135046482086
Training batch 32 / 32
Total batch reconstruction loss: 0.050685711205005646
Epoch [142/500], Train Loss: 0.0604, Validation Loss: 0.0611, Generator Loss: 12.2341, Discriminator Loss: 0.3369
Training epoch 143 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.06369909644126892
Training batch 2 / 32
Total batch reconstruction loss: 0.06473493576049805
Training batch 3 / 32
Total batch reconstruction loss: 0.0611690953373909
Training batch 4 / 32
Total batch reconstruction loss: 0.06238322705030441
Training batch 5 / 32
Total batch reconstruction loss: 0.06677104532718658
Training batch 6 / 32
Total batch reconstruction loss: 0.0642680823802948
Training batch 7 / 32
Total batch reconstruction loss: 0.06259210407733917
Training batch 8 / 32
Total batch reconstruction loss: 0.06278204172849655
Training batch 9 / 32
Total batch reconstruction loss: 0.05870606005191803
Training batch 10 / 32
Total batch reconstruction loss: 0.05910123139619827
Training batch 11 / 32
Total batch reconstruction loss: 0.06146271526813507
Training batch 12 / 32
Total batch reconstruction loss: 0.05966794490814209
Training batch 13 / 32
Total batch reconstruction loss: 0.0586247518658638
Training batch 14 / 32
Total batch reconstruction loss: 0.061332426965236664
Training batch 15 / 32
Total batch reconstruction loss: 0.06568551063537598
Training batch 16 / 32
Total batch reconstruction loss: 0.058191828429698944
Training batch 17 / 32
Total batch reconstruction loss: 0.05969211086630821
Training batch 18 / 32
Total batch reconstruction loss: 0.060045026242733
Training batch 19 / 32
Total batch reconstruction loss: 0.05762813985347748
Training batch 20 / 32
Total batch reconstruction loss: 0.06115549057722092
Training batch 21 / 32
Total batch reconstruction loss: 0.05881316959857941
Training batch 22 / 32
Total batch reconstruction loss: 0.06014810502529144
Training batch 23 / 32
Total batch reconstruction loss: 0.06283898651599884
Training batch 24 / 32
Total batch reconstruction loss: 0.060156308114528656
Training batch 25 / 32
Total batch reconstruction loss: 0.0573272705078125
Training batch 26 / 32
Total batch reconstruction loss: 0.059074483811855316
Training batch 27 / 32
Total batch reconstruction loss: 0.06666391342878342
Training batch 28 / 32
Total batch reconstruction loss: 0.061062827706336975
Training batch 29 / 32
Total batch reconstruction loss: 0.06258654594421387
Training batch 30 / 32
Total batch reconstruction loss: 0.06332714855670929
Training batch 31 / 32
Total batch reconstruction loss: 0.05954279750585556
Training batch 32 / 32
Total batch reconstruction loss: 0.08231186866760254
Epoch [143/500], Train Loss: 0.0613, Validation Loss: 0.0623, Generator Loss: 12.4552, Discriminator Loss: 0.3331
Training epoch 144 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.057316239923238754
Training batch 2 / 32
Total batch reconstruction loss: 0.06816799938678741
Training batch 3 / 32
Total batch reconstruction loss: 0.061136018484830856
Training batch 4 / 32
Total batch reconstruction loss: 0.059278592467308044
Training batch 5 / 32
Total batch reconstruction loss: 0.06196543574333191
Training batch 6 / 32
Total batch reconstruction loss: 0.05576265603303909
Training batch 7 / 32
Total batch reconstruction loss: 0.05789241939783096
Training batch 8 / 32
Total batch reconstruction loss: 0.060362812131643295
Training batch 9 / 32
Total batch reconstruction loss: 0.06263679265975952
Training batch 10 / 32
Total batch reconstruction loss: 0.06013993173837662
Training batch 11 / 32
Total batch reconstruction loss: 0.061745934188365936
Training batch 12 / 32
Total batch reconstruction loss: 0.05787047743797302
Training batch 13 / 32
Total batch reconstruction loss: 0.057785749435424805
Training batch 14 / 32
Total batch reconstruction loss: 0.05879154056310654
Training batch 15 / 32
Total batch reconstruction loss: 0.061401091516017914
Training batch 16 / 32
Total batch reconstruction loss: 0.05925918370485306
Training batch 17 / 32
Total batch reconstruction loss: 0.06307241320610046
Training batch 18 / 32
Total batch reconstruction loss: 0.0619850680232048
Training batch 19 / 32
Total batch reconstruction loss: 0.06304469704627991
Training batch 20 / 32
Total batch reconstruction loss: 0.05954531580209732
Training batch 21 / 32
Total batch reconstruction loss: 0.06005246937274933
Training batch 22 / 32
Total batch reconstruction loss: 0.06246419996023178
Training batch 23 / 32
Total batch reconstruction loss: 0.06038830429315567
Training batch 24 / 32
Total batch reconstruction loss: 0.06466931849718094
Training batch 25 / 32
Total batch reconstruction loss: 0.06355337798595428
Training batch 26 / 32
Total batch reconstruction loss: 0.06073548644781113
Training batch 27 / 32
Total batch reconstruction loss: 0.06156025826931
Training batch 28 / 32
Total batch reconstruction loss: 0.06505388766527176
Training batch 29 / 32
Total batch reconstruction loss: 0.06653463840484619
Training batch 30 / 32
Total batch reconstruction loss: 0.06368637830018997
Training batch 31 / 32
Total batch reconstruction loss: 0.06009373068809509
Training batch 32 / 32
Total batch reconstruction loss: 0.04814350977540016
Epoch [144/500], Train Loss: 0.0603, Validation Loss: 0.0627, Generator Loss: 12.2403, Discriminator Loss: 0.3009
Training epoch 145 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.05992509797215462
Training batch 2 / 32
Total batch reconstruction loss: 0.06388749182224274
Training batch 3 / 32
Total batch reconstruction loss: 0.059707559645175934
Training batch 4 / 32
Total batch reconstruction loss: 0.059054769575595856
Training batch 5 / 32
Total batch reconstruction loss: 0.0603998601436615
Training batch 6 / 32
Total batch reconstruction loss: 0.06333568692207336
Training batch 7 / 32
Total batch reconstruction loss: 0.059284526854753494
Training batch 8 / 32
Total batch reconstruction loss: 0.06378385424613953
Training batch 9 / 32
Total batch reconstruction loss: 0.056952010840177536
Training batch 10 / 32
Total batch reconstruction loss: 0.06564592570066452
Training batch 11 / 32
Total batch reconstruction loss: 0.05800490453839302
Training batch 12 / 32
Total batch reconstruction loss: 0.06209084391593933
Training batch 13 / 32
Total batch reconstruction loss: 0.06299909204244614
Training batch 14 / 32
Total batch reconstruction loss: 0.05754837393760681
Training batch 15 / 32
Total batch reconstruction loss: 0.06004590541124344
Training batch 16 / 32
Total batch reconstruction loss: 0.06105080246925354
Training batch 17 / 32
Total batch reconstruction loss: 0.06309453397989273
Training batch 18 / 32
Total batch reconstruction loss: 0.05867810547351837
Training batch 19 / 32
Total batch reconstruction loss: 0.06456191092729568
Training batch 20 / 32
Total batch reconstruction loss: 0.06073176860809326
Training batch 21 / 32
Total batch reconstruction loss: 0.05879027023911476
Training batch 22 / 32
Total batch reconstruction loss: 0.059146858751773834
Training batch 23 / 32
Total batch reconstruction loss: 0.0659250020980835
Training batch 24 / 32
Total batch reconstruction loss: 0.0620712973177433
Training batch 25 / 32
Total batch reconstruction loss: 0.05736994743347168
Training batch 26 / 32
Total batch reconstruction loss: 0.06112339720129967
Training batch 27 / 32
Total batch reconstruction loss: 0.05697968602180481
Training batch 28 / 32
Total batch reconstruction loss: 0.06543288379907608
Training batch 29 / 32
Total batch reconstruction loss: 0.06099794805049896
Training batch 30 / 32
Total batch reconstruction loss: 0.05761527270078659
Training batch 31 / 32
Total batch reconstruction loss: 0.05916678160429001
Training batch 32 / 32
Total batch reconstruction loss: 0.07906997203826904
Epoch [145/500], Train Loss: 0.0606, Validation Loss: 0.0604, Generator Loss: 12.3410, Discriminator Loss: 0.3274
Training epoch 146 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.05749526619911194
Training batch 2 / 32
Total batch reconstruction loss: 0.06493892520666122
Training batch 3 / 32
Total batch reconstruction loss: 0.06074431538581848
Training batch 4 / 32
Total batch reconstruction loss: 0.06041274964809418
Training batch 5 / 32
Total batch reconstruction loss: 0.06503686308860779
Training batch 6 / 32
Total batch reconstruction loss: 0.06222709268331528
Training batch 7 / 32
Total batch reconstruction loss: 0.06044568121433258
Training batch 8 / 32
Total batch reconstruction loss: 0.06339747458696365
Training batch 9 / 32
Total batch reconstruction loss: 0.059083327651023865
Training batch 10 / 32
Total batch reconstruction loss: 0.05897989124059677
Training batch 11 / 32
Total batch reconstruction loss: 0.06840665638446808
Training batch 12 / 32
Total batch reconstruction loss: 0.06450044363737106
Training batch 13 / 32
Total batch reconstruction loss: 0.06610534340143204
Training batch 14 / 32
Total batch reconstruction loss: 0.058825526386499405
Training batch 15 / 32
Total batch reconstruction loss: 0.06020975857973099
Training batch 16 / 32
Total batch reconstruction loss: 0.0628480315208435
Training batch 17 / 32
Total batch reconstruction loss: 0.06708203256130219
Training batch 18 / 32
Total batch reconstruction loss: 0.05848613381385803
Training batch 19 / 32
Total batch reconstruction loss: 0.06431609392166138
Training batch 20 / 32
Total batch reconstruction loss: 0.06336064636707306
Training batch 21 / 32
Total batch reconstruction loss: 0.06820769608020782
Training batch 22 / 32
Total batch reconstruction loss: 0.0643768385052681
Training batch 23 / 32
Total batch reconstruction loss: 0.061500031501054764
Training batch 24 / 32
Total batch reconstruction loss: 0.06457041949033737
Training batch 25 / 32
Total batch reconstruction loss: 0.06626416742801666
Training batch 26 / 32
Total batch reconstruction loss: 0.060605160892009735
Training batch 27 / 32
Total batch reconstruction loss: 0.06124747171998024
Training batch 28 / 32
Total batch reconstruction loss: 0.056473761796951294
Training batch 29 / 32
Total batch reconstruction loss: 0.061077967286109924
Training batch 30 / 32
Total batch reconstruction loss: 0.06097593158483505
Training batch 31 / 32
Total batch reconstruction loss: 0.060143012553453445
Training batch 32 / 32
Total batch reconstruction loss: 0.06105373054742813
Epoch [146/500], Train Loss: 0.0617, Validation Loss: 0.0630, Generator Loss: 12.5433, Discriminator Loss: 0.2976
Training epoch 147 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.0616147555410862
Training batch 2 / 32
Total batch reconstruction loss: 0.0636005774140358
Training batch 3 / 32
Total batch reconstruction loss: 0.06244281306862831
Training batch 4 / 32
Total batch reconstruction loss: 0.05975596606731415
Training batch 5 / 32
Total batch reconstruction loss: 0.06481362879276276
Training batch 6 / 32
Total batch reconstruction loss: 0.05940655618906021
Training batch 7 / 32
Total batch reconstruction loss: 0.0670105516910553
Training batch 8 / 32
Total batch reconstruction loss: 0.05861084163188934
Training batch 9 / 32
Total batch reconstruction loss: 0.05612148717045784
Training batch 10 / 32
Total batch reconstruction loss: 0.058083824813365936
Training batch 11 / 32
Total batch reconstruction loss: 0.058138541877269745
Training batch 12 / 32
Total batch reconstruction loss: 0.06198781728744507
Training batch 13 / 32
Total batch reconstruction loss: 0.06312143057584763
Training batch 14 / 32
Total batch reconstruction loss: 0.05998751148581505
Training batch 15 / 32
Total batch reconstruction loss: 0.0581643246114254
Training batch 16 / 32
Total batch reconstruction loss: 0.05984269827604294
Training batch 17 / 32
Total batch reconstruction loss: 0.06207782030105591
Training batch 18 / 32
Total batch reconstruction loss: 0.0664140060544014
Training batch 19 / 32
Total batch reconstruction loss: 0.0626344308257103
Training batch 20 / 32
Total batch reconstruction loss: 0.059554487466812134
Training batch 21 / 32
Total batch reconstruction loss: 0.061379220336675644
Training batch 22 / 32
Total batch reconstruction loss: 0.06319364905357361
Training batch 23 / 32
Total batch reconstruction loss: 0.05891022831201553
Training batch 24 / 32
Total batch reconstruction loss: 0.05949370190501213
Training batch 25 / 32
Total batch reconstruction loss: 0.06221447139978409
Training batch 26 / 32
Total batch reconstruction loss: 0.06559967994689941
Training batch 27 / 32
Total batch reconstruction loss: 0.060288265347480774
Training batch 28 / 32
Total batch reconstruction loss: 0.05627100169658661
Training batch 29 / 32
Total batch reconstruction loss: 0.06300617754459381
Training batch 30 / 32
Total batch reconstruction loss: 0.059889789670705795
Training batch 31 / 32
Total batch reconstruction loss: 0.062143824994564056
Training batch 32 / 32
Total batch reconstruction loss: 0.05544442683458328
Epoch [147/500], Train Loss: 0.0602, Validation Loss: 0.0629, Generator Loss: 12.2601, Discriminator Loss: 0.3251
Training epoch 148 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.06338229775428772
Training batch 2 / 32
Total batch reconstruction loss: 0.06275565177202225
Training batch 3 / 32
Total batch reconstruction loss: 0.06208687275648117
Training batch 4 / 32
Total batch reconstruction loss: 0.0665653795003891
Training batch 5 / 32
Total batch reconstruction loss: 0.06755802780389786
Training batch 6 / 32
Total batch reconstruction loss: 0.06533964723348618
Training batch 7 / 32
Total batch reconstruction loss: 0.06527106463909149
Training batch 8 / 32
Total batch reconstruction loss: 0.0678640827536583
Training batch 9 / 32
Total batch reconstruction loss: 0.06504414975643158
Training batch 10 / 32
Total batch reconstruction loss: 0.06257130205631256
Training batch 11 / 32
Total batch reconstruction loss: 0.0637550801038742
Training batch 12 / 32
Total batch reconstruction loss: 0.061060838401317596
Training batch 13 / 32
Total batch reconstruction loss: 0.06440430879592896
Training batch 14 / 32
Total batch reconstruction loss: 0.0657423734664917
Training batch 15 / 32
Total batch reconstruction loss: 0.06368151307106018
Training batch 16 / 32
Total batch reconstruction loss: 0.0608498677611351
Training batch 17 / 32
Total batch reconstruction loss: 0.06043039262294769
Training batch 18 / 32
Total batch reconstruction loss: 0.06178586930036545
Training batch 19 / 32
Total batch reconstruction loss: 0.06103724241256714
Training batch 20 / 32
Total batch reconstruction loss: 0.061506181955337524
Training batch 21 / 32
Total batch reconstruction loss: 0.060005005449056625
Training batch 22 / 32
Total batch reconstruction loss: 0.0644686371088028
Training batch 23 / 32
Total batch reconstruction loss: 0.0646081417798996
Training batch 24 / 32
Total batch reconstruction loss: 0.0628204345703125
Training batch 25 / 32
Total batch reconstruction loss: 0.06415769457817078
Training batch 26 / 32
Total batch reconstruction loss: 0.057247478514909744
Training batch 27 / 32
Total batch reconstruction loss: 0.06234423443675041
Training batch 28 / 32
Total batch reconstruction loss: 0.06217942386865616
Training batch 29 / 32
Total batch reconstruction loss: 0.059104785323143005
Training batch 30 / 32
Total batch reconstruction loss: 0.05658387392759323
Training batch 31 / 32
Total batch reconstruction loss: 0.05480920150876045
Training batch 32 / 32
Total batch reconstruction loss: 0.05406123772263527
Epoch [148/500], Train Loss: 0.0621, Validation Loss: 0.0659, Generator Loss: 12.5352, Discriminator Loss: 0.3240
Training epoch 149 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.06602025032043457
Training batch 2 / 32
Total batch reconstruction loss: 0.06431874632835388
Training batch 3 / 32
Total batch reconstruction loss: 0.06134195625782013
Training batch 4 / 32
Total batch reconstruction loss: 0.06053157150745392
Training batch 5 / 32
Total batch reconstruction loss: 0.06259739398956299
Training batch 6 / 32
Total batch reconstruction loss: 0.06936095654964447
Training batch 7 / 32
Total batch reconstruction loss: 0.06297718733549118
Training batch 8 / 32
Total batch reconstruction loss: 0.06154694780707359
Training batch 9 / 32
Total batch reconstruction loss: 0.06702279299497604
Training batch 10 / 32
Total batch reconstruction loss: 0.06233995035290718
Training batch 11 / 32
Total batch reconstruction loss: 0.060809917747974396
Training batch 12 / 32
Total batch reconstruction loss: 0.05870289355516434
Training batch 13 / 32
Total batch reconstruction loss: 0.06305927038192749
Training batch 14 / 32
Total batch reconstruction loss: 0.062277548015117645
Training batch 15 / 32
Total batch reconstruction loss: 0.05696965008974075
Training batch 16 / 32
Total batch reconstruction loss: 0.05758874863386154
Training batch 17 / 32
Total batch reconstruction loss: 0.06151794269680977
Training batch 18 / 32
Total batch reconstruction loss: 0.06577104330062866
Training batch 19 / 32
Total batch reconstruction loss: 0.061710961163043976
Training batch 20 / 32
Total batch reconstruction loss: 0.061675526201725006
Training batch 21 / 32
Total batch reconstruction loss: 0.06169254332780838
Training batch 22 / 32
Total batch reconstruction loss: 0.05847909301519394
Training batch 23 / 32
Total batch reconstruction loss: 0.05957389622926712
Training batch 24 / 32
Total batch reconstruction loss: 0.05719948187470436
Training batch 25 / 32
Total batch reconstruction loss: 0.06392507255077362
Training batch 26 / 32
Total batch reconstruction loss: 0.06259739398956299
Training batch 27 / 32
Total batch reconstruction loss: 0.06183565780520439
Training batch 28 / 32
Total batch reconstruction loss: 0.059812985360622406
Training batch 29 / 32
Total batch reconstruction loss: 0.059451624751091
Training batch 30 / 32
Total batch reconstruction loss: 0.06275244057178497
Training batch 31 / 32
Total batch reconstruction loss: 0.057351596653461456
Training batch 32 / 32
Total batch reconstruction loss: 0.06489820033311844
Epoch [149/500], Train Loss: 0.0608, Validation Loss: 0.0653, Generator Loss: 12.4327, Discriminator Loss: 0.3200
Training epoch 150 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.057058870792388916
Training batch 2 / 32
Total batch reconstruction loss: 0.062158964574337006
Training batch 3 / 32
Total batch reconstruction loss: 0.06395980715751648
Training batch 4 / 32
Total batch reconstruction loss: 0.06198892742395401
Training batch 5 / 32
Total batch reconstruction loss: 0.06013365834951401
Training batch 6 / 32
Total batch reconstruction loss: 0.06132662668824196
Training batch 7 / 32
Total batch reconstruction loss: 0.05988679453730583
Training batch 8 / 32
Total batch reconstruction loss: 0.06399619579315186
Training batch 9 / 32
Total batch reconstruction loss: 0.0580853708088398
Training batch 10 / 32
Total batch reconstruction loss: 0.06037139892578125
Training batch 11 / 32
Total batch reconstruction loss: 0.061997659504413605
Training batch 12 / 32
Total batch reconstruction loss: 0.06077747792005539
Training batch 13 / 32
Total batch reconstruction loss: 0.05872734263539314
Training batch 14 / 32
Total batch reconstruction loss: 0.058772385120391846
Training batch 15 / 32
Total batch reconstruction loss: 0.06020718812942505
Training batch 16 / 32
Total batch reconstruction loss: 0.058095961809158325
Training batch 17 / 32
Total batch reconstruction loss: 0.0626959577202797
Training batch 18 / 32
Total batch reconstruction loss: 0.061237867921590805
Training batch 19 / 32
Total batch reconstruction loss: 0.0581456795334816
Training batch 20 / 32
Total batch reconstruction loss: 0.061846181750297546
Training batch 21 / 32
Total batch reconstruction loss: 0.06292851269245148
Training batch 22 / 32
Total batch reconstruction loss: 0.06541771441698074
Training batch 23 / 32
Total batch reconstruction loss: 0.058415599167346954
Training batch 24 / 32
Total batch reconstruction loss: 0.05818958953022957
Training batch 25 / 32
Total batch reconstruction loss: 0.0662279725074768
Training batch 26 / 32
Total batch reconstruction loss: 0.06027204915881157
Training batch 27 / 32
Total batch reconstruction loss: 0.059901170432567596
Training batch 28 / 32
Total batch reconstruction loss: 0.0592508502304554
Training batch 29 / 32
Total batch reconstruction loss: 0.06312146782875061
Training batch 30 / 32
Total batch reconstruction loss: 0.0642586499452591
Training batch 31 / 32
Total batch reconstruction loss: 0.0633007138967514
Training batch 32 / 32
Total batch reconstruction loss: 0.06525421887636185
Epoch [150/500], Train Loss: 0.0604, Validation Loss: 0.0626, Generator Loss: 12.3166, Discriminator Loss: 0.3049
Training epoch 151 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.06575647741556168
Training batch 2 / 32
Total batch reconstruction loss: 0.060381531715393066
Training batch 3 / 32
Total batch reconstruction loss: 0.058000169694423676
Training batch 4 / 32
Total batch reconstruction loss: 0.06041060388088226
Training batch 5 / 32
Total batch reconstruction loss: 0.06254543364048004
Training batch 6 / 32
Total batch reconstruction loss: 0.06640242040157318
Training batch 7 / 32
Total batch reconstruction loss: 0.0671878531575203
Training batch 8 / 32
Total batch reconstruction loss: 0.06411095708608627
Training batch 9 / 32
Total batch reconstruction loss: 0.05952921509742737
Training batch 10 / 32
Total batch reconstruction loss: 0.06339842826128006
Training batch 11 / 32
Total batch reconstruction loss: 0.061239853501319885
Training batch 12 / 32
Total batch reconstruction loss: 0.06105850636959076
Training batch 13 / 32
Total batch reconstruction loss: 0.060337550938129425
Training batch 14 / 32
Total batch reconstruction loss: 0.061885811388492584
Training batch 15 / 32
Total batch reconstruction loss: 0.06340756267309189
Training batch 16 / 32
Total batch reconstruction loss: 0.05956702679395676
Training batch 17 / 32
Total batch reconstruction loss: 0.05928332731127739
Training batch 18 / 32
Total batch reconstruction loss: 0.06079045683145523
Training batch 19 / 32
Total batch reconstruction loss: 0.062120262533426285
Training batch 20 / 32
Total batch reconstruction loss: 0.057364560663700104
Training batch 21 / 32
Total batch reconstruction loss: 0.05592332035303116
Training batch 22 / 32
Total batch reconstruction loss: 0.06103919446468353
Training batch 23 / 32
Total batch reconstruction loss: 0.058529116213321686
Training batch 24 / 32
Total batch reconstruction loss: 0.06491690874099731
Training batch 25 / 32
Total batch reconstruction loss: 0.06233663856983185
Training batch 26 / 32
Total batch reconstruction loss: 0.05828133225440979
Training batch 27 / 32
Total batch reconstruction loss: 0.06431452929973602
Training batch 28 / 32
Total batch reconstruction loss: 0.0656442940235138
Training batch 29 / 32
Total batch reconstruction loss: 0.05440344288945198
Training batch 30 / 32
Total batch reconstruction loss: 0.057484231889247894
Training batch 31 / 32
Total batch reconstruction loss: 0.06251294910907745
Training batch 32 / 32
Total batch reconstruction loss: 0.05262406915426254
Epoch [151/500], Train Loss: 0.0605, Validation Loss: 0.0607, Generator Loss: 12.2737, Discriminator Loss: 0.3145
Training epoch 152 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.06264617294073105
Training batch 2 / 32
Total batch reconstruction loss: 0.060955654829740524
Training batch 3 / 32
Total batch reconstruction loss: 0.0654628798365593
Training batch 4 / 32
Total batch reconstruction loss: 0.0687522292137146
Training batch 5 / 32
Total batch reconstruction loss: 0.05827699229121208
Training batch 6 / 32
Total batch reconstruction loss: 0.061109017580747604
Training batch 7 / 32
Total batch reconstruction loss: 0.05960242822766304
Training batch 8 / 32
Total batch reconstruction loss: 0.06896279007196426
Training batch 9 / 32
Total batch reconstruction loss: 0.05854835361242294
Training batch 10 / 32
Total batch reconstruction loss: 0.06352628767490387
Training batch 11 / 32
Total batch reconstruction loss: 0.06512514501810074
Training batch 12 / 32
Total batch reconstruction loss: 0.06107121706008911
Training batch 13 / 32
Total batch reconstruction loss: 0.06622849404811859
Training batch 14 / 32
Total batch reconstruction loss: 0.0637601763010025
Training batch 15 / 32
Total batch reconstruction loss: 0.05655086785554886
Training batch 16 / 32
Total batch reconstruction loss: 0.05544154345989227
Training batch 17 / 32
Total batch reconstruction loss: 0.0656353235244751
Training batch 18 / 32
Total batch reconstruction loss: 0.06050260365009308
Training batch 19 / 32
Total batch reconstruction loss: 0.060686465352773666
Training batch 20 / 32
Total batch reconstruction loss: 0.0620713010430336
Training batch 21 / 32
Total batch reconstruction loss: 0.05985291674733162
Training batch 22 / 32
Total batch reconstruction loss: 0.0644373670220375
Training batch 23 / 32
Total batch reconstruction loss: 0.06084802374243736
Training batch 24 / 32
Total batch reconstruction loss: 0.05725342780351639
Training batch 25 / 32
Total batch reconstruction loss: 0.06185062229633331
Training batch 26 / 32
Total batch reconstruction loss: 0.05886738747358322
Training batch 27 / 32
Total batch reconstruction loss: 0.05863933265209198
Training batch 28 / 32
Total batch reconstruction loss: 0.059282295405864716
Training batch 29 / 32
Total batch reconstruction loss: 0.06115374714136124
Training batch 30 / 32
Total batch reconstruction loss: 0.06196810305118561
Training batch 31 / 32
Total batch reconstruction loss: 0.06658347696065903
Training batch 32 / 32
Total batch reconstruction loss: 0.08892175555229187
Epoch [152/500], Train Loss: 0.0617, Validation Loss: 0.0609, Generator Loss: 12.6000, Discriminator Loss: 0.3291
Training epoch 153 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.06400822103023529
Training batch 2 / 32
Total batch reconstruction loss: 0.06955760717391968
Training batch 3 / 32
Total batch reconstruction loss: 0.0616602748632431
Training batch 4 / 32
Total batch reconstruction loss: 0.05807139724493027
Training batch 5 / 32
Total batch reconstruction loss: 0.06606733798980713
Training batch 6 / 32
Total batch reconstruction loss: 0.058947157114744186
Training batch 7 / 32
Total batch reconstruction loss: 0.05912487953901291
Training batch 8 / 32
Total batch reconstruction loss: 0.05904658883810043
Training batch 9 / 32
Total batch reconstruction loss: 0.05943858623504639
Training batch 10 / 32
Total batch reconstruction loss: 0.06246772035956383
Training batch 11 / 32
Total batch reconstruction loss: 0.06572375446557999
Training batch 12 / 32
Total batch reconstruction loss: 0.056676026433706284
Training batch 13 / 32
Total batch reconstruction loss: 0.061295848339796066
Training batch 14 / 32
Total batch reconstruction loss: 0.060997918248176575
Training batch 15 / 32
Total batch reconstruction loss: 0.0630505234003067
Training batch 16 / 32
Total batch reconstruction loss: 0.05655921995639801
Training batch 17 / 32
Total batch reconstruction loss: 0.05893206596374512
Training batch 18 / 32
Total batch reconstruction loss: 0.0647631585597992
Training batch 19 / 32
Total batch reconstruction loss: 0.057211652398109436
Training batch 20 / 32
Total batch reconstruction loss: 0.06030373275279999
Training batch 21 / 32
Total batch reconstruction loss: 0.06542403995990753
Training batch 22 / 32
Total batch reconstruction loss: 0.05883641913533211
Training batch 23 / 32
Total batch reconstruction loss: 0.06527858972549438
Training batch 24 / 32
Total batch reconstruction loss: 0.06032662093639374
Training batch 25 / 32
Total batch reconstruction loss: 0.06266726553440094
Training batch 26 / 32
Total batch reconstruction loss: 0.06053023785352707
Training batch 27 / 32
Total batch reconstruction loss: 0.06337440013885498
Training batch 28 / 32
Total batch reconstruction loss: 0.058098018169403076
Training batch 29 / 32
Total batch reconstruction loss: 0.05937501788139343
Training batch 30 / 32
Total batch reconstruction loss: 0.06091555207967758
Training batch 31 / 32
Total batch reconstruction loss: 0.06104549020528793
Training batch 32 / 32
Total batch reconstruction loss: 0.06652509421110153
Epoch [153/500], Train Loss: 0.0608, Validation Loss: 0.0609, Generator Loss: 12.3602, Discriminator Loss: 0.3207
Training epoch 154 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.06420861184597015
Training batch 2 / 32
Total batch reconstruction loss: 0.06405236572027206
Training batch 3 / 32
Total batch reconstruction loss: 0.05977769196033478
Training batch 4 / 32
Total batch reconstruction loss: 0.061620742082595825
Training batch 5 / 32
Total batch reconstruction loss: 0.06414663791656494
Training batch 6 / 32
Total batch reconstruction loss: 0.06033306568861008
Training batch 7 / 32
Total batch reconstruction loss: 0.0599348247051239
Training batch 8 / 32
Total batch reconstruction loss: 0.058878108859062195
Training batch 9 / 32
Total batch reconstruction loss: 0.06056998297572136
Training batch 10 / 32
Total batch reconstruction loss: 0.06478078663349152
Training batch 11 / 32
Total batch reconstruction loss: 0.0621991902589798
Training batch 12 / 32
Total batch reconstruction loss: 0.06111174076795578
Training batch 13 / 32
Total batch reconstruction loss: 0.05693918094038963
Training batch 14 / 32
Total batch reconstruction loss: 0.059152111411094666
Training batch 15 / 32
Total batch reconstruction loss: 0.06528705358505249
Training batch 16 / 32
Total batch reconstruction loss: 0.06025637686252594
Training batch 17 / 32
Total batch reconstruction loss: 0.0627087727189064
Training batch 18 / 32
Total batch reconstruction loss: 0.0589287094771862
Training batch 19 / 32
Total batch reconstruction loss: 0.05993512272834778
Training batch 20 / 32
Total batch reconstruction loss: 0.06159862503409386
Training batch 21 / 32
Total batch reconstruction loss: 0.058645546436309814
Training batch 22 / 32
Total batch reconstruction loss: 0.06044543534517288
Training batch 23 / 32
Total batch reconstruction loss: 0.059649571776390076
Training batch 24 / 32
Total batch reconstruction loss: 0.06147034466266632
Training batch 25 / 32
Total batch reconstruction loss: 0.06282342970371246
Training batch 26 / 32
Total batch reconstruction loss: 0.05880381166934967
Training batch 27 / 32
Total batch reconstruction loss: 0.06177733838558197
Training batch 28 / 32
Total batch reconstruction loss: 0.06274797022342682
Training batch 29 / 32
Total batch reconstruction loss: 0.0616171658039093
Training batch 30 / 32
Total batch reconstruction loss: 0.05926390737295151
Training batch 31 / 32
Total batch reconstruction loss: 0.0578618198633194
Training batch 32 / 32
Total batch reconstruction loss: 0.047312162816524506
Epoch [154/500], Train Loss: 0.0597, Validation Loss: 0.0598, Generator Loss: 12.1958, Discriminator Loss: 0.3141
Training epoch 155 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.058766260743141174
Training batch 2 / 32
Total batch reconstruction loss: 0.05661676451563835
Training batch 3 / 32
Total batch reconstruction loss: 0.05777028948068619
Training batch 4 / 32
Total batch reconstruction loss: 0.06002380698919296
Training batch 5 / 32
Total batch reconstruction loss: 0.06393586844205856
Training batch 6 / 32
Total batch reconstruction loss: 0.06559989601373672
Training batch 7 / 32
Total batch reconstruction loss: 0.0639030858874321
Training batch 8 / 32
Total batch reconstruction loss: 0.06026637554168701
Training batch 9 / 32
Total batch reconstruction loss: 0.06461507081985474
Training batch 10 / 32
Total batch reconstruction loss: 0.05668605491518974
Training batch 11 / 32
Total batch reconstruction loss: 0.06545397639274597
Training batch 12 / 32
Total batch reconstruction loss: 0.05672137811779976
Training batch 13 / 32
Total batch reconstruction loss: 0.06665484607219696
Training batch 14 / 32
Total batch reconstruction loss: 0.05932790786027908
Training batch 15 / 32
Total batch reconstruction loss: 0.0684586688876152
Training batch 16 / 32
Total batch reconstruction loss: 0.06352750211954117
Training batch 17 / 32
Total batch reconstruction loss: 0.05737747997045517
Training batch 18 / 32
Total batch reconstruction loss: 0.05955909937620163
Training batch 19 / 32
Total batch reconstruction loss: 0.059272393584251404
Training batch 20 / 32
Total batch reconstruction loss: 0.057336337864398956
Training batch 21 / 32
Total batch reconstruction loss: 0.06226325407624245
Training batch 22 / 32
Total batch reconstruction loss: 0.05613001435995102
Training batch 23 / 32
Total batch reconstruction loss: 0.06361988931894302
Training batch 24 / 32
Total batch reconstruction loss: 0.05967947095632553
Training batch 25 / 32
Total batch reconstruction loss: 0.060928620398044586
Training batch 26 / 32
Total batch reconstruction loss: 0.06296053528785706
Training batch 27 / 32
Total batch reconstruction loss: 0.06451938301324844
Training batch 28 / 32
Total batch reconstruction loss: 0.06632228940725327
Training batch 29 / 32
Total batch reconstruction loss: 0.056717079132795334
Training batch 30 / 32
Total batch reconstruction loss: 0.062234699726104736
Training batch 31 / 32
Total batch reconstruction loss: 0.060354359447956085
Training batch 32 / 32
Total batch reconstruction loss: 0.05991421639919281
Epoch [155/500], Train Loss: 0.0609, Validation Loss: 0.0615, Generator Loss: 12.2996, Discriminator Loss: 0.3269
Training epoch 156 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.06081314757466316
Training batch 2 / 32
Total batch reconstruction loss: 0.05894361436367035
Training batch 3 / 32
Total batch reconstruction loss: 0.062058061361312866
Training batch 4 / 32
Total batch reconstruction loss: 0.057866763323545456
Training batch 5 / 32
Total batch reconstruction loss: 0.06391743570566177
Training batch 6 / 32
Total batch reconstruction loss: 0.058577895164489746
Training batch 7 / 32
Total batch reconstruction loss: 0.05967149883508682
Training batch 8 / 32
Total batch reconstruction loss: 0.06809884309768677
Training batch 9 / 32
Total batch reconstruction loss: 0.05877004563808441
Training batch 10 / 32
Total batch reconstruction loss: 0.05966392531991005
Training batch 11 / 32
Total batch reconstruction loss: 0.06089865788817406
Training batch 12 / 32
Total batch reconstruction loss: 0.06129416078329086
Training batch 13 / 32
Total batch reconstruction loss: 0.060692474246025085
Training batch 14 / 32
Total batch reconstruction loss: 0.0626436322927475
Training batch 15 / 32
Total batch reconstruction loss: 0.0568888783454895
Training batch 16 / 32
Total batch reconstruction loss: 0.06500303745269775
Training batch 17 / 32
Total batch reconstruction loss: 0.06316541135311127
Training batch 18 / 32
Total batch reconstruction loss: 0.056866489350795746
Training batch 19 / 32
Total batch reconstruction loss: 0.06367164105176926
Training batch 20 / 32
Total batch reconstruction loss: 0.06300221383571625
Training batch 21 / 32
Total batch reconstruction loss: 0.05833857133984566
Training batch 22 / 32
Total batch reconstruction loss: 0.059816863387823105
Training batch 23 / 32
Total batch reconstruction loss: 0.060813162475824356
Training batch 24 / 32
Total batch reconstruction loss: 0.06115014851093292
Training batch 25 / 32
Total batch reconstruction loss: 0.061835527420043945
Training batch 26 / 32
Total batch reconstruction loss: 0.057564862072467804
Training batch 27 / 32
Total batch reconstruction loss: 0.06349402666091919
Training batch 28 / 32
Total batch reconstruction loss: 0.061770204454660416
Training batch 29 / 32
Total batch reconstruction loss: 0.05873915180563927
Training batch 30 / 32
Total batch reconstruction loss: 0.061588265001773834
Training batch 31 / 32
Total batch reconstruction loss: 0.06490558385848999
Training batch 32 / 32
Total batch reconstruction loss: 0.054640211164951324
Epoch [156/500], Train Loss: 0.0603, Validation Loss: 0.0633, Generator Loss: 12.2358, Discriminator Loss: 0.3250
Training epoch 157 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.06257463991641998
Training batch 2 / 32
Total batch reconstruction loss: 0.06004262715578079
Training batch 3 / 32
Total batch reconstruction loss: 0.06528063118457794
Training batch 4 / 32
Total batch reconstruction loss: 0.06234673783183098
Training batch 5 / 32
Total batch reconstruction loss: 0.06571153551340103
Training batch 6 / 32
Total batch reconstruction loss: 0.05852267146110535
Training batch 7 / 32
Total batch reconstruction loss: 0.06290865689516068
Training batch 8 / 32
Total batch reconstruction loss: 0.06862912327051163
Training batch 9 / 32
Total batch reconstruction loss: 0.06426795572042465
Training batch 10 / 32
Total batch reconstruction loss: 0.06065726280212402
Training batch 11 / 32
Total batch reconstruction loss: 0.058830395340919495
Training batch 12 / 32
Total batch reconstruction loss: 0.06577860563993454
Training batch 13 / 32
Total batch reconstruction loss: 0.061588775366544724
Training batch 14 / 32
Total batch reconstruction loss: 0.05630152300000191
Training batch 15 / 32
Total batch reconstruction loss: 0.06776582449674606
Training batch 16 / 32
Total batch reconstruction loss: 0.06063050776720047
Training batch 17 / 32
Total batch reconstruction loss: 0.06149415671825409
Training batch 18 / 32
Total batch reconstruction loss: 0.05706996098160744
Training batch 19 / 32
Total batch reconstruction loss: 0.061117179691791534
Training batch 20 / 32
Total batch reconstruction loss: 0.05986355245113373
Training batch 21 / 32
Total batch reconstruction loss: 0.05990784615278244
Training batch 22 / 32
Total batch reconstruction loss: 0.060453176498413086
Training batch 23 / 32
Total batch reconstruction loss: 0.059324607253074646
Training batch 24 / 32
Total batch reconstruction loss: 0.05796639621257782
Training batch 25 / 32
Total batch reconstruction loss: 0.05873238295316696
Training batch 26 / 32
Total batch reconstruction loss: 0.059282831847667694
Training batch 27 / 32
Total batch reconstruction loss: 0.05624193325638771
Training batch 28 / 32
Total batch reconstruction loss: 0.0603979229927063
Training batch 29 / 32
Total batch reconstruction loss: 0.054923802614212036
Training batch 30 / 32
Total batch reconstruction loss: 0.06791020929813385
Training batch 31 / 32
Total batch reconstruction loss: 0.060841724276542664
Training batch 32 / 32
Total batch reconstruction loss: 0.047543011605739594
Epoch [157/500], Train Loss: 0.0607, Validation Loss: 0.0582, Generator Loss: 12.2356, Discriminator Loss: 0.3061
Training epoch 158 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.05605749040842056
Training batch 2 / 32
Total batch reconstruction loss: 0.06004009395837784
Training batch 3 / 32
Total batch reconstruction loss: 0.060518376529216766
Training batch 4 / 32
Total batch reconstruction loss: 0.05945520102977753
Training batch 5 / 32
Total batch reconstruction loss: 0.06623438000679016
Training batch 6 / 32
Total batch reconstruction loss: 0.06018710881471634
Training batch 7 / 32
Total batch reconstruction loss: 0.06120409071445465
Training batch 8 / 32
Total batch reconstruction loss: 0.05671500042080879
Training batch 9 / 32
Total batch reconstruction loss: 0.05823154002428055
Training batch 10 / 32
Total batch reconstruction loss: 0.06508436053991318
Training batch 11 / 32
Total batch reconstruction loss: 0.06193601340055466
Training batch 12 / 32
Total batch reconstruction loss: 0.06803129613399506
Training batch 13 / 32
Total batch reconstruction loss: 0.06203947961330414
Training batch 14 / 32
Total batch reconstruction loss: 0.0575382262468338
Training batch 15 / 32
Total batch reconstruction loss: 0.06439118087291718
Training batch 16 / 32
Total batch reconstruction loss: 0.061593685299158096
Training batch 17 / 32
Total batch reconstruction loss: 0.0626300647854805
Training batch 18 / 32
Total batch reconstruction loss: 0.05749993026256561
Training batch 19 / 32
Total batch reconstruction loss: 0.06289352476596832
Training batch 20 / 32
Total batch reconstruction loss: 0.061411455273628235
Training batch 21 / 32
Total batch reconstruction loss: 0.05541107431054115
Training batch 22 / 32
Total batch reconstruction loss: 0.06155095994472504
Training batch 23 / 32
Total batch reconstruction loss: 0.06149624288082123
Training batch 24 / 32
Total batch reconstruction loss: 0.054978784173727036
Training batch 25 / 32
Total batch reconstruction loss: 0.060484133660793304
Training batch 26 / 32
Total batch reconstruction loss: 0.06007014214992523
Training batch 27 / 32
Total batch reconstruction loss: 0.06207679212093353
Training batch 28 / 32
Total batch reconstruction loss: 0.059385996311903
Training batch 29 / 32
Total batch reconstruction loss: 0.05973632633686066
Training batch 30 / 32
Total batch reconstruction loss: 0.06449141353368759
Training batch 31 / 32
Total batch reconstruction loss: 0.05709758773446083
Training batch 32 / 32
Total batch reconstruction loss: 0.04758133739233017
Epoch [158/500], Train Loss: 0.0595, Validation Loss: 0.0596, Generator Loss: 12.1166, Discriminator Loss: 0.3221
Training epoch 159 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.06277215480804443
Training batch 2 / 32
Total batch reconstruction loss: 0.06001465022563934
Training batch 3 / 32
Total batch reconstruction loss: 0.06644785404205322
Training batch 4 / 32
Total batch reconstruction loss: 0.06279736757278442
Training batch 5 / 32
Total batch reconstruction loss: 0.05920937657356262
Training batch 6 / 32
Total batch reconstruction loss: 0.055815987288951874
Training batch 7 / 32
Total batch reconstruction loss: 0.06207789480686188
Training batch 8 / 32
Total batch reconstruction loss: 0.05813642218708992
Training batch 9 / 32
Total batch reconstruction loss: 0.0579279363155365
Training batch 10 / 32
Total batch reconstruction loss: 0.059717386960983276
Training batch 11 / 32
Total batch reconstruction loss: 0.06611557304859161
Training batch 12 / 32
Total batch reconstruction loss: 0.057825539261102676
Training batch 13 / 32
Total batch reconstruction loss: 0.05719529837369919
Training batch 14 / 32
Total batch reconstruction loss: 0.06182248890399933
Training batch 15 / 32
Total batch reconstruction loss: 0.06423066556453705
Training batch 16 / 32
Total batch reconstruction loss: 0.05821644142270088
Training batch 17 / 32
Total batch reconstruction loss: 0.059173546731472015
Training batch 18 / 32
Total batch reconstruction loss: 0.05998498946428299
Training batch 19 / 32
Total batch reconstruction loss: 0.05764255300164223
Training batch 20 / 32
Total batch reconstruction loss: 0.06572200357913971
Training batch 21 / 32
Total batch reconstruction loss: 0.05698889493942261
Training batch 22 / 32
Total batch reconstruction loss: 0.06225457787513733
Training batch 23 / 32
Total batch reconstruction loss: 0.06240389496088028
Training batch 24 / 32
Total batch reconstruction loss: 0.06359349191188812
Training batch 25 / 32
Total batch reconstruction loss: 0.06806951761245728
Training batch 26 / 32
Total batch reconstruction loss: 0.06464473903179169
Training batch 27 / 32
Total batch reconstruction loss: 0.06141521409153938
Training batch 28 / 32
Total batch reconstruction loss: 0.061262086033821106
Training batch 29 / 32
Total batch reconstruction loss: 0.06535017490386963
Training batch 30 / 32
Total batch reconstruction loss: 0.06265763193368912
Training batch 31 / 32
Total batch reconstruction loss: 0.06089214235544205
Training batch 32 / 32
Total batch reconstruction loss: 0.08140525221824646
Epoch [159/500], Train Loss: 0.0610, Validation Loss: 0.0608, Generator Loss: 12.4676, Discriminator Loss: 0.3240
Training epoch 160 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.06204238533973694
Training batch 2 / 32
Total batch reconstruction loss: 0.06647223979234695
Training batch 3 / 32
Total batch reconstruction loss: 0.06381459534168243
Training batch 4 / 32
Total batch reconstruction loss: 0.06280364096164703
Training batch 5 / 32
Total batch reconstruction loss: 0.0621773898601532
Training batch 6 / 32
Total batch reconstruction loss: 0.06093665212392807
Training batch 7 / 32
Total batch reconstruction loss: 0.06011530011892319
Training batch 8 / 32
Total batch reconstruction loss: 0.06083637475967407
Training batch 9 / 32
Total batch reconstruction loss: 0.059921953827142715
Training batch 10 / 32
Total batch reconstruction loss: 0.06298631429672241
Training batch 11 / 32
Total batch reconstruction loss: 0.057930152863264084
Training batch 12 / 32
Total batch reconstruction loss: 0.06052082031965256
Training batch 13 / 32
Total batch reconstruction loss: 0.0586470402777195
Training batch 14 / 32
Total batch reconstruction loss: 0.0604468509554863
Training batch 15 / 32
Total batch reconstruction loss: 0.06727868318557739
Training batch 16 / 32
Total batch reconstruction loss: 0.06348946690559387
Training batch 17 / 32
Total batch reconstruction loss: 0.06354321539402008
Training batch 18 / 32
Total batch reconstruction loss: 0.06054332107305527
Training batch 19 / 32
Total batch reconstruction loss: 0.057593829929828644
Training batch 20 / 32
Total batch reconstruction loss: 0.059036433696746826
Training batch 21 / 32
Total batch reconstruction loss: 0.0625963807106018
Training batch 22 / 32
Total batch reconstruction loss: 0.06083317846059799
Training batch 23 / 32
Total batch reconstruction loss: 0.05952422320842743
Training batch 24 / 32
Total batch reconstruction loss: 0.06137271970510483
Training batch 25 / 32
Total batch reconstruction loss: 0.06177952140569687
Training batch 26 / 32
Total batch reconstruction loss: 0.05589873343706131
Training batch 27 / 32
Total batch reconstruction loss: 0.059772707521915436
Training batch 28 / 32
Total batch reconstruction loss: 0.0584508553147316
Training batch 29 / 32
Total batch reconstruction loss: 0.06397786736488342
Training batch 30 / 32
Total batch reconstruction loss: 0.06116374582052231
Training batch 31 / 32
Total batch reconstruction loss: 0.05629289150238037
Training batch 32 / 32
Total batch reconstruction loss: 0.06452479958534241
Epoch [160/500], Train Loss: 0.0603, Validation Loss: 0.0596, Generator Loss: 12.3217, Discriminator Loss: 0.2922
Training epoch 161 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.0597844198346138
Training batch 2 / 32
Total batch reconstruction loss: 0.05667830631136894
Training batch 3 / 32
Total batch reconstruction loss: 0.06002202257514
Training batch 4 / 32
Total batch reconstruction loss: 0.06644421070814133
Training batch 5 / 32
Total batch reconstruction loss: 0.06042748689651489
Training batch 6 / 32
Total batch reconstruction loss: 0.055816881358623505
Training batch 7 / 32
Total batch reconstruction loss: 0.058696240186691284
Training batch 8 / 32
Total batch reconstruction loss: 0.05924234911799431
Training batch 9 / 32
Total batch reconstruction loss: 0.06002354994416237
Training batch 10 / 32
Total batch reconstruction loss: 0.060863494873046875
Training batch 11 / 32
Total batch reconstruction loss: 0.05972316116094589
Training batch 12 / 32
Total batch reconstruction loss: 0.060870978981256485
Training batch 13 / 32
Total batch reconstruction loss: 0.05873633548617363
Training batch 14 / 32
Total batch reconstruction loss: 0.05658002197742462
Training batch 15 / 32
Total batch reconstruction loss: 0.05855736508965492
Training batch 16 / 32
Total batch reconstruction loss: 0.05587266385555267
Training batch 17 / 32
Total batch reconstruction loss: 0.06471294164657593
Training batch 18 / 32
Total batch reconstruction loss: 0.05933511257171631
Training batch 19 / 32
Total batch reconstruction loss: 0.06526455283164978
Training batch 20 / 32
Total batch reconstruction loss: 0.06112050265073776
Training batch 21 / 32
Total batch reconstruction loss: 0.0606636106967926
Training batch 22 / 32
Total batch reconstruction loss: 0.05884820222854614
Training batch 23 / 32
Total batch reconstruction loss: 0.05950240418314934
Training batch 24 / 32
Total batch reconstruction loss: 0.06121515855193138
Training batch 25 / 32
Total batch reconstruction loss: 0.07289150357246399
Training batch 26 / 32
Total batch reconstruction loss: 0.05899471789598465
Training batch 27 / 32
Total batch reconstruction loss: 0.05580200254917145
Training batch 28 / 32
Total batch reconstruction loss: 0.061376817524433136
Training batch 29 / 32
Total batch reconstruction loss: 0.061659589409828186
Training batch 30 / 32
Total batch reconstruction loss: 0.06241253763437271
Training batch 31 / 32
Total batch reconstruction loss: 0.061086881905794144
Training batch 32 / 32
Total batch reconstruction loss: 0.04794873297214508
Epoch [161/500], Train Loss: 0.0588, Validation Loss: 0.0596, Generator Loss: 12.0797, Discriminator Loss: 0.3118
Training epoch 162 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.057235151529312134
Training batch 2 / 32
Total batch reconstruction loss: 0.06132316589355469
Training batch 3 / 32
Total batch reconstruction loss: 0.05920335277915001
Training batch 4 / 32
Total batch reconstruction loss: 0.06457623839378357
Training batch 5 / 32
Total batch reconstruction loss: 0.05854769051074982
Training batch 6 / 32
Total batch reconstruction loss: 0.060637228190898895
Training batch 7 / 32
Total batch reconstruction loss: 0.0587044358253479
Training batch 8 / 32
Total batch reconstruction loss: 0.06215076521039009
Training batch 9 / 32
Total batch reconstruction loss: 0.06374742090702057
Training batch 10 / 32
Total batch reconstruction loss: 0.05749449506402016
Training batch 11 / 32
Total batch reconstruction loss: 0.058825746178627014
Training batch 12 / 32
Total batch reconstruction loss: 0.06117982789874077
Training batch 13 / 32
Total batch reconstruction loss: 0.06013139337301254
Training batch 14 / 32
Total batch reconstruction loss: 0.0628674328327179
Training batch 15 / 32
Total batch reconstruction loss: 0.0630766898393631
Training batch 16 / 32
Total batch reconstruction loss: 0.06639174371957779
Training batch 17 / 32
Total batch reconstruction loss: 0.06464073807001114
Training batch 18 / 32
Total batch reconstruction loss: 0.06137923151254654
Training batch 19 / 32
Total batch reconstruction loss: 0.05744854360818863
Training batch 20 / 32
Total batch reconstruction loss: 0.06750089675188065
Training batch 21 / 32
Total batch reconstruction loss: 0.058111220598220825
Training batch 22 / 32
Total batch reconstruction loss: 0.06282701343297958
Training batch 23 / 32
Total batch reconstruction loss: 0.0677085816860199
Training batch 24 / 32
Total batch reconstruction loss: 0.05688135698437691
Training batch 25 / 32
Total batch reconstruction loss: 0.0588759109377861
Training batch 26 / 32
Total batch reconstruction loss: 0.06256174296140671
Training batch 27 / 32
Total batch reconstruction loss: 0.06249178946018219
Training batch 28 / 32
Total batch reconstruction loss: 0.06484101712703705
Training batch 29 / 32
Total batch reconstruction loss: 0.059500034898519516
Training batch 30 / 32
Total batch reconstruction loss: 0.06488016247749329
Training batch 31 / 32
Total batch reconstruction loss: 0.054777178913354874
Training batch 32 / 32
Total batch reconstruction loss: 0.053676802664995193
Epoch [162/500], Train Loss: 0.0605, Validation Loss: 0.0646, Generator Loss: 12.2785, Discriminator Loss: 0.3256
Training epoch 163 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.05867081135511398
Training batch 2 / 32
Total batch reconstruction loss: 0.06331907212734222
Training batch 3 / 32
Total batch reconstruction loss: 0.06273965537548065
Training batch 4 / 32
Total batch reconstruction loss: 0.061894889920949936
Training batch 5 / 32
Total batch reconstruction loss: 0.06476722657680511
Training batch 6 / 32
Total batch reconstruction loss: 0.06147924065589905
Training batch 7 / 32
Total batch reconstruction loss: 0.06153953820466995
Training batch 8 / 32
Total batch reconstruction loss: 0.06256134063005447
Training batch 9 / 32
Total batch reconstruction loss: 0.06318753957748413
Training batch 10 / 32
Total batch reconstruction loss: 0.06075058877468109
Training batch 11 / 32
Total batch reconstruction loss: 0.06708128750324249
Training batch 12 / 32
Total batch reconstruction loss: 0.05929847061634064
Training batch 13 / 32
Total batch reconstruction loss: 0.06648918986320496
Training batch 14 / 32
Total batch reconstruction loss: 0.06132236868143082
Training batch 15 / 32
Total batch reconstruction loss: 0.06085992977023125
Training batch 16 / 32
Total batch reconstruction loss: 0.061097897589206696
Training batch 17 / 32
Total batch reconstruction loss: 0.06340766698122025
Training batch 18 / 32
Total batch reconstruction loss: 0.06342753767967224
Training batch 19 / 32
Total batch reconstruction loss: 0.0603066086769104
Training batch 20 / 32
Total batch reconstruction loss: 0.05941876769065857
Training batch 21 / 32
Total batch reconstruction loss: 0.053985871374607086
Training batch 22 / 32
Total batch reconstruction loss: 0.05926946550607681
Training batch 23 / 32
Total batch reconstruction loss: 0.060791317373514175
Training batch 24 / 32
Total batch reconstruction loss: 0.058589525520801544
Training batch 25 / 32
Total batch reconstruction loss: 0.05909699201583862
Training batch 26 / 32
Total batch reconstruction loss: 0.05993625149130821
Training batch 27 / 32
Total batch reconstruction loss: 0.06084434688091278
Training batch 28 / 32
Total batch reconstruction loss: 0.05771562084555626
Training batch 29 / 32
Total batch reconstruction loss: 0.05506295710802078
Training batch 30 / 32
Total batch reconstruction loss: 0.06610596925020218
Training batch 31 / 32
Total batch reconstruction loss: 0.058177649974823
Training batch 32 / 32
Total batch reconstruction loss: 0.06389543414115906
Epoch [163/500], Train Loss: 0.0601, Validation Loss: 0.0592, Generator Loss: 12.2986, Discriminator Loss: 0.3215
Training epoch 164 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.058314815163612366
Training batch 2 / 32
Total batch reconstruction loss: 0.06059424951672554
Training batch 3 / 32
Total batch reconstruction loss: 0.06065855920314789
Training batch 4 / 32
Total batch reconstruction loss: 0.059924520552158356
Training batch 5 / 32
Total batch reconstruction loss: 0.06080295145511627
Training batch 6 / 32
Total batch reconstruction loss: 0.06151717156171799
Training batch 7 / 32
Total batch reconstruction loss: 0.06424359977245331
Training batch 8 / 32
Total batch reconstruction loss: 0.07074963301420212
Training batch 9 / 32
Total batch reconstruction loss: 0.06156070530414581
Training batch 10 / 32
Total batch reconstruction loss: 0.060729995369911194
Training batch 11 / 32
Total batch reconstruction loss: 0.06737199425697327
Training batch 12 / 32
Total batch reconstruction loss: 0.060346852988004684
Training batch 13 / 32
Total batch reconstruction loss: 0.058201465755701065
Training batch 14 / 32
Total batch reconstruction loss: 0.05937792360782623
Training batch 15 / 32
Total batch reconstruction loss: 0.06024909019470215
Training batch 16 / 32
Total batch reconstruction loss: 0.05693300813436508
Training batch 17 / 32
Total batch reconstruction loss: 0.059722647070884705
Training batch 18 / 32
Total batch reconstruction loss: 0.058710191398859024
Training batch 19 / 32
Total batch reconstruction loss: 0.06759503483772278
Training batch 20 / 32
Total batch reconstruction loss: 0.06315386295318604
Training batch 21 / 32
Total batch reconstruction loss: 0.05605514347553253
Training batch 22 / 32
Total batch reconstruction loss: 0.06133590266108513
Training batch 23 / 32
Total batch reconstruction loss: 0.0566045381128788
Training batch 24 / 32
Total batch reconstruction loss: 0.05983482673764229
Training batch 25 / 32
Total batch reconstruction loss: 0.06132900342345238
Training batch 26 / 32
Total batch reconstruction loss: 0.05921410024166107
Training batch 27 / 32
Total batch reconstruction loss: 0.059567540884017944
Training batch 28 / 32
Total batch reconstruction loss: 0.0645252913236618
Training batch 29 / 32
Total batch reconstruction loss: 0.059977974742650986
Training batch 30 / 32
Total batch reconstruction loss: 0.06069307029247284
Training batch 31 / 32
Total batch reconstruction loss: 0.059654414653778076
Training batch 32 / 32
Total batch reconstruction loss: 0.07907877117395401
Epoch [164/500], Train Loss: 0.0607, Validation Loss: 0.0609, Generator Loss: 12.3611, Discriminator Loss: 0.3343
Training epoch 165 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.061477504670619965
Training batch 2 / 32
Total batch reconstruction loss: 0.05632605403661728
Training batch 3 / 32
Total batch reconstruction loss: 0.06138710677623749
Training batch 4 / 32
Total batch reconstruction loss: 0.06319472193717957
Training batch 5 / 32
Total batch reconstruction loss: 0.06724517047405243
Training batch 6 / 32
Total batch reconstruction loss: 0.05697460472583771
Training batch 7 / 32
Total batch reconstruction loss: 0.06681251525878906
Training batch 8 / 32
Total batch reconstruction loss: 0.06187308207154274
Training batch 9 / 32
Total batch reconstruction loss: 0.06001513451337814
Training batch 10 / 32
Total batch reconstruction loss: 0.06094832718372345
Training batch 11 / 32
Total batch reconstruction loss: 0.05879952013492584
Training batch 12 / 32
Total batch reconstruction loss: 0.05799642577767372
Training batch 13 / 32
Total batch reconstruction loss: 0.06101152300834656
Training batch 14 / 32
Total batch reconstruction loss: 0.05653791502118111
Training batch 15 / 32
Total batch reconstruction loss: 0.0679045021533966
Training batch 16 / 32
Total batch reconstruction loss: 0.06085629016160965
Training batch 17 / 32
Total batch reconstruction loss: 0.06241820007562637
Training batch 18 / 32
Total batch reconstruction loss: 0.05750822275876999
Training batch 19 / 32
Total batch reconstruction loss: 0.06084132567048073
Training batch 20 / 32
Total batch reconstruction loss: 0.06563548743724823
Training batch 21 / 32
Total batch reconstruction loss: 0.06056542694568634
Training batch 22 / 32
Total batch reconstruction loss: 0.06205994635820389
Training batch 23 / 32
Total batch reconstruction loss: 0.05955827981233597
Training batch 24 / 32
Total batch reconstruction loss: 0.06196167320013046
Training batch 25 / 32
Total batch reconstruction loss: 0.05718325078487396
Training batch 26 / 32
Total batch reconstruction loss: 0.060027942061424255
Training batch 27 / 32
Total batch reconstruction loss: 0.06228254735469818
Training batch 28 / 32
Total batch reconstruction loss: 0.05795656144618988
Training batch 29 / 32
Total batch reconstruction loss: 0.05762043595314026
Training batch 30 / 32
Total batch reconstruction loss: 0.06118308752775192
Training batch 31 / 32
Total batch reconstruction loss: 0.06963328272104263
Training batch 32 / 32
Total batch reconstruction loss: 0.05392228811979294
Epoch [165/500], Train Loss: 0.0605, Validation Loss: 0.0627, Generator Loss: 12.2472, Discriminator Loss: 0.3326
Training epoch 166 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.05941535905003548
Training batch 2 / 32
Total batch reconstruction loss: 0.06356823444366455
Training batch 3 / 32
Total batch reconstruction loss: 0.062368907034397125
Training batch 4 / 32
Total batch reconstruction loss: 0.059601619839668274
Training batch 5 / 32
Total batch reconstruction loss: 0.057896196842193604
Training batch 6 / 32
Total batch reconstruction loss: 0.05742869898676872
Training batch 7 / 32
Total batch reconstruction loss: 0.05502898991107941
Training batch 8 / 32
Total batch reconstruction loss: 0.059697456657886505
Training batch 9 / 32
Total batch reconstruction loss: 0.06220933794975281
Training batch 10 / 32
Total batch reconstruction loss: 0.06268266588449478
Training batch 11 / 32
Total batch reconstruction loss: 0.06646373867988586
Training batch 12 / 32
Total batch reconstruction loss: 0.06296879798173904
Training batch 13 / 32
Total batch reconstruction loss: 0.06696487963199615
Training batch 14 / 32
Total batch reconstruction loss: 0.06230814754962921
Training batch 15 / 32
Total batch reconstruction loss: 0.05948463827371597
Training batch 16 / 32
Total batch reconstruction loss: 0.061628274619579315
Training batch 17 / 32
Total batch reconstruction loss: 0.06261585652828217
Training batch 18 / 32
Total batch reconstruction loss: 0.059266380965709686
Training batch 19 / 32
Total batch reconstruction loss: 0.05965116620063782
Training batch 20 / 32
Total batch reconstruction loss: 0.05932324752211571
Training batch 21 / 32
Total batch reconstruction loss: 0.05741618201136589
Training batch 22 / 32
Total batch reconstruction loss: 0.06827636063098907
Training batch 23 / 32
Total batch reconstruction loss: 0.05764450877904892
Training batch 24 / 32
Total batch reconstruction loss: 0.05780524015426636
Training batch 25 / 32
Total batch reconstruction loss: 0.05905253440141678
Training batch 26 / 32
Total batch reconstruction loss: 0.058534227311611176
Training batch 27 / 32
Total batch reconstruction loss: 0.05964775010943413
Training batch 28 / 32
Total batch reconstruction loss: 0.06566651165485382
Training batch 29 / 32
Total batch reconstruction loss: 0.05646958202123642
Training batch 30 / 32
Total batch reconstruction loss: 0.05871177837252617
Training batch 31 / 32
Total batch reconstruction loss: 0.06058695167303085
Training batch 32 / 32
Total batch reconstruction loss: 0.05045326054096222
Epoch [166/500], Train Loss: 0.0598, Validation Loss: 0.0607, Generator Loss: 12.1419, Discriminator Loss: 0.3115
Training epoch 167 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.0587259978055954
Training batch 2 / 32
Total batch reconstruction loss: 0.061345115303993225
Training batch 3 / 32
Total batch reconstruction loss: 0.06625822186470032
Training batch 4 / 32
Total batch reconstruction loss: 0.06299612671136856
Training batch 5 / 32
Total batch reconstruction loss: 0.060981232672929764
Training batch 6 / 32
Total batch reconstruction loss: 0.06388428807258606
Training batch 7 / 32
Total batch reconstruction loss: 0.06576483696699142
Training batch 8 / 32
Total batch reconstruction loss: 0.06086856499314308
Training batch 9 / 32
Total batch reconstruction loss: 0.06365559995174408
Training batch 10 / 32
Total batch reconstruction loss: 0.06049920618534088
Training batch 11 / 32
Total batch reconstruction loss: 0.06307421624660492
Training batch 12 / 32
Total batch reconstruction loss: 0.06277309358119965
Training batch 13 / 32
Total batch reconstruction loss: 0.06250859797000885
Training batch 14 / 32
Total batch reconstruction loss: 0.061803001910448074
Training batch 15 / 32
Total batch reconstruction loss: 0.056468408554792404
Training batch 16 / 32
Total batch reconstruction loss: 0.058711938560009
Training batch 17 / 32
Total batch reconstruction loss: 0.056275539100170135
Training batch 18 / 32
Total batch reconstruction loss: 0.06077500432729721
Training batch 19 / 32
Total batch reconstruction loss: 0.05563054233789444
Training batch 20 / 32
Total batch reconstruction loss: 0.06400901079177856
Training batch 21 / 32
Total batch reconstruction loss: 0.06489987671375275
Training batch 22 / 32
Total batch reconstruction loss: 0.06490438431501389
Training batch 23 / 32
Total batch reconstruction loss: 0.05817660689353943
Training batch 24 / 32
Total batch reconstruction loss: 0.05744312331080437
Training batch 25 / 32
Total batch reconstruction loss: 0.05812153220176697
Training batch 26 / 32
Total batch reconstruction loss: 0.06276766955852509
Training batch 27 / 32
Total batch reconstruction loss: 0.057862669229507446
Training batch 28 / 32
Total batch reconstruction loss: 0.06349190324544907
Training batch 29 / 32
Total batch reconstruction loss: 0.06067834794521332
Training batch 30 / 32
Total batch reconstruction loss: 0.05637573450803757
Training batch 31 / 32
Total batch reconstruction loss: 0.05953214317560196
Training batch 32 / 32
Total batch reconstruction loss: 0.07857175171375275
Epoch [167/500], Train Loss: 0.0607, Validation Loss: 0.0587, Generator Loss: 12.3901, Discriminator Loss: 0.3102
Training epoch 168 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.06438741087913513
Training batch 2 / 32
Total batch reconstruction loss: 0.06204567104578018
Training batch 3 / 32
Total batch reconstruction loss: 0.06377111375331879
Training batch 4 / 32
Total batch reconstruction loss: 0.06639538705348969
Training batch 5 / 32
Total batch reconstruction loss: 0.06303771585226059
Training batch 6 / 32
Total batch reconstruction loss: 0.05966009199619293
Training batch 7 / 32
Total batch reconstruction loss: 0.05899890139698982
Training batch 8 / 32
Total batch reconstruction loss: 0.06061892211437225
Training batch 9 / 32
Total batch reconstruction loss: 0.06043839454650879
Training batch 10 / 32
Total batch reconstruction loss: 0.06365382671356201
Training batch 11 / 32
Total batch reconstruction loss: 0.0639813095331192
Training batch 12 / 32
Total batch reconstruction loss: 0.06013811379671097
Training batch 13 / 32
Total batch reconstruction loss: 0.06186460331082344
Training batch 14 / 32
Total batch reconstruction loss: 0.06035742908716202
Training batch 15 / 32
Total batch reconstruction loss: 0.06061791628599167
Training batch 16 / 32
Total batch reconstruction loss: 0.06282693147659302
Training batch 17 / 32
Total batch reconstruction loss: 0.05871075019240379
Training batch 18 / 32
Total batch reconstruction loss: 0.06219935417175293
Training batch 19 / 32
Total batch reconstruction loss: 0.057510096579790115
Training batch 20 / 32
Total batch reconstruction loss: 0.06053992360830307
Training batch 21 / 32
Total batch reconstruction loss: 0.06479009985923767
Training batch 22 / 32
Total batch reconstruction loss: 0.06261250376701355
Training batch 23 / 32
Total batch reconstruction loss: 0.0634143054485321
Training batch 24 / 32
Total batch reconstruction loss: 0.06632663309574127
Training batch 25 / 32
Total batch reconstruction loss: 0.06202947348356247
Training batch 26 / 32
Total batch reconstruction loss: 0.059938423335552216
Training batch 27 / 32
Total batch reconstruction loss: 0.0574529767036438
Training batch 28 / 32
Total batch reconstruction loss: 0.05918930098414421
Training batch 29 / 32
Total batch reconstruction loss: 0.06258586049079895
Training batch 30 / 32
Total batch reconstruction loss: 0.06078667193651199
Training batch 31 / 32
Total batch reconstruction loss: 0.058226559311151505
Training batch 32 / 32
Total batch reconstruction loss: 0.04967436566948891
Epoch [168/500], Train Loss: 0.0603, Validation Loss: 0.0603, Generator Loss: 12.3264, Discriminator Loss: 0.2982
Training epoch 169 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.059322960674762726
Training batch 2 / 32
Total batch reconstruction loss: 0.06102091819047928
Training batch 3 / 32
Total batch reconstruction loss: 0.06189132481813431
Training batch 4 / 32
Total batch reconstruction loss: 0.05747947096824646
Training batch 5 / 32
Total batch reconstruction loss: 0.06365218758583069
Training batch 6 / 32
Total batch reconstruction loss: 0.06172993406653404
Training batch 7 / 32
Total batch reconstruction loss: 0.05979335680603981
Training batch 8 / 32
Total batch reconstruction loss: 0.06546098738908768
Training batch 9 / 32
Total batch reconstruction loss: 0.0580785796046257
Training batch 10 / 32
Total batch reconstruction loss: 0.061062462627887726
Training batch 11 / 32
Total batch reconstruction loss: 0.057168081402778625
Training batch 12 / 32
Total batch reconstruction loss: 0.05992642790079117
Training batch 13 / 32
Total batch reconstruction loss: 0.061089739203453064
Training batch 14 / 32
Total batch reconstruction loss: 0.05952570587396622
Training batch 15 / 32
Total batch reconstruction loss: 0.06330534815788269
Training batch 16 / 32
Total batch reconstruction loss: 0.06402377784252167
Training batch 17 / 32
Total batch reconstruction loss: 0.06256656348705292
Training batch 18 / 32
Total batch reconstruction loss: 0.06171946972608566
Training batch 19 / 32
Total batch reconstruction loss: 0.05910838395357132
Training batch 20 / 32
Total batch reconstruction loss: 0.06170971691608429
Training batch 21 / 32
Total batch reconstruction loss: 0.06109859421849251
Training batch 22 / 32
Total batch reconstruction loss: 0.06363995373249054
Training batch 23 / 32
Total batch reconstruction loss: 0.06442658603191376
Training batch 24 / 32
Total batch reconstruction loss: 0.06113982945680618
Training batch 25 / 32
Total batch reconstruction loss: 0.059485841542482376
Training batch 26 / 32
Total batch reconstruction loss: 0.0588616281747818
Training batch 27 / 32
Total batch reconstruction loss: 0.05850149691104889
Training batch 28 / 32
Total batch reconstruction loss: 0.06272576004266739
Training batch 29 / 32
Total batch reconstruction loss: 0.06338382512331009
Training batch 30 / 32
Total batch reconstruction loss: 0.06452145427465439
Training batch 31 / 32
Total batch reconstruction loss: 0.06022954732179642
Training batch 32 / 32
Total batch reconstruction loss: 0.05863099917769432
Epoch [169/500], Train Loss: 0.0608, Validation Loss: 0.0616, Generator Loss: 12.2983, Discriminator Loss: 0.3134
Training epoch 170 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.060744524002075195
Training batch 2 / 32
Total batch reconstruction loss: 0.05758122354745865
Training batch 3 / 32
Total batch reconstruction loss: 0.06578296422958374
Training batch 4 / 32
Total batch reconstruction loss: 0.06047876179218292
Training batch 5 / 32
Total batch reconstruction loss: 0.0646190494298935
Training batch 6 / 32
Total batch reconstruction loss: 0.058914292603731155
Training batch 7 / 32
Total batch reconstruction loss: 0.0592043474316597
Training batch 8 / 32
Total batch reconstruction loss: 0.05757245421409607
Training batch 9 / 32
Total batch reconstruction loss: 0.06010299175977707
Training batch 10 / 32
Total batch reconstruction loss: 0.060030046850442886
Training batch 11 / 32
Total batch reconstruction loss: 0.06271752715110779
Training batch 12 / 32
Total batch reconstruction loss: 0.05752631276845932
Training batch 13 / 32
Total batch reconstruction loss: 0.06619158387184143
Training batch 14 / 32
Total batch reconstruction loss: 0.061911441385746
Training batch 15 / 32
Total batch reconstruction loss: 0.05987759679555893
Training batch 16 / 32
Total batch reconstruction loss: 0.06808486580848694
Training batch 17 / 32
Total batch reconstruction loss: 0.06151947006583214
Training batch 18 / 32
Total batch reconstruction loss: 0.060464005917310715
Training batch 19 / 32
Total batch reconstruction loss: 0.059969641268253326
Training batch 20 / 32
Total batch reconstruction loss: 0.058386314660310745
Training batch 21 / 32
Total batch reconstruction loss: 0.05944644659757614
Training batch 22 / 32
Total batch reconstruction loss: 0.05743527412414551
Training batch 23 / 32
Total batch reconstruction loss: 0.06079564243555069
Training batch 24 / 32
Total batch reconstruction loss: 0.06231679767370224
Training batch 25 / 32
Total batch reconstruction loss: 0.056332703679800034
Training batch 26 / 32
Total batch reconstruction loss: 0.06029312685132027
Training batch 27 / 32
Total batch reconstruction loss: 0.062393516302108765
Training batch 28 / 32
Total batch reconstruction loss: 0.06208684295415878
Training batch 29 / 32
Total batch reconstruction loss: 0.057775575667619705
Training batch 30 / 32
Total batch reconstruction loss: 0.060741670429706573
Training batch 31 / 32
Total batch reconstruction loss: 0.05848691239953041
Training batch 32 / 32
Total batch reconstruction loss: 0.06259667873382568
Epoch [170/500], Train Loss: 0.0599, Validation Loss: 0.0582, Generator Loss: 12.2123, Discriminator Loss: 0.3106
Training epoch 171 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.060239944607019424
Training batch 2 / 32
Total batch reconstruction loss: 0.06365363299846649
Training batch 3 / 32
Total batch reconstruction loss: 0.05764087289571762
Training batch 4 / 32
Total batch reconstruction loss: 0.05660201236605644
Training batch 5 / 32
Total batch reconstruction loss: 0.05538932979106903
Training batch 6 / 32
Total batch reconstruction loss: 0.05983718857169151
Training batch 7 / 32
Total batch reconstruction loss: 0.05960647016763687
Training batch 8 / 32
Total batch reconstruction loss: 0.05972117930650711
Training batch 9 / 32
Total batch reconstruction loss: 0.05773922801017761
Training batch 10 / 32
Total batch reconstruction loss: 0.06229866296052933
Training batch 11 / 32
Total batch reconstruction loss: 0.06516074389219284
Training batch 12 / 32
Total batch reconstruction loss: 0.05869714170694351
Training batch 13 / 32
Total batch reconstruction loss: 0.06076722592115402
Training batch 14 / 32
Total batch reconstruction loss: 0.05678117275238037
Training batch 15 / 32
Total batch reconstruction loss: 0.06165916845202446
Training batch 16 / 32
Total batch reconstruction loss: 0.05857532098889351
Training batch 17 / 32
Total batch reconstruction loss: 0.06609182804822922
Training batch 18 / 32
Total batch reconstruction loss: 0.05985530465841293
Training batch 19 / 32
Total batch reconstruction loss: 0.058621667325496674
Training batch 20 / 32
Total batch reconstruction loss: 0.061560098081827164
Training batch 21 / 32
Total batch reconstruction loss: 0.0651647299528122
Training batch 22 / 32
Total batch reconstruction loss: 0.060044802725315094
Training batch 23 / 32
Total batch reconstruction loss: 0.058562446385622025
Training batch 24 / 32
Total batch reconstruction loss: 0.062031544744968414
Training batch 25 / 32
Total batch reconstruction loss: 0.06550240516662598
Training batch 26 / 32
Total batch reconstruction loss: 0.06078919768333435
Training batch 27 / 32
Total batch reconstruction loss: 0.06015912815928459
Training batch 28 / 32
Total batch reconstruction loss: 0.06221291422843933
Training batch 29 / 32
Total batch reconstruction loss: 0.06241992861032486
Training batch 30 / 32
Total batch reconstruction loss: 0.0641101598739624
Training batch 31 / 32
Total batch reconstruction loss: 0.06483162939548492
Training batch 32 / 32
Total batch reconstruction loss: 0.05284632369875908
Epoch [171/500], Train Loss: 0.0599, Validation Loss: 0.0602, Generator Loss: 12.1889, Discriminator Loss: 0.3200
Training epoch 172 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.06653319299221039
Training batch 2 / 32
Total batch reconstruction loss: 0.058963656425476074
Training batch 3 / 32
Total batch reconstruction loss: 0.06402862817049026
Training batch 4 / 32
Total batch reconstruction loss: 0.05769586190581322
Training batch 5 / 32
Total batch reconstruction loss: 0.06090342625975609
Training batch 6 / 32
Total batch reconstruction loss: 0.06175367161631584
Training batch 7 / 32
Total batch reconstruction loss: 0.06055846065282822
Training batch 8 / 32
Total batch reconstruction loss: 0.05974279344081879
Training batch 9 / 32
Total batch reconstruction loss: 0.0661735087633133
Training batch 10 / 32
Total batch reconstruction loss: 0.06315293908119202
Training batch 11 / 32
Total batch reconstruction loss: 0.06113523989915848
Training batch 12 / 32
Total batch reconstruction loss: 0.05737895518541336
Training batch 13 / 32
Total batch reconstruction loss: 0.06250052154064178
Training batch 14 / 32
Total batch reconstruction loss: 0.058755576610565186
Training batch 15 / 32
Total batch reconstruction loss: 0.05636162683367729
Training batch 16 / 32
Total batch reconstruction loss: 0.05967426300048828
Training batch 17 / 32
Total batch reconstruction loss: 0.06115562841296196
Training batch 18 / 32
Total batch reconstruction loss: 0.06173088774085045
Training batch 19 / 32
Total batch reconstruction loss: 0.060894519090652466
Training batch 20 / 32
Total batch reconstruction loss: 0.05825192481279373
Training batch 21 / 32
Total batch reconstruction loss: 0.062453191727399826
Training batch 22 / 32
Total batch reconstruction loss: 0.059656284749507904
Training batch 23 / 32
Total batch reconstruction loss: 0.06275332719087601
Training batch 24 / 32
Total batch reconstruction loss: 0.059073008596897125
Training batch 25 / 32
Total batch reconstruction loss: 0.0619928203523159
Training batch 26 / 32
Total batch reconstruction loss: 0.05874735116958618
Training batch 27 / 32
Total batch reconstruction loss: 0.05637741833925247
Training batch 28 / 32
Total batch reconstruction loss: 0.058369800448417664
Training batch 29 / 32
Total batch reconstruction loss: 0.06326992064714432
Training batch 30 / 32
Total batch reconstruction loss: 0.061652641743421555
Training batch 31 / 32
Total batch reconstruction loss: 0.06042974814772606
Training batch 32 / 32
Total batch reconstruction loss: 0.05476021766662598
Epoch [172/500], Train Loss: 0.0598, Validation Loss: 0.0589, Generator Loss: 12.1768, Discriminator Loss: 0.3159
Training epoch 173 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.06646087020635605
Training batch 2 / 32
Total batch reconstruction loss: 0.05633324757218361
Training batch 3 / 32
Total batch reconstruction loss: 0.060997843742370605
Training batch 4 / 32
Total batch reconstruction loss: 0.05959126725792885
Training batch 5 / 32
Total batch reconstruction loss: 0.06094469875097275
Training batch 6 / 32
Total batch reconstruction loss: 0.06621786952018738
Training batch 7 / 32
Total batch reconstruction loss: 0.05918404459953308
Training batch 8 / 32
Total batch reconstruction loss: 0.06456369161605835
Training batch 9 / 32
Total batch reconstruction loss: 0.05995543673634529
Training batch 10 / 32
Total batch reconstruction loss: 0.06450729072093964
Training batch 11 / 32
Total batch reconstruction loss: 0.058050207793712616
Training batch 12 / 32
Total batch reconstruction loss: 0.05851663649082184
Training batch 13 / 32
Total batch reconstruction loss: 0.062432028353214264
Training batch 14 / 32
Total batch reconstruction loss: 0.06051827222108841
Training batch 15 / 32
Total batch reconstruction loss: 0.06078318879008293
Training batch 16 / 32
Total batch reconstruction loss: 0.060865871608257294
Training batch 17 / 32
Total batch reconstruction loss: 0.059018753468990326
Training batch 18 / 32
Total batch reconstruction loss: 0.059894390404224396
Training batch 19 / 32
Total batch reconstruction loss: 0.05998697876930237
Training batch 20 / 32
Total batch reconstruction loss: 0.06064169853925705
Training batch 21 / 32
Total batch reconstruction loss: 0.06695152819156647
Training batch 22 / 32
Total batch reconstruction loss: 0.060381703078746796
Training batch 23 / 32
Total batch reconstruction loss: 0.05804311856627464
Training batch 24 / 32
Total batch reconstruction loss: 0.06397371739149094
Training batch 25 / 32
Total batch reconstruction loss: 0.06326939165592194
Training batch 26 / 32
Total batch reconstruction loss: 0.06360994279384613
Training batch 27 / 32
Total batch reconstruction loss: 0.060464367270469666
Training batch 28 / 32
Total batch reconstruction loss: 0.06304235756397247
Training batch 29 / 32
Total batch reconstruction loss: 0.05841486155986786
Training batch 30 / 32
Total batch reconstruction loss: 0.06147110089659691
Training batch 31 / 32
Total batch reconstruction loss: 0.056219346821308136
Training batch 32 / 32
Total batch reconstruction loss: 0.051891934126615524
Epoch [173/500], Train Loss: 0.0598, Validation Loss: 0.0596, Generator Loss: 12.2443, Discriminator Loss: 0.3118
Training epoch 174 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.05919335037469864
Training batch 2 / 32
Total batch reconstruction loss: 0.05883948132395744
Training batch 3 / 32
Total batch reconstruction loss: 0.06493566185235977
Training batch 4 / 32
Total batch reconstruction loss: 0.05800314620137215
Training batch 5 / 32
Total batch reconstruction loss: 0.0634554773569107
Training batch 6 / 32
Total batch reconstruction loss: 0.05844860523939133
Training batch 7 / 32
Total batch reconstruction loss: 0.05970010161399841
Training batch 8 / 32
Total batch reconstruction loss: 0.06204596161842346
Training batch 9 / 32
Total batch reconstruction loss: 0.05851256474852562
Training batch 10 / 32
Total batch reconstruction loss: 0.05873260647058487
Training batch 11 / 32
Total batch reconstruction loss: 0.06552091985940933
Training batch 12 / 32
Total batch reconstruction loss: 0.06061635911464691
Training batch 13 / 32
Total batch reconstruction loss: 0.055774640291929245
Training batch 14 / 32
Total batch reconstruction loss: 0.05853138864040375
Training batch 15 / 32
Total batch reconstruction loss: 0.06556452810764313
Training batch 16 / 32
Total batch reconstruction loss: 0.056218571960926056
Training batch 17 / 32
Total batch reconstruction loss: 0.06136198341846466
Training batch 18 / 32
Total batch reconstruction loss: 0.06033392250537872
Training batch 19 / 32
Total batch reconstruction loss: 0.06492160260677338
Training batch 20 / 32
Total batch reconstruction loss: 0.05995512753725052
Training batch 21 / 32
Total batch reconstruction loss: 0.061685413122177124
Training batch 22 / 32
Total batch reconstruction loss: 0.058667782694101334
Training batch 23 / 32
Total batch reconstruction loss: 0.060075435787439346
Training batch 24 / 32
Total batch reconstruction loss: 0.059195809066295624
Training batch 25 / 32
Total batch reconstruction loss: 0.05589670315384865
Training batch 26 / 32
Total batch reconstruction loss: 0.060185547918081284
Training batch 27 / 32
Total batch reconstruction loss: 0.06676267832517624
Training batch 28 / 32
Total batch reconstruction loss: 0.056011032313108444
Training batch 29 / 32
Total batch reconstruction loss: 0.06322246789932251
Training batch 30 / 32
Total batch reconstruction loss: 0.05910313129425049
Training batch 31 / 32
Total batch reconstruction loss: 0.06341004371643066
Training batch 32 / 32
Total batch reconstruction loss: 0.06272627413272858
Epoch [174/500], Train Loss: 0.0595, Validation Loss: 0.0599, Generator Loss: 12.1809, Discriminator Loss: 0.3250
Training epoch 175 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.05965013802051544
Training batch 2 / 32
Total batch reconstruction loss: 0.059658292680978775
Training batch 3 / 32
Total batch reconstruction loss: 0.06383457779884338
Training batch 4 / 32
Total batch reconstruction loss: 0.060468077659606934
Training batch 5 / 32
Total batch reconstruction loss: 0.06633886694908142
Training batch 6 / 32
Total batch reconstruction loss: 0.060857079923152924
Training batch 7 / 32
Total batch reconstruction loss: 0.06009971350431442
Training batch 8 / 32
Total batch reconstruction loss: 0.05840545892715454
Training batch 9 / 32
Total batch reconstruction loss: 0.057993315160274506
Training batch 10 / 32
Total batch reconstruction loss: 0.05865085497498512
Training batch 11 / 32
Total batch reconstruction loss: 0.06282038986682892
Training batch 12 / 32
Total batch reconstruction loss: 0.05424285680055618
Training batch 13 / 32
Total batch reconstruction loss: 0.06119884178042412
Training batch 14 / 32
Total batch reconstruction loss: 0.062764473259449
Training batch 15 / 32
Total batch reconstruction loss: 0.05731756240129471
Training batch 16 / 32
Total batch reconstruction loss: 0.06606976687908173
Training batch 17 / 32
Total batch reconstruction loss: 0.059982698410749435
Training batch 18 / 32
Total batch reconstruction loss: 0.0563468299806118
Training batch 19 / 32
Total batch reconstruction loss: 0.06055415794253349
Training batch 20 / 32
Total batch reconstruction loss: 0.055138375610113144
Training batch 21 / 32
Total batch reconstruction loss: 0.06387193500995636
Training batch 22 / 32
Total batch reconstruction loss: 0.05938250198960304
Training batch 23 / 32
Total batch reconstruction loss: 0.06186237186193466
Training batch 24 / 32
Total batch reconstruction loss: 0.05681609734892845
Training batch 25 / 32
Total batch reconstruction loss: 0.06318998336791992
Training batch 26 / 32
Total batch reconstruction loss: 0.05993123725056648
Training batch 27 / 32
Total batch reconstruction loss: 0.059908781200647354
Training batch 28 / 32
Total batch reconstruction loss: 0.05878600478172302
Training batch 29 / 32
Total batch reconstruction loss: 0.06384347379207611
Training batch 30 / 32
Total batch reconstruction loss: 0.0599159300327301
Training batch 31 / 32
Total batch reconstruction loss: 0.05871140956878662
Training batch 32 / 32
Total batch reconstruction loss: 0.05451934039592743
Epoch [175/500], Train Loss: 0.0592, Validation Loss: 0.0592, Generator Loss: 12.0948, Discriminator Loss: 0.3137
Training epoch 176 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.06141044944524765
Training batch 2 / 32
Total batch reconstruction loss: 0.06129959598183632
Training batch 3 / 32
Total batch reconstruction loss: 0.058208636939525604
Training batch 4 / 32
Total batch reconstruction loss: 0.06386982649564743
Training batch 5 / 32
Total batch reconstruction loss: 0.0651022270321846
Training batch 6 / 32
Total batch reconstruction loss: 0.057332031428813934
Training batch 7 / 32
Total batch reconstruction loss: 0.06197524443268776
Training batch 8 / 32
Total batch reconstruction loss: 0.05837604030966759
Training batch 9 / 32
Total batch reconstruction loss: 0.06289876252412796
Training batch 10 / 32
Total batch reconstruction loss: 0.058821093291044235
Training batch 11 / 32
Total batch reconstruction loss: 0.06294995546340942
Training batch 12 / 32
Total batch reconstruction loss: 0.05755738541483879
Training batch 13 / 32
Total batch reconstruction loss: 0.05893472954630852
Training batch 14 / 32
Total batch reconstruction loss: 0.059524260461330414
Training batch 15 / 32
Total batch reconstruction loss: 0.061196163296699524
Training batch 16 / 32
Total batch reconstruction loss: 0.05455721169710159
Training batch 17 / 32
Total batch reconstruction loss: 0.062082983553409576
Training batch 18 / 32
Total batch reconstruction loss: 0.0606619194149971
Training batch 19 / 32
Total batch reconstruction loss: 0.0648750588297844
Training batch 20 / 32
Total batch reconstruction loss: 0.056104451417922974
Training batch 21 / 32
Total batch reconstruction loss: 0.06144484132528305
Training batch 22 / 32
Total batch reconstruction loss: 0.06159621477127075
Training batch 23 / 32
Total batch reconstruction loss: 0.06358703225851059
Training batch 24 / 32
Total batch reconstruction loss: 0.061020560562610626
Training batch 25 / 32
Total batch reconstruction loss: 0.06058020889759064
Training batch 26 / 32
Total batch reconstruction loss: 0.05704810470342636
Training batch 27 / 32
Total batch reconstruction loss: 0.058601170778274536
Training batch 28 / 32
Total batch reconstruction loss: 0.06118784472346306
Training batch 29 / 32
Total batch reconstruction loss: 0.06407081335783005
Training batch 30 / 32
Total batch reconstruction loss: 0.0640643984079361
Training batch 31 / 32
Total batch reconstruction loss: 0.06160188093781471
Training batch 32 / 32
Total batch reconstruction loss: 0.07017368823289871
Epoch [176/500], Train Loss: 0.0602, Validation Loss: 0.0608, Generator Loss: 12.2612, Discriminator Loss: 0.3378
Training epoch 177 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.059933438897132874
Training batch 2 / 32
Total batch reconstruction loss: 0.06088322028517723
Training batch 3 / 32
Total batch reconstruction loss: 0.06590086966753006
Training batch 4 / 32
Total batch reconstruction loss: 0.05878718942403793
Training batch 5 / 32
Total batch reconstruction loss: 0.05938391387462616
Training batch 6 / 32
Total batch reconstruction loss: 0.06025397777557373
Training batch 7 / 32
Total batch reconstruction loss: 0.059144776314496994
Training batch 8 / 32
Total batch reconstruction loss: 0.06511226296424866
Training batch 9 / 32
Total batch reconstruction loss: 0.06335201114416122
Training batch 10 / 32
Total batch reconstruction loss: 0.06297605484724045
Training batch 11 / 32
Total batch reconstruction loss: 0.06207642704248428
Training batch 12 / 32
Total batch reconstruction loss: 0.06100386381149292
Training batch 13 / 32
Total batch reconstruction loss: 0.05929552763700485
Training batch 14 / 32
Total batch reconstruction loss: 0.05838194489479065
Training batch 15 / 32
Total batch reconstruction loss: 0.05892274156212807
Training batch 16 / 32
Total batch reconstruction loss: 0.05867673456668854
Training batch 17 / 32
Total batch reconstruction loss: 0.0603165328502655
Training batch 18 / 32
Total batch reconstruction loss: 0.05894405394792557
Training batch 19 / 32
Total batch reconstruction loss: 0.06443095207214355
Training batch 20 / 32
Total batch reconstruction loss: 0.05969049036502838
Training batch 21 / 32
Total batch reconstruction loss: 0.06075316667556763
Training batch 22 / 32
Total batch reconstruction loss: 0.059425510466098785
Training batch 23 / 32
Total batch reconstruction loss: 0.06043267995119095
Training batch 24 / 32
Total batch reconstruction loss: 0.05922827497124672
Training batch 25 / 32
Total batch reconstruction loss: 0.06429895013570786
Training batch 26 / 32
Total batch reconstruction loss: 0.05706269294023514
Training batch 27 / 32
Total batch reconstruction loss: 0.06246891990303993
Training batch 28 / 32
Total batch reconstruction loss: 0.06686832010746002
Training batch 29 / 32
Total batch reconstruction loss: 0.06391549855470657
Training batch 30 / 32
Total batch reconstruction loss: 0.06304924935102463
Training batch 31 / 32
Total batch reconstruction loss: 0.062306471168994904
Training batch 32 / 32
Total batch reconstruction loss: 0.07439303398132324
Epoch [177/500], Train Loss: 0.0607, Validation Loss: 0.0603, Generator Loss: 12.3952, Discriminator Loss: 0.3173
Training epoch 178 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.06414006650447845
Training batch 2 / 32
Total batch reconstruction loss: 0.05964059382677078
Training batch 3 / 32
Total batch reconstruction loss: 0.059209465980529785
Training batch 4 / 32
Total batch reconstruction loss: 0.06220228224992752
Training batch 5 / 32
Total batch reconstruction loss: 0.05949302017688751
Training batch 6 / 32
Total batch reconstruction loss: 0.0631580576300621
Training batch 7 / 32
Total batch reconstruction loss: 0.06196392700076103
Training batch 8 / 32
Total batch reconstruction loss: 0.06528521329164505
Training batch 9 / 32
Total batch reconstruction loss: 0.06252853572368622
Training batch 10 / 32
Total batch reconstruction loss: 0.06295136362314224
Training batch 11 / 32
Total batch reconstruction loss: 0.05904010683298111
Training batch 12 / 32
Total batch reconstruction loss: 0.06253217160701752
Training batch 13 / 32
Total batch reconstruction loss: 0.0648389607667923
Training batch 14 / 32
Total batch reconstruction loss: 0.057416271418333054
Training batch 15 / 32
Total batch reconstruction loss: 0.06561578810214996
Training batch 16 / 32
Total batch reconstruction loss: 0.06537573039531708
Training batch 17 / 32
Total batch reconstruction loss: 0.06105874851346016
Training batch 18 / 32
Total batch reconstruction loss: 0.061667539179325104
Training batch 19 / 32
Total batch reconstruction loss: 0.0593520849943161
Training batch 20 / 32
Total batch reconstruction loss: 0.05450989678502083
Training batch 21 / 32
Total batch reconstruction loss: 0.059438493102788925
Training batch 22 / 32
Total batch reconstruction loss: 0.05918639153242111
Training batch 23 / 32
Total batch reconstruction loss: 0.062320537865161896
Training batch 24 / 32
Total batch reconstruction loss: 0.06070574373006821
Training batch 25 / 32
Total batch reconstruction loss: 0.05914539098739624
Training batch 26 / 32
Total batch reconstruction loss: 0.062186725437641144
Training batch 27 / 32
Total batch reconstruction loss: 0.057220131158828735
Training batch 28 / 32
Total batch reconstruction loss: 0.0627908855676651
Training batch 29 / 32
Total batch reconstruction loss: 0.0628521740436554
Training batch 30 / 32
Total batch reconstruction loss: 0.06088889762759209
Training batch 31 / 32
Total batch reconstruction loss: 0.059969741851091385
Training batch 32 / 32
Total batch reconstruction loss: 0.0527007170021534
Epoch [178/500], Train Loss: 0.0603, Validation Loss: 0.0598, Generator Loss: 12.2704, Discriminator Loss: 0.3158
Training epoch 179 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.05900096148252487
Training batch 2 / 32
Total batch reconstruction loss: 0.058036427944898605
Training batch 3 / 32
Total batch reconstruction loss: 0.06154229864478111
Training batch 4 / 32
Total batch reconstruction loss: 0.05876070261001587
Training batch 5 / 32
Total batch reconstruction loss: 0.06227609142661095
Training batch 6 / 32
Total batch reconstruction loss: 0.05776958167552948
Training batch 7 / 32
Total batch reconstruction loss: 0.0655234232544899
Training batch 8 / 32
Total batch reconstruction loss: 0.05977611243724823
Training batch 9 / 32
Total batch reconstruction loss: 0.06392920017242432
Training batch 10 / 32
Total batch reconstruction loss: 0.06039990484714508
Training batch 11 / 32
Total batch reconstruction loss: 0.06136094033718109
Training batch 12 / 32
Total batch reconstruction loss: 0.0634787529706955
Training batch 13 / 32
Total batch reconstruction loss: 0.06339235603809357
Training batch 14 / 32
Total batch reconstruction loss: 0.06192393973469734
Training batch 15 / 32
Total batch reconstruction loss: 0.06349793076515198
Training batch 16 / 32
Total batch reconstruction loss: 0.05950659140944481
Training batch 17 / 32
Total batch reconstruction loss: 0.05611245334148407
Training batch 18 / 32
Total batch reconstruction loss: 0.05984051525592804
Training batch 19 / 32
Total batch reconstruction loss: 0.059655897319316864
Training batch 20 / 32
Total batch reconstruction loss: 0.06328307092189789
Training batch 21 / 32
Total batch reconstruction loss: 0.05727869272232056
Training batch 22 / 32
Total batch reconstruction loss: 0.058978501707315445
Training batch 23 / 32
Total batch reconstruction loss: 0.0624837763607502
Training batch 24 / 32
Total batch reconstruction loss: 0.06056845933198929
Training batch 25 / 32
Total batch reconstruction loss: 0.05927620828151703
Training batch 26 / 32
Total batch reconstruction loss: 0.0629725381731987
Training batch 27 / 32
Total batch reconstruction loss: 0.05877523869276047
Training batch 28 / 32
Total batch reconstruction loss: 0.05999818816781044
Training batch 29 / 32
Total batch reconstruction loss: 0.05987439304590225
Training batch 30 / 32
Total batch reconstruction loss: 0.060659654438495636
Training batch 31 / 32
Total batch reconstruction loss: 0.05831116810441017
Training batch 32 / 32
Total batch reconstruction loss: 0.06642709672451019
Epoch [179/500], Train Loss: 0.0599, Validation Loss: 0.0596, Generator Loss: 12.2318, Discriminator Loss: 0.3092
Training epoch 180 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.06644248217344284
Training batch 2 / 32
Total batch reconstruction loss: 0.06645167618989944
Training batch 3 / 32
Total batch reconstruction loss: 0.05901581048965454
Training batch 4 / 32
Total batch reconstruction loss: 0.06136060506105423
Training batch 5 / 32
Total batch reconstruction loss: 0.05719209834933281
Training batch 6 / 32
Total batch reconstruction loss: 0.06310462951660156
Training batch 7 / 32
Total batch reconstruction loss: 0.06062128394842148
Training batch 8 / 32
Total batch reconstruction loss: 0.06140407919883728
Training batch 9 / 32
Total batch reconstruction loss: 0.06543188542127609
Training batch 10 / 32
Total batch reconstruction loss: 0.06649474054574966
Training batch 11 / 32
Total batch reconstruction loss: 0.06203575059771538
Training batch 12 / 32
Total batch reconstruction loss: 0.06482574343681335
Training batch 13 / 32
Total batch reconstruction loss: 0.05780696123838425
Training batch 14 / 32
Total batch reconstruction loss: 0.06119297444820404
Training batch 15 / 32
Total batch reconstruction loss: 0.055836644023656845
Training batch 16 / 32
Total batch reconstruction loss: 0.060222841799259186
Training batch 17 / 32
Total batch reconstruction loss: 0.06563349068164825
Training batch 18 / 32
Total batch reconstruction loss: 0.062497906386852264
Training batch 19 / 32
Total batch reconstruction loss: 0.06088874861598015
Training batch 20 / 32
Total batch reconstruction loss: 0.06471945345401764
Training batch 21 / 32
Total batch reconstruction loss: 0.06422871351242065
Training batch 22 / 32
Total batch reconstruction loss: 0.05922508239746094
Training batch 23 / 32
Total batch reconstruction loss: 0.06352108716964722
Training batch 24 / 32
Total batch reconstruction loss: 0.05771378055214882
Training batch 25 / 32
Total batch reconstruction loss: 0.06126101315021515
Training batch 26 / 32
Total batch reconstruction loss: 0.06269706785678864
Training batch 27 / 32
Total batch reconstruction loss: 0.06493699550628662
Training batch 28 / 32
Total batch reconstruction loss: 0.06135476380586624
Training batch 29 / 32
Total batch reconstruction loss: 0.05954306572675705
Training batch 30 / 32
Total batch reconstruction loss: 0.06252630800008774
Training batch 31 / 32
Total batch reconstruction loss: 0.059109970927238464
Training batch 32 / 32
Total batch reconstruction loss: 0.058818601071834564
Epoch [180/500], Train Loss: 0.0611, Validation Loss: 0.0619, Generator Loss: 12.4291, Discriminator Loss: 0.3204
Training epoch 181 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.06299224495887756
Training batch 2 / 32
Total batch reconstruction loss: 0.06310191750526428
Training batch 3 / 32
Total batch reconstruction loss: 0.06170642375946045
Training batch 4 / 32
Total batch reconstruction loss: 0.05645160377025604
Training batch 5 / 32
Total batch reconstruction loss: 0.06461676955223083
Training batch 6 / 32
Total batch reconstruction loss: 0.06015976518392563
Training batch 7 / 32
Total batch reconstruction loss: 0.060930896550416946
Training batch 8 / 32
Total batch reconstruction loss: 0.05951595678925514
Training batch 9 / 32
Total batch reconstruction loss: 0.06273014098405838
Training batch 10 / 32
Total batch reconstruction loss: 0.06767267733812332
Training batch 11 / 32
Total batch reconstruction loss: 0.06267106533050537
Training batch 12 / 32
Total batch reconstruction loss: 0.05983172729611397
Training batch 13 / 32
Total batch reconstruction loss: 0.05790295824408531
Training batch 14 / 32
Total batch reconstruction loss: 0.06124194711446762
Training batch 15 / 32
Total batch reconstruction loss: 0.067885622382164
Training batch 16 / 32
Total batch reconstruction loss: 0.06279467046260834
Training batch 17 / 32
Total batch reconstruction loss: 0.06317250430583954
Training batch 18 / 32
Total batch reconstruction loss: 0.05625244230031967
Training batch 19 / 32
Total batch reconstruction loss: 0.057607512921094894
Training batch 20 / 32
Total batch reconstruction loss: 0.06081005558371544
Training batch 21 / 32
Total batch reconstruction loss: 0.060782525688409805
Training batch 22 / 32
Total batch reconstruction loss: 0.06278951466083527
Training batch 23 / 32
Total batch reconstruction loss: 0.0600636824965477
Training batch 24 / 32
Total batch reconstruction loss: 0.056937362998723984
Training batch 25 / 32
Total batch reconstruction loss: 0.06257802248001099
Training batch 26 / 32
Total batch reconstruction loss: 0.06091699004173279
Training batch 27 / 32
Total batch reconstruction loss: 0.0561794675886631
Training batch 28 / 32
Total batch reconstruction loss: 0.06042850390076637
Training batch 29 / 32
Total batch reconstruction loss: 0.06333477795124054
Training batch 30 / 32
Total batch reconstruction loss: 0.06104576587677002
Training batch 31 / 32
Total batch reconstruction loss: 0.06264159083366394
Training batch 32 / 32
Total batch reconstruction loss: 0.06600393354892731
Epoch [181/500], Train Loss: 0.0605, Validation Loss: 0.0594, Generator Loss: 12.3362, Discriminator Loss: 0.3302
Training epoch 182 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.0662422850728035
Training batch 2 / 32
Total batch reconstruction loss: 0.0579051598906517
Training batch 3 / 32
Total batch reconstruction loss: 0.06181758642196655
Training batch 4 / 32
Total batch reconstruction loss: 0.06461133062839508
Training batch 5 / 32
Total batch reconstruction loss: 0.05839064344763756
Training batch 6 / 32
Total batch reconstruction loss: 0.06140565499663353
Training batch 7 / 32
Total batch reconstruction loss: 0.06320883333683014
Training batch 8 / 32
Total batch reconstruction loss: 0.057985819876194
Training batch 9 / 32
Total batch reconstruction loss: 0.059891730546951294
Training batch 10 / 32
Total batch reconstruction loss: 0.05591734126210213
Training batch 11 / 32
Total batch reconstruction loss: 0.06043761596083641
Training batch 12 / 32
Total batch reconstruction loss: 0.060455989092588425
Training batch 13 / 32
Total batch reconstruction loss: 0.060452889651060104
Training batch 14 / 32
Total batch reconstruction loss: 0.06419724971055984
Training batch 15 / 32
Total batch reconstruction loss: 0.061190709471702576
Training batch 16 / 32
Total batch reconstruction loss: 0.06164172664284706
Training batch 17 / 32
Total batch reconstruction loss: 0.05662161111831665
Training batch 18 / 32
Total batch reconstruction loss: 0.05917798727750778
Training batch 19 / 32
Total batch reconstruction loss: 0.06367902457714081
Training batch 20 / 32
Total batch reconstruction loss: 0.06706111878156662
Training batch 21 / 32
Total batch reconstruction loss: 0.06582556664943695
Training batch 22 / 32
Total batch reconstruction loss: 0.0594189390540123
Training batch 23 / 32
Total batch reconstruction loss: 0.05772550404071808
Training batch 24 / 32
Total batch reconstruction loss: 0.0598539263010025
Training batch 25 / 32
Total batch reconstruction loss: 0.06077180430293083
Training batch 26 / 32
Total batch reconstruction loss: 0.05856829136610031
Training batch 27 / 32
Total batch reconstruction loss: 0.059045273810625076
Training batch 28 / 32
Total batch reconstruction loss: 0.06066980957984924
Training batch 29 / 32
Total batch reconstruction loss: 0.06727628409862518
Training batch 30 / 32
Total batch reconstruction loss: 0.06003386899828911
Training batch 31 / 32
Total batch reconstruction loss: 0.06435377895832062
Training batch 32 / 32
Total batch reconstruction loss: 0.05972990393638611
Epoch [182/500], Train Loss: 0.0603, Validation Loss: 0.0604, Generator Loss: 12.2903, Discriminator Loss: 0.3250
Training epoch 183 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.05810226500034332
Training batch 2 / 32
Total batch reconstruction loss: 0.06352731585502625
Training batch 3 / 32
Total batch reconstruction loss: 0.06048168987035751
Training batch 4 / 32
Total batch reconstruction loss: 0.07201234996318817
Training batch 5 / 32
Total batch reconstruction loss: 0.06154141575098038
Training batch 6 / 32
Total batch reconstruction loss: 0.05963389575481415
Training batch 7 / 32
Total batch reconstruction loss: 0.059900134801864624
Training batch 8 / 32
Total batch reconstruction loss: 0.061257582157850266
Training batch 9 / 32
Total batch reconstruction loss: 0.0587591715157032
Training batch 10 / 32
Total batch reconstruction loss: 0.0623328797519207
Training batch 11 / 32
Total batch reconstruction loss: 0.058431532233953476
Training batch 12 / 32
Total batch reconstruction loss: 0.05716633051633835
Training batch 13 / 32
Total batch reconstruction loss: 0.06245509535074234
Training batch 14 / 32
Total batch reconstruction loss: 0.06081385910511017
Training batch 15 / 32
Total batch reconstruction loss: 0.05830441415309906
Training batch 16 / 32
Total batch reconstruction loss: 0.061738837510347366
Training batch 17 / 32
Total batch reconstruction loss: 0.05943980813026428
Training batch 18 / 32
Total batch reconstruction loss: 0.05845710262656212
Training batch 19 / 32
Total batch reconstruction loss: 0.05865929648280144
Training batch 20 / 32
Total batch reconstruction loss: 0.06248308718204498
Training batch 21 / 32
Total batch reconstruction loss: 0.05961742252111435
Training batch 22 / 32
Total batch reconstruction loss: 0.061861854046583176
Training batch 23 / 32
Total batch reconstruction loss: 0.064662404358387
Training batch 24 / 32
Total batch reconstruction loss: 0.05551989749073982
Training batch 25 / 32
Total batch reconstruction loss: 0.0613078698515892
Training batch 26 / 32
Total batch reconstruction loss: 0.06344497203826904
Training batch 27 / 32
Total batch reconstruction loss: 0.06079418957233429
Training batch 28 / 32
Total batch reconstruction loss: 0.05919511616230011
Training batch 29 / 32
Total batch reconstruction loss: 0.05887625738978386
Training batch 30 / 32
Total batch reconstruction loss: 0.06067761778831482
Training batch 31 / 32
Total batch reconstruction loss: 0.058606814593076706
Training batch 32 / 32
Total batch reconstruction loss: 0.06802880764007568
Epoch [183/500], Train Loss: 0.0598, Validation Loss: 0.0594, Generator Loss: 12.2579, Discriminator Loss: 0.3020
Training epoch 184 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.05871061235666275
Training batch 2 / 32
Total batch reconstruction loss: 0.06046415865421295
Training batch 3 / 32
Total batch reconstruction loss: 0.06087641045451164
Training batch 4 / 32
Total batch reconstruction loss: 0.061218298971652985
Training batch 5 / 32
Total batch reconstruction loss: 0.05679875239729881
Training batch 6 / 32
Total batch reconstruction loss: 0.056664854288101196
Training batch 7 / 32
Total batch reconstruction loss: 0.05711892247200012
Training batch 8 / 32
Total batch reconstruction loss: 0.0617152564227581
Training batch 9 / 32
Total batch reconstruction loss: 0.07047297060489655
Training batch 10 / 32
Total batch reconstruction loss: 0.05754958093166351
Training batch 11 / 32
Total batch reconstruction loss: 0.06267224997282028
Training batch 12 / 32
Total batch reconstruction loss: 0.05939307436347008
Training batch 13 / 32
Total batch reconstruction loss: 0.058921266347169876
Training batch 14 / 32
Total batch reconstruction loss: 0.05756019800901413
Training batch 15 / 32
Total batch reconstruction loss: 0.058538805693387985
Training batch 16 / 32
Total batch reconstruction loss: 0.0589737668633461
Training batch 17 / 32
Total batch reconstruction loss: 0.06013423576951027
Training batch 18 / 32
Total batch reconstruction loss: 0.06093757227063179
Training batch 19 / 32
Total batch reconstruction loss: 0.062279898673295975
Training batch 20 / 32
Total batch reconstruction loss: 0.05849125236272812
Training batch 21 / 32
Total batch reconstruction loss: 0.06396865099668503
Training batch 22 / 32
Total batch reconstruction loss: 0.059194568544626236
Training batch 23 / 32
Total batch reconstruction loss: 0.0686553567647934
Training batch 24 / 32
Total batch reconstruction loss: 0.0655839592218399
Training batch 25 / 32
Total batch reconstruction loss: 0.056683510541915894
Training batch 26 / 32
Total batch reconstruction loss: 0.05959885194897652
Training batch 27 / 32
Total batch reconstruction loss: 0.06327599287033081
Training batch 28 / 32
Total batch reconstruction loss: 0.057574689388275146
Training batch 29 / 32
Total batch reconstruction loss: 0.0560215525329113
Training batch 30 / 32
Total batch reconstruction loss: 0.060656413435935974
Training batch 31 / 32
Total batch reconstruction loss: 0.06239514797925949
Training batch 32 / 32
Total batch reconstruction loss: 0.049961552023887634
Epoch [184/500], Train Loss: 0.0592, Validation Loss: 0.0588, Generator Loss: 12.0969, Discriminator Loss: 0.2999
Training epoch 185 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.05905347317457199
Training batch 2 / 32
Total batch reconstruction loss: 0.05775715410709381
Training batch 3 / 32
Total batch reconstruction loss: 0.0567578487098217
Training batch 4 / 32
Total batch reconstruction loss: 0.06357578933238983
Training batch 5 / 32
Total batch reconstruction loss: 0.05869431793689728
Training batch 6 / 32
Total batch reconstruction loss: 0.062145985662937164
Training batch 7 / 32
Total batch reconstruction loss: 0.060073260217905045
Training batch 8 / 32
Total batch reconstruction loss: 0.060891829431056976
Training batch 9 / 32
Total batch reconstruction loss: 0.06495220214128494
Training batch 10 / 32
Total batch reconstruction loss: 0.06357190012931824
Training batch 11 / 32
Total batch reconstruction loss: 0.05922381579875946
Training batch 12 / 32
Total batch reconstruction loss: 0.057618647813797
Training batch 13 / 32
Total batch reconstruction loss: 0.05946998670697212
Training batch 14 / 32
Total batch reconstruction loss: 0.06135665252804756
Training batch 15 / 32
Total batch reconstruction loss: 0.06178370118141174
Training batch 16 / 32
Total batch reconstruction loss: 0.06085561215877533
Training batch 17 / 32
Total batch reconstruction loss: 0.058737851679325104
Training batch 18 / 32
Total batch reconstruction loss: 0.05741819739341736
Training batch 19 / 32
Total batch reconstruction loss: 0.057716004550457
Training batch 20 / 32
Total batch reconstruction loss: 0.058058224618434906
Training batch 21 / 32
Total batch reconstruction loss: 0.05996214225888252
Training batch 22 / 32
Total batch reconstruction loss: 0.05705820769071579
Training batch 23 / 32
Total batch reconstruction loss: 0.05960443243384361
Training batch 24 / 32
Total batch reconstruction loss: 0.06337903439998627
Training batch 25 / 32
Total batch reconstruction loss: 0.05994834005832672
Training batch 26 / 32
Total batch reconstruction loss: 0.06021662801504135
Training batch 27 / 32
Total batch reconstruction loss: 0.06032547727227211
Training batch 28 / 32
Total batch reconstruction loss: 0.06371121108531952
Training batch 29 / 32
Total batch reconstruction loss: 0.06018831580877304
Training batch 30 / 32
Total batch reconstruction loss: 0.05937962979078293
Training batch 31 / 32
Total batch reconstruction loss: 0.05917932093143463
Training batch 32 / 32
Total batch reconstruction loss: 0.06791840493679047
Epoch [185/500], Train Loss: 0.0593, Validation Loss: 0.0598, Generator Loss: 12.1359, Discriminator Loss: 0.3183
Training epoch 186 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.059177763760089874
Training batch 2 / 32
Total batch reconstruction loss: 0.06420701742172241
Training batch 3 / 32
Total batch reconstruction loss: 0.06021317094564438
Training batch 4 / 32
Total batch reconstruction loss: 0.06779550015926361
Training batch 5 / 32
Total batch reconstruction loss: 0.06386522948741913
Training batch 6 / 32
Total batch reconstruction loss: 0.05957828462123871
Training batch 7 / 32
Total batch reconstruction loss: 0.05932530015707016
Training batch 8 / 32
Total batch reconstruction loss: 0.05694129317998886
Training batch 9 / 32
Total batch reconstruction loss: 0.05857344716787338
Training batch 10 / 32
Total batch reconstruction loss: 0.06135249882936478
Training batch 11 / 32
Total batch reconstruction loss: 0.06029348820447922
Training batch 12 / 32
Total batch reconstruction loss: 0.06415065377950668
Training batch 13 / 32
Total batch reconstruction loss: 0.06007027253508568
Training batch 14 / 32
Total batch reconstruction loss: 0.060819439589977264
Training batch 15 / 32
Total batch reconstruction loss: 0.05923214182257652
Training batch 16 / 32
Total batch reconstruction loss: 0.06204720586538315
Training batch 17 / 32
Total batch reconstruction loss: 0.06359490007162094
Training batch 18 / 32
Total batch reconstruction loss: 0.05756882205605507
Training batch 19 / 32
Total batch reconstruction loss: 0.05920569971203804
Training batch 20 / 32
Total batch reconstruction loss: 0.05631390959024429
Training batch 21 / 32
Total batch reconstruction loss: 0.0651627779006958
Training batch 22 / 32
Total batch reconstruction loss: 0.057492561638355255
Training batch 23 / 32
Total batch reconstruction loss: 0.05948822945356369
Training batch 24 / 32
Total batch reconstruction loss: 0.06095733493566513
Training batch 25 / 32
Total batch reconstruction loss: 0.06047807261347771
Training batch 26 / 32
Total batch reconstruction loss: 0.060805097222328186
Training batch 27 / 32
Total batch reconstruction loss: 0.06239630654454231
Training batch 28 / 32
Total batch reconstruction loss: 0.05878489464521408
Training batch 29 / 32
Total batch reconstruction loss: 0.059795089066028595
Training batch 30 / 32
Total batch reconstruction loss: 0.056812554597854614
Training batch 31 / 32
Total batch reconstruction loss: 0.05776984617114067
Training batch 32 / 32
Total batch reconstruction loss: 0.05393751710653305
Epoch [186/500], Train Loss: 0.0592, Validation Loss: 0.0626, Generator Loss: 12.1278, Discriminator Loss: 0.3084
Training epoch 187 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.059313997626304626
Training batch 2 / 32
Total batch reconstruction loss: 0.06073552370071411
Training batch 3 / 32
Total batch reconstruction loss: 0.06255071610212326
Training batch 4 / 32
Total batch reconstruction loss: 0.059511005878448486
Training batch 5 / 32
Total batch reconstruction loss: 0.061228763312101364
Training batch 6 / 32
Total batch reconstruction loss: 0.06445957720279694
Training batch 7 / 32
Total batch reconstruction loss: 0.05669986456632614
Training batch 8 / 32
Total batch reconstruction loss: 0.06357750296592712
Training batch 9 / 32
Total batch reconstruction loss: 0.06198941171169281
Training batch 10 / 32
Total batch reconstruction loss: 0.06180102750658989
Training batch 11 / 32
Total batch reconstruction loss: 0.06590673327445984
Training batch 12 / 32
Total batch reconstruction loss: 0.06336233764886856
Training batch 13 / 32
Total batch reconstruction loss: 0.059212084859609604
Training batch 14 / 32
Total batch reconstruction loss: 0.06307784467935562
Training batch 15 / 32
Total batch reconstruction loss: 0.06107857823371887
Training batch 16 / 32
Total batch reconstruction loss: 0.06315857172012329
Training batch 17 / 32
Total batch reconstruction loss: 0.060767389833927155
Training batch 18 / 32
Total batch reconstruction loss: 0.06269334256649017
Training batch 19 / 32
Total batch reconstruction loss: 0.05620729923248291
Training batch 20 / 32
Total batch reconstruction loss: 0.0573962926864624
Training batch 21 / 32
Total batch reconstruction loss: 0.05734883248806
Training batch 22 / 32
Total batch reconstruction loss: 0.05962072312831879
Training batch 23 / 32
Total batch reconstruction loss: 0.06208279728889465
Training batch 24 / 32
Total batch reconstruction loss: 0.05951584130525589
Training batch 25 / 32
Total batch reconstruction loss: 0.0608847513794899
Training batch 26 / 32
Total batch reconstruction loss: 0.05576597899198532
Training batch 27 / 32
Total batch reconstruction loss: 0.06966634094715118
Training batch 28 / 32
Total batch reconstruction loss: 0.057731714099645615
Training batch 29 / 32
Total batch reconstruction loss: 0.0632464587688446
Training batch 30 / 32
Total batch reconstruction loss: 0.06504574418067932
Training batch 31 / 32
Total batch reconstruction loss: 0.05638624727725983
Training batch 32 / 32
Total batch reconstruction loss: 0.04871563985943794
Epoch [187/500], Train Loss: 0.0602, Validation Loss: 0.0601, Generator Loss: 12.2079, Discriminator Loss: 0.3105
Training epoch 188 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.0688132792711258
Training batch 2 / 32
Total batch reconstruction loss: 0.06149301677942276
Training batch 3 / 32
Total batch reconstruction loss: 0.06203313544392586
Training batch 4 / 32
Total batch reconstruction loss: 0.058791399002075195
Training batch 5 / 32
Total batch reconstruction loss: 0.057435162365436554
Training batch 6 / 32
Total batch reconstruction loss: 0.05949100852012634
Training batch 7 / 32
Total batch reconstruction loss: 0.059347331523895264
Training batch 8 / 32
Total batch reconstruction loss: 0.06331086158752441
Training batch 9 / 32
Total batch reconstruction loss: 0.057483598589897156
Training batch 10 / 32
Total batch reconstruction loss: 0.06561750173568726
Training batch 11 / 32
Total batch reconstruction loss: 0.056269362568855286
Training batch 12 / 32
Total batch reconstruction loss: 0.06427711993455887
Training batch 13 / 32
Total batch reconstruction loss: 0.05897640064358711
Training batch 14 / 32
Total batch reconstruction loss: 0.05876821279525757
Training batch 15 / 32
Total batch reconstruction loss: 0.05797652155160904
Training batch 16 / 32
Total batch reconstruction loss: 0.06118152290582657
Training batch 17 / 32
Total batch reconstruction loss: 0.06151551008224487
Training batch 18 / 32
Total batch reconstruction loss: 0.05990014225244522
Training batch 19 / 32
Total batch reconstruction loss: 0.062123991549015045
Training batch 20 / 32
Total batch reconstruction loss: 0.05852549150586128
Training batch 21 / 32
Total batch reconstruction loss: 0.060446929186582565
Training batch 22 / 32
Total batch reconstruction loss: 0.060925692319869995
Training batch 23 / 32
Total batch reconstruction loss: 0.06643730401992798
Training batch 24 / 32
Total batch reconstruction loss: 0.0639035627245903
Training batch 25 / 32
Total batch reconstruction loss: 0.06156223639845848
Training batch 26 / 32
Total batch reconstruction loss: 0.05995214730501175
Training batch 27 / 32
Total batch reconstruction loss: 0.05815834924578667
Training batch 28 / 32
Total batch reconstruction loss: 0.06269755959510803
Training batch 29 / 32
Total batch reconstruction loss: 0.06031487137079239
Training batch 30 / 32
Total batch reconstruction loss: 0.059294238686561584
Training batch 31 / 32
Total batch reconstruction loss: 0.060159750282764435
Training batch 32 / 32
Total batch reconstruction loss: 0.05500286817550659
Epoch [188/500], Train Loss: 0.0603, Validation Loss: 0.0615, Generator Loss: 12.2002, Discriminator Loss: 0.3289
Training epoch 189 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.06480340659618378
Training batch 2 / 32
Total batch reconstruction loss: 0.06171351671218872
Training batch 3 / 32
Total batch reconstruction loss: 0.061663173139095306
Training batch 4 / 32
Total batch reconstruction loss: 0.05665775388479233
Training batch 5 / 32
Total batch reconstruction loss: 0.059819482266902924
Training batch 6 / 32
Total batch reconstruction loss: 0.061853501945734024
Training batch 7 / 32
Total batch reconstruction loss: 0.06458023935556412
Training batch 8 / 32
Total batch reconstruction loss: 0.06506051123142242
Training batch 9 / 32
Total batch reconstruction loss: 0.06016173213720322
Training batch 10 / 32
Total batch reconstruction loss: 0.06195182353258133
Training batch 11 / 32
Total batch reconstruction loss: 0.06345245242118835
Training batch 12 / 32
Total batch reconstruction loss: 0.06407099217176437
Training batch 13 / 32
Total batch reconstruction loss: 0.06424568593502045
Training batch 14 / 32
Total batch reconstruction loss: 0.05726272612810135
Training batch 15 / 32
Total batch reconstruction loss: 0.06313635408878326
Training batch 16 / 32
Total batch reconstruction loss: 0.06593203544616699
Training batch 17 / 32
Total batch reconstruction loss: 0.06154634803533554
Training batch 18 / 32
Total batch reconstruction loss: 0.06056620925664902
Training batch 19 / 32
Total batch reconstruction loss: 0.06062886863946915
Training batch 20 / 32
Total batch reconstruction loss: 0.05863118916749954
Training batch 21 / 32
Total batch reconstruction loss: 0.05503435060381889
Training batch 22 / 32
Total batch reconstruction loss: 0.06237560883164406
Training batch 23 / 32
Total batch reconstruction loss: 0.05736929923295975
Training batch 24 / 32
Total batch reconstruction loss: 0.058670856058597565
Training batch 25 / 32
Total batch reconstruction loss: 0.06105741485953331
Training batch 26 / 32
Total batch reconstruction loss: 0.05886343866586685
Training batch 27 / 32
Total batch reconstruction loss: 0.05520216375589371
Training batch 28 / 32
Total batch reconstruction loss: 0.0583876296877861
Training batch 29 / 32
Total batch reconstruction loss: 0.06278599798679352
Training batch 30 / 32
Total batch reconstruction loss: 0.06276634335517883
Training batch 31 / 32
Total batch reconstruction loss: 0.06023017317056656
Training batch 32 / 32
Total batch reconstruction loss: 0.04996854066848755
Epoch [189/500], Train Loss: 0.0597, Validation Loss: 0.0609, Generator Loss: 12.1937, Discriminator Loss: 0.3213
Training epoch 190 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.06700894236564636
Training batch 2 / 32
Total batch reconstruction loss: 0.059974342584609985
Training batch 3 / 32
Total batch reconstruction loss: 0.05807717144489288
Training batch 4 / 32
Total batch reconstruction loss: 0.06397928297519684
Training batch 5 / 32
Total batch reconstruction loss: 0.05953472852706909
Training batch 6 / 32
Total batch reconstruction loss: 0.061128053814172745
Training batch 7 / 32
Total batch reconstruction loss: 0.059965528547763824
Training batch 8 / 32
Total batch reconstruction loss: 0.05659913271665573
Training batch 9 / 32
Total batch reconstruction loss: 0.058259960263967514
Training batch 10 / 32
Total batch reconstruction loss: 0.0635203868150711
Training batch 11 / 32
Total batch reconstruction loss: 0.061163321137428284
Training batch 12 / 32
Total batch reconstruction loss: 0.0570426806807518
Training batch 13 / 32
Total batch reconstruction loss: 0.0596441775560379
Training batch 14 / 32
Total batch reconstruction loss: 0.058988235890865326
Training batch 15 / 32
Total batch reconstruction loss: 0.05718798562884331
Training batch 16 / 32
Total batch reconstruction loss: 0.059597503393888474
Training batch 17 / 32
Total batch reconstruction loss: 0.06341078877449036
Training batch 18 / 32
Total batch reconstruction loss: 0.0636545941233635
Training batch 19 / 32
Total batch reconstruction loss: 0.055392589420080185
Training batch 20 / 32
Total batch reconstruction loss: 0.06250247359275818
Training batch 21 / 32
Total batch reconstruction loss: 0.061118967831134796
Training batch 22 / 32
Total batch reconstruction loss: 0.06475885957479477
Training batch 23 / 32
Total batch reconstruction loss: 0.05924838036298752
Training batch 24 / 32
Total batch reconstruction loss: 0.06629464030265808
Training batch 25 / 32
Total batch reconstruction loss: 0.06025570258498192
Training batch 26 / 32
Total batch reconstruction loss: 0.06092822551727295
Training batch 27 / 32
Total batch reconstruction loss: 0.05813232809305191
Training batch 28 / 32
Total batch reconstruction loss: 0.057124316692352295
Training batch 29 / 32
Total batch reconstruction loss: 0.058316633105278015
Training batch 30 / 32
Total batch reconstruction loss: 0.0581655278801918
Training batch 31 / 32
Total batch reconstruction loss: 0.058411531150341034
Training batch 32 / 32
Total batch reconstruction loss: 0.04930105805397034
Epoch [190/500], Train Loss: 0.0589, Validation Loss: 0.0586, Generator Loss: 12.0519, Discriminator Loss: 0.3331
Training epoch 191 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.06183095648884773
Training batch 2 / 32
Total batch reconstruction loss: 0.055044155567884445
Training batch 3 / 32
Total batch reconstruction loss: 0.059573348611593246
Training batch 4 / 32
Total batch reconstruction loss: 0.059192754328250885
Training batch 5 / 32
Total batch reconstruction loss: 0.06384553015232086
Training batch 6 / 32
Total batch reconstruction loss: 0.06412763148546219
Training batch 7 / 32
Total batch reconstruction loss: 0.05897629261016846
Training batch 8 / 32
Total batch reconstruction loss: 0.060819149017333984
Training batch 9 / 32
Total batch reconstruction loss: 0.06085657700896263
Training batch 10 / 32
Total batch reconstruction loss: 0.05903097614645958
Training batch 11 / 32
Total batch reconstruction loss: 0.05899177864193916
Training batch 12 / 32
Total batch reconstruction loss: 0.0594310536980629
Training batch 13 / 32
Total batch reconstruction loss: 0.05807671695947647
Training batch 14 / 32
Total batch reconstruction loss: 0.061996590346097946
Training batch 15 / 32
Total batch reconstruction loss: 0.061258718371391296
Training batch 16 / 32
Total batch reconstruction loss: 0.0637715756893158
Training batch 17 / 32
Total batch reconstruction loss: 0.05783119425177574
Training batch 18 / 32
Total batch reconstruction loss: 0.059391532093286514
Training batch 19 / 32
Total batch reconstruction loss: 0.059166595339775085
Training batch 20 / 32
Total batch reconstruction loss: 0.06063811853528023
Training batch 21 / 32
Total batch reconstruction loss: 0.05949783697724342
Training batch 22 / 32
Total batch reconstruction loss: 0.05677109211683273
Training batch 23 / 32
Total batch reconstruction loss: 0.05979616940021515
Training batch 24 / 32
Total batch reconstruction loss: 0.06660923361778259
Training batch 25 / 32
Total batch reconstruction loss: 0.05720949172973633
Training batch 26 / 32
Total batch reconstruction loss: 0.06100800633430481
Training batch 27 / 32
Total batch reconstruction loss: 0.05985032021999359
Training batch 28 / 32
Total batch reconstruction loss: 0.05986649915575981
Training batch 29 / 32
Total batch reconstruction loss: 0.06491027027368546
Training batch 30 / 32
Total batch reconstruction loss: 0.06022690236568451
Training batch 31 / 32
Total batch reconstruction loss: 0.05648047477006912
Training batch 32 / 32
Total batch reconstruction loss: 0.06484706699848175
Epoch [191/500], Train Loss: 0.0592, Validation Loss: 0.0595, Generator Loss: 12.1497, Discriminator Loss: 0.2919
Training epoch 192 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.061254553496837616
Training batch 2 / 32
Total batch reconstruction loss: 0.05920882895588875
Training batch 3 / 32
Total batch reconstruction loss: 0.06071476638317108
Training batch 4 / 32
Total batch reconstruction loss: 0.06799157708883286
Training batch 5 / 32
Total batch reconstruction loss: 0.0619194321334362
Training batch 6 / 32
Total batch reconstruction loss: 0.05978744849562645
Training batch 7 / 32
Total batch reconstruction loss: 0.05834332853555679
Training batch 8 / 32
Total batch reconstruction loss: 0.06202102452516556
Training batch 9 / 32
Total batch reconstruction loss: 0.060200534760951996
Training batch 10 / 32
Total batch reconstruction loss: 0.05994989722967148
Training batch 11 / 32
Total batch reconstruction loss: 0.05899959057569504
Training batch 12 / 32
Total batch reconstruction loss: 0.062266089022159576
Training batch 13 / 32
Total batch reconstruction loss: 0.05900121480226517
Training batch 14 / 32
Total batch reconstruction loss: 0.05783023312687874
Training batch 15 / 32
Total batch reconstruction loss: 0.062085047364234924
Training batch 16 / 32
Total batch reconstruction loss: 0.0661860853433609
Training batch 17 / 32
Total batch reconstruction loss: 0.06035719811916351
Training batch 18 / 32
Total batch reconstruction loss: 0.05728559195995331
Training batch 19 / 32
Total batch reconstruction loss: 0.05560411512851715
Training batch 20 / 32
Total batch reconstruction loss: 0.055014751851558685
Training batch 21 / 32
Total batch reconstruction loss: 0.06709782779216766
Training batch 22 / 32
Total batch reconstruction loss: 0.058629367500543594
Training batch 23 / 32
Total batch reconstruction loss: 0.061057236045598984
Training batch 24 / 32
Total batch reconstruction loss: 0.06116204708814621
Training batch 25 / 32
Total batch reconstruction loss: 0.058432649821043015
Training batch 26 / 32
Total batch reconstruction loss: 0.05843322351574898
Training batch 27 / 32
Total batch reconstruction loss: 0.05933820456266403
Training batch 28 / 32
Total batch reconstruction loss: 0.059785716235637665
Training batch 29 / 32
Total batch reconstruction loss: 0.06036188080906868
Training batch 30 / 32
Total batch reconstruction loss: 0.06089099869132042
Training batch 31 / 32
Total batch reconstruction loss: 0.06589303910732269
Training batch 32 / 32
Total batch reconstruction loss: 0.06304677575826645
Epoch [192/500], Train Loss: 0.0597, Validation Loss: 0.0586, Generator Loss: 12.1878, Discriminator Loss: 0.3253
Training epoch 193 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.06166825443506241
Training batch 2 / 32
Total batch reconstruction loss: 0.05781402811408043
Training batch 3 / 32
Total batch reconstruction loss: 0.05486808717250824
Training batch 4 / 32
Total batch reconstruction loss: 0.0627271831035614
Training batch 5 / 32
Total batch reconstruction loss: 0.0680103749036789
Training batch 6 / 32
Total batch reconstruction loss: 0.06104607880115509
Training batch 7 / 32
Total batch reconstruction loss: 0.06312108784914017
Training batch 8 / 32
Total batch reconstruction loss: 0.059481289237737656
Training batch 9 / 32
Total batch reconstruction loss: 0.05907588079571724
Training batch 10 / 32
Total batch reconstruction loss: 0.061724092811346054
Training batch 11 / 32
Total batch reconstruction loss: 0.06061626225709915
Training batch 12 / 32
Total batch reconstruction loss: 0.061451591551303864
Training batch 13 / 32
Total batch reconstruction loss: 0.05828971043229103
Training batch 14 / 32
Total batch reconstruction loss: 0.06586529314517975
Training batch 15 / 32
Total batch reconstruction loss: 0.06035279482603073
Training batch 16 / 32
Total batch reconstruction loss: 0.06274176388978958
Training batch 17 / 32
Total batch reconstruction loss: 0.05641520023345947
Training batch 18 / 32
Total batch reconstruction loss: 0.0610528290271759
Training batch 19 / 32
Total batch reconstruction loss: 0.06094738095998764
Training batch 20 / 32
Total batch reconstruction loss: 0.05660494789481163
Training batch 21 / 32
Total batch reconstruction loss: 0.05864732712507248
Training batch 22 / 32
Total batch reconstruction loss: 0.05980084836483002
Training batch 23 / 32
Total batch reconstruction loss: 0.05701720714569092
Training batch 24 / 32
Total batch reconstruction loss: 0.06305354833602905
Training batch 25 / 32
Total batch reconstruction loss: 0.05487988889217377
Training batch 26 / 32
Total batch reconstruction loss: 0.06702899932861328
Training batch 27 / 32
Total batch reconstruction loss: 0.06695093959569931
Training batch 28 / 32
Total batch reconstruction loss: 0.05571727454662323
Training batch 29 / 32
Total batch reconstruction loss: 0.05897916108369827
Training batch 30 / 32
Total batch reconstruction loss: 0.060544706881046295
Training batch 31 / 32
Total batch reconstruction loss: 0.05785468965768814
Training batch 32 / 32
Total batch reconstruction loss: 0.05565895885229111
Epoch [193/500], Train Loss: 0.0592, Validation Loss: 0.0604, Generator Loss: 12.1359, Discriminator Loss: 0.3131
Training epoch 194 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.060398709028959274
Training batch 2 / 32
Total batch reconstruction loss: 0.06715792417526245
Training batch 3 / 32
Total batch reconstruction loss: 0.06818480789661407
Training batch 4 / 32
Total batch reconstruction loss: 0.058248370885849
Training batch 5 / 32
Total batch reconstruction loss: 0.06205039471387863
Training batch 6 / 32
Total batch reconstruction loss: 0.06230516359210014
Training batch 7 / 32
Total batch reconstruction loss: 0.061578039079904556
Training batch 8 / 32
Total batch reconstruction loss: 0.06126959249377251
Training batch 9 / 32
Total batch reconstruction loss: 0.06091117113828659
Training batch 10 / 32
Total batch reconstruction loss: 0.0642610713839531
Training batch 11 / 32
Total batch reconstruction loss: 0.06104377284646034
Training batch 12 / 32
Total batch reconstruction loss: 0.06154106557369232
Training batch 13 / 32
Total batch reconstruction loss: 0.06435780227184296
Training batch 14 / 32
Total batch reconstruction loss: 0.055131688714027405
Training batch 15 / 32
Total batch reconstruction loss: 0.05545970797538757
Training batch 16 / 32
Total batch reconstruction loss: 0.060754310339689255
Training batch 17 / 32
Total batch reconstruction loss: 0.06350201368331909
Training batch 18 / 32
Total batch reconstruction loss: 0.05634193494915962
Training batch 19 / 32
Total batch reconstruction loss: 0.05843230336904526
Training batch 20 / 32
Total batch reconstruction loss: 0.05874785780906677
Training batch 21 / 32
Total batch reconstruction loss: 0.06330803036689758
Training batch 22 / 32
Total batch reconstruction loss: 0.05840175226330757
Training batch 23 / 32
Total batch reconstruction loss: 0.057999446988105774
Training batch 24 / 32
Total batch reconstruction loss: 0.060648784041404724
Training batch 25 / 32
Total batch reconstruction loss: 0.05565492808818817
Training batch 26 / 32
Total batch reconstruction loss: 0.059812311083078384
Training batch 27 / 32
Total batch reconstruction loss: 0.06403792649507523
Training batch 28 / 32
Total batch reconstruction loss: 0.06393314898014069
Training batch 29 / 32
Total batch reconstruction loss: 0.059961602091789246
Training batch 30 / 32
Total batch reconstruction loss: 0.06130160391330719
Training batch 31 / 32
Total batch reconstruction loss: 0.06108495965600014
Training batch 32 / 32
Total batch reconstruction loss: 0.06540839374065399
Epoch [194/500], Train Loss: 0.0603, Validation Loss: 0.0605, Generator Loss: 12.2817, Discriminator Loss: 0.3157
Training epoch 195 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.05799420177936554
Training batch 2 / 32
Total batch reconstruction loss: 0.06162041053175926
Training batch 3 / 32
Total batch reconstruction loss: 0.05665983259677887
Training batch 4 / 32
Total batch reconstruction loss: 0.059915054589509964
Training batch 5 / 32
Total batch reconstruction loss: 0.05905325338244438
Training batch 6 / 32
Total batch reconstruction loss: 0.05819026380777359
Training batch 7 / 32
Total batch reconstruction loss: 0.05835955590009689
Training batch 8 / 32
Total batch reconstruction loss: 0.058705247938632965
Training batch 9 / 32
Total batch reconstruction loss: 0.060345131903886795
Training batch 10 / 32
Total batch reconstruction loss: 0.05812731012701988
Training batch 11 / 32
Total batch reconstruction loss: 0.06762395799160004
Training batch 12 / 32
Total batch reconstruction loss: 0.06263826042413712
Training batch 13 / 32
Total batch reconstruction loss: 0.057471856474876404
Training batch 14 / 32
Total batch reconstruction loss: 0.0593310222029686
Training batch 15 / 32
Total batch reconstruction loss: 0.058418937027454376
Training batch 16 / 32
Total batch reconstruction loss: 0.06300664693117142
Training batch 17 / 32
Total batch reconstruction loss: 0.06011655181646347
Training batch 18 / 32
Total batch reconstruction loss: 0.057231605052948
Training batch 19 / 32
Total batch reconstruction loss: 0.057879459112882614
Training batch 20 / 32
Total batch reconstruction loss: 0.06524501740932465
Training batch 21 / 32
Total batch reconstruction loss: 0.06022508442401886
Training batch 22 / 32
Total batch reconstruction loss: 0.062328726053237915
Training batch 23 / 32
Total batch reconstruction loss: 0.06054803729057312
Training batch 24 / 32
Total batch reconstruction loss: 0.05817446857690811
Training batch 25 / 32
Total batch reconstruction loss: 0.05964120849967003
Training batch 26 / 32
Total batch reconstruction loss: 0.055863916873931885
Training batch 27 / 32
Total batch reconstruction loss: 0.05869048088788986
Training batch 28 / 32
Total batch reconstruction loss: 0.06058128923177719
Training batch 29 / 32
Total batch reconstruction loss: 0.06886053085327148
Training batch 30 / 32
Total batch reconstruction loss: 0.06456159800291061
Training batch 31 / 32
Total batch reconstruction loss: 0.06356322765350342
Training batch 32 / 32
Total batch reconstruction loss: 0.06654931604862213
Epoch [195/500], Train Loss: 0.0595, Validation Loss: 0.0586, Generator Loss: 12.1884, Discriminator Loss: 0.3095
Training epoch 196 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.06124080717563629
Training batch 2 / 32
Total batch reconstruction loss: 0.05872604250907898
Training batch 3 / 32
Total batch reconstruction loss: 0.06096542626619339
Training batch 4 / 32
Total batch reconstruction loss: 0.06254582852125168
Training batch 5 / 32
Total batch reconstruction loss: 0.06545305997133255
Training batch 6 / 32
Total batch reconstruction loss: 0.060385994613170624
Training batch 7 / 32
Total batch reconstruction loss: 0.05819900333881378
Training batch 8 / 32
Total batch reconstruction loss: 0.05732186883687973
Training batch 9 / 32
Total batch reconstruction loss: 0.061097949743270874
Training batch 10 / 32
Total batch reconstruction loss: 0.056286439299583435
Training batch 11 / 32
Total batch reconstruction loss: 0.06490612775087357
Training batch 12 / 32
Total batch reconstruction loss: 0.0612289123237133
Training batch 13 / 32
Total batch reconstruction loss: 0.06396088004112244
Training batch 14 / 32
Total batch reconstruction loss: 0.056203559041023254
Training batch 15 / 32
Total batch reconstruction loss: 0.06313630193471909
Training batch 16 / 32
Total batch reconstruction loss: 0.05887426808476448
Training batch 17 / 32
Total batch reconstruction loss: 0.0618261992931366
Training batch 18 / 32
Total batch reconstruction loss: 0.059070486575365067
Training batch 19 / 32
Total batch reconstruction loss: 0.05946611240506172
Training batch 20 / 32
Total batch reconstruction loss: 0.05779455602169037
Training batch 21 / 32
Total batch reconstruction loss: 0.05694898962974548
Training batch 22 / 32
Total batch reconstruction loss: 0.0676192045211792
Training batch 23 / 32
Total batch reconstruction loss: 0.06202299892902374
Training batch 24 / 32
Total batch reconstruction loss: 0.06138204038143158
Training batch 25 / 32
Total batch reconstruction loss: 0.06260731816291809
Training batch 26 / 32
Total batch reconstruction loss: 0.06217934563755989
Training batch 27 / 32
Total batch reconstruction loss: 0.05805394798517227
Training batch 28 / 32
Total batch reconstruction loss: 0.05670870095491409
Training batch 29 / 32
Total batch reconstruction loss: 0.060830265283584595
Training batch 30 / 32
Total batch reconstruction loss: 0.06254003942012787
Training batch 31 / 32
Total batch reconstruction loss: 0.05747982859611511
Training batch 32 / 32
Total batch reconstruction loss: 0.0659344419836998
Epoch [196/500], Train Loss: 0.0596, Validation Loss: 0.0591, Generator Loss: 12.2197, Discriminator Loss: 0.3084
Training epoch 197 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.06109321862459183
Training batch 2 / 32
Total batch reconstruction loss: 0.0638236477971077
Training batch 3 / 32
Total batch reconstruction loss: 0.06268049031496048
Training batch 4 / 32
Total batch reconstruction loss: 0.05787774920463562
Training batch 5 / 32
Total batch reconstruction loss: 0.05712568759918213
Training batch 6 / 32
Total batch reconstruction loss: 0.0642232894897461
Training batch 7 / 32
Total batch reconstruction loss: 0.06095116585493088
Training batch 8 / 32
Total batch reconstruction loss: 0.06008126959204674
Training batch 9 / 32
Total batch reconstruction loss: 0.06484507024288177
Training batch 10 / 32
Total batch reconstruction loss: 0.06209883838891983
Training batch 11 / 32
Total batch reconstruction loss: 0.060416191816329956
Training batch 12 / 32
Total batch reconstruction loss: 0.059695832431316376
Training batch 13 / 32
Total batch reconstruction loss: 0.059865836054086685
Training batch 14 / 32
Total batch reconstruction loss: 0.05936770141124725
Training batch 15 / 32
Total batch reconstruction loss: 0.05680780112743378
Training batch 16 / 32
Total batch reconstruction loss: 0.0613606721162796
Training batch 17 / 32
Total batch reconstruction loss: 0.06054111197590828
Training batch 18 / 32
Total batch reconstruction loss: 0.05492032319307327
Training batch 19 / 32
Total batch reconstruction loss: 0.057452403008937836
Training batch 20 / 32
Total batch reconstruction loss: 0.057021088898181915
Training batch 21 / 32
Total batch reconstruction loss: 0.06443677842617035
Training batch 22 / 32
Total batch reconstruction loss: 0.05993761867284775
Training batch 23 / 32
Total batch reconstruction loss: 0.060552965849637985
Training batch 24 / 32
Total batch reconstruction loss: 0.0599244087934494
Training batch 25 / 32
Total batch reconstruction loss: 0.0572379045188427
Training batch 26 / 32
Total batch reconstruction loss: 0.05930662900209427
Training batch 27 / 32
Total batch reconstruction loss: 0.06315620988607407
Training batch 28 / 32
Total batch reconstruction loss: 0.05940946936607361
Training batch 29 / 32
Total batch reconstruction loss: 0.06325303018093109
Training batch 30 / 32
Total batch reconstruction loss: 0.06193992868065834
Training batch 31 / 32
Total batch reconstruction loss: 0.05882902443408966
Training batch 32 / 32
Total batch reconstruction loss: 0.06451068073511124
Epoch [197/500], Train Loss: 0.0594, Validation Loss: 0.0585, Generator Loss: 12.1556, Discriminator Loss: 0.3311
Training epoch 198 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.05789788067340851
Training batch 2 / 32
Total batch reconstruction loss: 0.060165971517562866
Training batch 3 / 32
Total batch reconstruction loss: 0.06292198598384857
Training batch 4 / 32
Total batch reconstruction loss: 0.05979186296463013
Training batch 5 / 32
Total batch reconstruction loss: 0.05832841247320175
Training batch 6 / 32
Total batch reconstruction loss: 0.059552356600761414
Training batch 7 / 32
Total batch reconstruction loss: 0.06310772150754929
Training batch 8 / 32
Total batch reconstruction loss: 0.06136472895741463
Training batch 9 / 32
Total batch reconstruction loss: 0.05991426110267639
Training batch 10 / 32
Total batch reconstruction loss: 0.06604789197444916
Training batch 11 / 32
Total batch reconstruction loss: 0.05970568209886551
Training batch 12 / 32
Total batch reconstruction loss: 0.05864648520946503
Training batch 13 / 32
Total batch reconstruction loss: 0.06360666453838348
Training batch 14 / 32
Total batch reconstruction loss: 0.059332698583602905
Training batch 15 / 32
Total batch reconstruction loss: 0.0633273646235466
Training batch 16 / 32
Total batch reconstruction loss: 0.05867825821042061
Training batch 17 / 32
Total batch reconstruction loss: 0.06197899580001831
Training batch 18 / 32
Total batch reconstruction loss: 0.06110997870564461
Training batch 19 / 32
Total batch reconstruction loss: 0.05990638583898544
Training batch 20 / 32
Total batch reconstruction loss: 0.060661669820547104
Training batch 21 / 32
Total batch reconstruction loss: 0.06027091294527054
Training batch 22 / 32
Total batch reconstruction loss: 0.06519131362438202
Training batch 23 / 32
Total batch reconstruction loss: 0.05946968495845795
Training batch 24 / 32
Total batch reconstruction loss: 0.05748383700847626
Training batch 25 / 32
Total batch reconstruction loss: 0.06198100745677948
Training batch 26 / 32
Total batch reconstruction loss: 0.05866522714495659
Training batch 27 / 32
Total batch reconstruction loss: 0.05602892115712166
Training batch 28 / 32
Total batch reconstruction loss: 0.0574195459485054
Training batch 29 / 32
Total batch reconstruction loss: 0.0642336905002594
Training batch 30 / 32
Total batch reconstruction loss: 0.05757814273238182
Training batch 31 / 32
Total batch reconstruction loss: 0.06584879755973816
Training batch 32 / 32
Total batch reconstruction loss: 0.049571551382541656
Epoch [198/500], Train Loss: 0.0595, Validation Loss: 0.0590, Generator Loss: 12.1274, Discriminator Loss: 0.3224
Training epoch 199 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.06043578311800957
Training batch 2 / 32
Total batch reconstruction loss: 0.060657478868961334
Training batch 3 / 32
Total batch reconstruction loss: 0.05840624123811722
Training batch 4 / 32
Total batch reconstruction loss: 0.0575723759829998
Training batch 5 / 32
Total batch reconstruction loss: 0.05791633203625679
Training batch 6 / 32
Total batch reconstruction loss: 0.057981446385383606
Training batch 7 / 32
Total batch reconstruction loss: 0.06125721335411072
Training batch 8 / 32
Total batch reconstruction loss: 0.058586277067661285
Training batch 9 / 32
Total batch reconstruction loss: 0.060817599296569824
Training batch 10 / 32
Total batch reconstruction loss: 0.05802750959992409
Training batch 11 / 32
Total batch reconstruction loss: 0.06384941935539246
Training batch 12 / 32
Total batch reconstruction loss: 0.06330998986959457
Training batch 13 / 32
Total batch reconstruction loss: 0.05837036669254303
Training batch 14 / 32
Total batch reconstruction loss: 0.05955352261662483
Training batch 15 / 32
Total batch reconstruction loss: 0.0598144456744194
Training batch 16 / 32
Total batch reconstruction loss: 0.058066993951797485
Training batch 17 / 32
Total batch reconstruction loss: 0.06312564015388489
Training batch 18 / 32
Total batch reconstruction loss: 0.06579811125993729
Training batch 19 / 32
Total batch reconstruction loss: 0.05894477665424347
Training batch 20 / 32
Total batch reconstruction loss: 0.06221059337258339
Training batch 21 / 32
Total batch reconstruction loss: 0.05634192004799843
Training batch 22 / 32
Total batch reconstruction loss: 0.05982322245836258
Training batch 23 / 32
Total batch reconstruction loss: 0.06239759176969528
Training batch 24 / 32
Total batch reconstruction loss: 0.06134570389986038
Training batch 25 / 32
Total batch reconstruction loss: 0.058349862694740295
Training batch 26 / 32
Total batch reconstruction loss: 0.06139744073152542
Training batch 27 / 32
Total batch reconstruction loss: 0.06604933738708496
Training batch 28 / 32
Total batch reconstruction loss: 0.06296822428703308
Training batch 29 / 32
Total batch reconstruction loss: 0.06352484971284866
Training batch 30 / 32
Total batch reconstruction loss: 0.06114739179611206
Training batch 31 / 32
Total batch reconstruction loss: 0.05934009328484535
Training batch 32 / 32
Total batch reconstruction loss: 0.05490884929895401
Epoch [199/500], Train Loss: 0.0590, Validation Loss: 0.0598, Generator Loss: 12.1600, Discriminator Loss: 0.3030
Training epoch 200 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.055898115038871765
Training batch 2 / 32
Total batch reconstruction loss: 0.05877359211444855
Training batch 3 / 32
Total batch reconstruction loss: 0.057688988745212555
Training batch 4 / 32
Total batch reconstruction loss: 0.06276226788759232
Training batch 5 / 32
Total batch reconstruction loss: 0.06187216192483902
Training batch 6 / 32
Total batch reconstruction loss: 0.06358358263969421
Training batch 7 / 32
Total batch reconstruction loss: 0.06440222263336182
Training batch 8 / 32
Total batch reconstruction loss: 0.057492006570100784
Training batch 9 / 32
Total batch reconstruction loss: 0.06033767759799957
Training batch 10 / 32
Total batch reconstruction loss: 0.061261918395757675
Training batch 11 / 32
Total batch reconstruction loss: 0.058814603835344315
Training batch 12 / 32
Total batch reconstruction loss: 0.0608992725610733
Training batch 13 / 32
Total batch reconstruction loss: 0.061060868203639984
Training batch 14 / 32
Total batch reconstruction loss: 0.06435361504554749
Training batch 15 / 32
Total batch reconstruction loss: 0.05733954533934593
Training batch 16 / 32
Total batch reconstruction loss: 0.061736881732940674
Training batch 17 / 32
Total batch reconstruction loss: 0.058219969272613525
Training batch 18 / 32
Total batch reconstruction loss: 0.06009354069828987
Training batch 19 / 32
Total batch reconstruction loss: 0.056058771908283234
Training batch 20 / 32
Total batch reconstruction loss: 0.05921277403831482
Training batch 21 / 32
Total batch reconstruction loss: 0.05928429588675499
Training batch 22 / 32
Total batch reconstruction loss: 0.060145772993564606
Training batch 23 / 32
Total batch reconstruction loss: 0.06118465214967728
Training batch 24 / 32
Total batch reconstruction loss: 0.058751024305820465
Training batch 25 / 32
Total batch reconstruction loss: 0.061076849699020386
Training batch 26 / 32
Total batch reconstruction loss: 0.056981202214956284
Training batch 27 / 32
Total batch reconstruction loss: 0.06677160412073135
Training batch 28 / 32
Total batch reconstruction loss: 0.05902361869812012
Training batch 29 / 32
Total batch reconstruction loss: 0.05931508541107178
Training batch 30 / 32
Total batch reconstruction loss: 0.05799733102321625
Training batch 31 / 32
Total batch reconstruction loss: 0.05891507863998413
Training batch 32 / 32
Total batch reconstruction loss: 0.047548603266477585
Epoch [200/500], Train Loss: 0.0584, Validation Loss: 0.0585, Generator Loss: 12.0036, Discriminator Loss: 0.3122
Training epoch 201 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.058365918695926666
Training batch 2 / 32
Total batch reconstruction loss: 0.05887313932180405
Training batch 3 / 32
Total batch reconstruction loss: 0.058924004435539246
Training batch 4 / 32
Total batch reconstruction loss: 0.05888604372739792
Training batch 5 / 32
Total batch reconstruction loss: 0.05833329260349274
Training batch 6 / 32
Total batch reconstruction loss: 0.06006345525383949
Training batch 7 / 32
Total batch reconstruction loss: 0.05796437710523605
Training batch 8 / 32
Total batch reconstruction loss: 0.06463433057069778
Training batch 9 / 32
Total batch reconstruction loss: 0.059180598706007004
Training batch 10 / 32
Total batch reconstruction loss: 0.060830388218164444
Training batch 11 / 32
Total batch reconstruction loss: 0.05765578895807266
Training batch 12 / 32
Total batch reconstruction loss: 0.06174509599804878
Training batch 13 / 32
Total batch reconstruction loss: 0.06269224733114243
Training batch 14 / 32
Total batch reconstruction loss: 0.06022839993238449
Training batch 15 / 32
Total batch reconstruction loss: 0.059259817004203796
Training batch 16 / 32
Total batch reconstruction loss: 0.059591952711343765
Training batch 17 / 32
Total batch reconstruction loss: 0.06140682101249695
Training batch 18 / 32
Total batch reconstruction loss: 0.05960916355252266
Training batch 19 / 32
Total batch reconstruction loss: 0.06255410611629486
Training batch 20 / 32
Total batch reconstruction loss: 0.05655381828546524
Training batch 21 / 32
Total batch reconstruction loss: 0.05652628093957901
Training batch 22 / 32
Total batch reconstruction loss: 0.06507246196269989
Training batch 23 / 32
Total batch reconstruction loss: 0.05894209444522858
Training batch 24 / 32
Total batch reconstruction loss: 0.05924931913614273
Training batch 25 / 32
Total batch reconstruction loss: 0.05852213501930237
Training batch 26 / 32
Total batch reconstruction loss: 0.05887272581458092
Training batch 27 / 32
Total batch reconstruction loss: 0.0668727457523346
Training batch 28 / 32
Total batch reconstruction loss: 0.05800550431013107
Training batch 29 / 32
Total batch reconstruction loss: 0.06278875470161438
Training batch 30 / 32
Total batch reconstruction loss: 0.06053542345762253
Training batch 31 / 32
Total batch reconstruction loss: 0.05531996488571167
Training batch 32 / 32
Total batch reconstruction loss: 0.07547023892402649
Epoch [201/500], Train Loss: 0.0591, Validation Loss: 0.0585, Generator Loss: 12.1597, Discriminator Loss: 0.3096
Training epoch 202 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.056488342583179474
Training batch 2 / 32
Total batch reconstruction loss: 0.058865517377853394
Training batch 3 / 32
Total batch reconstruction loss: 0.06416158378124237
Training batch 4 / 32
Total batch reconstruction loss: 0.062275826930999756
Training batch 5 / 32
Total batch reconstruction loss: 0.05945975333452225
Training batch 6 / 32
Total batch reconstruction loss: 0.0627431571483612
Training batch 7 / 32
Total batch reconstruction loss: 0.06334195286035538
Training batch 8 / 32
Total batch reconstruction loss: 0.0671767145395279
Training batch 9 / 32
Total batch reconstruction loss: 0.0594034343957901
Training batch 10 / 32
Total batch reconstruction loss: 0.0649566575884819
Training batch 11 / 32
Total batch reconstruction loss: 0.06395541876554489
Training batch 12 / 32
Total batch reconstruction loss: 0.05940123647451401
Training batch 13 / 32
Total batch reconstruction loss: 0.05685289949178696
Training batch 14 / 32
Total batch reconstruction loss: 0.060584597289562225
Training batch 15 / 32
Total batch reconstruction loss: 0.056434232741594315
Training batch 16 / 32
Total batch reconstruction loss: 0.06043509393930435
Training batch 17 / 32
Total batch reconstruction loss: 0.05951014161109924
Training batch 18 / 32
Total batch reconstruction loss: 0.06282240152359009
Training batch 19 / 32
Total batch reconstruction loss: 0.059971775859594345
Training batch 20 / 32
Total batch reconstruction loss: 0.05747320502996445
Training batch 21 / 32
Total batch reconstruction loss: 0.06060432642698288
Training batch 22 / 32
Total batch reconstruction loss: 0.05972879007458687
Training batch 23 / 32
Total batch reconstruction loss: 0.06210307031869888
Training batch 24 / 32
Total batch reconstruction loss: 0.06312516331672668
Training batch 25 / 32
Total batch reconstruction loss: 0.05778668075799942
Training batch 26 / 32
Total batch reconstruction loss: 0.05557141825556755
Training batch 27 / 32
Total batch reconstruction loss: 0.06517741084098816
Training batch 28 / 32
Total batch reconstruction loss: 0.058378785848617554
Training batch 29 / 32
Total batch reconstruction loss: 0.060087770223617554
Training batch 30 / 32
Total batch reconstruction loss: 0.060500048100948334
Training batch 31 / 32
Total batch reconstruction loss: 0.059034522622823715
Training batch 32 / 32
Total batch reconstruction loss: 0.06073382869362831
Epoch [202/500], Train Loss: 0.0594, Validation Loss: 0.0592, Generator Loss: 12.1894, Discriminator Loss: 0.3153
Training epoch 203 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.06648021191358566
Training batch 2 / 32
Total batch reconstruction loss: 0.06500384211540222
Training batch 3 / 32
Total batch reconstruction loss: 0.059130050241947174
Training batch 4 / 32
Total batch reconstruction loss: 0.05985359847545624
Training batch 5 / 32
Total batch reconstruction loss: 0.06173938512802124
Training batch 6 / 32
Total batch reconstruction loss: 0.05816196650266647
Training batch 7 / 32
Total batch reconstruction loss: 0.06282724440097809
Training batch 8 / 32
Total batch reconstruction loss: 0.05971892550587654
Training batch 9 / 32
Total batch reconstruction loss: 0.056934237480163574
Training batch 10 / 32
Total batch reconstruction loss: 0.061933502554893494
Training batch 11 / 32
Total batch reconstruction loss: 0.05994800850749016
Training batch 12 / 32
Total batch reconstruction loss: 0.05585695803165436
Training batch 13 / 32
Total batch reconstruction loss: 0.05933763086795807
Training batch 14 / 32
Total batch reconstruction loss: 0.05904654040932655
Training batch 15 / 32
Total batch reconstruction loss: 0.06075751781463623
Training batch 16 / 32
Total batch reconstruction loss: 0.05727533996105194
Training batch 17 / 32
Total batch reconstruction loss: 0.058530911803245544
Training batch 18 / 32
Total batch reconstruction loss: 0.05648992210626602
Training batch 19 / 32
Total batch reconstruction loss: 0.06627550721168518
Training batch 20 / 32
Total batch reconstruction loss: 0.06186213344335556
Training batch 21 / 32
Total batch reconstruction loss: 0.056436747312545776
Training batch 22 / 32
Total batch reconstruction loss: 0.06034216284751892
Training batch 23 / 32
Total batch reconstruction loss: 0.05874539166688919
Training batch 24 / 32
Total batch reconstruction loss: 0.06088638678193092
Training batch 25 / 32
Total batch reconstruction loss: 0.058714691549539566
Training batch 26 / 32
Total batch reconstruction loss: 0.06165724992752075
Training batch 27 / 32
Total batch reconstruction loss: 0.06347044557332993
Training batch 28 / 32
Total batch reconstruction loss: 0.05565767362713814
Training batch 29 / 32
Total batch reconstruction loss: 0.05821014568209648
Training batch 30 / 32
Total batch reconstruction loss: 0.0648597925901413
Training batch 31 / 32
Total batch reconstruction loss: 0.058122098445892334
Training batch 32 / 32
Total batch reconstruction loss: 0.06982243806123734
Epoch [203/500], Train Loss: 0.0592, Validation Loss: 0.0588, Generator Loss: 12.1643, Discriminator Loss: 0.3142
Training epoch 204 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.06264980137348175
Training batch 2 / 32
Total batch reconstruction loss: 0.05988924950361252
Training batch 3 / 32
Total batch reconstruction loss: 0.05849922448396683
Training batch 4 / 32
Total batch reconstruction loss: 0.056714631617069244
Training batch 5 / 32
Total batch reconstruction loss: 0.062093690037727356
Training batch 6 / 32
Total batch reconstruction loss: 0.06266477704048157
Training batch 7 / 32
Total batch reconstruction loss: 0.0612199604511261
Training batch 8 / 32
Total batch reconstruction loss: 0.06155683845281601
Training batch 9 / 32
Total batch reconstruction loss: 0.06394834071397781
Training batch 10 / 32
Total batch reconstruction loss: 0.06335264444351196
Training batch 11 / 32
Total batch reconstruction loss: 0.058979690074920654
Training batch 12 / 32
Total batch reconstruction loss: 0.060665570199489594
Training batch 13 / 32
Total batch reconstruction loss: 0.05910289287567139
Training batch 14 / 32
Total batch reconstruction loss: 0.05969836190342903
Training batch 15 / 32
Total batch reconstruction loss: 0.06181733310222626
Training batch 16 / 32
Total batch reconstruction loss: 0.05652649700641632
Training batch 17 / 32
Total batch reconstruction loss: 0.06376798450946808
Training batch 18 / 32
Total batch reconstruction loss: 0.05807524919509888
Training batch 19 / 32
Total batch reconstruction loss: 0.06109694391489029
Training batch 20 / 32
Total batch reconstruction loss: 0.06075522303581238
Training batch 21 / 32
Total batch reconstruction loss: 0.062357209622859955
Training batch 22 / 32
Total batch reconstruction loss: 0.0621497668325901
Training batch 23 / 32
Total batch reconstruction loss: 0.05947195738554001
Training batch 24 / 32
Total batch reconstruction loss: 0.06167908012866974
Training batch 25 / 32
Total batch reconstruction loss: 0.061992183327674866
Training batch 26 / 32
Total batch reconstruction loss: 0.0591578409075737
Training batch 27 / 32
Total batch reconstruction loss: 0.05936317890882492
Training batch 28 / 32
Total batch reconstruction loss: 0.05906578153371811
Training batch 29 / 32
Total batch reconstruction loss: 0.05572739988565445
Training batch 30 / 32
Total batch reconstruction loss: 0.06034616753458977
Training batch 31 / 32
Total batch reconstruction loss: 0.060235895216464996
Training batch 32 / 32
Total batch reconstruction loss: 0.059382542967796326
Epoch [204/500], Train Loss: 0.0595, Validation Loss: 0.0590, Generator Loss: 12.1583, Discriminator Loss: 0.3145
Training epoch 205 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.056708917021751404
Training batch 2 / 32
Total batch reconstruction loss: 0.06505464017391205
Training batch 3 / 32
Total batch reconstruction loss: 0.06081129238009453
Training batch 4 / 32
Total batch reconstruction loss: 0.06047865003347397
Training batch 5 / 32
Total batch reconstruction loss: 0.05768507346510887
Training batch 6 / 32
Total batch reconstruction loss: 0.059020884335041046
Training batch 7 / 32
Total batch reconstruction loss: 0.062421951442956924
Training batch 8 / 32
Total batch reconstruction loss: 0.060957230627536774
Training batch 9 / 32
Total batch reconstruction loss: 0.05940675735473633
Training batch 10 / 32
Total batch reconstruction loss: 0.06144805997610092
Training batch 11 / 32
Total batch reconstruction loss: 0.059439074248075485
Training batch 12 / 32
Total batch reconstruction loss: 0.06158490851521492
Training batch 13 / 32
Total batch reconstruction loss: 0.05942422151565552
Training batch 14 / 32
Total batch reconstruction loss: 0.05828968435525894
Training batch 15 / 32
Total batch reconstruction loss: 0.05870484933257103
Training batch 16 / 32
Total batch reconstruction loss: 0.05862419307231903
Training batch 17 / 32
Total batch reconstruction loss: 0.07078999280929565
Training batch 18 / 32
Total batch reconstruction loss: 0.05883864313364029
Training batch 19 / 32
Total batch reconstruction loss: 0.06077336519956589
Training batch 20 / 32
Total batch reconstruction loss: 0.0568876676261425
Training batch 21 / 32
Total batch reconstruction loss: 0.05994280427694321
Training batch 22 / 32
Total batch reconstruction loss: 0.05577573552727699
Training batch 23 / 32
Total batch reconstruction loss: 0.06037011370062828
Training batch 24 / 32
Total batch reconstruction loss: 0.05923301354050636
Training batch 25 / 32
Total batch reconstruction loss: 0.06330963224172592
Training batch 26 / 32
Total batch reconstruction loss: 0.05881129577755928
Training batch 27 / 32
Total batch reconstruction loss: 0.06539467722177505
Training batch 28 / 32
Total batch reconstruction loss: 0.05957304686307907
Training batch 29 / 32
Total batch reconstruction loss: 0.05458729714155197
Training batch 30 / 32
Total batch reconstruction loss: 0.057392217218875885
Training batch 31 / 32
Total batch reconstruction loss: 0.05952455848455429
Training batch 32 / 32
Total batch reconstruction loss: 0.06207956001162529
Epoch [205/500], Train Loss: 0.0588, Validation Loss: 0.0591, Generator Loss: 12.0865, Discriminator Loss: 0.3264
Training epoch 206 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.05832725018262863
Training batch 2 / 32
Total batch reconstruction loss: 0.05995498597621918
Training batch 3 / 32
Total batch reconstruction loss: 0.06331343203783035
Training batch 4 / 32
Total batch reconstruction loss: 0.058093685656785965
Training batch 5 / 32
Total batch reconstruction loss: 0.06315955519676208
Training batch 6 / 32
Total batch reconstruction loss: 0.0573834665119648
Training batch 7 / 32
Total batch reconstruction loss: 0.06658865511417389
Training batch 8 / 32
Total batch reconstruction loss: 0.0635761097073555
Training batch 9 / 32
Total batch reconstruction loss: 0.06141063570976257
Training batch 10 / 32
Total batch reconstruction loss: 0.05743721127510071
Training batch 11 / 32
Total batch reconstruction loss: 0.06270822882652283
Training batch 12 / 32
Total batch reconstruction loss: 0.06079607829451561
Training batch 13 / 32
Total batch reconstruction loss: 0.06426675617694855
Training batch 14 / 32
Total batch reconstruction loss: 0.05951067805290222
Training batch 15 / 32
Total batch reconstruction loss: 0.062476906925439835
Training batch 16 / 32
Total batch reconstruction loss: 0.061237674206495285
Training batch 17 / 32
Total batch reconstruction loss: 0.05287613719701767
Training batch 18 / 32
Total batch reconstruction loss: 0.058729387819767
Training batch 19 / 32
Total batch reconstruction loss: 0.06158395856618881
Training batch 20 / 32
Total batch reconstruction loss: 0.0548330694437027
Training batch 21 / 32
Total batch reconstruction loss: 0.06636900454759598
Training batch 22 / 32
Total batch reconstruction loss: 0.0670483186841011
Training batch 23 / 32
Total batch reconstruction loss: 0.05942266806960106
Training batch 24 / 32
Total batch reconstruction loss: 0.05886180326342583
Training batch 25 / 32
Total batch reconstruction loss: 0.05667824298143387
Training batch 26 / 32
Total batch reconstruction loss: 0.06774716824293137
Training batch 27 / 32
Total batch reconstruction loss: 0.06109541654586792
Training batch 28 / 32
Total batch reconstruction loss: 0.059442419558763504
Training batch 29 / 32
Total batch reconstruction loss: 0.05808905139565468
Training batch 30 / 32
Total batch reconstruction loss: 0.06178662180900574
Training batch 31 / 32
Total batch reconstruction loss: 0.057455144822597504
Training batch 32 / 32
Total batch reconstruction loss: 0.06301570683717728
Epoch [206/500], Train Loss: 0.0600, Validation Loss: 0.0598, Generator Loss: 12.2274, Discriminator Loss: 0.3134
Training epoch 207 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.06465595960617065
Training batch 2 / 32
Total batch reconstruction loss: 0.05877559632062912
Training batch 3 / 32
Total batch reconstruction loss: 0.06179368495941162
Training batch 4 / 32
Total batch reconstruction loss: 0.059877801686525345
Training batch 5 / 32
Total batch reconstruction loss: 0.05942815542221069
Training batch 6 / 32
Total batch reconstruction loss: 0.0570608526468277
Training batch 7 / 32
Total batch reconstruction loss: 0.06406262516975403
Training batch 8 / 32
Total batch reconstruction loss: 0.06099208444356918
Training batch 9 / 32
Total batch reconstruction loss: 0.05714920163154602
Training batch 10 / 32
Total batch reconstruction loss: 0.06050194054841995
Training batch 11 / 32
Total batch reconstruction loss: 0.06246749684214592
Training batch 12 / 32
Total batch reconstruction loss: 0.061376508325338364
Training batch 13 / 32
Total batch reconstruction loss: 0.06283146142959595
Training batch 14 / 32
Total batch reconstruction loss: 0.05677952617406845
Training batch 15 / 32
Total batch reconstruction loss: 0.057533763349056244
Training batch 16 / 32
Total batch reconstruction loss: 0.057580575346946716
Training batch 17 / 32
Total batch reconstruction loss: 0.059038400650024414
Training batch 18 / 32
Total batch reconstruction loss: 0.061685360968112946
Training batch 19 / 32
Total batch reconstruction loss: 0.05879978835582733
Training batch 20 / 32
Total batch reconstruction loss: 0.05822642147541046
Training batch 21 / 32
Total batch reconstruction loss: 0.060927413403987885
Training batch 22 / 32
Total batch reconstruction loss: 0.05742723122239113
Training batch 23 / 32
Total batch reconstruction loss: 0.060850970447063446
Training batch 24 / 32
Total batch reconstruction loss: 0.06332845240831375
Training batch 25 / 32
Total batch reconstruction loss: 0.06377263367176056
Training batch 26 / 32
Total batch reconstruction loss: 0.06083928048610687
Training batch 27 / 32
Total batch reconstruction loss: 0.06270357966423035
Training batch 28 / 32
Total batch reconstruction loss: 0.05872465297579765
Training batch 29 / 32
Total batch reconstruction loss: 0.06063046306371689
Training batch 30 / 32
Total batch reconstruction loss: 0.06074340641498566
Training batch 31 / 32
Total batch reconstruction loss: 0.05535687878727913
Training batch 32 / 32
Total batch reconstruction loss: 0.051562003791332245
Epoch [207/500], Train Loss: 0.0586, Validation Loss: 0.0616, Generator Loss: 12.0533, Discriminator Loss: 0.3177
Training epoch 208 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.058480218052864075
Training batch 2 / 32
Total batch reconstruction loss: 0.0654897391796112
Training batch 3 / 32
Total batch reconstruction loss: 0.05884094536304474
Training batch 4 / 32
Total batch reconstruction loss: 0.06305087357759476
Training batch 5 / 32
Total batch reconstruction loss: 0.060738932341337204
Training batch 6 / 32
Total batch reconstruction loss: 0.06145195662975311
Training batch 7 / 32
Total batch reconstruction loss: 0.05861179530620575
Training batch 8 / 32
Total batch reconstruction loss: 0.05591592937707901
Training batch 9 / 32
Total batch reconstruction loss: 0.06333232671022415
Training batch 10 / 32
Total batch reconstruction loss: 0.05897658318281174
Training batch 11 / 32
Total batch reconstruction loss: 0.0617169588804245
Training batch 12 / 32
Total batch reconstruction loss: 0.05609015375375748
Training batch 13 / 32
Total batch reconstruction loss: 0.061193760484457016
Training batch 14 / 32
Total batch reconstruction loss: 0.05634632706642151
Training batch 15 / 32
Total batch reconstruction loss: 0.05998220294713974
Training batch 16 / 32
Total batch reconstruction loss: 0.06242276728153229
Training batch 17 / 32
Total batch reconstruction loss: 0.06898598372936249
Training batch 18 / 32
Total batch reconstruction loss: 0.0596657358109951
Training batch 19 / 32
Total batch reconstruction loss: 0.05684801936149597
Training batch 20 / 32
Total batch reconstruction loss: 0.06138933077454567
Training batch 21 / 32
Total batch reconstruction loss: 0.0654318556189537
Training batch 22 / 32
Total batch reconstruction loss: 0.05414876341819763
Training batch 23 / 32
Total batch reconstruction loss: 0.060864098370075226
Training batch 24 / 32
Total batch reconstruction loss: 0.0606512576341629
Training batch 25 / 32
Total batch reconstruction loss: 0.06422094255685806
Training batch 26 / 32
Total batch reconstruction loss: 0.05923675745725632
Training batch 27 / 32
Total batch reconstruction loss: 0.060492757707834244
Training batch 28 / 32
Total batch reconstruction loss: 0.06199125945568085
Training batch 29 / 32
Total batch reconstruction loss: 0.060247909277677536
Training batch 30 / 32
Total batch reconstruction loss: 0.06039140000939369
Training batch 31 / 32
Total batch reconstruction loss: 0.06168518587946892
Training batch 32 / 32
Total batch reconstruction loss: 0.06102024018764496
Epoch [208/500], Train Loss: 0.0597, Validation Loss: 0.0600, Generator Loss: 12.1966, Discriminator Loss: 0.3050
Training epoch 209 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.06401420384645462
Training batch 2 / 32
Total batch reconstruction loss: 0.06661951541900635
Training batch 3 / 32
Total batch reconstruction loss: 0.06187120079994202
Training batch 4 / 32
Total batch reconstruction loss: 0.05886406451463699
Training batch 5 / 32
Total batch reconstruction loss: 0.06191083788871765
Training batch 6 / 32
Total batch reconstruction loss: 0.05932184308767319
Training batch 7 / 32
Total batch reconstruction loss: 0.06310606002807617
Training batch 8 / 32
Total batch reconstruction loss: 0.06407007575035095
Training batch 9 / 32
Total batch reconstruction loss: 0.05830644443631172
Training batch 10 / 32
Total batch reconstruction loss: 0.05631719529628754
Training batch 11 / 32
Total batch reconstruction loss: 0.06055092439055443
Training batch 12 / 32
Total batch reconstruction loss: 0.06067592650651932
Training batch 13 / 32
Total batch reconstruction loss: 0.05956605076789856
Training batch 14 / 32
Total batch reconstruction loss: 0.05719301849603653
Training batch 15 / 32
Total batch reconstruction loss: 0.06342107057571411
Training batch 16 / 32
Total batch reconstruction loss: 0.060992710292339325
Training batch 17 / 32
Total batch reconstruction loss: 0.05570681393146515
Training batch 18 / 32
Total batch reconstruction loss: 0.0594678595662117
Training batch 19 / 32
Total batch reconstruction loss: 0.05992467701435089
Training batch 20 / 32
Total batch reconstruction loss: 0.05833417922258377
Training batch 21 / 32
Total batch reconstruction loss: 0.059362150728702545
Training batch 22 / 32
Total batch reconstruction loss: 0.060272179543972015
Training batch 23 / 32
Total batch reconstruction loss: 0.05469372123479843
Training batch 24 / 32
Total batch reconstruction loss: 0.06520610302686691
Training batch 25 / 32
Total batch reconstruction loss: 0.06296058744192123
Training batch 26 / 32
Total batch reconstruction loss: 0.061211515218019485
Training batch 27 / 32
Total batch reconstruction loss: 0.06127661466598511
Training batch 28 / 32
Total batch reconstruction loss: 0.05955009162425995
Training batch 29 / 32
Total batch reconstruction loss: 0.05564502626657486
Training batch 30 / 32
Total batch reconstruction loss: 0.060518063604831696
Training batch 31 / 32
Total batch reconstruction loss: 0.06282827258110046
Training batch 32 / 32
Total batch reconstruction loss: 0.053577788174152374
Epoch [209/500], Train Loss: 0.0589, Validation Loss: 0.0596, Generator Loss: 12.1168, Discriminator Loss: 0.3138
Training epoch 210 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.058801449835300446
Training batch 2 / 32
Total batch reconstruction loss: 0.06385824829339981
Training batch 3 / 32
Total batch reconstruction loss: 0.06117018312215805
Training batch 4 / 32
Total batch reconstruction loss: 0.05726486071944237
Training batch 5 / 32
Total batch reconstruction loss: 0.05585550516843796
Training batch 6 / 32
Total batch reconstruction loss: 0.06242251396179199
Training batch 7 / 32
Total batch reconstruction loss: 0.060338497161865234
Training batch 8 / 32
Total batch reconstruction loss: 0.05835005268454552
Training batch 9 / 32
Total batch reconstruction loss: 0.06054828315973282
Training batch 10 / 32
Total batch reconstruction loss: 0.058832645416259766
Training batch 11 / 32
Total batch reconstruction loss: 0.05932653695344925
Training batch 12 / 32
Total batch reconstruction loss: 0.05873482674360275
Training batch 13 / 32
Total batch reconstruction loss: 0.05966273695230484
Training batch 14 / 32
Total batch reconstruction loss: 0.06831751018762589
Training batch 15 / 32
Total batch reconstruction loss: 0.06483282148838043
Training batch 16 / 32
Total batch reconstruction loss: 0.05877570062875748
Training batch 17 / 32
Total batch reconstruction loss: 0.06140458583831787
Training batch 18 / 32
Total batch reconstruction loss: 0.05849616229534149
Training batch 19 / 32
Total batch reconstruction loss: 0.06348744034767151
Training batch 20 / 32
Total batch reconstruction loss: 0.0589628666639328
Training batch 21 / 32
Total batch reconstruction loss: 0.06165227293968201
Training batch 22 / 32
Total batch reconstruction loss: 0.06210172176361084
Training batch 23 / 32
Total batch reconstruction loss: 0.05886555463075638
Training batch 24 / 32
Total batch reconstruction loss: 0.05863874778151512
Training batch 25 / 32
Total batch reconstruction loss: 0.061294905841350555
Training batch 26 / 32
Total batch reconstruction loss: 0.0616205632686615
Training batch 27 / 32
Total batch reconstruction loss: 0.05961678922176361
Training batch 28 / 32
Total batch reconstruction loss: 0.06753851473331451
Training batch 29 / 32
Total batch reconstruction loss: 0.055105410516262054
Training batch 30 / 32
Total batch reconstruction loss: 0.06048749387264252
Training batch 31 / 32
Total batch reconstruction loss: 0.06094451993703842
Training batch 32 / 32
Total batch reconstruction loss: 0.05733741447329521
Epoch [210/500], Train Loss: 0.0595, Validation Loss: 0.0590, Generator Loss: 12.1721, Discriminator Loss: 0.2979
Training epoch 211 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.054301269352436066
Training batch 2 / 32
Total batch reconstruction loss: 0.06040717661380768
Training batch 3 / 32
Total batch reconstruction loss: 0.06122910976409912
Training batch 4 / 32
Total batch reconstruction loss: 0.06112151965498924
Training batch 5 / 32
Total batch reconstruction loss: 0.05997387692332268
Training batch 6 / 32
Total batch reconstruction loss: 0.0635635256767273
Training batch 7 / 32
Total batch reconstruction loss: 0.057748645544052124
Training batch 8 / 32
Total batch reconstruction loss: 0.060978662222623825
Training batch 9 / 32
Total batch reconstruction loss: 0.06316249072551727
Training batch 10 / 32
Total batch reconstruction loss: 0.05857821926474571
Training batch 11 / 32
Total batch reconstruction loss: 0.06294888257980347
Training batch 12 / 32
Total batch reconstruction loss: 0.057893916964530945
Training batch 13 / 32
Total batch reconstruction loss: 0.06366442143917084
Training batch 14 / 32
Total batch reconstruction loss: 0.056534409523010254
Training batch 15 / 32
Total batch reconstruction loss: 0.06794773042201996
Training batch 16 / 32
Total batch reconstruction loss: 0.06262966990470886
Training batch 17 / 32
Total batch reconstruction loss: 0.05974514037370682
Training batch 18 / 32
Total batch reconstruction loss: 0.06219639629125595
Training batch 19 / 32
Total batch reconstruction loss: 0.06340838968753815
Training batch 20 / 32
Total batch reconstruction loss: 0.06314931064844131
Training batch 21 / 32
Total batch reconstruction loss: 0.05943949893116951
Training batch 22 / 32
Total batch reconstruction loss: 0.058065265417099
Training batch 23 / 32
Total batch reconstruction loss: 0.057599522173404694
Training batch 24 / 32
Total batch reconstruction loss: 0.05854245647788048
Training batch 25 / 32
Total batch reconstruction loss: 0.05946524441242218
Training batch 26 / 32
Total batch reconstruction loss: 0.06389336287975311
Training batch 27 / 32
Total batch reconstruction loss: 0.0573045015335083
Training batch 28 / 32
Total batch reconstruction loss: 0.059655047953128815
Training batch 29 / 32
Total batch reconstruction loss: 0.06549009680747986
Training batch 30 / 32
Total batch reconstruction loss: 0.059249237179756165
Training batch 31 / 32
Total batch reconstruction loss: 0.05796261131763458
Training batch 32 / 32
Total batch reconstruction loss: 0.07313218712806702
Epoch [211/500], Train Loss: 0.0593, Validation Loss: 0.0617, Generator Loss: 12.2730, Discriminator Loss: 0.2999
Training epoch 212 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.06115424260497093
Training batch 2 / 32
Total batch reconstruction loss: 0.06035877391695976
Training batch 3 / 32
Total batch reconstruction loss: 0.057874567806720734
Training batch 4 / 32
Total batch reconstruction loss: 0.059519462287425995
Training batch 5 / 32
Total batch reconstruction loss: 0.058614157140254974
Training batch 6 / 32
Total batch reconstruction loss: 0.06177052855491638
Training batch 7 / 32
Total batch reconstruction loss: 0.060525670647621155
Training batch 8 / 32
Total batch reconstruction loss: 0.059655748307704926
Training batch 9 / 32
Total batch reconstruction loss: 0.06200052425265312
Training batch 10 / 32
Total batch reconstruction loss: 0.06397043168544769
Training batch 11 / 32
Total batch reconstruction loss: 0.06296384334564209
Training batch 12 / 32
Total batch reconstruction loss: 0.06594256311655045
Training batch 13 / 32
Total batch reconstruction loss: 0.056841544806957245
Training batch 14 / 32
Total batch reconstruction loss: 0.06267564743757248
Training batch 15 / 32
Total batch reconstruction loss: 0.06424960494041443
Training batch 16 / 32
Total batch reconstruction loss: 0.06191633641719818
Training batch 17 / 32
Total batch reconstruction loss: 0.056979767978191376
Training batch 18 / 32
Total batch reconstruction loss: 0.058543071150779724
Training batch 19 / 32
Total batch reconstruction loss: 0.06032610684633255
Training batch 20 / 32
Total batch reconstruction loss: 0.05753426253795624
Training batch 21 / 32
Total batch reconstruction loss: 0.06282376497983932
Training batch 22 / 32
Total batch reconstruction loss: 0.05909200385212898
Training batch 23 / 32
Total batch reconstruction loss: 0.05759921669960022
Training batch 24 / 32
Total batch reconstruction loss: 0.060425758361816406
Training batch 25 / 32
Total batch reconstruction loss: 0.06459799408912659
Training batch 26 / 32
Total batch reconstruction loss: 0.06179903447628021
Training batch 27 / 32
Total batch reconstruction loss: 0.061898812651634216
Training batch 28 / 32
Total batch reconstruction loss: 0.06222120672464371
Training batch 29 / 32
Total batch reconstruction loss: 0.06298636645078659
Training batch 30 / 32
Total batch reconstruction loss: 0.058620646595954895
Training batch 31 / 32
Total batch reconstruction loss: 0.06114218384027481
Training batch 32 / 32
Total batch reconstruction loss: 0.059118352830410004
Epoch [212/500], Train Loss: 0.0598, Validation Loss: 0.0605, Generator Loss: 12.2356, Discriminator Loss: 0.3076
Training epoch 213 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.059431158006191254
Training batch 2 / 32
Total batch reconstruction loss: 0.06100166589021683
Training batch 3 / 32
Total batch reconstruction loss: 0.0637623518705368
Training batch 4 / 32
Total batch reconstruction loss: 0.058858729898929596
Training batch 5 / 32
Total batch reconstruction loss: 0.05784420669078827
Training batch 6 / 32
Total batch reconstruction loss: 0.05934373289346695
Training batch 7 / 32
Total batch reconstruction loss: 0.0583643913269043
Training batch 8 / 32
Total batch reconstruction loss: 0.05772721767425537
Training batch 9 / 32
Total batch reconstruction loss: 0.06657329201698303
Training batch 10 / 32
Total batch reconstruction loss: 0.06633636355400085
Training batch 11 / 32
Total batch reconstruction loss: 0.06392013281583786
Training batch 12 / 32
Total batch reconstruction loss: 0.06570014357566833
Training batch 13 / 32
Total batch reconstruction loss: 0.05917910858988762
Training batch 14 / 32
Total batch reconstruction loss: 0.05896038934588432
Training batch 15 / 32
Total batch reconstruction loss: 0.06241593137383461
Training batch 16 / 32
Total batch reconstruction loss: 0.06137441098690033
Training batch 17 / 32
Total batch reconstruction loss: 0.06380908191204071
Training batch 18 / 32
Total batch reconstruction loss: 0.056832365691661835
Training batch 19 / 32
Total batch reconstruction loss: 0.06368522346019745
Training batch 20 / 32
Total batch reconstruction loss: 0.05726069211959839
Training batch 21 / 32
Total batch reconstruction loss: 0.060544662177562714
Training batch 22 / 32
Total batch reconstruction loss: 0.06353990733623505
Training batch 23 / 32
Total batch reconstruction loss: 0.05801117792725563
Training batch 24 / 32
Total batch reconstruction loss: 0.05784423649311066
Training batch 25 / 32
Total batch reconstruction loss: 0.06173612177371979
Training batch 26 / 32
Total batch reconstruction loss: 0.05731240287423134
Training batch 27 / 32
Total batch reconstruction loss: 0.05915256589651108
Training batch 28 / 32
Total batch reconstruction loss: 0.05891653522849083
Training batch 29 / 32
Total batch reconstruction loss: 0.0594731941819191
Training batch 30 / 32
Total batch reconstruction loss: 0.06243827939033508
Training batch 31 / 32
Total batch reconstruction loss: 0.05532604083418846
Training batch 32 / 32
Total batch reconstruction loss: 0.051447562873363495
Epoch [213/500], Train Loss: 0.0593, Validation Loss: 0.0595, Generator Loss: 12.1229, Discriminator Loss: 0.3087
Training epoch 214 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.060710471123456955
Training batch 2 / 32
Total batch reconstruction loss: 0.06751102209091187
Training batch 3 / 32
Total batch reconstruction loss: 0.06041121855378151
Training batch 4 / 32
Total batch reconstruction loss: 0.05520695075392723
Training batch 5 / 32
Total batch reconstruction loss: 0.060276392847299576
Training batch 6 / 32
Total batch reconstruction loss: 0.055928025394678116
Training batch 7 / 32
Total batch reconstruction loss: 0.06076648458838463
Training batch 8 / 32
Total batch reconstruction loss: 0.05910991132259369
Training batch 9 / 32
Total batch reconstruction loss: 0.05672961845993996
Training batch 10 / 32
Total batch reconstruction loss: 0.05713098496198654
Training batch 11 / 32
Total batch reconstruction loss: 0.06476663053035736
Training batch 12 / 32
Total batch reconstruction loss: 0.05586552619934082
Training batch 13 / 32
Total batch reconstruction loss: 0.06256559491157532
Training batch 14 / 32
Total batch reconstruction loss: 0.05736003816127777
Training batch 15 / 32
Total batch reconstruction loss: 0.062252819538116455
Training batch 16 / 32
Total batch reconstruction loss: 0.060495346784591675
Training batch 17 / 32
Total batch reconstruction loss: 0.06277161836624146
Training batch 18 / 32
Total batch reconstruction loss: 0.05603833869099617
Training batch 19 / 32
Total batch reconstruction loss: 0.058844514191150665
Training batch 20 / 32
Total batch reconstruction loss: 0.07311766594648361
Training batch 21 / 32
Total batch reconstruction loss: 0.0643797516822815
Training batch 22 / 32
Total batch reconstruction loss: 0.06319410353899002
Training batch 23 / 32
Total batch reconstruction loss: 0.05927200987935066
Training batch 24 / 32
Total batch reconstruction loss: 0.0592171736061573
Training batch 25 / 32
Total batch reconstruction loss: 0.06177707016468048
Training batch 26 / 32
Total batch reconstruction loss: 0.05918944627046585
Training batch 27 / 32
Total batch reconstruction loss: 0.057310961186885834
Training batch 28 / 32
Total batch reconstruction loss: 0.057541318237781525
Training batch 29 / 32
Total batch reconstruction loss: 0.057766906917095184
Training batch 30 / 32
Total batch reconstruction loss: 0.05792266130447388
Training batch 31 / 32
Total batch reconstruction loss: 0.06668414920568466
Training batch 32 / 32
Total batch reconstruction loss: 0.057481639087200165
Epoch [214/500], Train Loss: 0.0590, Validation Loss: 0.0606, Generator Loss: 12.1276, Discriminator Loss: 0.3142
Training epoch 215 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.06506634503602982
Training batch 2 / 32
Total batch reconstruction loss: 0.05633557587862015
Training batch 3 / 32
Total batch reconstruction loss: 0.058800309896469116
Training batch 4 / 32
Total batch reconstruction loss: 0.058264099061489105
Training batch 5 / 32
Total batch reconstruction loss: 0.05903013050556183
Training batch 6 / 32
Total batch reconstruction loss: 0.05980813875794411
Training batch 7 / 32
Total batch reconstruction loss: 0.06429588794708252
Training batch 8 / 32
Total batch reconstruction loss: 0.05937030538916588
Training batch 9 / 32
Total batch reconstruction loss: 0.06153416633605957
Training batch 10 / 32
Total batch reconstruction loss: 0.060338664799928665
Training batch 11 / 32
Total batch reconstruction loss: 0.060362301766872406
Training batch 12 / 32
Total batch reconstruction loss: 0.0613899827003479
Training batch 13 / 32
Total batch reconstruction loss: 0.061415866017341614
Training batch 14 / 32
Total batch reconstruction loss: 0.05899633839726448
Training batch 15 / 32
Total batch reconstruction loss: 0.06132105737924576
Training batch 16 / 32
Total batch reconstruction loss: 0.0599580779671669
Training batch 17 / 32
Total batch reconstruction loss: 0.05998265743255615
Training batch 18 / 32
Total batch reconstruction loss: 0.0586913600564003
Training batch 19 / 32
Total batch reconstruction loss: 0.05978399142622948
Training batch 20 / 32
Total batch reconstruction loss: 0.05887933820486069
Training batch 21 / 32
Total batch reconstruction loss: 0.06337226927280426
Training batch 22 / 32
Total batch reconstruction loss: 0.059511154890060425
Training batch 23 / 32
Total batch reconstruction loss: 0.06705065071582794
Training batch 24 / 32
Total batch reconstruction loss: 0.05749497562646866
Training batch 25 / 32
Total batch reconstruction loss: 0.05858892947435379
Training batch 26 / 32
Total batch reconstruction loss: 0.060281090438365936
Training batch 27 / 32
Total batch reconstruction loss: 0.05841134488582611
Training batch 28 / 32
Total batch reconstruction loss: 0.0647396445274353
Training batch 29 / 32
Total batch reconstruction loss: 0.05829700827598572
Training batch 30 / 32
Total batch reconstruction loss: 0.06024550274014473
Training batch 31 / 32
Total batch reconstruction loss: 0.064692422747612
Training batch 32 / 32
Total batch reconstruction loss: 0.10032700002193451
Epoch [215/500], Train Loss: 0.0603, Validation Loss: 0.0582, Generator Loss: 12.4160, Discriminator Loss: 0.3307
Training epoch 216 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.05639701709151268
Training batch 2 / 32
Total batch reconstruction loss: 0.06066417694091797
Training batch 3 / 32
Total batch reconstruction loss: 0.06424445658922195
Training batch 4 / 32
Total batch reconstruction loss: 0.05939290672540665
Training batch 5 / 32
Total batch reconstruction loss: 0.05900764465332031
Training batch 6 / 32
Total batch reconstruction loss: 0.0597824789583683
Training batch 7 / 32
Total batch reconstruction loss: 0.06066500395536423
Training batch 8 / 32
Total batch reconstruction loss: 0.06620708107948303
Training batch 9 / 32
Total batch reconstruction loss: 0.0592227429151535
Training batch 10 / 32
Total batch reconstruction loss: 0.058118898421525955
Training batch 11 / 32
Total batch reconstruction loss: 0.06185681372880936
Training batch 12 / 32
Total batch reconstruction loss: 0.06114581227302551
Training batch 13 / 32
Total batch reconstruction loss: 0.06198018789291382
Training batch 14 / 32
Total batch reconstruction loss: 0.06030655652284622
Training batch 15 / 32
Total batch reconstruction loss: 0.05853290110826492
Training batch 16 / 32
Total batch reconstruction loss: 0.060565341264009476
Training batch 17 / 32
Total batch reconstruction loss: 0.06018800288438797
Training batch 18 / 32
Total batch reconstruction loss: 0.0577242337167263
Training batch 19 / 32
Total batch reconstruction loss: 0.05535915493965149
Training batch 20 / 32
Total batch reconstruction loss: 0.05754219740629196
Training batch 21 / 32
Total batch reconstruction loss: 0.06446573883295059
Training batch 22 / 32
Total batch reconstruction loss: 0.057598233222961426
Training batch 23 / 32
Total batch reconstruction loss: 0.058488499373197556
Training batch 24 / 32
Total batch reconstruction loss: 0.065992571413517
Training batch 25 / 32
Total batch reconstruction loss: 0.059968456625938416
Training batch 26 / 32
Total batch reconstruction loss: 0.062312446534633636
Training batch 27 / 32
Total batch reconstruction loss: 0.06285466998815536
Training batch 28 / 32
Total batch reconstruction loss: 0.0590042844414711
Training batch 29 / 32
Total batch reconstruction loss: 0.05915076285600662
Training batch 30 / 32
Total batch reconstruction loss: 0.058856260031461716
Training batch 31 / 32
Total batch reconstruction loss: 0.0581999272108078
Training batch 32 / 32
Total batch reconstruction loss: 0.062211260199546814
Epoch [216/500], Train Loss: 0.0589, Validation Loss: 0.0594, Generator Loss: 12.1184, Discriminator Loss: 0.3102
Training epoch 217 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.06350798904895782
Training batch 2 / 32
Total batch reconstruction loss: 0.057934001088142395
Training batch 3 / 32
Total batch reconstruction loss: 0.05957094579935074
Training batch 4 / 32
Total batch reconstruction loss: 0.05788592994213104
Training batch 5 / 32
Total batch reconstruction loss: 0.05861438810825348
Training batch 6 / 32
Total batch reconstruction loss: 0.0579722598195076
Training batch 7 / 32
Total batch reconstruction loss: 0.06028169393539429
Training batch 8 / 32
Total batch reconstruction loss: 0.05771584063768387
Training batch 9 / 32
Total batch reconstruction loss: 0.06565214693546295
Training batch 10 / 32
Total batch reconstruction loss: 0.06240171194076538
Training batch 11 / 32
Total batch reconstruction loss: 0.061335489153862
Training batch 12 / 32
Total batch reconstruction loss: 0.05884905532002449
Training batch 13 / 32
Total batch reconstruction loss: 0.06117973476648331
Training batch 14 / 32
Total batch reconstruction loss: 0.060539547353982925
Training batch 15 / 32
Total batch reconstruction loss: 0.05762239173054695
Training batch 16 / 32
Total batch reconstruction loss: 0.06148449331521988
Training batch 17 / 32
Total batch reconstruction loss: 0.05621561408042908
Training batch 18 / 32
Total batch reconstruction loss: 0.05666856840252876
Training batch 19 / 32
Total batch reconstruction loss: 0.05716533958911896
Training batch 20 / 32
Total batch reconstruction loss: 0.05835800990462303
Training batch 21 / 32
Total batch reconstruction loss: 0.0634695515036583
Training batch 22 / 32
Total batch reconstruction loss: 0.05915423855185509
Training batch 23 / 32
Total batch reconstruction loss: 0.06433829665184021
Training batch 24 / 32
Total batch reconstruction loss: 0.05909431725740433
Training batch 25 / 32
Total batch reconstruction loss: 0.05996298789978027
Training batch 26 / 32
Total batch reconstruction loss: 0.05848655104637146
Training batch 27 / 32
Total batch reconstruction loss: 0.06537001579999924
Training batch 28 / 32
Total batch reconstruction loss: 0.05859976261854172
Training batch 29 / 32
Total batch reconstruction loss: 0.057642076164484024
Training batch 30 / 32
Total batch reconstruction loss: 0.05939148738980293
Training batch 31 / 32
Total batch reconstruction loss: 0.05819714441895485
Training batch 32 / 32
Total batch reconstruction loss: 0.08122684061527252
Epoch [217/500], Train Loss: 0.0591, Validation Loss: 0.0591, Generator Loss: 12.1718, Discriminator Loss: 0.3095
Training epoch 218 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.05977611243724823
Training batch 2 / 32
Total batch reconstruction loss: 0.058688413351774216
Training batch 3 / 32
Total batch reconstruction loss: 0.06130285561084747
Training batch 4 / 32
Total batch reconstruction loss: 0.05885013937950134
Training batch 5 / 32
Total batch reconstruction loss: 0.056889794766902924
Training batch 6 / 32
Total batch reconstruction loss: 0.056098297238349915
Training batch 7 / 32
Total batch reconstruction loss: 0.06300225853919983
Training batch 8 / 32
Total batch reconstruction loss: 0.05774317681789398
Training batch 9 / 32
Total batch reconstruction loss: 0.0635119155049324
Training batch 10 / 32
Total batch reconstruction loss: 0.06274938583374023
Training batch 11 / 32
Total batch reconstruction loss: 0.05846758931875229
Training batch 12 / 32
Total batch reconstruction loss: 0.06121998280286789
Training batch 13 / 32
Total batch reconstruction loss: 0.06215345859527588
Training batch 14 / 32
Total batch reconstruction loss: 0.06357090175151825
Training batch 15 / 32
Total batch reconstruction loss: 0.05834425985813141
Training batch 16 / 32
Total batch reconstruction loss: 0.058046258985996246
Training batch 17 / 32
Total batch reconstruction loss: 0.06286861002445221
Training batch 18 / 32
Total batch reconstruction loss: 0.05845407024025917
Training batch 19 / 32
Total batch reconstruction loss: 0.05845579504966736
Training batch 20 / 32
Total batch reconstruction loss: 0.05610598623752594
Training batch 21 / 32
Total batch reconstruction loss: 0.05933685600757599
Training batch 22 / 32
Total batch reconstruction loss: 0.0618869923055172
Training batch 23 / 32
Total batch reconstruction loss: 0.05457082390785217
Training batch 24 / 32
Total batch reconstruction loss: 0.06178336217999458
Training batch 25 / 32
Total batch reconstruction loss: 0.061082370579242706
Training batch 26 / 32
Total batch reconstruction loss: 0.059384822845458984
Training batch 27 / 32
Total batch reconstruction loss: 0.05661121755838394
Training batch 28 / 32
Total batch reconstruction loss: 0.06743931770324707
Training batch 29 / 32
Total batch reconstruction loss: 0.06128048896789551
Training batch 30 / 32
Total batch reconstruction loss: 0.06157746538519859
Training batch 31 / 32
Total batch reconstruction loss: 0.0692482739686966
Training batch 32 / 32
Total batch reconstruction loss: 0.054533906280994415
Epoch [218/500], Train Loss: 0.0591, Validation Loss: 0.0592, Generator Loss: 12.1037, Discriminator Loss: 0.3074
Training epoch 219 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.062194786965847015
Training batch 2 / 32
Total batch reconstruction loss: 0.05923330783843994
Training batch 3 / 32
Total batch reconstruction loss: 0.056434206664562225
Training batch 4 / 32
Total batch reconstruction loss: 0.06483292579650879
Training batch 5 / 32
Total batch reconstruction loss: 0.06524977087974548
Training batch 6 / 32
Total batch reconstruction loss: 0.058093711733818054
Training batch 7 / 32
Total batch reconstruction loss: 0.05853278189897537
Training batch 8 / 32
Total batch reconstruction loss: 0.05742920562624931
Training batch 9 / 32
Total batch reconstruction loss: 0.059271588921546936
Training batch 10 / 32
Total batch reconstruction loss: 0.06185309961438179
Training batch 11 / 32
Total batch reconstruction loss: 0.05712716653943062
Training batch 12 / 32
Total batch reconstruction loss: 0.054689109325408936
Training batch 13 / 32
Total batch reconstruction loss: 0.06343872845172882
Training batch 14 / 32
Total batch reconstruction loss: 0.05691085383296013
Training batch 15 / 32
Total batch reconstruction loss: 0.06430495530366898
Training batch 16 / 32
Total batch reconstruction loss: 0.05749977007508278
Training batch 17 / 32
Total batch reconstruction loss: 0.060747578740119934
Training batch 18 / 32
Total batch reconstruction loss: 0.05962610989809036
Training batch 19 / 32
Total batch reconstruction loss: 0.05750185623764992
Training batch 20 / 32
Total batch reconstruction loss: 0.058442626148462296
Training batch 21 / 32
Total batch reconstruction loss: 0.06120889633893967
Training batch 22 / 32
Total batch reconstruction loss: 0.058211155235767365
Training batch 23 / 32
Total batch reconstruction loss: 0.05815678834915161
Training batch 24 / 32
Total batch reconstruction loss: 0.06651522219181061
Training batch 25 / 32
Total batch reconstruction loss: 0.06101270020008087
Training batch 26 / 32
Total batch reconstruction loss: 0.05619475245475769
Training batch 27 / 32
Total batch reconstruction loss: 0.06429872661828995
Training batch 28 / 32
Total batch reconstruction loss: 0.0614212229847908
Training batch 29 / 32
Total batch reconstruction loss: 0.05717327073216438
Training batch 30 / 32
Total batch reconstruction loss: 0.05742279440164566
Training batch 31 / 32
Total batch reconstruction loss: 0.05718396604061127
Training batch 32 / 32
Total batch reconstruction loss: 0.055206164717674255
Epoch [219/500], Train Loss: 0.0583, Validation Loss: 0.0581, Generator Loss: 11.9892, Discriminator Loss: 0.3176
Training epoch 220 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.05593959614634514
Training batch 2 / 32
Total batch reconstruction loss: 0.05828312784433365
Training batch 3 / 32
Total batch reconstruction loss: 0.06171215698122978
Training batch 4 / 32
Total batch reconstruction loss: 0.0579807423055172
Training batch 5 / 32
Total batch reconstruction loss: 0.057317666709423065
Training batch 6 / 32
Total batch reconstruction loss: 0.0555441677570343
Training batch 7 / 32
Total batch reconstruction loss: 0.05780676007270813
Training batch 8 / 32
Total batch reconstruction loss: 0.06519444286823273
Training batch 9 / 32
Total batch reconstruction loss: 0.06327685713768005
Training batch 10 / 32
Total batch reconstruction loss: 0.06018039956688881
Training batch 11 / 32
Total batch reconstruction loss: 0.06466665118932724
Training batch 12 / 32
Total batch reconstruction loss: 0.060186922550201416
Training batch 13 / 32
Total batch reconstruction loss: 0.0603998526930809
Training batch 14 / 32
Total batch reconstruction loss: 0.05855661630630493
Training batch 15 / 32
Total batch reconstruction loss: 0.06549130380153656
Training batch 16 / 32
Total batch reconstruction loss: 0.06484163552522659
Training batch 17 / 32
Total batch reconstruction loss: 0.05867619067430496
Training batch 18 / 32
Total batch reconstruction loss: 0.06370757520198822
Training batch 19 / 32
Total batch reconstruction loss: 0.058314450085163116
Training batch 20 / 32
Total batch reconstruction loss: 0.061848267912864685
Training batch 21 / 32
Total batch reconstruction loss: 0.05880594998598099
Training batch 22 / 32
Total batch reconstruction loss: 0.054469525814056396
Training batch 23 / 32
Total batch reconstruction loss: 0.05708577483892441
Training batch 24 / 32
Total batch reconstruction loss: 0.06151018291711807
Training batch 25 / 32
Total batch reconstruction loss: 0.05982428789138794
Training batch 26 / 32
Total batch reconstruction loss: 0.06124985218048096
Training batch 27 / 32
Total batch reconstruction loss: 0.061119649559259415
Training batch 28 / 32
Total batch reconstruction loss: 0.062155839055776596
Training batch 29 / 32
Total batch reconstruction loss: 0.05826404690742493
Training batch 30 / 32
Total batch reconstruction loss: 0.058893803507089615
Training batch 31 / 32
Total batch reconstruction loss: 0.05957048386335373
Training batch 32 / 32
Total batch reconstruction loss: 0.05731254816055298
Epoch [220/500], Train Loss: 0.0586, Validation Loss: 0.0619, Generator Loss: 12.0565, Discriminator Loss: 0.3459
Training epoch 221 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.055596910417079926
Training batch 2 / 32
Total batch reconstruction loss: 0.06123495101928711
Training batch 3 / 32
Total batch reconstruction loss: 0.056709736585617065
Training batch 4 / 32
Total batch reconstruction loss: 0.06319637596607208
Training batch 5 / 32
Total batch reconstruction loss: 0.06279390305280685
Training batch 6 / 32
Total batch reconstruction loss: 0.05993518978357315
Training batch 7 / 32
Total batch reconstruction loss: 0.06148551404476166
Training batch 8 / 32
Total batch reconstruction loss: 0.06400854885578156
Training batch 9 / 32
Total batch reconstruction loss: 0.06099248677492142
Training batch 10 / 32
Total batch reconstruction loss: 0.06326249241828918
Training batch 11 / 32
Total batch reconstruction loss: 0.06013359874486923
Training batch 12 / 32
Total batch reconstruction loss: 0.06341229379177094
Training batch 13 / 32
Total batch reconstruction loss: 0.06290316581726074
Training batch 14 / 32
Total batch reconstruction loss: 0.05918462201952934
Training batch 15 / 32
Total batch reconstruction loss: 0.05881185457110405
Training batch 16 / 32
Total batch reconstruction loss: 0.06135067343711853
Training batch 17 / 32
Total batch reconstruction loss: 0.05857241898775101
Training batch 18 / 32
Total batch reconstruction loss: 0.05987799912691116
Training batch 19 / 32
Total batch reconstruction loss: 0.06467381119728088
Training batch 20 / 32
Total batch reconstruction loss: 0.05970902740955353
Training batch 21 / 32
Total batch reconstruction loss: 0.05757927894592285
Training batch 22 / 32
Total batch reconstruction loss: 0.06039557233452797
Training batch 23 / 32
Total batch reconstruction loss: 0.06097598373889923
Training batch 24 / 32
Total batch reconstruction loss: 0.05765515938401222
Training batch 25 / 32
Total batch reconstruction loss: 0.05821996182203293
Training batch 26 / 32
Total batch reconstruction loss: 0.06106477230787277
Training batch 27 / 32
Total batch reconstruction loss: 0.05960787460207939
Training batch 28 / 32
Total batch reconstruction loss: 0.05882551521062851
Training batch 29 / 32
Total batch reconstruction loss: 0.06309492886066437
Training batch 30 / 32
Total batch reconstruction loss: 0.05972881615161896
Training batch 31 / 32
Total batch reconstruction loss: 0.061349283903837204
Training batch 32 / 32
Total batch reconstruction loss: 0.09811477363109589
Epoch [221/500], Train Loss: 0.0605, Validation Loss: 0.0602, Generator Loss: 12.4164, Discriminator Loss: 0.3133
Training epoch 222 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.05705945938825607
Training batch 2 / 32
Total batch reconstruction loss: 0.06085677444934845
Training batch 3 / 32
Total batch reconstruction loss: 0.06505861133337021
Training batch 4 / 32
Total batch reconstruction loss: 0.05970938876271248
Training batch 5 / 32
Total batch reconstruction loss: 0.06393449008464813
Training batch 6 / 32
Total batch reconstruction loss: 0.06288807094097137
Training batch 7 / 32
Total batch reconstruction loss: 0.05718667060136795
Training batch 8 / 32
Total batch reconstruction loss: 0.0677824318408966
Training batch 9 / 32
Total batch reconstruction loss: 0.06075930967926979
Training batch 10 / 32
Total batch reconstruction loss: 0.061355289071798325
Training batch 11 / 32
Total batch reconstruction loss: 0.05940695106983185
Training batch 12 / 32
Total batch reconstruction loss: 0.05509175360202789
Training batch 13 / 32
Total batch reconstruction loss: 0.06111886352300644
Training batch 14 / 32
Total batch reconstruction loss: 0.05858089029788971
Training batch 15 / 32
Total batch reconstruction loss: 0.06348109245300293
Training batch 16 / 32
Total batch reconstruction loss: 0.06145452708005905
Training batch 17 / 32
Total batch reconstruction loss: 0.06197166442871094
Training batch 18 / 32
Total batch reconstruction loss: 0.06155015528202057
Training batch 19 / 32
Total batch reconstruction loss: 0.06463636457920074
Training batch 20 / 32
Total batch reconstruction loss: 0.06081143021583557
Training batch 21 / 32
Total batch reconstruction loss: 0.06208527088165283
Training batch 22 / 32
Total batch reconstruction loss: 0.05582650005817413
Training batch 23 / 32
Total batch reconstruction loss: 0.06226172670722008
Training batch 24 / 32
Total batch reconstruction loss: 0.056152936071157455
Training batch 25 / 32
Total batch reconstruction loss: 0.058131229132413864
Training batch 26 / 32
Total batch reconstruction loss: 0.05534079670906067
Training batch 27 / 32
Total batch reconstruction loss: 0.06167756766080856
Training batch 28 / 32
Total batch reconstruction loss: 0.05995715409517288
Training batch 29 / 32
Total batch reconstruction loss: 0.060335323214530945
Training batch 30 / 32
Total batch reconstruction loss: 0.05641452595591545
Training batch 31 / 32
Total batch reconstruction loss: 0.05625000596046448
Training batch 32 / 32
Total batch reconstruction loss: 0.07046246528625488
Epoch [222/500], Train Loss: 0.0594, Validation Loss: 0.0581, Generator Loss: 12.1930, Discriminator Loss: 0.3088
Training epoch 223 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.06033127382397652
Training batch 2 / 32
Total batch reconstruction loss: 0.06005479395389557
Training batch 3 / 32
Total batch reconstruction loss: 0.059500329196453094
Training batch 4 / 32
Total batch reconstruction loss: 0.062149692326784134
Training batch 5 / 32
Total batch reconstruction loss: 0.06365068256855011
Training batch 6 / 32
Total batch reconstruction loss: 0.05727195739746094
Training batch 7 / 32
Total batch reconstruction loss: 0.058623164892196655
Training batch 8 / 32
Total batch reconstruction loss: 0.06067729741334915
Training batch 9 / 32
Total batch reconstruction loss: 0.060628604143857956
Training batch 10 / 32
Total batch reconstruction loss: 0.05944084748625755
Training batch 11 / 32
Total batch reconstruction loss: 0.05691302567720413
Training batch 12 / 32
Total batch reconstruction loss: 0.05968009680509567
Training batch 13 / 32
Total batch reconstruction loss: 0.05811399221420288
Training batch 14 / 32
Total batch reconstruction loss: 0.06446070224046707
Training batch 15 / 32
Total batch reconstruction loss: 0.05663764849305153
Training batch 16 / 32
Total batch reconstruction loss: 0.05555561184883118
Training batch 17 / 32
Total batch reconstruction loss: 0.061563313007354736
Training batch 18 / 32
Total batch reconstruction loss: 0.05795042961835861
Training batch 19 / 32
Total batch reconstruction loss: 0.06439168751239777
Training batch 20 / 32
Total batch reconstruction loss: 0.060208626091480255
Training batch 21 / 32
Total batch reconstruction loss: 0.06139596551656723
Training batch 22 / 32
Total batch reconstruction loss: 0.06340298056602478
Training batch 23 / 32
Total batch reconstruction loss: 0.06450027972459793
Training batch 24 / 32
Total batch reconstruction loss: 0.06503129750490189
Training batch 25 / 32
Total batch reconstruction loss: 0.06690165400505066
Training batch 26 / 32
Total batch reconstruction loss: 0.05804283916950226
Training batch 27 / 32
Total batch reconstruction loss: 0.06496022641658783
Training batch 28 / 32
Total batch reconstruction loss: 0.058838024735450745
Training batch 29 / 32
Total batch reconstruction loss: 0.06641086935997009
Training batch 30 / 32
Total batch reconstruction loss: 0.055027540773153305
Training batch 31 / 32
Total batch reconstruction loss: 0.05946679040789604
Training batch 32 / 32
Total batch reconstruction loss: 0.051510319113731384
Epoch [223/500], Train Loss: 0.0595, Validation Loss: 0.0591, Generator Loss: 12.1509, Discriminator Loss: 0.3199
Training epoch 224 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.05867768079042435
Training batch 2 / 32
Total batch reconstruction loss: 0.06807143986225128
Training batch 3 / 32
Total batch reconstruction loss: 0.06443570554256439
Training batch 4 / 32
Total batch reconstruction loss: 0.06351953744888306
Training batch 5 / 32
Total batch reconstruction loss: 0.05605987086892128
Training batch 6 / 32
Total batch reconstruction loss: 0.06173843517899513
Training batch 7 / 32
Total batch reconstruction loss: 0.061225712299346924
Training batch 8 / 32
Total batch reconstruction loss: 0.057841066271066666
Training batch 9 / 32
Total batch reconstruction loss: 0.06064508110284805
Training batch 10 / 32
Total batch reconstruction loss: 0.056088753044605255
Training batch 11 / 32
Total batch reconstruction loss: 0.060588538646698
Training batch 12 / 32
Total batch reconstruction loss: 0.06421799212694168
Training batch 13 / 32
Total batch reconstruction loss: 0.061404962092638016
Training batch 14 / 32
Total batch reconstruction loss: 0.06092310696840286
Training batch 15 / 32
Total batch reconstruction loss: 0.05842017009854317
Training batch 16 / 32
Total batch reconstruction loss: 0.05945641174912453
Training batch 17 / 32
Total batch reconstruction loss: 0.06576405465602875
Training batch 18 / 32
Total batch reconstruction loss: 0.06027659773826599
Training batch 19 / 32
Total batch reconstruction loss: 0.060905203223228455
Training batch 20 / 32
Total batch reconstruction loss: 0.06286701560020447
Training batch 21 / 32
Total batch reconstruction loss: 0.0554577112197876
Training batch 22 / 32
Total batch reconstruction loss: 0.05819457024335861
Training batch 23 / 32
Total batch reconstruction loss: 0.058217283338308334
Training batch 24 / 32
Total batch reconstruction loss: 0.05665358155965805
Training batch 25 / 32
Total batch reconstruction loss: 0.05960985645651817
Training batch 26 / 32
Total batch reconstruction loss: 0.06229177117347717
Training batch 27 / 32
Total batch reconstruction loss: 0.06330297887325287
Training batch 28 / 32
Total batch reconstruction loss: 0.058871325105428696
Training batch 29 / 32
Total batch reconstruction loss: 0.05897852033376694
Training batch 30 / 32
Total batch reconstruction loss: 0.05883728712797165
Training batch 31 / 32
Total batch reconstruction loss: 0.053757570683956146
Training batch 32 / 32
Total batch reconstruction loss: 0.051053501665592194
Epoch [224/500], Train Loss: 0.0589, Validation Loss: 0.0588, Generator Loss: 12.0706, Discriminator Loss: 0.2968
Training epoch 225 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.06387496739625931
Training batch 2 / 32
Total batch reconstruction loss: 0.059484757483005524
Training batch 3 / 32
Total batch reconstruction loss: 0.061034977436065674
Training batch 4 / 32
Total batch reconstruction loss: 0.059973422437906265
Training batch 5 / 32
Total batch reconstruction loss: 0.05698072910308838
Training batch 6 / 32
Total batch reconstruction loss: 0.060146450996398926
Training batch 7 / 32
Total batch reconstruction loss: 0.06026097387075424
Training batch 8 / 32
Total batch reconstruction loss: 0.061396803706884384
Training batch 9 / 32
Total batch reconstruction loss: 0.06070581451058388
Training batch 10 / 32
Total batch reconstruction loss: 0.06101725623011589
Training batch 11 / 32
Total batch reconstruction loss: 0.06385671347379684
Training batch 12 / 32
Total batch reconstruction loss: 0.060396403074264526
Training batch 13 / 32
Total batch reconstruction loss: 0.05914320424199104
Training batch 14 / 32
Total batch reconstruction loss: 0.057621654123067856
Training batch 15 / 32
Total batch reconstruction loss: 0.06205899268388748
Training batch 16 / 32
Total batch reconstruction loss: 0.05633469671010971
Training batch 17 / 32
Total batch reconstruction loss: 0.06772672384977341
Training batch 18 / 32
Total batch reconstruction loss: 0.06001821905374527
Training batch 19 / 32
Total batch reconstruction loss: 0.06181759014725685
Training batch 20 / 32
Total batch reconstruction loss: 0.05931485444307327
Training batch 21 / 32
Total batch reconstruction loss: 0.05863417685031891
Training batch 22 / 32
Total batch reconstruction loss: 0.06364314258098602
Training batch 23 / 32
Total batch reconstruction loss: 0.056930482387542725
Training batch 24 / 32
Total batch reconstruction loss: 0.0564064159989357
Training batch 25 / 32
Total batch reconstruction loss: 0.05644559860229492
Training batch 26 / 32
Total batch reconstruction loss: 0.061329424381256104
Training batch 27 / 32
Total batch reconstruction loss: 0.06057488173246384
Training batch 28 / 32
Total batch reconstruction loss: 0.057699281722307205
Training batch 29 / 32
Total batch reconstruction loss: 0.056999627500772476
Training batch 30 / 32
Total batch reconstruction loss: 0.05992916226387024
Training batch 31 / 32
Total batch reconstruction loss: 0.05711060017347336
Training batch 32 / 32
Total batch reconstruction loss: 0.07127973437309265
Epoch [225/500], Train Loss: 0.0592, Validation Loss: 0.0587, Generator Loss: 12.1519, Discriminator Loss: 0.2889
Training epoch 226 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.061658136546611786
Training batch 2 / 32
Total batch reconstruction loss: 0.058305300772190094
Training batch 3 / 32
Total batch reconstruction loss: 0.057902298867702484
Training batch 4 / 32
Total batch reconstruction loss: 0.06220962107181549
Training batch 5 / 32
Total batch reconstruction loss: 0.05646887421607971
Training batch 6 / 32
Total batch reconstruction loss: 0.05850524455308914
Training batch 7 / 32
Total batch reconstruction loss: 0.06028889864683151
Training batch 8 / 32
Total batch reconstruction loss: 0.062336307018995285
Training batch 9 / 32
Total batch reconstruction loss: 0.05757087469100952
Training batch 10 / 32
Total batch reconstruction loss: 0.061880774796009064
Training batch 11 / 32
Total batch reconstruction loss: 0.06110694259405136
Training batch 12 / 32
Total batch reconstruction loss: 0.06811277568340302
Training batch 13 / 32
Total batch reconstruction loss: 0.06387399137020111
Training batch 14 / 32
Total batch reconstruction loss: 0.05745091289281845
Training batch 15 / 32
Total batch reconstruction loss: 0.05826665461063385
Training batch 16 / 32
Total batch reconstruction loss: 0.06364096701145172
Training batch 17 / 32
Total batch reconstruction loss: 0.05862846225500107
Training batch 18 / 32
Total batch reconstruction loss: 0.053886719048023224
Training batch 19 / 32
Total batch reconstruction loss: 0.06041490659117699
Training batch 20 / 32
Total batch reconstruction loss: 0.05997750535607338
Training batch 21 / 32
Total batch reconstruction loss: 0.0646592304110527
Training batch 22 / 32
Total batch reconstruction loss: 0.061095595359802246
Training batch 23 / 32
Total batch reconstruction loss: 0.05990785360336304
Training batch 24 / 32
Total batch reconstruction loss: 0.05580199137330055
Training batch 25 / 32
Total batch reconstruction loss: 0.05536126345396042
Training batch 26 / 32
Total batch reconstruction loss: 0.0612398125231266
Training batch 27 / 32
Total batch reconstruction loss: 0.06281918287277222
Training batch 28 / 32
Total batch reconstruction loss: 0.060039062052965164
Training batch 29 / 32
Total batch reconstruction loss: 0.060428451746702194
Training batch 30 / 32
Total batch reconstruction loss: 0.06483298540115356
Training batch 31 / 32
Total batch reconstruction loss: 0.06279265880584717
Training batch 32 / 32
Total batch reconstruction loss: 0.06910449266433716
Epoch [226/500], Train Loss: 0.0595, Validation Loss: 0.0608, Generator Loss: 12.1966, Discriminator Loss: 0.3197
Training epoch 227 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.0605996772646904
Training batch 2 / 32
Total batch reconstruction loss: 0.05934222787618637
Training batch 3 / 32
Total batch reconstruction loss: 0.06102856621146202
Training batch 4 / 32
Total batch reconstruction loss: 0.06130903214216232
Training batch 5 / 32
Total batch reconstruction loss: 0.057285062968730927
Training batch 6 / 32
Total batch reconstruction loss: 0.059477340430021286
Training batch 7 / 32
Total batch reconstruction loss: 0.062343791127204895
Training batch 8 / 32
Total batch reconstruction loss: 0.05980559438467026
Training batch 9 / 32
Total batch reconstruction loss: 0.0609440915286541
Training batch 10 / 32
Total batch reconstruction loss: 0.0621805265545845
Training batch 11 / 32
Total batch reconstruction loss: 0.062167540192604065
Training batch 12 / 32
Total batch reconstruction loss: 0.06089533865451813
Training batch 13 / 32
Total batch reconstruction loss: 0.05708622932434082
Training batch 14 / 32
Total batch reconstruction loss: 0.06260943412780762
Training batch 15 / 32
Total batch reconstruction loss: 0.06280380487442017
Training batch 16 / 32
Total batch reconstruction loss: 0.06144304201006889
Training batch 17 / 32
Total batch reconstruction loss: 0.059156302362680435
Training batch 18 / 32
Total batch reconstruction loss: 0.061587221920490265
Training batch 19 / 32
Total batch reconstruction loss: 0.05810592323541641
Training batch 20 / 32
Total batch reconstruction loss: 0.06138119101524353
Training batch 21 / 32
Total batch reconstruction loss: 0.062263790518045425
Training batch 22 / 32
Total batch reconstruction loss: 0.06385537981987
Training batch 23 / 32
Total batch reconstruction loss: 0.06330528110265732
Training batch 24 / 32
Total batch reconstruction loss: 0.061256274580955505
Training batch 25 / 32
Total batch reconstruction loss: 0.05840777978301048
Training batch 26 / 32
Total batch reconstruction loss: 0.06070522591471672
Training batch 27 / 32
Total batch reconstruction loss: 0.05979844182729721
Training batch 28 / 32
Total batch reconstruction loss: 0.06300440430641174
Training batch 29 / 32
Total batch reconstruction loss: 0.05740635097026825
Training batch 30 / 32
Total batch reconstruction loss: 0.05779421329498291
Training batch 31 / 32
Total batch reconstruction loss: 0.055725354701280594
Training batch 32 / 32
Total batch reconstruction loss: 0.0573178306221962
Epoch [227/500], Train Loss: 0.0594, Validation Loss: 0.0584, Generator Loss: 12.1447, Discriminator Loss: 0.3215
Training epoch 228 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.06107950583100319
Training batch 2 / 32
Total batch reconstruction loss: 0.05958021432161331
Training batch 3 / 32
Total batch reconstruction loss: 0.06184081733226776
Training batch 4 / 32
Total batch reconstruction loss: 0.0612221322953701
Training batch 5 / 32
Total batch reconstruction loss: 0.06042790412902832
Training batch 6 / 32
Total batch reconstruction loss: 0.057774245738983154
Training batch 7 / 32
Total batch reconstruction loss: 0.060849569737911224
Training batch 8 / 32
Total batch reconstruction loss: 0.059879694133996964
Training batch 9 / 32
Total batch reconstruction loss: 0.05746733397245407
Training batch 10 / 32
Total batch reconstruction loss: 0.06399191915988922
Training batch 11 / 32
Total batch reconstruction loss: 0.05812549218535423
Training batch 12 / 32
Total batch reconstruction loss: 0.05933144688606262
Training batch 13 / 32
Total batch reconstruction loss: 0.062011733651161194
Training batch 14 / 32
Total batch reconstruction loss: 0.06668343394994736
Training batch 15 / 32
Total batch reconstruction loss: 0.057965122163295746
Training batch 16 / 32
Total batch reconstruction loss: 0.05925162136554718
Training batch 17 / 32
Total batch reconstruction loss: 0.06322725117206573
Training batch 18 / 32
Total batch reconstruction loss: 0.05863790214061737
Training batch 19 / 32
Total batch reconstruction loss: 0.05696627497673035
Training batch 20 / 32
Total batch reconstruction loss: 0.06210660934448242
Training batch 21 / 32
Total batch reconstruction loss: 0.06358714401721954
Training batch 22 / 32
Total batch reconstruction loss: 0.06194765120744705
Training batch 23 / 32
Total batch reconstruction loss: 0.058839768171310425
Training batch 24 / 32
Total batch reconstruction loss: 0.06313244998455048
Training batch 25 / 32
Total batch reconstruction loss: 0.06303533911705017
Training batch 26 / 32
Total batch reconstruction loss: 0.055447302758693695
Training batch 27 / 32
Total batch reconstruction loss: 0.058388419449329376
Training batch 28 / 32
Total batch reconstruction loss: 0.06023033708333969
Training batch 29 / 32
Total batch reconstruction loss: 0.06710699200630188
Training batch 30 / 32
Total batch reconstruction loss: 0.05777013674378395
Training batch 31 / 32
Total batch reconstruction loss: 0.05873720347881317
Training batch 32 / 32
Total batch reconstruction loss: 0.05277064070105553
Epoch [228/500], Train Loss: 0.0590, Validation Loss: 0.0594, Generator Loss: 12.1353, Discriminator Loss: 0.3070
Training epoch 229 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.058740630745887756
Training batch 2 / 32
Total batch reconstruction loss: 0.057618193328380585
Training batch 3 / 32
Total batch reconstruction loss: 0.06304191797971725
Training batch 4 / 32
Total batch reconstruction loss: 0.06357630342245102
Training batch 5 / 32
Total batch reconstruction loss: 0.059811800718307495
Training batch 6 / 32
Total batch reconstruction loss: 0.06101348251104355
Training batch 7 / 32
Total batch reconstruction loss: 0.06111360713839531
Training batch 8 / 32
Total batch reconstruction loss: 0.06018778681755066
Training batch 9 / 32
Total batch reconstruction loss: 0.06418488919734955
Training batch 10 / 32
Total batch reconstruction loss: 0.06523576378822327
Training batch 11 / 32
Total batch reconstruction loss: 0.06075018271803856
Training batch 12 / 32
Total batch reconstruction loss: 0.058811821043491364
Training batch 13 / 32
Total batch reconstruction loss: 0.058558523654937744
Training batch 14 / 32
Total batch reconstruction loss: 0.05649014562368393
Training batch 15 / 32
Total batch reconstruction loss: 0.06021576002240181
Training batch 16 / 32
Total batch reconstruction loss: 0.062189556658267975
Training batch 17 / 32
Total batch reconstruction loss: 0.05635290592908859
Training batch 18 / 32
Total batch reconstruction loss: 0.06129797548055649
Training batch 19 / 32
Total batch reconstruction loss: 0.06067311391234398
Training batch 20 / 32
Total batch reconstruction loss: 0.062051378190517426
Training batch 21 / 32
Total batch reconstruction loss: 0.058645229786634445
Training batch 22 / 32
Total batch reconstruction loss: 0.05776561051607132
Training batch 23 / 32
Total batch reconstruction loss: 0.05910586938261986
Training batch 24 / 32
Total batch reconstruction loss: 0.056038882583379745
Training batch 25 / 32
Total batch reconstruction loss: 0.06063308194279671
Training batch 26 / 32
Total batch reconstruction loss: 0.058089450001716614
Training batch 27 / 32
Total batch reconstruction loss: 0.0654943436384201
Training batch 28 / 32
Total batch reconstruction loss: 0.05952412635087967
Training batch 29 / 32
Total batch reconstruction loss: 0.05905899778008461
Training batch 30 / 32
Total batch reconstruction loss: 0.06191685050725937
Training batch 31 / 32
Total batch reconstruction loss: 0.061459433287382126
Training batch 32 / 32
Total batch reconstruction loss: 0.06392451375722885
Epoch [229/500], Train Loss: 0.0592, Validation Loss: 0.0602, Generator Loss: 12.1412, Discriminator Loss: 0.3342
Training epoch 230 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.05733397603034973
Training batch 2 / 32
Total batch reconstruction loss: 0.06120990216732025
Training batch 3 / 32
Total batch reconstruction loss: 0.057333484292030334
Training batch 4 / 32
Total batch reconstruction loss: 0.058378878980875015
Training batch 5 / 32
Total batch reconstruction loss: 0.059649594128131866
Training batch 6 / 32
Total batch reconstruction loss: 0.06254655867815018
Training batch 7 / 32
Total batch reconstruction loss: 0.06225437670946121
Training batch 8 / 32
Total batch reconstruction loss: 0.05771098658442497
Training batch 9 / 32
Total batch reconstruction loss: 0.05963391065597534
Training batch 10 / 32
Total batch reconstruction loss: 0.0600280798971653
Training batch 11 / 32
Total batch reconstruction loss: 0.0584859773516655
Training batch 12 / 32
Total batch reconstruction loss: 0.059468209743499756
Training batch 13 / 32
Total batch reconstruction loss: 0.06042458862066269
Training batch 14 / 32
Total batch reconstruction loss: 0.06509062647819519
Training batch 15 / 32
Total batch reconstruction loss: 0.06107032299041748
Training batch 16 / 32
Total batch reconstruction loss: 0.059566158801317215
Training batch 17 / 32
Total batch reconstruction loss: 0.059386201202869415
Training batch 18 / 32
Total batch reconstruction loss: 0.05742406100034714
Training batch 19 / 32
Total batch reconstruction loss: 0.057281240820884705
Training batch 20 / 32
Total batch reconstruction loss: 0.05886906385421753
Training batch 21 / 32
Total batch reconstruction loss: 0.0620909258723259
Training batch 22 / 32
Total batch reconstruction loss: 0.0561758428812027
Training batch 23 / 32
Total batch reconstruction loss: 0.05917532742023468
Training batch 24 / 32
Total batch reconstruction loss: 0.06608562171459198
Training batch 25 / 32
Total batch reconstruction loss: 0.06210634857416153
Training batch 26 / 32
Total batch reconstruction loss: 0.064963698387146
Training batch 27 / 32
Total batch reconstruction loss: 0.05956104397773743
Training batch 28 / 32
Total batch reconstruction loss: 0.05752146616578102
Training batch 29 / 32
Total batch reconstruction loss: 0.0619363971054554
Training batch 30 / 32
Total batch reconstruction loss: 0.06419326364994049
Training batch 31 / 32
Total batch reconstruction loss: 0.05913154035806656
Training batch 32 / 32
Total batch reconstruction loss: 0.057379238307476044
Epoch [230/500], Train Loss: 0.0587, Validation Loss: 0.0583, Generator Loss: 12.0908, Discriminator Loss: 0.3143
Training epoch 231 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.059878572821617126
Training batch 2 / 32
Total batch reconstruction loss: 0.06325646489858627
Training batch 3 / 32
Total batch reconstruction loss: 0.060903578996658325
Training batch 4 / 32
Total batch reconstruction loss: 0.06218664348125458
Training batch 5 / 32
Total batch reconstruction loss: 0.06234978884458542
Training batch 6 / 32
Total batch reconstruction loss: 0.060197871178388596
Training batch 7 / 32
Total batch reconstruction loss: 0.060755737125873566
Training batch 8 / 32
Total batch reconstruction loss: 0.05879231542348862
Training batch 9 / 32
Total batch reconstruction loss: 0.058483317494392395
Training batch 10 / 32
Total batch reconstruction loss: 0.05672234669327736
Training batch 11 / 32
Total batch reconstruction loss: 0.06103671342134476
Training batch 12 / 32
Total batch reconstruction loss: 0.060489244759082794
Training batch 13 / 32
Total batch reconstruction loss: 0.05924025923013687
Training batch 14 / 32
Total batch reconstruction loss: 0.06041792035102844
Training batch 15 / 32
Total batch reconstruction loss: 0.0571533627808094
Training batch 16 / 32
Total batch reconstruction loss: 0.059470050036907196
Training batch 17 / 32
Total batch reconstruction loss: 0.05856739729642868
Training batch 18 / 32
Total batch reconstruction loss: 0.06337295472621918
Training batch 19 / 32
Total batch reconstruction loss: 0.0600915402173996
Training batch 20 / 32
Total batch reconstruction loss: 0.06203465908765793
Training batch 21 / 32
Total batch reconstruction loss: 0.06083659082651138
Training batch 22 / 32
Total batch reconstruction loss: 0.05950178578495979
Training batch 23 / 32
Total batch reconstruction loss: 0.06213203817605972
Training batch 24 / 32
Total batch reconstruction loss: 0.057336412370204926
Training batch 25 / 32
Total batch reconstruction loss: 0.05877685546875
Training batch 26 / 32
Total batch reconstruction loss: 0.06259448826313019
Training batch 27 / 32
Total batch reconstruction loss: 0.05848489701747894
Training batch 28 / 32
Total batch reconstruction loss: 0.06004359573125839
Training batch 29 / 32
Total batch reconstruction loss: 0.06077657639980316
Training batch 30 / 32
Total batch reconstruction loss: 0.06215306371450424
Training batch 31 / 32
Total batch reconstruction loss: 0.05956007540225983
Training batch 32 / 32
Total batch reconstruction loss: 0.048182956874370575
Epoch [231/500], Train Loss: 0.0586, Validation Loss: 0.0577, Generator Loss: 12.0534, Discriminator Loss: 0.2993
Training epoch 232 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.05759429931640625
Training batch 2 / 32
Total batch reconstruction loss: 0.05939708650112152
Training batch 3 / 32
Total batch reconstruction loss: 0.06108177453279495
Training batch 4 / 32
Total batch reconstruction loss: 0.05928343906998634
Training batch 5 / 32
Total batch reconstruction loss: 0.062105897814035416
Training batch 6 / 32
Total batch reconstruction loss: 0.05960208922624588
Training batch 7 / 32
Total batch reconstruction loss: 0.05927302688360214
Training batch 8 / 32
Total batch reconstruction loss: 0.05832798406481743
Training batch 9 / 32
Total batch reconstruction loss: 0.060433682054281235
Training batch 10 / 32
Total batch reconstruction loss: 0.05590009689331055
Training batch 11 / 32
Total batch reconstruction loss: 0.057777710258960724
Training batch 12 / 32
Total batch reconstruction loss: 0.058250539004802704
Training batch 13 / 32
Total batch reconstruction loss: 0.05797816067934036
Training batch 14 / 32
Total batch reconstruction loss: 0.060372598469257355
Training batch 15 / 32
Total batch reconstruction loss: 0.05926579236984253
Training batch 16 / 32
Total batch reconstruction loss: 0.056129440665245056
Training batch 17 / 32
Total batch reconstruction loss: 0.061606571078300476
Training batch 18 / 32
Total batch reconstruction loss: 0.05915837734937668
Training batch 19 / 32
Total batch reconstruction loss: 0.05861911550164223
Training batch 20 / 32
Total batch reconstruction loss: 0.057248055934906006
Training batch 21 / 32
Total batch reconstruction loss: 0.060055699199438095
Training batch 22 / 32
Total batch reconstruction loss: 0.06308141350746155
Training batch 23 / 32
Total batch reconstruction loss: 0.06280101835727692
Training batch 24 / 32
Total batch reconstruction loss: 0.057816218584775925
Training batch 25 / 32
Total batch reconstruction loss: 0.06448712199926376
Training batch 26 / 32
Total batch reconstruction loss: 0.06263566762208939
Training batch 27 / 32
Total batch reconstruction loss: 0.06143717095255852
Training batch 28 / 32
Total batch reconstruction loss: 0.06141260638833046
Training batch 29 / 32
Total batch reconstruction loss: 0.057111456990242004
Training batch 30 / 32
Total batch reconstruction loss: 0.06258095800876617
Training batch 31 / 32
Total batch reconstruction loss: 0.061284855008125305
Training batch 32 / 32
Total batch reconstruction loss: 0.06142521649599075
Epoch [232/500], Train Loss: 0.0586, Validation Loss: 0.0605, Generator Loss: 12.0389, Discriminator Loss: 0.3273
Training epoch 233 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.06328369677066803
Training batch 2 / 32
Total batch reconstruction loss: 0.056005559861660004
Training batch 3 / 32
Total batch reconstruction loss: 0.05757947266101837
Training batch 4 / 32
Total batch reconstruction loss: 0.06201772391796112
Training batch 5 / 32
Total batch reconstruction loss: 0.061475083231925964
Training batch 6 / 32
Total batch reconstruction loss: 0.06324850022792816
Training batch 7 / 32
Total batch reconstruction loss: 0.05824314057826996
Training batch 8 / 32
Total batch reconstruction loss: 0.05670434236526489
Training batch 9 / 32
Total batch reconstruction loss: 0.06367708742618561
Training batch 10 / 32
Total batch reconstruction loss: 0.060367196798324585
Training batch 11 / 32
Total batch reconstruction loss: 0.05811036750674248
Training batch 12 / 32
Total batch reconstruction loss: 0.06226576119661331
Training batch 13 / 32
Total batch reconstruction loss: 0.05701899155974388
Training batch 14 / 32
Total batch reconstruction loss: 0.05866653472185135
Training batch 15 / 32
Total batch reconstruction loss: 0.056401900947093964
Training batch 16 / 32
Total batch reconstruction loss: 0.05809418112039566
Training batch 17 / 32
Total batch reconstruction loss: 0.05627676099538803
Training batch 18 / 32
Total batch reconstruction loss: 0.0626373142004013
Training batch 19 / 32
Total batch reconstruction loss: 0.06402992457151413
Training batch 20 / 32
Total batch reconstruction loss: 0.06044105067849159
Training batch 21 / 32
Total batch reconstruction loss: 0.06155794486403465
Training batch 22 / 32
Total batch reconstruction loss: 0.06051446869969368
Training batch 23 / 32
Total batch reconstruction loss: 0.06264577805995941
Training batch 24 / 32
Total batch reconstruction loss: 0.05971648171544075
Training batch 25 / 32
Total batch reconstruction loss: 0.06113894283771515
Training batch 26 / 32
Total batch reconstruction loss: 0.058487821370363235
Training batch 27 / 32
Total batch reconstruction loss: 0.056074582040309906
Training batch 28 / 32
Total batch reconstruction loss: 0.05996415391564369
Training batch 29 / 32
Total batch reconstruction loss: 0.058732226490974426
Training batch 30 / 32
Total batch reconstruction loss: 0.05848746746778488
Training batch 31 / 32
Total batch reconstruction loss: 0.05925360321998596
Training batch 32 / 32
Total batch reconstruction loss: 0.0642099604010582
Epoch [233/500], Train Loss: 0.0585, Validation Loss: 0.0595, Generator Loss: 12.0572, Discriminator Loss: 0.3119
Training epoch 234 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.05782010778784752
Training batch 2 / 32
Total batch reconstruction loss: 0.05789146572351456
Training batch 3 / 32
Total batch reconstruction loss: 0.06549833714962006
Training batch 4 / 32
Total batch reconstruction loss: 0.05628962814807892
Training batch 5 / 32
Total batch reconstruction loss: 0.05675250291824341
Training batch 6 / 32
Total batch reconstruction loss: 0.060639068484306335
Training batch 7 / 32
Total batch reconstruction loss: 0.06359122693538666
Training batch 8 / 32
Total batch reconstruction loss: 0.06592870503664017
Training batch 9 / 32
Total batch reconstruction loss: 0.06164206564426422
Training batch 10 / 32
Total batch reconstruction loss: 0.05912758409976959
Training batch 11 / 32
Total batch reconstruction loss: 0.06002706289291382
Training batch 12 / 32
Total batch reconstruction loss: 0.05752023681998253
Training batch 13 / 32
Total batch reconstruction loss: 0.05746184289455414
Training batch 14 / 32
Total batch reconstruction loss: 0.05658154934644699
Training batch 15 / 32
Total batch reconstruction loss: 0.06234871596097946
Training batch 16 / 32
Total batch reconstruction loss: 0.0582648441195488
Training batch 17 / 32
Total batch reconstruction loss: 0.06512686610221863
Training batch 18 / 32
Total batch reconstruction loss: 0.06111196428537369
Training batch 19 / 32
Total batch reconstruction loss: 0.0609687939286232
Training batch 20 / 32
Total batch reconstruction loss: 0.06457690894603729
Training batch 21 / 32
Total batch reconstruction loss: 0.05796649307012558
Training batch 22 / 32
Total batch reconstruction loss: 0.06355694681406021
Training batch 23 / 32
Total batch reconstruction loss: 0.06046638637781143
Training batch 24 / 32
Total batch reconstruction loss: 0.05703344941139221
Training batch 25 / 32
Total batch reconstruction loss: 0.05928458273410797
Training batch 26 / 32
Total batch reconstruction loss: 0.058856088668107986
Training batch 27 / 32
Total batch reconstruction loss: 0.06126166507601738
Training batch 28 / 32
Total batch reconstruction loss: 0.06086353957653046
Training batch 29 / 32
Total batch reconstruction loss: 0.05657527968287468
Training batch 30 / 32
Total batch reconstruction loss: 0.06251204758882523
Training batch 31 / 32
Total batch reconstruction loss: 0.0586371049284935
Training batch 32 / 32
Total batch reconstruction loss: 0.06213945895433426
Epoch [234/500], Train Loss: 0.0587, Validation Loss: 0.0583, Generator Loss: 12.1300, Discriminator Loss: 0.3026
Training epoch 235 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.061196766793727875
Training batch 2 / 32
Total batch reconstruction loss: 0.05906302109360695
Training batch 3 / 32
Total batch reconstruction loss: 0.0556807741522789
Training batch 4 / 32
Total batch reconstruction loss: 0.054886773228645325
Training batch 5 / 32
Total batch reconstruction loss: 0.06118125468492508
Training batch 6 / 32
Total batch reconstruction loss: 0.06673063337802887
Training batch 7 / 32
Total batch reconstruction loss: 0.061464838683605194
Training batch 8 / 32
Total batch reconstruction loss: 0.061306633055210114
Training batch 9 / 32
Total batch reconstruction loss: 0.06504552811384201
Training batch 10 / 32
Total batch reconstruction loss: 0.06011717766523361
Training batch 11 / 32
Total batch reconstruction loss: 0.06432770937681198
Training batch 12 / 32
Total batch reconstruction loss: 0.0583525225520134
Training batch 13 / 32
Total batch reconstruction loss: 0.05879133194684982
Training batch 14 / 32
Total batch reconstruction loss: 0.056423261761665344
Training batch 15 / 32
Total batch reconstruction loss: 0.05656082555651665
Training batch 16 / 32
Total batch reconstruction loss: 0.06304171681404114
Training batch 17 / 32
Total batch reconstruction loss: 0.0638221949338913
Training batch 18 / 32
Total batch reconstruction loss: 0.05937604978680611
Training batch 19 / 32
Total batch reconstruction loss: 0.06568530946969986
Training batch 20 / 32
Total batch reconstruction loss: 0.06005527079105377
Training batch 21 / 32
Total batch reconstruction loss: 0.05535167455673218
Training batch 22 / 32
Total batch reconstruction loss: 0.06011895090341568
Training batch 23 / 32
Total batch reconstruction loss: 0.05891551077365875
Training batch 24 / 32
Total batch reconstruction loss: 0.06027035042643547
Training batch 25 / 32
Total batch reconstruction loss: 0.06369297951459885
Training batch 26 / 32
Total batch reconstruction loss: 0.06023622676730156
Training batch 27 / 32
Total batch reconstruction loss: 0.05760617554187775
Training batch 28 / 32
Total batch reconstruction loss: 0.05974492430686951
Training batch 29 / 32
Total batch reconstruction loss: 0.05834217369556427
Training batch 30 / 32
Total batch reconstruction loss: 0.05873430520296097
Training batch 31 / 32
Total batch reconstruction loss: 0.05591735988855362
Training batch 32 / 32
Total batch reconstruction loss: 0.04409497231245041
Epoch [235/500], Train Loss: 0.0582, Validation Loss: 0.0593, Generator Loss: 11.9827, Discriminator Loss: 0.3204
Training epoch 236 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.06021144613623619
Training batch 2 / 32
Total batch reconstruction loss: 0.05997002497315407
Training batch 3 / 32
Total batch reconstruction loss: 0.055713120847940445
Training batch 4 / 32
Total batch reconstruction loss: 0.05891750007867813
Training batch 5 / 32
Total batch reconstruction loss: 0.06557594239711761
Training batch 6 / 32
Total batch reconstruction loss: 0.061528775840997696
Training batch 7 / 32
Total batch reconstruction loss: 0.059508487582206726
Training batch 8 / 32
Total batch reconstruction loss: 0.057756565511226654
Training batch 9 / 32
Total batch reconstruction loss: 0.05983882024884224
Training batch 10 / 32
Total batch reconstruction loss: 0.05971085652709007
Training batch 11 / 32
Total batch reconstruction loss: 0.061999835073947906
Training batch 12 / 32
Total batch reconstruction loss: 0.05737593024969101
Training batch 13 / 32
Total batch reconstruction loss: 0.06050490215420723
Training batch 14 / 32
Total batch reconstruction loss: 0.06696982681751251
Training batch 15 / 32
Total batch reconstruction loss: 0.061285752803087234
Training batch 16 / 32
Total batch reconstruction loss: 0.05723877251148224
Training batch 17 / 32
Total batch reconstruction loss: 0.05850515514612198
Training batch 18 / 32
Total batch reconstruction loss: 0.06379171460866928
Training batch 19 / 32
Total batch reconstruction loss: 0.05718033015727997
Training batch 20 / 32
Total batch reconstruction loss: 0.05763746425509453
Training batch 21 / 32
Total batch reconstruction loss: 0.05803605541586876
Training batch 22 / 32
Total batch reconstruction loss: 0.057052385061979294
Training batch 23 / 32
Total batch reconstruction loss: 0.061084549874067307
Training batch 24 / 32
Total batch reconstruction loss: 0.060893379151821136
Training batch 25 / 32
Total batch reconstruction loss: 0.05604042857885361
Training batch 26 / 32
Total batch reconstruction loss: 0.05875808000564575
Training batch 27 / 32
Total batch reconstruction loss: 0.06633599102497101
Training batch 28 / 32
Total batch reconstruction loss: 0.05802539736032486
Training batch 29 / 32
Total batch reconstruction loss: 0.05467825382947922
Training batch 30 / 32
Total batch reconstruction loss: 0.0647338479757309
Training batch 31 / 32
Total batch reconstruction loss: 0.0616205632686615
Training batch 32 / 32
Total batch reconstruction loss: 0.05672258138656616
Epoch [236/500], Train Loss: 0.0588, Validation Loss: 0.0587, Generator Loss: 12.0417, Discriminator Loss: 0.3125
Training epoch 237 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.06293618679046631
Training batch 2 / 32
Total batch reconstruction loss: 0.0640546903014183
Training batch 3 / 32
Total batch reconstruction loss: 0.0648384764790535
Training batch 4 / 32
Total batch reconstruction loss: 0.05924499034881592
Training batch 5 / 32
Total batch reconstruction loss: 0.05893074721097946
Training batch 6 / 32
Total batch reconstruction loss: 0.06132620573043823
Training batch 7 / 32
Total batch reconstruction loss: 0.06000061333179474
Training batch 8 / 32
Total batch reconstruction loss: 0.0662364512681961
Training batch 9 / 32
Total batch reconstruction loss: 0.05964832752943039
Training batch 10 / 32
Total batch reconstruction loss: 0.05497273430228233
Training batch 11 / 32
Total batch reconstruction loss: 0.06338279694318771
Training batch 12 / 32
Total batch reconstruction loss: 0.0585719533264637
Training batch 13 / 32
Total batch reconstruction loss: 0.05694512277841568
Training batch 14 / 32
Total batch reconstruction loss: 0.06337971985340118
Training batch 15 / 32
Total batch reconstruction loss: 0.05661546066403389
Training batch 16 / 32
Total batch reconstruction loss: 0.06453271210193634
Training batch 17 / 32
Total batch reconstruction loss: 0.05675062537193298
Training batch 18 / 32
Total batch reconstruction loss: 0.05937265604734421
Training batch 19 / 32
Total batch reconstruction loss: 0.06418760865926743
Training batch 20 / 32
Total batch reconstruction loss: 0.05991624295711517
Training batch 21 / 32
Total batch reconstruction loss: 0.059359535574913025
Training batch 22 / 32
Total batch reconstruction loss: 0.059983205050230026
Training batch 23 / 32
Total batch reconstruction loss: 0.058996085077524185
Training batch 24 / 32
Total batch reconstruction loss: 0.062104351818561554
Training batch 25 / 32
Total batch reconstruction loss: 0.06061850115656853
Training batch 26 / 32
Total batch reconstruction loss: 0.05611381679773331
Training batch 27 / 32
Total batch reconstruction loss: 0.05891641601920128
Training batch 28 / 32
Total batch reconstruction loss: 0.05763161927461624
Training batch 29 / 32
Total batch reconstruction loss: 0.06367803364992142
Training batch 30 / 32
Total batch reconstruction loss: 0.05666830390691757
Training batch 31 / 32
Total batch reconstruction loss: 0.056641511619091034
Training batch 32 / 32
Total batch reconstruction loss: 0.05167226493358612
Epoch [237/500], Train Loss: 0.0585, Validation Loss: 0.0580, Generator Loss: 12.0515, Discriminator Loss: 0.3283
Training epoch 238 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.06351455301046371
Training batch 2 / 32
Total batch reconstruction loss: 0.06056647375226021
Training batch 3 / 32
Total batch reconstruction loss: 0.05707588046789169
Training batch 4 / 32
Total batch reconstruction loss: 0.05849101394414902
Training batch 5 / 32
Total batch reconstruction loss: 0.05618928745388985
Training batch 6 / 32
Total batch reconstruction loss: 0.054423749446868896
Training batch 7 / 32
Total batch reconstruction loss: 0.06141691654920578
Training batch 8 / 32
Total batch reconstruction loss: 0.06240928918123245
Training batch 9 / 32
Total batch reconstruction loss: 0.05979729816317558
Training batch 10 / 32
Total batch reconstruction loss: 0.05639621615409851
Training batch 11 / 32
Total batch reconstruction loss: 0.06349808722734451
Training batch 12 / 32
Total batch reconstruction loss: 0.05831075459718704
Training batch 13 / 32
Total batch reconstruction loss: 0.06032637879252434
Training batch 14 / 32
Total batch reconstruction loss: 0.05845491960644722
Training batch 15 / 32
Total batch reconstruction loss: 0.05790133774280548
Training batch 16 / 32
Total batch reconstruction loss: 0.05972778797149658
Training batch 17 / 32
Total batch reconstruction loss: 0.05964032560586929
Training batch 18 / 32
Total batch reconstruction loss: 0.06164844334125519
Training batch 19 / 32
Total batch reconstruction loss: 0.058904021978378296
Training batch 20 / 32
Total batch reconstruction loss: 0.06586902588605881
Training batch 21 / 32
Total batch reconstruction loss: 0.06427520513534546
Training batch 22 / 32
Total batch reconstruction loss: 0.06350190937519073
Training batch 23 / 32
Total batch reconstruction loss: 0.0581069141626358
Training batch 24 / 32
Total batch reconstruction loss: 0.05993857979774475
Training batch 25 / 32
Total batch reconstruction loss: 0.0595037043094635
Training batch 26 / 32
Total batch reconstruction loss: 0.06217087805271149
Training batch 27 / 32
Total batch reconstruction loss: 0.05974564328789711
Training batch 28 / 32
Total batch reconstruction loss: 0.05879157781600952
Training batch 29 / 32
Total batch reconstruction loss: 0.05689026787877083
Training batch 30 / 32
Total batch reconstruction loss: 0.06163172796368599
Training batch 31 / 32
Total batch reconstruction loss: 0.06364697217941284
Training batch 32 / 32
Total batch reconstruction loss: 0.0483778640627861
Epoch [238/500], Train Loss: 0.0588, Validation Loss: 0.0607, Generator Loss: 12.0162, Discriminator Loss: 0.3064
Training epoch 239 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.06098143756389618
Training batch 2 / 32
Total batch reconstruction loss: 0.06221913918852806
Training batch 3 / 32
Total batch reconstruction loss: 0.06199878826737404
Training batch 4 / 32
Total batch reconstruction loss: 0.057270921766757965
Training batch 5 / 32
Total batch reconstruction loss: 0.056388288736343384
Training batch 6 / 32
Total batch reconstruction loss: 0.06195633485913277
Training batch 7 / 32
Total batch reconstruction loss: 0.058851469308137894
Training batch 8 / 32
Total batch reconstruction loss: 0.05597565323114395
Training batch 9 / 32
Total batch reconstruction loss: 0.058114491403102875
Training batch 10 / 32
Total batch reconstruction loss: 0.06015722453594208
Training batch 11 / 32
Total batch reconstruction loss: 0.0604439377784729
Training batch 12 / 32
Total batch reconstruction loss: 0.06011137366294861
Training batch 13 / 32
Total batch reconstruction loss: 0.05660886690020561
Training batch 14 / 32
Total batch reconstruction loss: 0.058691591024398804
Training batch 15 / 32
Total batch reconstruction loss: 0.06260229647159576
Training batch 16 / 32
Total batch reconstruction loss: 0.06186429411172867
Training batch 17 / 32
Total batch reconstruction loss: 0.05668717622756958
Training batch 18 / 32
Total batch reconstruction loss: 0.0638330727815628
Training batch 19 / 32
Total batch reconstruction loss: 0.0609925277531147
Training batch 20 / 32
Total batch reconstruction loss: 0.062140192836523056
Training batch 21 / 32
Total batch reconstruction loss: 0.06020984798669815
Training batch 22 / 32
Total batch reconstruction loss: 0.05946432799100876
Training batch 23 / 32
Total batch reconstruction loss: 0.057095155119895935
Training batch 24 / 32
Total batch reconstruction loss: 0.06178019940853119
Training batch 25 / 32
Total batch reconstruction loss: 0.06038108095526695
Training batch 26 / 32
Total batch reconstruction loss: 0.05836869776248932
Training batch 27 / 32
Total batch reconstruction loss: 0.05947547405958176
Training batch 28 / 32
Total batch reconstruction loss: 0.056636691093444824
Training batch 29 / 32
Total batch reconstruction loss: 0.054703518748283386
Training batch 30 / 32
Total batch reconstruction loss: 0.05770588293671608
Training batch 31 / 32
Total batch reconstruction loss: 0.05698647350072861
Training batch 32 / 32
Total batch reconstruction loss: 0.05299318954348564
Epoch [239/500], Train Loss: 0.0576, Validation Loss: 0.0576, Generator Loss: 11.8955, Discriminator Loss: 0.3313
Training epoch 240 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.06493890285491943
Training batch 2 / 32
Total batch reconstruction loss: 0.06343024224042892
Training batch 3 / 32
Total batch reconstruction loss: 0.06035633385181427
Training batch 4 / 32
Total batch reconstruction loss: 0.06280256062746048
Training batch 5 / 32
Total batch reconstruction loss: 0.06428655236959457
Training batch 6 / 32
Total batch reconstruction loss: 0.06177755072712898
Training batch 7 / 32
Total batch reconstruction loss: 0.061642035841941833
Training batch 8 / 32
Total batch reconstruction loss: 0.05673603713512421
Training batch 9 / 32
Total batch reconstruction loss: 0.06415362656116486
Training batch 10 / 32
Total batch reconstruction loss: 0.060528464615345
Training batch 11 / 32
Total batch reconstruction loss: 0.05868726968765259
Training batch 12 / 32
Total batch reconstruction loss: 0.0587121844291687
Training batch 13 / 32
Total batch reconstruction loss: 0.05917588248848915
Training batch 14 / 32
Total batch reconstruction loss: 0.05757648125290871
Training batch 15 / 32
Total batch reconstruction loss: 0.05607157200574875
Training batch 16 / 32
Total batch reconstruction loss: 0.05855612829327583
Training batch 17 / 32
Total batch reconstruction loss: 0.061223626136779785
Training batch 18 / 32
Total batch reconstruction loss: 0.059180401265621185
Training batch 19 / 32
Total batch reconstruction loss: 0.057656437158584595
Training batch 20 / 32
Total batch reconstruction loss: 0.059212468564510345
Training batch 21 / 32
Total batch reconstruction loss: 0.05849098414182663
Training batch 22 / 32
Total batch reconstruction loss: 0.06252611428499222
Training batch 23 / 32
Total batch reconstruction loss: 0.055217668414115906
Training batch 24 / 32
Total batch reconstruction loss: 0.05770137906074524
Training batch 25 / 32
Total batch reconstruction loss: 0.05613262951374054
Training batch 26 / 32
Total batch reconstruction loss: 0.0576627179980278
Training batch 27 / 32
Total batch reconstruction loss: 0.06246425583958626
Training batch 28 / 32
Total batch reconstruction loss: 0.062395911663770676
Training batch 29 / 32
Total batch reconstruction loss: 0.060678549110889435
Training batch 30 / 32
Total batch reconstruction loss: 0.06239417940378189
Training batch 31 / 32
Total batch reconstruction loss: 0.05709737166762352
Training batch 32 / 32
Total batch reconstruction loss: 0.08651367574930191
Epoch [240/500], Train Loss: 0.0594, Validation Loss: 0.0590, Generator Loss: 12.2332, Discriminator Loss: 0.3184
Training epoch 241 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.05725185573101044
Training batch 2 / 32
Total batch reconstruction loss: 0.05525851249694824
Training batch 3 / 32
Total batch reconstruction loss: 0.0635644793510437
Training batch 4 / 32
Total batch reconstruction loss: 0.06244203448295593
Training batch 5 / 32
Total batch reconstruction loss: 0.06071728095412254
Training batch 6 / 32
Total batch reconstruction loss: 0.06290729343891144
Training batch 7 / 32
Total batch reconstruction loss: 0.05835449695587158
Training batch 8 / 32
Total batch reconstruction loss: 0.06306637823581696
Training batch 9 / 32
Total batch reconstruction loss: 0.06025702506303787
Training batch 10 / 32
Total batch reconstruction loss: 0.05390963330864906
Training batch 11 / 32
Total batch reconstruction loss: 0.05777209997177124
Training batch 12 / 32
Total batch reconstruction loss: 0.06125187873840332
Training batch 13 / 32
Total batch reconstruction loss: 0.05832646042108536
Training batch 14 / 32
Total batch reconstruction loss: 0.06295078992843628
Training batch 15 / 32
Total batch reconstruction loss: 0.06366457790136337
Training batch 16 / 32
Total batch reconstruction loss: 0.05783674865961075
Training batch 17 / 32
Total batch reconstruction loss: 0.05633106082677841
Training batch 18 / 32
Total batch reconstruction loss: 0.06439484655857086
Training batch 19 / 32
Total batch reconstruction loss: 0.057812273502349854
Training batch 20 / 32
Total batch reconstruction loss: 0.06128868833184242
Training batch 21 / 32
Total batch reconstruction loss: 0.06648819148540497
Training batch 22 / 32
Total batch reconstruction loss: 0.055665627121925354
Training batch 23 / 32
Total batch reconstruction loss: 0.06136458367109299
Training batch 24 / 32
Total batch reconstruction loss: 0.05882411077618599
Training batch 25 / 32
Total batch reconstruction loss: 0.0583883635699749
Training batch 26 / 32
Total batch reconstruction loss: 0.05886948108673096
Training batch 27 / 32
Total batch reconstruction loss: 0.056870944797992706
Training batch 28 / 32
Total batch reconstruction loss: 0.057755377143621445
Training batch 29 / 32
Total batch reconstruction loss: 0.0639272928237915
Training batch 30 / 32
Total batch reconstruction loss: 0.06105796620249748
Training batch 31 / 32
Total batch reconstruction loss: 0.060193102806806564
Training batch 32 / 32
Total batch reconstruction loss: 0.055951956659555435
Epoch [241/500], Train Loss: 0.0584, Validation Loss: 0.0605, Generator Loss: 12.0293, Discriminator Loss: 0.3269
Training epoch 242 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.05975139141082764
Training batch 2 / 32
Total batch reconstruction loss: 0.06037440150976181
Training batch 3 / 32
Total batch reconstruction loss: 0.059713251888751984
Training batch 4 / 32
Total batch reconstruction loss: 0.06378595530986786
Training batch 5 / 32
Total batch reconstruction loss: 0.0596841536462307
Training batch 6 / 32
Total batch reconstruction loss: 0.06033766269683838
Training batch 7 / 32
Total batch reconstruction loss: 0.05875402316451073
Training batch 8 / 32
Total batch reconstruction loss: 0.05799490213394165
Training batch 9 / 32
Total batch reconstruction loss: 0.05947766453027725
Training batch 10 / 32
Total batch reconstruction loss: 0.06193871051073074
Training batch 11 / 32
Total batch reconstruction loss: 0.058357883244752884
Training batch 12 / 32
Total batch reconstruction loss: 0.05954393744468689
Training batch 13 / 32
Total batch reconstruction loss: 0.0587807260453701
Training batch 14 / 32
Total batch reconstruction loss: 0.0572177954018116
Training batch 15 / 32
Total batch reconstruction loss: 0.06144115701317787
Training batch 16 / 32
Total batch reconstruction loss: 0.05965191125869751
Training batch 17 / 32
Total batch reconstruction loss: 0.06198880821466446
Training batch 18 / 32
Total batch reconstruction loss: 0.060688719153404236
Training batch 19 / 32
Total batch reconstruction loss: 0.05888071283698082
Training batch 20 / 32
Total batch reconstruction loss: 0.054943397641181946
Training batch 21 / 32
Total batch reconstruction loss: 0.06215084344148636
Training batch 22 / 32
Total batch reconstruction loss: 0.059616394340991974
Training batch 23 / 32
Total batch reconstruction loss: 0.06052630394697189
Training batch 24 / 32
Total batch reconstruction loss: 0.061276935040950775
Training batch 25 / 32
Total batch reconstruction loss: 0.06261954456567764
Training batch 26 / 32
Total batch reconstruction loss: 0.0590859092772007
Training batch 27 / 32
Total batch reconstruction loss: 0.05851886421442032
Training batch 28 / 32
Total batch reconstruction loss: 0.05964624881744385
Training batch 29 / 32
Total batch reconstruction loss: 0.0615832544863224
Training batch 30 / 32
Total batch reconstruction loss: 0.060407605022192
Training batch 31 / 32
Total batch reconstruction loss: 0.05679631605744362
Training batch 32 / 32
Total batch reconstruction loss: 0.04793032258749008
Epoch [242/500], Train Loss: 0.0582, Validation Loss: 0.0580, Generator Loss: 11.9677, Discriminator Loss: 0.3182
Training epoch 243 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.061670102179050446
Training batch 2 / 32
Total batch reconstruction loss: 0.055296145379543304
Training batch 3 / 32
Total batch reconstruction loss: 0.0638207346200943
Training batch 4 / 32
Total batch reconstruction loss: 0.057279013097286224
Training batch 5 / 32
Total batch reconstruction loss: 0.058429569005966187
Training batch 6 / 32
Total batch reconstruction loss: 0.057108595967292786
Training batch 7 / 32
Total batch reconstruction loss: 0.060380131006240845
Training batch 8 / 32
Total batch reconstruction loss: 0.059045664966106415
Training batch 9 / 32
Total batch reconstruction loss: 0.05868018418550491
Training batch 10 / 32
Total batch reconstruction loss: 0.058579616248607635
Training batch 11 / 32
Total batch reconstruction loss: 0.060495298355817795
Training batch 12 / 32
Total batch reconstruction loss: 0.057758983224630356
Training batch 13 / 32
Total batch reconstruction loss: 0.06214787811040878
Training batch 14 / 32
Total batch reconstruction loss: 0.05816078931093216
Training batch 15 / 32
Total batch reconstruction loss: 0.05617513135075569
Training batch 16 / 32
Total batch reconstruction loss: 0.060289982706308365
Training batch 17 / 32
Total batch reconstruction loss: 0.0630173608660698
Training batch 18 / 32
Total batch reconstruction loss: 0.0612056739628315
Training batch 19 / 32
Total batch reconstruction loss: 0.05455443635582924
Training batch 20 / 32
Total batch reconstruction loss: 0.05858957767486572
Training batch 21 / 32
Total batch reconstruction loss: 0.05943210422992706
Training batch 22 / 32
Total batch reconstruction loss: 0.06202585995197296
Training batch 23 / 32
Total batch reconstruction loss: 0.05721215903759003
Training batch 24 / 32
Total batch reconstruction loss: 0.06628945469856262
Training batch 25 / 32
Total batch reconstruction loss: 0.05620792508125305
Training batch 26 / 32
Total batch reconstruction loss: 0.059711288660764694
Training batch 27 / 32
Total batch reconstruction loss: 0.062155574560165405
Training batch 28 / 32
Total batch reconstruction loss: 0.059076227247714996
Training batch 29 / 32
Total batch reconstruction loss: 0.06667192280292511
Training batch 30 / 32
Total batch reconstruction loss: 0.062313299626111984
Training batch 31 / 32
Total batch reconstruction loss: 0.0610821507871151
Training batch 32 / 32
Total batch reconstruction loss: 0.056055475026369095
Epoch [243/500], Train Loss: 0.0585, Validation Loss: 0.0584, Generator Loss: 12.0215, Discriminator Loss: 0.3131
Training epoch 244 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.06034711375832558
Training batch 2 / 32
Total batch reconstruction loss: 0.061844974756240845
Training batch 3 / 32
Total batch reconstruction loss: 0.06134609878063202
Training batch 4 / 32
Total batch reconstruction loss: 0.0642940104007721
Training batch 5 / 32
Total batch reconstruction loss: 0.056983210146427155
Training batch 6 / 32
Total batch reconstruction loss: 0.06229502707719803
Training batch 7 / 32
Total batch reconstruction loss: 0.06070604920387268
Training batch 8 / 32
Total batch reconstruction loss: 0.06336991488933563
Training batch 9 / 32
Total batch reconstruction loss: 0.05960769206285477
Training batch 10 / 32
Total batch reconstruction loss: 0.06347281485795975
Training batch 11 / 32
Total batch reconstruction loss: 0.05784670636057854
Training batch 12 / 32
Total batch reconstruction loss: 0.06099390238523483
Training batch 13 / 32
Total batch reconstruction loss: 0.061463240534067154
Training batch 14 / 32
Total batch reconstruction loss: 0.061460673809051514
Training batch 15 / 32
Total batch reconstruction loss: 0.060477279126644135
Training batch 16 / 32
Total batch reconstruction loss: 0.057734306901693344
Training batch 17 / 32
Total batch reconstruction loss: 0.06151951849460602
Training batch 18 / 32
Total batch reconstruction loss: 0.0546148344874382
Training batch 19 / 32
Total batch reconstruction loss: 0.06347859650850296
Training batch 20 / 32
Total batch reconstruction loss: 0.05896523594856262
Training batch 21 / 32
Total batch reconstruction loss: 0.05815421789884567
Training batch 22 / 32
Total batch reconstruction loss: 0.05706388130784035
Training batch 23 / 32
Total batch reconstruction loss: 0.06116325408220291
Training batch 24 / 32
Total batch reconstruction loss: 0.057964026927948
Training batch 25 / 32
Total batch reconstruction loss: 0.05556074529886246
Training batch 26 / 32
Total batch reconstruction loss: 0.05467088520526886
Training batch 27 / 32
Total batch reconstruction loss: 0.05769076943397522
Training batch 28 / 32
Total batch reconstruction loss: 0.0576106496155262
Training batch 29 / 32
Total batch reconstruction loss: 0.058448366820812225
Training batch 30 / 32
Total batch reconstruction loss: 0.05650746077299118
Training batch 31 / 32
Total batch reconstruction loss: 0.061302680522203445
Training batch 32 / 32
Total batch reconstruction loss: 0.06326468288898468
Epoch [244/500], Train Loss: 0.0582, Validation Loss: 0.0577, Generator Loss: 12.0192, Discriminator Loss: 0.3184
Training epoch 245 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.05934528261423111
Training batch 2 / 32
Total batch reconstruction loss: 0.059838198125362396
Training batch 3 / 32
Total batch reconstruction loss: 0.06048308312892914
Training batch 4 / 32
Total batch reconstruction loss: 0.059844691306352615
Training batch 5 / 32
Total batch reconstruction loss: 0.05780314654111862
Training batch 6 / 32
Total batch reconstruction loss: 0.05918967351317406
Training batch 7 / 32
Total batch reconstruction loss: 0.06325475126504898
Training batch 8 / 32
Total batch reconstruction loss: 0.06365732103586197
Training batch 9 / 32
Total batch reconstruction loss: 0.06257158517837524
Training batch 10 / 32
Total batch reconstruction loss: 0.06091691181063652
Training batch 11 / 32
Total batch reconstruction loss: 0.05798488110303879
Training batch 12 / 32
Total batch reconstruction loss: 0.0558130107820034
Training batch 13 / 32
Total batch reconstruction loss: 0.06049282103776932
Training batch 14 / 32
Total batch reconstruction loss: 0.05749941244721413
Training batch 15 / 32
Total batch reconstruction loss: 0.05805394798517227
Training batch 16 / 32
Total batch reconstruction loss: 0.05608091503381729
Training batch 17 / 32
Total batch reconstruction loss: 0.059691790491342545
Training batch 18 / 32
Total batch reconstruction loss: 0.062428705394268036
Training batch 19 / 32
Total batch reconstruction loss: 0.05897705256938934
Training batch 20 / 32
Total batch reconstruction loss: 0.05616948753595352
Training batch 21 / 32
Total batch reconstruction loss: 0.06140957027673721
Training batch 22 / 32
Total batch reconstruction loss: 0.0588708370923996
Training batch 23 / 32
Total batch reconstruction loss: 0.06108727306127548
Training batch 24 / 32
Total batch reconstruction loss: 0.0626465454697609
Training batch 25 / 32
Total batch reconstruction loss: 0.05627766251564026
Training batch 26 / 32
Total batch reconstruction loss: 0.0583481527864933
Training batch 27 / 32
Total batch reconstruction loss: 0.06082145869731903
Training batch 28 / 32
Total batch reconstruction loss: 0.054996512830257416
Training batch 29 / 32
Total batch reconstruction loss: 0.06234865263104439
Training batch 30 / 32
Total batch reconstruction loss: 0.06436900794506073
Training batch 31 / 32
Total batch reconstruction loss: 0.058396559208631516
Training batch 32 / 32
Total batch reconstruction loss: 0.05701912194490433
Epoch [245/500], Train Loss: 0.0578, Validation Loss: 0.0608, Generator Loss: 11.9935, Discriminator Loss: 0.3029
Training epoch 246 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.05607006698846817
Training batch 2 / 32
Total batch reconstruction loss: 0.058148570358753204
Training batch 3 / 32
Total batch reconstruction loss: 0.06349477916955948
Training batch 4 / 32
Total batch reconstruction loss: 0.06566716730594635
Training batch 5 / 32
Total batch reconstruction loss: 0.056655168533325195
Training batch 6 / 32
Total batch reconstruction loss: 0.06338812410831451
Training batch 7 / 32
Total batch reconstruction loss: 0.061613474041223526
Training batch 8 / 32
Total batch reconstruction loss: 0.05922802910208702
Training batch 9 / 32
Total batch reconstruction loss: 0.060160502791404724
Training batch 10 / 32
Total batch reconstruction loss: 0.05846124514937401
Training batch 11 / 32
Total batch reconstruction loss: 0.05953339487314224
Training batch 12 / 32
Total batch reconstruction loss: 0.05620315298438072
Training batch 13 / 32
Total batch reconstruction loss: 0.058052875101566315
Training batch 14 / 32
Total batch reconstruction loss: 0.06051865220069885
Training batch 15 / 32
Total batch reconstruction loss: 0.06299477815628052
Training batch 16 / 32
Total batch reconstruction loss: 0.05780355632305145
Training batch 17 / 32
Total batch reconstruction loss: 0.06464329361915588
Training batch 18 / 32
Total batch reconstruction loss: 0.0690026581287384
Training batch 19 / 32
Total batch reconstruction loss: 0.05991678684949875
Training batch 20 / 32
Total batch reconstruction loss: 0.05806652829051018
Training batch 21 / 32
Total batch reconstruction loss: 0.05573320388793945
Training batch 22 / 32
Total batch reconstruction loss: 0.060874324291944504
Training batch 23 / 32
Total batch reconstruction loss: 0.061594072729349136
Training batch 24 / 32
Total batch reconstruction loss: 0.0586603581905365
Training batch 25 / 32
Total batch reconstruction loss: 0.06466203182935715
Training batch 26 / 32
Total batch reconstruction loss: 0.05859099328517914
Training batch 27 / 32
Total batch reconstruction loss: 0.051400210708379745
Training batch 28 / 32
Total batch reconstruction loss: 0.05618913471698761
Training batch 29 / 32
Total batch reconstruction loss: 0.05642983317375183
Training batch 30 / 32
Total batch reconstruction loss: 0.06105532869696617
Training batch 31 / 32
Total batch reconstruction loss: 0.05882706865668297
Training batch 32 / 32
Total batch reconstruction loss: 0.0491785854101181
Epoch [246/500], Train Loss: 0.0580, Validation Loss: 0.0581, Generator Loss: 11.9620, Discriminator Loss: 0.3193
Training epoch 247 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.06174708902835846
Training batch 2 / 32
Total batch reconstruction loss: 0.059051722288131714
Training batch 3 / 32
Total batch reconstruction loss: 0.060235634446144104
Training batch 4 / 32
Total batch reconstruction loss: 0.06268799304962158
Training batch 5 / 32
Total batch reconstruction loss: 0.05775679275393486
Training batch 6 / 32
Total batch reconstruction loss: 0.06513099372386932
Training batch 7 / 32
Total batch reconstruction loss: 0.05768241360783577
Training batch 8 / 32
Total batch reconstruction loss: 0.06254522502422333
Training batch 9 / 32
Total batch reconstruction loss: 0.05982428789138794
Training batch 10 / 32
Total batch reconstruction loss: 0.05637108534574509
Training batch 11 / 32
Total batch reconstruction loss: 0.0654904693365097
Training batch 12 / 32
Total batch reconstruction loss: 0.05663973093032837
Training batch 13 / 32
Total batch reconstruction loss: 0.062446534633636475
Training batch 14 / 32
Total batch reconstruction loss: 0.06383759528398514
Training batch 15 / 32
Total batch reconstruction loss: 0.057105060666799545
Training batch 16 / 32
Total batch reconstruction loss: 0.05840525031089783
Training batch 17 / 32
Total batch reconstruction loss: 0.06466646492481232
Training batch 18 / 32
Total batch reconstruction loss: 0.05883744731545448
Training batch 19 / 32
Total batch reconstruction loss: 0.058706432580947876
Training batch 20 / 32
Total batch reconstruction loss: 0.05855081230401993
Training batch 21 / 32
Total batch reconstruction loss: 0.057535767555236816
Training batch 22 / 32
Total batch reconstruction loss: 0.05868896469473839
Training batch 23 / 32
Total batch reconstruction loss: 0.05859364941716194
Training batch 24 / 32
Total batch reconstruction loss: 0.05709070712327957
Training batch 25 / 32
Total batch reconstruction loss: 0.05884917452931404
Training batch 26 / 32
Total batch reconstruction loss: 0.0547085776925087
Training batch 27 / 32
Total batch reconstruction loss: 0.06437273323535919
Training batch 28 / 32
Total batch reconstruction loss: 0.0575634203851223
Training batch 29 / 32
Total batch reconstruction loss: 0.0552641786634922
Training batch 30 / 32
Total batch reconstruction loss: 0.05701979249715805
Training batch 31 / 32
Total batch reconstruction loss: 0.061186037957668304
Training batch 32 / 32
Total batch reconstruction loss: 0.05616457760334015
Epoch [247/500], Train Loss: 0.0584, Validation Loss: 0.0598, Generator Loss: 11.9709, Discriminator Loss: 0.3206
Training epoch 248 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.06033600866794586
Training batch 2 / 32
Total batch reconstruction loss: 0.062746062874794
Training batch 3 / 32
Total batch reconstruction loss: 0.06303699314594269
Training batch 4 / 32
Total batch reconstruction loss: 0.057485565543174744
Training batch 5 / 32
Total batch reconstruction loss: 0.05979035049676895
Training batch 6 / 32
Total batch reconstruction loss: 0.06239563971757889
Training batch 7 / 32
Total batch reconstruction loss: 0.05967283248901367
Training batch 8 / 32
Total batch reconstruction loss: 0.05805405229330063
Training batch 9 / 32
Total batch reconstruction loss: 0.059972770512104034
Training batch 10 / 32
Total batch reconstruction loss: 0.06007859483361244
Training batch 11 / 32
Total batch reconstruction loss: 0.059014976024627686
Training batch 12 / 32
Total batch reconstruction loss: 0.05900606885552406
Training batch 13 / 32
Total batch reconstruction loss: 0.0590938962996006
Training batch 14 / 32
Total batch reconstruction loss: 0.05721185356378555
Training batch 15 / 32
Total batch reconstruction loss: 0.05697169899940491
Training batch 16 / 32
Total batch reconstruction loss: 0.05972527712583542
Training batch 17 / 32
Total batch reconstruction loss: 0.06117520108819008
Training batch 18 / 32
Total batch reconstruction loss: 0.06273301690816879
Training batch 19 / 32
Total batch reconstruction loss: 0.06345872581005096
Training batch 20 / 32
Total batch reconstruction loss: 0.05872522294521332
Training batch 21 / 32
Total batch reconstruction loss: 0.06136676296591759
Training batch 22 / 32
Total batch reconstruction loss: 0.06451618671417236
Training batch 23 / 32
Total batch reconstruction loss: 0.05685598775744438
Training batch 24 / 32
Total batch reconstruction loss: 0.05830742418766022
Training batch 25 / 32
Total batch reconstruction loss: 0.05542527884244919
Training batch 26 / 32
Total batch reconstruction loss: 0.056797780096530914
Training batch 27 / 32
Total batch reconstruction loss: 0.0569443479180336
Training batch 28 / 32
Total batch reconstruction loss: 0.06074829399585724
Training batch 29 / 32
Total batch reconstruction loss: 0.06012362241744995
Training batch 30 / 32
Total batch reconstruction loss: 0.05953492224216461
Training batch 31 / 32
Total batch reconstruction loss: 0.06457296013832092
Training batch 32 / 32
Total batch reconstruction loss: 0.05365467816591263
Epoch [248/500], Train Loss: 0.0586, Validation Loss: 0.0575, Generator Loss: 12.0149, Discriminator Loss: 0.3044
Training epoch 249 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.0587882474064827
Training batch 2 / 32
Total batch reconstruction loss: 0.06088480353355408
Training batch 3 / 32
Total batch reconstruction loss: 0.05864611268043518
Training batch 4 / 32
Total batch reconstruction loss: 0.05959022790193558
Training batch 5 / 32
Total batch reconstruction loss: 0.054526034742593765
Training batch 6 / 32
Total batch reconstruction loss: 0.05520869791507721
Training batch 7 / 32
Total batch reconstruction loss: 0.05674012750387192
Training batch 8 / 32
Total batch reconstruction loss: 0.061719607561826706
Training batch 9 / 32
Total batch reconstruction loss: 0.06139864772558212
Training batch 10 / 32
Total batch reconstruction loss: 0.05979534238576889
Training batch 11 / 32
Total batch reconstruction loss: 0.06110692769289017
Training batch 12 / 32
Total batch reconstruction loss: 0.06114932522177696
Training batch 13 / 32
Total batch reconstruction loss: 0.06041041016578674
Training batch 14 / 32
Total batch reconstruction loss: 0.05897274613380432
Training batch 15 / 32
Total batch reconstruction loss: 0.05506446212530136
Training batch 16 / 32
Total batch reconstruction loss: 0.059689000248909
Training batch 17 / 32
Total batch reconstruction loss: 0.05532026290893555
Training batch 18 / 32
Total batch reconstruction loss: 0.06117147207260132
Training batch 19 / 32
Total batch reconstruction loss: 0.05985117703676224
Training batch 20 / 32
Total batch reconstruction loss: 0.06358987092971802
Training batch 21 / 32
Total batch reconstruction loss: 0.060083240270614624
Training batch 22 / 32
Total batch reconstruction loss: 0.0591730996966362
Training batch 23 / 32
Total batch reconstruction loss: 0.06351357698440552
Training batch 24 / 32
Total batch reconstruction loss: 0.06032080203294754
Training batch 25 / 32
Total batch reconstruction loss: 0.06173533946275711
Training batch 26 / 32
Total batch reconstruction loss: 0.06073923408985138
Training batch 27 / 32
Total batch reconstruction loss: 0.06053842976689339
Training batch 28 / 32
Total batch reconstruction loss: 0.0582367479801178
Training batch 29 / 32
Total batch reconstruction loss: 0.060346975922584534
Training batch 30 / 32
Total batch reconstruction loss: 0.058176446706056595
Training batch 31 / 32
Total batch reconstruction loss: 0.06161414086818695
Training batch 32 / 32
Total batch reconstruction loss: 0.0581820122897625
Epoch [249/500], Train Loss: 0.0583, Validation Loss: 0.0591, Generator Loss: 11.9801, Discriminator Loss: 0.3209
Training epoch 250 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.05730510130524635
Training batch 2 / 32
Total batch reconstruction loss: 0.056372568011283875
Training batch 3 / 32
Total batch reconstruction loss: 0.06395044177770615
Training batch 4 / 32
Total batch reconstruction loss: 0.06167551130056381
Training batch 5 / 32
Total batch reconstruction loss: 0.057955458760261536
Training batch 6 / 32
Total batch reconstruction loss: 0.05752811208367348
Training batch 7 / 32
Total batch reconstruction loss: 0.05952383950352669
Training batch 8 / 32
Total batch reconstruction loss: 0.06406904011964798
Training batch 9 / 32
Total batch reconstruction loss: 0.06600762903690338
Training batch 10 / 32
Total batch reconstruction loss: 0.056830696761608124
Training batch 11 / 32
Total batch reconstruction loss: 0.05613251030445099
Training batch 12 / 32
Total batch reconstruction loss: 0.06387022137641907
Training batch 13 / 32
Total batch reconstruction loss: 0.06157085299491882
Training batch 14 / 32
Total batch reconstruction loss: 0.06344084441661835
Training batch 15 / 32
Total batch reconstruction loss: 0.0570748969912529
Training batch 16 / 32
Total batch reconstruction loss: 0.058870162814855576
Training batch 17 / 32
Total batch reconstruction loss: 0.061388153582811356
Training batch 18 / 32
Total batch reconstruction loss: 0.059766024351119995
Training batch 19 / 32
Total batch reconstruction loss: 0.054645150899887085
Training batch 20 / 32
Total batch reconstruction loss: 0.06171214580535889
Training batch 21 / 32
Total batch reconstruction loss: 0.057983458042144775
Training batch 22 / 32
Total batch reconstruction loss: 0.06014364957809448
Training batch 23 / 32
Total batch reconstruction loss: 0.05560356751084328
Training batch 24 / 32
Total batch reconstruction loss: 0.06177617609500885
Training batch 25 / 32
Total batch reconstruction loss: 0.058660440146923065
Training batch 26 / 32
Total batch reconstruction loss: 0.05737614259123802
Training batch 27 / 32
Total batch reconstruction loss: 0.05658576264977455
Training batch 28 / 32
Total batch reconstruction loss: 0.061296261847019196
Training batch 29 / 32
Total batch reconstruction loss: 0.05909100919961929
Training batch 30 / 32
Total batch reconstruction loss: 0.05820515379309654
Training batch 31 / 32
Total batch reconstruction loss: 0.059356871992349625
Training batch 32 / 32
Total batch reconstruction loss: 0.0670078918337822
Epoch [250/500], Train Loss: 0.0584, Validation Loss: 0.0569, Generator Loss: 12.0308, Discriminator Loss: 0.3026
Training epoch 251 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.05694544315338135
Training batch 2 / 32
Total batch reconstruction loss: 0.0643533319234848
Training batch 3 / 32
Total batch reconstruction loss: 0.06180230528116226
Training batch 4 / 32
Total batch reconstruction loss: 0.05550297349691391
Training batch 5 / 32
Total batch reconstruction loss: 0.05935034900903702
Training batch 6 / 32
Total batch reconstruction loss: 0.05933555215597153
Training batch 7 / 32
Total batch reconstruction loss: 0.05779300630092621
Training batch 8 / 32
Total batch reconstruction loss: 0.05731985718011856
Training batch 9 / 32
Total batch reconstruction loss: 0.05917789041996002
Training batch 10 / 32
Total batch reconstruction loss: 0.063475102186203
Training batch 11 / 32
Total batch reconstruction loss: 0.06113786622881889
Training batch 12 / 32
Total batch reconstruction loss: 0.06301987916231155
Training batch 13 / 32
Total batch reconstruction loss: 0.06190546974539757
Training batch 14 / 32
Total batch reconstruction loss: 0.05472806468605995
Training batch 15 / 32
Total batch reconstruction loss: 0.05867442488670349
Training batch 16 / 32
Total batch reconstruction loss: 0.058250028640031815
Training batch 17 / 32
Total batch reconstruction loss: 0.060697946697473526
Training batch 18 / 32
Total batch reconstruction loss: 0.05675378069281578
Training batch 19 / 32
Total batch reconstruction loss: 0.05896276980638504
Training batch 20 / 32
Total batch reconstruction loss: 0.06292247772216797
Training batch 21 / 32
Total batch reconstruction loss: 0.0612204447388649
Training batch 22 / 32
Total batch reconstruction loss: 0.060149066150188446
Training batch 23 / 32
Total batch reconstruction loss: 0.06002534553408623
Training batch 24 / 32
Total batch reconstruction loss: 0.060010913759469986
Training batch 25 / 32
Total batch reconstruction loss: 0.059648577123880386
Training batch 26 / 32
Total batch reconstruction loss: 0.061079174280166626
Training batch 27 / 32
Total batch reconstruction loss: 0.058232493698596954
Training batch 28 / 32
Total batch reconstruction loss: 0.06375721096992493
Training batch 29 / 32
Total batch reconstruction loss: 0.06056590378284454
Training batch 30 / 32
Total batch reconstruction loss: 0.061182357370853424
Training batch 31 / 32
Total batch reconstruction loss: 0.06230315938591957
Training batch 32 / 32
Total batch reconstruction loss: 0.05571625381708145
Epoch [251/500], Train Loss: 0.0586, Validation Loss: 0.0581, Generator Loss: 12.0470, Discriminator Loss: 0.3111
Training epoch 252 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.06076148897409439
Training batch 2 / 32
Total batch reconstruction loss: 0.05962979793548584
Training batch 3 / 32
Total batch reconstruction loss: 0.059034690260887146
Training batch 4 / 32
Total batch reconstruction loss: 0.0676833987236023
Training batch 5 / 32
Total batch reconstruction loss: 0.06170300394296646
Training batch 6 / 32
Total batch reconstruction loss: 0.06227867305278778
Training batch 7 / 32
Total batch reconstruction loss: 0.061545711010694504
Training batch 8 / 32
Total batch reconstruction loss: 0.05808304622769356
Training batch 9 / 32
Total batch reconstruction loss: 0.06238125264644623
Training batch 10 / 32
Total batch reconstruction loss: 0.057637427002191544
Training batch 11 / 32
Total batch reconstruction loss: 0.06141502410173416
Training batch 12 / 32
Total batch reconstruction loss: 0.06353240460157394
Training batch 13 / 32
Total batch reconstruction loss: 0.05982603505253792
Training batch 14 / 32
Total batch reconstruction loss: 0.05868598073720932
Training batch 15 / 32
Total batch reconstruction loss: 0.05850469321012497
Training batch 16 / 32
Total batch reconstruction loss: 0.0632588341832161
Training batch 17 / 32
Total batch reconstruction loss: 0.058167338371276855
Training batch 18 / 32
Total batch reconstruction loss: 0.05910992622375488
Training batch 19 / 32
Total batch reconstruction loss: 0.06110931932926178
Training batch 20 / 32
Total batch reconstruction loss: 0.058041494339704514
Training batch 21 / 32
Total batch reconstruction loss: 0.0618569552898407
Training batch 22 / 32
Total batch reconstruction loss: 0.06101357191801071
Training batch 23 / 32
Total batch reconstruction loss: 0.05523131787776947
Training batch 24 / 32
Total batch reconstruction loss: 0.059257492423057556
Training batch 25 / 32
Total batch reconstruction loss: 0.061138227581977844
Training batch 26 / 32
Total batch reconstruction loss: 0.05705612897872925
Training batch 27 / 32
Total batch reconstruction loss: 0.05997033417224884
Training batch 28 / 32
Total batch reconstruction loss: 0.06169223040342331
Training batch 29 / 32
Total batch reconstruction loss: 0.05723459646105766
Training batch 30 / 32
Total batch reconstruction loss: 0.05735401064157486
Training batch 31 / 32
Total batch reconstruction loss: 0.05858587101101875
Training batch 32 / 32
Total batch reconstruction loss: 0.10281623899936676
Epoch [252/500], Train Loss: 0.0597, Validation Loss: 0.0583, Generator Loss: 12.3509, Discriminator Loss: 0.3234
Training epoch 253 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.061740897595882416
Training batch 2 / 32
Total batch reconstruction loss: 0.05682116746902466
Training batch 3 / 32
Total batch reconstruction loss: 0.05899977684020996
Training batch 4 / 32
Total batch reconstruction loss: 0.0595390610396862
Training batch 5 / 32
Total batch reconstruction loss: 0.06282554566860199
Training batch 6 / 32
Total batch reconstruction loss: 0.06364075839519501
Training batch 7 / 32
Total batch reconstruction loss: 0.06019942834973335
Training batch 8 / 32
Total batch reconstruction loss: 0.06548918783664703
Training batch 9 / 32
Total batch reconstruction loss: 0.0575217679142952
Training batch 10 / 32
Total batch reconstruction loss: 0.06066674739122391
Training batch 11 / 32
Total batch reconstruction loss: 0.05827479436993599
Training batch 12 / 32
Total batch reconstruction loss: 0.05963550880551338
Training batch 13 / 32
Total batch reconstruction loss: 0.057546935975551605
Training batch 14 / 32
Total batch reconstruction loss: 0.06491000205278397
Training batch 15 / 32
Total batch reconstruction loss: 0.05806581676006317
Training batch 16 / 32
Total batch reconstruction loss: 0.06024181842803955
Training batch 17 / 32
Total batch reconstruction loss: 0.06171794980764389
Training batch 18 / 32
Total batch reconstruction loss: 0.058859579265117645
Training batch 19 / 32
Total batch reconstruction loss: 0.05852237343788147
Training batch 20 / 32
Total batch reconstruction loss: 0.055938005447387695
Training batch 21 / 32
Total batch reconstruction loss: 0.05601570010185242
Training batch 22 / 32
Total batch reconstruction loss: 0.054947398602962494
Training batch 23 / 32
Total batch reconstruction loss: 0.058024510741233826
Training batch 24 / 32
Total batch reconstruction loss: 0.058414027094841
Training batch 25 / 32
Total batch reconstruction loss: 0.06630848348140717
Training batch 26 / 32
Total batch reconstruction loss: 0.06081435829401016
Training batch 27 / 32
Total batch reconstruction loss: 0.054946862161159515
Training batch 28 / 32
Total batch reconstruction loss: 0.05828918516635895
Training batch 29 / 32
Total batch reconstruction loss: 0.05567213520407677
Training batch 30 / 32
Total batch reconstruction loss: 0.062222011387348175
Training batch 31 / 32
Total batch reconstruction loss: 0.05769757926464081
Training batch 32 / 32
Total batch reconstruction loss: 0.05587904900312424
Epoch [253/500], Train Loss: 0.0579, Validation Loss: 0.0580, Generator Loss: 11.9524, Discriminator Loss: 0.3028
Training epoch 254 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.05920640379190445
Training batch 2 / 32
Total batch reconstruction loss: 0.059054743498563766
Training batch 3 / 32
Total batch reconstruction loss: 0.05755152925848961
Training batch 4 / 32
Total batch reconstruction loss: 0.06353000551462173
Training batch 5 / 32
Total batch reconstruction loss: 0.06119580194354057
Training batch 6 / 32
Total batch reconstruction loss: 0.05961224436759949
Training batch 7 / 32
Total batch reconstruction loss: 0.057119011878967285
Training batch 8 / 32
Total batch reconstruction loss: 0.05913008749485016
Training batch 9 / 32
Total batch reconstruction loss: 0.058406103402376175
Training batch 10 / 32
Total batch reconstruction loss: 0.05845528468489647
Training batch 11 / 32
Total batch reconstruction loss: 0.05835575982928276
Training batch 12 / 32
Total batch reconstruction loss: 0.06472783535718918
Training batch 13 / 32
Total batch reconstruction loss: 0.0643087849020958
Training batch 14 / 32
Total batch reconstruction loss: 0.059746697545051575
Training batch 15 / 32
Total batch reconstruction loss: 0.06150504946708679
Training batch 16 / 32
Total batch reconstruction loss: 0.06493079662322998
Training batch 17 / 32
Total batch reconstruction loss: 0.05689340829849243
Training batch 18 / 32
Total batch reconstruction loss: 0.057699982076883316
Training batch 19 / 32
Total batch reconstruction loss: 0.058098070323467255
Training batch 20 / 32
Total batch reconstruction loss: 0.0649719312787056
Training batch 21 / 32
Total batch reconstruction loss: 0.05958472192287445
Training batch 22 / 32
Total batch reconstruction loss: 0.05897165834903717
Training batch 23 / 32
Total batch reconstruction loss: 0.062406595796346664
Training batch 24 / 32
Total batch reconstruction loss: 0.05716570466756821
Training batch 25 / 32
Total batch reconstruction loss: 0.05777069553732872
Training batch 26 / 32
Total batch reconstruction loss: 0.058872148394584656
Training batch 27 / 32
Total batch reconstruction loss: 0.05808299779891968
Training batch 28 / 32
Total batch reconstruction loss: 0.060229651629924774
Training batch 29 / 32
Total batch reconstruction loss: 0.0551224946975708
Training batch 30 / 32
Total batch reconstruction loss: 0.060014113783836365
Training batch 31 / 32
Total batch reconstruction loss: 0.0570126548409462
Training batch 32 / 32
Total batch reconstruction loss: 0.048824332654476166
Epoch [254/500], Train Loss: 0.0577, Validation Loss: 0.0599, Generator Loss: 11.9358, Discriminator Loss: 0.3175
Training epoch 255 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.05553431063890457
Training batch 2 / 32
Total batch reconstruction loss: 0.05870102718472481
Training batch 3 / 32
Total batch reconstruction loss: 0.06479012221097946
Training batch 4 / 32
Total batch reconstruction loss: 0.06154265254735947
Training batch 5 / 32
Total batch reconstruction loss: 0.057823117822408676
Training batch 6 / 32
Total batch reconstruction loss: 0.05797431617975235
Training batch 7 / 32
Total batch reconstruction loss: 0.05716075748205185
Training batch 8 / 32
Total batch reconstruction loss: 0.06274352222681046
Training batch 9 / 32
Total batch reconstruction loss: 0.06031117960810661
Training batch 10 / 32
Total batch reconstruction loss: 0.060048192739486694
Training batch 11 / 32
Total batch reconstruction loss: 0.0622314028441906
Training batch 12 / 32
Total batch reconstruction loss: 0.0582195445895195
Training batch 13 / 32
Total batch reconstruction loss: 0.06011402979493141
Training batch 14 / 32
Total batch reconstruction loss: 0.05992898344993591
Training batch 15 / 32
Total batch reconstruction loss: 0.0611252523958683
Training batch 16 / 32
Total batch reconstruction loss: 0.06005442887544632
Training batch 17 / 32
Total batch reconstruction loss: 0.058231014758348465
Training batch 18 / 32
Total batch reconstruction loss: 0.05572187900543213
Training batch 19 / 32
Total batch reconstruction loss: 0.05886448547244072
Training batch 20 / 32
Total batch reconstruction loss: 0.05632221698760986
Training batch 21 / 32
Total batch reconstruction loss: 0.06061384826898575
Training batch 22 / 32
Total batch reconstruction loss: 0.0594966784119606
Training batch 23 / 32
Total batch reconstruction loss: 0.06440119445323944
Training batch 24 / 32
Total batch reconstruction loss: 0.05897275358438492
Training batch 25 / 32
Total batch reconstruction loss: 0.058749884366989136
Training batch 26 / 32
Total batch reconstruction loss: 0.05794849246740341
Training batch 27 / 32
Total batch reconstruction loss: 0.05960012227296829
Training batch 28 / 32
Total batch reconstruction loss: 0.06152551621198654
Training batch 29 / 32
Total batch reconstruction loss: 0.06105632334947586
Training batch 30 / 32
Total batch reconstruction loss: 0.05776244401931763
Training batch 31 / 32
Total batch reconstruction loss: 0.05754774808883667
Training batch 32 / 32
Total batch reconstruction loss: 0.04991085082292557
Epoch [255/500], Train Loss: 0.0577, Validation Loss: 0.0577, Generator Loss: 11.9051, Discriminator Loss: 0.3252
Training epoch 256 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.06083988398313522
Training batch 2 / 32
Total batch reconstruction loss: 0.06157389283180237
Training batch 3 / 32
Total batch reconstruction loss: 0.059317365288734436
Training batch 4 / 32
Total batch reconstruction loss: 0.05930210277438164
Training batch 5 / 32
Total batch reconstruction loss: 0.05644858255982399
Training batch 6 / 32
Total batch reconstruction loss: 0.059039629995822906
Training batch 7 / 32
Total batch reconstruction loss: 0.05892360210418701
Training batch 8 / 32
Total batch reconstruction loss: 0.054794397205114365
Training batch 9 / 32
Total batch reconstruction loss: 0.05810665339231491
Training batch 10 / 32
Total batch reconstruction loss: 0.0585336796939373
Training batch 11 / 32
Total batch reconstruction loss: 0.06365882605314255
Training batch 12 / 32
Total batch reconstruction loss: 0.05482572317123413
Training batch 13 / 32
Total batch reconstruction loss: 0.05930885300040245
Training batch 14 / 32
Total batch reconstruction loss: 0.057293374091386795
Training batch 15 / 32
Total batch reconstruction loss: 0.0599140003323555
Training batch 16 / 32
Total batch reconstruction loss: 0.06342644989490509
Training batch 17 / 32
Total batch reconstruction loss: 0.06311909854412079
Training batch 18 / 32
Total batch reconstruction loss: 0.05519087612628937
Training batch 19 / 32
Total batch reconstruction loss: 0.05706961452960968
Training batch 20 / 32
Total batch reconstruction loss: 0.05810582637786865
Training batch 21 / 32
Total batch reconstruction loss: 0.05977749824523926
Training batch 22 / 32
Total batch reconstruction loss: 0.06496519595384598
Training batch 23 / 32
Total batch reconstruction loss: 0.05765631049871445
Training batch 24 / 32
Total batch reconstruction loss: 0.059850871562957764
Training batch 25 / 32
Total batch reconstruction loss: 0.056733936071395874
Training batch 26 / 32
Total batch reconstruction loss: 0.06224226951599121
Training batch 27 / 32
Total batch reconstruction loss: 0.06358283758163452
Training batch 28 / 32
Total batch reconstruction loss: 0.06087445467710495
Training batch 29 / 32
Total batch reconstruction loss: 0.062363915145397186
Training batch 30 / 32
Total batch reconstruction loss: 0.06209210306406021
Training batch 31 / 32
Total batch reconstruction loss: 0.05948138236999512
Training batch 32 / 32
Total batch reconstruction loss: 0.05121748149394989
Epoch [256/500], Train Loss: 0.0582, Validation Loss: 0.0568, Generator Loss: 11.9416, Discriminator Loss: 0.3132
Training epoch 257 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.05792766064405441
Training batch 2 / 32
Total batch reconstruction loss: 0.06267895549535751
Training batch 3 / 32
Total batch reconstruction loss: 0.0618877038359642
Training batch 4 / 32
Total batch reconstruction loss: 0.0596679151058197
Training batch 5 / 32
Total batch reconstruction loss: 0.06256595253944397
Training batch 6 / 32
Total batch reconstruction loss: 0.054772280156612396
Training batch 7 / 32
Total batch reconstruction loss: 0.05656138435006142
Training batch 8 / 32
Total batch reconstruction loss: 0.06072000041604042
Training batch 9 / 32
Total batch reconstruction loss: 0.060108110308647156
Training batch 10 / 32
Total batch reconstruction loss: 0.05686205253005028
Training batch 11 / 32
Total batch reconstruction loss: 0.059849195182323456
Training batch 12 / 32
Total batch reconstruction loss: 0.06042919307947159
Training batch 13 / 32
Total batch reconstruction loss: 0.05612998455762863
Training batch 14 / 32
Total batch reconstruction loss: 0.062141068279743195
Training batch 15 / 32
Total batch reconstruction loss: 0.05976954475045204
Training batch 16 / 32
Total batch reconstruction loss: 0.06468478590250015
Training batch 17 / 32
Total batch reconstruction loss: 0.05376925319433212
Training batch 18 / 32
Total batch reconstruction loss: 0.05850847065448761
Training batch 19 / 32
Total batch reconstruction loss: 0.05897584557533264
Training batch 20 / 32
Total batch reconstruction loss: 0.06295229494571686
Training batch 21 / 32
Total batch reconstruction loss: 0.06044573336839676
Training batch 22 / 32
Total batch reconstruction loss: 0.05599649250507355
Training batch 23 / 32
Total batch reconstruction loss: 0.059123385697603226
Training batch 24 / 32
Total batch reconstruction loss: 0.05552569776773453
Training batch 25 / 32
Total batch reconstruction loss: 0.0637136697769165
Training batch 26 / 32
Total batch reconstruction loss: 0.059927456080913544
Training batch 27 / 32
Total batch reconstruction loss: 0.055717602372169495
Training batch 28 / 32
Total batch reconstruction loss: 0.06291861087083817
Training batch 29 / 32
Total batch reconstruction loss: 0.059903621673583984
Training batch 30 / 32
Total batch reconstruction loss: 0.058611102402210236
Training batch 31 / 32
Total batch reconstruction loss: 0.06329487264156342
Training batch 32 / 32
Total batch reconstruction loss: 0.057645149528980255
Epoch [257/500], Train Loss: 0.0582, Validation Loss: 0.0596, Generator Loss: 11.9770, Discriminator Loss: 0.3062
Training epoch 258 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.0559144988656044
Training batch 2 / 32
Total batch reconstruction loss: 0.05553673207759857
Training batch 3 / 32
Total batch reconstruction loss: 0.06239335983991623
Training batch 4 / 32
Total batch reconstruction loss: 0.06004245951771736
Training batch 5 / 32
Total batch reconstruction loss: 0.06232842057943344
Training batch 6 / 32
Total batch reconstruction loss: 0.05821477621793747
Training batch 7 / 32
Total batch reconstruction loss: 0.06361323595046997
Training batch 8 / 32
Total batch reconstruction loss: 0.06070874631404877
Training batch 9 / 32
Total batch reconstruction loss: 0.05815255269408226
Training batch 10 / 32
Total batch reconstruction loss: 0.06559482216835022
Training batch 11 / 32
Total batch reconstruction loss: 0.06198621541261673
Training batch 12 / 32
Total batch reconstruction loss: 0.061429377645254135
Training batch 13 / 32
Total batch reconstruction loss: 0.05563943460583687
Training batch 14 / 32
Total batch reconstruction loss: 0.05909046530723572
Training batch 15 / 32
Total batch reconstruction loss: 0.057746391743421555
Training batch 16 / 32
Total batch reconstruction loss: 0.06406201422214508
Training batch 17 / 32
Total batch reconstruction loss: 0.05712614208459854
Training batch 18 / 32
Total batch reconstruction loss: 0.06386102735996246
Training batch 19 / 32
Total batch reconstruction loss: 0.059365980327129364
Training batch 20 / 32
Total batch reconstruction loss: 0.05843900516629219
Training batch 21 / 32
Total batch reconstruction loss: 0.0650767907500267
Training batch 22 / 32
Total batch reconstruction loss: 0.06293752789497375
Training batch 23 / 32
Total batch reconstruction loss: 0.05794117972254753
Training batch 24 / 32
Total batch reconstruction loss: 0.059672437608242035
Training batch 25 / 32
Total batch reconstruction loss: 0.06277810037136078
Training batch 26 / 32
Total batch reconstruction loss: 0.05932522565126419
Training batch 27 / 32
Total batch reconstruction loss: 0.05874200165271759
Training batch 28 / 32
Total batch reconstruction loss: 0.05925079807639122
Training batch 29 / 32
Total batch reconstruction loss: 0.05647890269756317
Training batch 30 / 32
Total batch reconstruction loss: 0.0569910854101181
Training batch 31 / 32
Total batch reconstruction loss: 0.06307615339756012
Training batch 32 / 32
Total batch reconstruction loss: 0.054593078792095184
Epoch [258/500], Train Loss: 0.0584, Validation Loss: 0.0593, Generator Loss: 12.0573, Discriminator Loss: 0.3117
Training epoch 259 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.06020886451005936
Training batch 2 / 32
Total batch reconstruction loss: 0.05845015496015549
Training batch 3 / 32
Total batch reconstruction loss: 0.058808423578739166
Training batch 4 / 32
Total batch reconstruction loss: 0.05659642070531845
Training batch 5 / 32
Total batch reconstruction loss: 0.056809574365615845
Training batch 6 / 32
Total batch reconstruction loss: 0.057681869715452194
Training batch 7 / 32
Total batch reconstruction loss: 0.05511874705553055
Training batch 8 / 32
Total batch reconstruction loss: 0.057559333741664886
Training batch 9 / 32
Total batch reconstruction loss: 0.061018187552690506
Training batch 10 / 32
Total batch reconstruction loss: 0.06461049616336823
Training batch 11 / 32
Total batch reconstruction loss: 0.058282751590013504
Training batch 12 / 32
Total batch reconstruction loss: 0.062464937567710876
Training batch 13 / 32
Total batch reconstruction loss: 0.0614306777715683
Training batch 14 / 32
Total batch reconstruction loss: 0.059299372136592865
Training batch 15 / 32
Total batch reconstruction loss: 0.0642118826508522
Training batch 16 / 32
Total batch reconstruction loss: 0.05852191895246506
Training batch 17 / 32
Total batch reconstruction loss: 0.057680465281009674
Training batch 18 / 32
Total batch reconstruction loss: 0.05787859857082367
Training batch 19 / 32
Total batch reconstruction loss: 0.05781087651848793
Training batch 20 / 32
Total batch reconstruction loss: 0.057896535843610764
Training batch 21 / 32
Total batch reconstruction loss: 0.05951331928372383
Training batch 22 / 32
Total batch reconstruction loss: 0.05875593051314354
Training batch 23 / 32
Total batch reconstruction loss: 0.05951136350631714
Training batch 24 / 32
Total batch reconstruction loss: 0.06437525153160095
Training batch 25 / 32
Total batch reconstruction loss: 0.05861957371234894
Training batch 26 / 32
Total batch reconstruction loss: 0.05863969400525093
Training batch 27 / 32
Total batch reconstruction loss: 0.06606005132198334
Training batch 28 / 32
Total batch reconstruction loss: 0.05883871391415596
Training batch 29 / 32
Total batch reconstruction loss: 0.06339411437511444
Training batch 30 / 32
Total batch reconstruction loss: 0.05889951065182686
Training batch 31 / 32
Total batch reconstruction loss: 0.060914576053619385
Training batch 32 / 32
Total batch reconstruction loss: 0.0466790534555912
Epoch [259/500], Train Loss: 0.0579, Validation Loss: 0.0591, Generator Loss: 11.9264, Discriminator Loss: 0.3153
Training epoch 260 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.05900868400931358
Training batch 2 / 32
Total batch reconstruction loss: 0.05624573305249214
Training batch 3 / 32
Total batch reconstruction loss: 0.059092871844768524
Training batch 4 / 32
Total batch reconstruction loss: 0.05783424526453018
Training batch 5 / 32
Total batch reconstruction loss: 0.061242081224918365
Training batch 6 / 32
Total batch reconstruction loss: 0.05526914820075035
Training batch 7 / 32
Total batch reconstruction loss: 0.05759575963020325
Training batch 8 / 32
Total batch reconstruction loss: 0.05793919414281845
Training batch 9 / 32
Total batch reconstruction loss: 0.0652136504650116
Training batch 10 / 32
Total batch reconstruction loss: 0.057452090084552765
Training batch 11 / 32
Total batch reconstruction loss: 0.06358536332845688
Training batch 12 / 32
Total batch reconstruction loss: 0.061058409512043
Training batch 13 / 32
Total batch reconstruction loss: 0.05713977664709091
Training batch 14 / 32
Total batch reconstruction loss: 0.056085966527462006
Training batch 15 / 32
Total batch reconstruction loss: 0.061070702970027924
Training batch 16 / 32
Total batch reconstruction loss: 0.0641973540186882
Training batch 17 / 32
Total batch reconstruction loss: 0.06096643954515457
Training batch 18 / 32
Total batch reconstruction loss: 0.06031222268939018
Training batch 19 / 32
Total batch reconstruction loss: 0.056514739990234375
Training batch 20 / 32
Total batch reconstruction loss: 0.060395073145627975
Training batch 21 / 32
Total batch reconstruction loss: 0.059633493423461914
Training batch 22 / 32
Total batch reconstruction loss: 0.05821407213807106
Training batch 23 / 32
Total batch reconstruction loss: 0.059056356549263
Training batch 24 / 32
Total batch reconstruction loss: 0.060427792370319366
Training batch 25 / 32
Total batch reconstruction loss: 0.060322653502225876
Training batch 26 / 32
Total batch reconstruction loss: 0.054785147309303284
Training batch 27 / 32
Total batch reconstruction loss: 0.05807255208492279
Training batch 28 / 32
Total batch reconstruction loss: 0.0636434257030487
Training batch 29 / 32
Total batch reconstruction loss: 0.060478925704956055
Training batch 30 / 32
Total batch reconstruction loss: 0.06240332871675491
Training batch 31 / 32
Total batch reconstruction loss: 0.05686439946293831
Training batch 32 / 32
Total batch reconstruction loss: 0.09680764377117157
Epoch [260/500], Train Loss: 0.0590, Validation Loss: 0.0576, Generator Loss: 12.1785, Discriminator Loss: 0.3284
Training epoch 261 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.06017589569091797
Training batch 2 / 32
Total batch reconstruction loss: 0.056995294988155365
Training batch 3 / 32
Total batch reconstruction loss: 0.06154736876487732
Training batch 4 / 32
Total batch reconstruction loss: 0.057051241397857666
Training batch 5 / 32
Total batch reconstruction loss: 0.06424207240343094
Training batch 6 / 32
Total batch reconstruction loss: 0.05826748535037041
Training batch 7 / 32
Total batch reconstruction loss: 0.05338793247938156
Training batch 8 / 32
Total batch reconstruction loss: 0.06038329005241394
Training batch 9 / 32
Total batch reconstruction loss: 0.05814461410045624
Training batch 10 / 32
Total batch reconstruction loss: 0.06459693610668182
Training batch 11 / 32
Total batch reconstruction loss: 0.059783756732940674
Training batch 12 / 32
Total batch reconstruction loss: 0.0675429254770279
Training batch 13 / 32
Total batch reconstruction loss: 0.06345824152231216
Training batch 14 / 32
Total batch reconstruction loss: 0.06377524882555008
Training batch 15 / 32
Total batch reconstruction loss: 0.060118600726127625
Training batch 16 / 32
Total batch reconstruction loss: 0.05401374772191048
Training batch 17 / 32
Total batch reconstruction loss: 0.06004390865564346
Training batch 18 / 32
Total batch reconstruction loss: 0.06063840538263321
Training batch 19 / 32
Total batch reconstruction loss: 0.06145233288407326
Training batch 20 / 32
Total batch reconstruction loss: 0.06316941976547241
Training batch 21 / 32
Total batch reconstruction loss: 0.05915137752890587
Training batch 22 / 32
Total batch reconstruction loss: 0.062070898711681366
Training batch 23 / 32
Total batch reconstruction loss: 0.06019990146160126
Training batch 24 / 32
Total batch reconstruction loss: 0.058028556406497955
Training batch 25 / 32
Total batch reconstruction loss: 0.05910744518041611
Training batch 26 / 32
Total batch reconstruction loss: 0.0560806579887867
Training batch 27 / 32
Total batch reconstruction loss: 0.05688432604074478
Training batch 28 / 32
Total batch reconstruction loss: 0.06473718583583832
Training batch 29 / 32
Total batch reconstruction loss: 0.05874675512313843
Training batch 30 / 32
Total batch reconstruction loss: 0.05717659741640091
Training batch 31 / 32
Total batch reconstruction loss: 0.06328987330198288
Training batch 32 / 32
Total batch reconstruction loss: 0.06213497370481491
Epoch [261/500], Train Loss: 0.0584, Validation Loss: 0.0585, Generator Loss: 12.1131, Discriminator Loss: 0.3099
Training epoch 262 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.06102960929274559
Training batch 2 / 32
Total batch reconstruction loss: 0.058197297155857086
Training batch 3 / 32
Total batch reconstruction loss: 0.057669974863529205
Training batch 4 / 32
Total batch reconstruction loss: 0.06110319867730141
Training batch 5 / 32
Total batch reconstruction loss: 0.05719064176082611
Training batch 6 / 32
Total batch reconstruction loss: 0.06472249329090118
Training batch 7 / 32
Total batch reconstruction loss: 0.05818040668964386
Training batch 8 / 32
Total batch reconstruction loss: 0.05607171356678009
Training batch 9 / 32
Total batch reconstruction loss: 0.06215040385723114
Training batch 10 / 32
Total batch reconstruction loss: 0.05995824187994003
Training batch 11 / 32
Total batch reconstruction loss: 0.06041169539093971
Training batch 12 / 32
Total batch reconstruction loss: 0.0653495043516159
Training batch 13 / 32
Total batch reconstruction loss: 0.057833172380924225
Training batch 14 / 32
Total batch reconstruction loss: 0.059993065893650055
Training batch 15 / 32
Total batch reconstruction loss: 0.05643518269062042
Training batch 16 / 32
Total batch reconstruction loss: 0.059634190052747726
Training batch 17 / 32
Total batch reconstruction loss: 0.05744759738445282
Training batch 18 / 32
Total batch reconstruction loss: 0.05960065871477127
Training batch 19 / 32
Total batch reconstruction loss: 0.06045451760292053
Training batch 20 / 32
Total batch reconstruction loss: 0.05893278867006302
Training batch 21 / 32
Total batch reconstruction loss: 0.06382192671298981
Training batch 22 / 32
Total batch reconstruction loss: 0.06348656117916107
Training batch 23 / 32
Total batch reconstruction loss: 0.05918611213564873
Training batch 24 / 32
Total batch reconstruction loss: 0.0592745803296566
Training batch 25 / 32
Total batch reconstruction loss: 0.05459369719028473
Training batch 26 / 32
Total batch reconstruction loss: 0.05943745747208595
Training batch 27 / 32
Total batch reconstruction loss: 0.06145094335079193
Training batch 28 / 32
Total batch reconstruction loss: 0.058309368789196014
Training batch 29 / 32
Total batch reconstruction loss: 0.05953143164515495
Training batch 30 / 32
Total batch reconstruction loss: 0.05887435004115105
Training batch 31 / 32
Total batch reconstruction loss: 0.059528134763240814
Training batch 32 / 32
Total batch reconstruction loss: 0.061194077134132385
Epoch [262/500], Train Loss: 0.0578, Validation Loss: 0.0587, Generator Loss: 12.0225, Discriminator Loss: 0.3051
Training epoch 263 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.05784345418214798
Training batch 2 / 32
Total batch reconstruction loss: 0.061927616596221924
Training batch 3 / 32
Total batch reconstruction loss: 0.0629856288433075
Training batch 4 / 32
Total batch reconstruction loss: 0.059267424046993256
Training batch 5 / 32
Total batch reconstruction loss: 0.05979396030306816
Training batch 6 / 32
Total batch reconstruction loss: 0.06458435952663422
Training batch 7 / 32
Total batch reconstruction loss: 0.06711234152317047
Training batch 8 / 32
Total batch reconstruction loss: 0.05802853778004646
Training batch 9 / 32
Total batch reconstruction loss: 0.05493747815489769
Training batch 10 / 32
Total batch reconstruction loss: 0.06105349212884903
Training batch 11 / 32
Total batch reconstruction loss: 0.061855919659137726
Training batch 12 / 32
Total batch reconstruction loss: 0.05809340626001358
Training batch 13 / 32
Total batch reconstruction loss: 0.05985531210899353
Training batch 14 / 32
Total batch reconstruction loss: 0.057572029531002045
Training batch 15 / 32
Total batch reconstruction loss: 0.06051100045442581
Training batch 16 / 32
Total batch reconstruction loss: 0.061046428978443146
Training batch 17 / 32
Total batch reconstruction loss: 0.06124303117394447
Training batch 18 / 32
Total batch reconstruction loss: 0.057278022170066833
Training batch 19 / 32
Total batch reconstruction loss: 0.06025012582540512
Training batch 20 / 32
Total batch reconstruction loss: 0.05812691152095795
Training batch 21 / 32
Total batch reconstruction loss: 0.05881236121058464
Training batch 22 / 32
Total batch reconstruction loss: 0.062106773257255554
Training batch 23 / 32
Total batch reconstruction loss: 0.06114756688475609
Training batch 24 / 32
Total batch reconstruction loss: 0.05675366893410683
Training batch 25 / 32
Total batch reconstruction loss: 0.05601438134908676
Training batch 26 / 32
Total batch reconstruction loss: 0.057431790977716446
Training batch 27 / 32
Total batch reconstruction loss: 0.05478684604167938
Training batch 28 / 32
Total batch reconstruction loss: 0.05777295306324959
Training batch 29 / 32
Total batch reconstruction loss: 0.06195668876171112
Training batch 30 / 32
Total batch reconstruction loss: 0.05872490257024765
Training batch 31 / 32
Total batch reconstruction loss: 0.060251254588365555
Training batch 32 / 32
Total batch reconstruction loss: 0.05642799660563469
Epoch [263/500], Train Loss: 0.0579, Validation Loss: 0.0582, Generator Loss: 11.9739, Discriminator Loss: 0.3167
Training epoch 264 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.05957842618227005
Training batch 2 / 32
Total batch reconstruction loss: 0.06036609038710594
Training batch 3 / 32
Total batch reconstruction loss: 0.0553382933139801
Training batch 4 / 32
Total batch reconstruction loss: 0.05466723069548607
Training batch 5 / 32
Total batch reconstruction loss: 0.055714454501867294
Training batch 6 / 32
Total batch reconstruction loss: 0.068292036652565
Training batch 7 / 32
Total batch reconstruction loss: 0.06263960897922516
Training batch 8 / 32
Total batch reconstruction loss: 0.05776739865541458
Training batch 9 / 32
Total batch reconstruction loss: 0.059656888246536255
Training batch 10 / 32
Total batch reconstruction loss: 0.062097154557704926
Training batch 11 / 32
Total batch reconstruction loss: 0.05838622897863388
Training batch 12 / 32
Total batch reconstruction loss: 0.06198788434267044
Training batch 13 / 32
Total batch reconstruction loss: 0.0596051961183548
Training batch 14 / 32
Total batch reconstruction loss: 0.06078852340579033
Training batch 15 / 32
Total batch reconstruction loss: 0.0628034844994545
Training batch 16 / 32
Total batch reconstruction loss: 0.055111974477767944
Training batch 17 / 32
Total batch reconstruction loss: 0.06098334863781929
Training batch 18 / 32
Total batch reconstruction loss: 0.06389547884464264
Training batch 19 / 32
Total batch reconstruction loss: 0.060367170721292496
Training batch 20 / 32
Total batch reconstruction loss: 0.057647839188575745
Training batch 21 / 32
Total batch reconstruction loss: 0.059505246579647064
Training batch 22 / 32
Total batch reconstruction loss: 0.059054531157016754
Training batch 23 / 32
Total batch reconstruction loss: 0.0636986792087555
Training batch 24 / 32
Total batch reconstruction loss: 0.05795881897211075
Training batch 25 / 32
Total batch reconstruction loss: 0.06435728818178177
Training batch 26 / 32
Total batch reconstruction loss: 0.057645659893751144
Training batch 27 / 32
Total batch reconstruction loss: 0.0607539527118206
Training batch 28 / 32
Total batch reconstruction loss: 0.058220162987709045
Training batch 29 / 32
Total batch reconstruction loss: 0.05688704550266266
Training batch 30 / 32
Total batch reconstruction loss: 0.05706661567091942
Training batch 31 / 32
Total batch reconstruction loss: 0.06639726459980011
Training batch 32 / 32
Total batch reconstruction loss: 0.05680183321237564
Epoch [264/500], Train Loss: 0.0587, Validation Loss: 0.0573, Generator Loss: 12.0331, Discriminator Loss: 0.3396
Training epoch 265 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.060372404754161835
Training batch 2 / 32
Total batch reconstruction loss: 0.06260323524475098
Training batch 3 / 32
Total batch reconstruction loss: 0.060961268842220306
Training batch 4 / 32
Total batch reconstruction loss: 0.059551287442445755
Training batch 5 / 32
Total batch reconstruction loss: 0.059889838099479675
Training batch 6 / 32
Total batch reconstruction loss: 0.05946663022041321
Training batch 7 / 32
Total batch reconstruction loss: 0.05740249156951904
Training batch 8 / 32
Total batch reconstruction loss: 0.05573159456253052
Training batch 9 / 32
Total batch reconstruction loss: 0.059167519211769104
Training batch 10 / 32
Total batch reconstruction loss: 0.05987625941634178
Training batch 11 / 32
Total batch reconstruction loss: 0.05975706875324249
Training batch 12 / 32
Total batch reconstruction loss: 0.05514076352119446
Training batch 13 / 32
Total batch reconstruction loss: 0.06342753767967224
Training batch 14 / 32
Total batch reconstruction loss: 0.05585265904664993
Training batch 15 / 32
Total batch reconstruction loss: 0.06611403822898865
Training batch 16 / 32
Total batch reconstruction loss: 0.059425804764032364
Training batch 17 / 32
Total batch reconstruction loss: 0.060713887214660645
Training batch 18 / 32
Total batch reconstruction loss: 0.060655876994132996
Training batch 19 / 32
Total batch reconstruction loss: 0.05698470026254654
Training batch 20 / 32
Total batch reconstruction loss: 0.06571999192237854
Training batch 21 / 32
Total batch reconstruction loss: 0.05804520100355148
Training batch 22 / 32
Total batch reconstruction loss: 0.057979341596364975
Training batch 23 / 32
Total batch reconstruction loss: 0.056159231811761856
Training batch 24 / 32
Total batch reconstruction loss: 0.056247346103191376
Training batch 25 / 32
Total batch reconstruction loss: 0.055473487824201584
Training batch 26 / 32
Total batch reconstruction loss: 0.06641104817390442
Training batch 27 / 32
Total batch reconstruction loss: 0.059321045875549316
Training batch 28 / 32
Total batch reconstruction loss: 0.05978164076805115
Training batch 29 / 32
Total batch reconstruction loss: 0.061719201505184174
Training batch 30 / 32
Total batch reconstruction loss: 0.05864865332841873
Training batch 31 / 32
Total batch reconstruction loss: 0.06062065809965134
Training batch 32 / 32
Total batch reconstruction loss: 0.05358336120843887
Epoch [265/500], Train Loss: 0.0580, Validation Loss: 0.0577, Generator Loss: 11.9618, Discriminator Loss: 0.3286
Training epoch 266 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.06053415313363075
Training batch 2 / 32
Total batch reconstruction loss: 0.05771920084953308
Training batch 3 / 32
Total batch reconstruction loss: 0.06611688435077667
Training batch 4 / 32
Total batch reconstruction loss: 0.060008056461811066
Training batch 5 / 32
Total batch reconstruction loss: 0.05886578559875488
Training batch 6 / 32
Total batch reconstruction loss: 0.05483335256576538
Training batch 7 / 32
Total batch reconstruction loss: 0.060592055320739746
Training batch 8 / 32
Total batch reconstruction loss: 0.05847066268324852
Training batch 9 / 32
Total batch reconstruction loss: 0.058518219739198685
Training batch 10 / 32
Total batch reconstruction loss: 0.06285892426967621
Training batch 11 / 32
Total batch reconstruction loss: 0.05658923462033272
Training batch 12 / 32
Total batch reconstruction loss: 0.0601801872253418
Training batch 13 / 32
Total batch reconstruction loss: 0.06109544634819031
Training batch 14 / 32
Total batch reconstruction loss: 0.0574435219168663
Training batch 15 / 32
Total batch reconstruction loss: 0.058711133897304535
Training batch 16 / 32
Total batch reconstruction loss: 0.05707913264632225
Training batch 17 / 32
Total batch reconstruction loss: 0.06503032147884369
Training batch 18 / 32
Total batch reconstruction loss: 0.0650734007358551
Training batch 19 / 32
Total batch reconstruction loss: 0.05820561945438385
Training batch 20 / 32
Total batch reconstruction loss: 0.06163802742958069
Training batch 21 / 32
Total batch reconstruction loss: 0.05905758589506149
Training batch 22 / 32
Total batch reconstruction loss: 0.06009538471698761
Training batch 23 / 32
Total batch reconstruction loss: 0.06068888306617737
Training batch 24 / 32
Total batch reconstruction loss: 0.057721737772226334
Training batch 25 / 32
Total batch reconstruction loss: 0.05467762425541878
Training batch 26 / 32
Total batch reconstruction loss: 0.06625252962112427
Training batch 27 / 32
Total batch reconstruction loss: 0.06239704415202141
Training batch 28 / 32
Total batch reconstruction loss: 0.057791486382484436
Training batch 29 / 32
Total batch reconstruction loss: 0.06280968338251114
Training batch 30 / 32
Total batch reconstruction loss: 0.05696158856153488
Training batch 31 / 32
Total batch reconstruction loss: 0.05685381963849068
Training batch 32 / 32
Total batch reconstruction loss: 0.05247262120246887
Epoch [266/500], Train Loss: 0.0581, Validation Loss: 0.0575, Generator Loss: 11.9973, Discriminator Loss: 0.3081
Training epoch 267 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.05642452463507652
Training batch 2 / 32
Total batch reconstruction loss: 0.06416971981525421
Training batch 3 / 32
Total batch reconstruction loss: 0.05829475075006485
Training batch 4 / 32
Total batch reconstruction loss: 0.06505396962165833
Training batch 5 / 32
Total batch reconstruction loss: 0.06598740816116333
Training batch 6 / 32
Total batch reconstruction loss: 0.06115765869617462
Training batch 7 / 32
Total batch reconstruction loss: 0.05944756418466568
Training batch 8 / 32
Total batch reconstruction loss: 0.06009107828140259
Training batch 9 / 32
Total batch reconstruction loss: 0.05745406821370125
Training batch 10 / 32
Total batch reconstruction loss: 0.06048469617962837
Training batch 11 / 32
Total batch reconstruction loss: 0.05544821545481682
Training batch 12 / 32
Total batch reconstruction loss: 0.06297136843204498
Training batch 13 / 32
Total batch reconstruction loss: 0.05672454833984375
Training batch 14 / 32
Total batch reconstruction loss: 0.059121694415807724
Training batch 15 / 32
Total batch reconstruction loss: 0.06431031972169876
Training batch 16 / 32
Total batch reconstruction loss: 0.062312621623277664
Training batch 17 / 32
Total batch reconstruction loss: 0.05999719351530075
Training batch 18 / 32
Total batch reconstruction loss: 0.057768311351537704
Training batch 19 / 32
Total batch reconstruction loss: 0.0565413236618042
Training batch 20 / 32
Total batch reconstruction loss: 0.0632714182138443
Training batch 21 / 32
Total batch reconstruction loss: 0.05962951481342316
Training batch 22 / 32
Total batch reconstruction loss: 0.05746325105428696
Training batch 23 / 32
Total batch reconstruction loss: 0.06149885430932045
Training batch 24 / 32
Total batch reconstruction loss: 0.06149798631668091
Training batch 25 / 32
Total batch reconstruction loss: 0.058872222900390625
Training batch 26 / 32
Total batch reconstruction loss: 0.05584215372800827
Training batch 27 / 32
Total batch reconstruction loss: 0.05985783040523529
Training batch 28 / 32
Total batch reconstruction loss: 0.05979527533054352
Training batch 29 / 32
Total batch reconstruction loss: 0.05781197547912598
Training batch 30 / 32
Total batch reconstruction loss: 0.05867009609937668
Training batch 31 / 32
Total batch reconstruction loss: 0.06068382412195206
Training batch 32 / 32
Total batch reconstruction loss: 0.05479602888226509
Epoch [267/500], Train Loss: 0.0583, Validation Loss: 0.0578, Generator Loss: 12.0209, Discriminator Loss: 0.3245
Training epoch 268 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.05821332335472107
Training batch 2 / 32
Total batch reconstruction loss: 0.05915253609418869
Training batch 3 / 32
Total batch reconstruction loss: 0.05924452096223831
Training batch 4 / 32
Total batch reconstruction loss: 0.059260908514261246
Training batch 5 / 32
Total batch reconstruction loss: 0.05696582794189453
Training batch 6 / 32
Total batch reconstruction loss: 0.06368240714073181
Training batch 7 / 32
Total batch reconstruction loss: 0.06014808267354965
Training batch 8 / 32
Total batch reconstruction loss: 0.059399932622909546
Training batch 9 / 32
Total batch reconstruction loss: 0.05891065299510956
Training batch 10 / 32
Total batch reconstruction loss: 0.06485828757286072
Training batch 11 / 32
Total batch reconstruction loss: 0.0594874769449234
Training batch 12 / 32
Total batch reconstruction loss: 0.058432385325431824
Training batch 13 / 32
Total batch reconstruction loss: 0.06286744028329849
Training batch 14 / 32
Total batch reconstruction loss: 0.058243557810783386
Training batch 15 / 32
Total batch reconstruction loss: 0.05856452137231827
Training batch 16 / 32
Total batch reconstruction loss: 0.060523275285959244
Training batch 17 / 32
Total batch reconstruction loss: 0.054699257016181946
Training batch 18 / 32
Total batch reconstruction loss: 0.06648917496204376
Training batch 19 / 32
Total batch reconstruction loss: 0.05683305859565735
Training batch 20 / 32
Total batch reconstruction loss: 0.059605248272418976
Training batch 21 / 32
Total batch reconstruction loss: 0.05813651159405708
Training batch 22 / 32
Total batch reconstruction loss: 0.06146106868982315
Training batch 23 / 32
Total batch reconstruction loss: 0.05574214830994606
Training batch 24 / 32
Total batch reconstruction loss: 0.06196719408035278
Training batch 25 / 32
Total batch reconstruction loss: 0.060603510588407516
Training batch 26 / 32
Total batch reconstruction loss: 0.05658339709043503
Training batch 27 / 32
Total batch reconstruction loss: 0.05941401422023773
Training batch 28 / 32
Total batch reconstruction loss: 0.058717548847198486
Training batch 29 / 32
Total batch reconstruction loss: 0.056792110204696655
Training batch 30 / 32
Total batch reconstruction loss: 0.06034255772829056
Training batch 31 / 32
Total batch reconstruction loss: 0.06021181866526604
Training batch 32 / 32
Total batch reconstruction loss: 0.05032455176115036
Epoch [268/500], Train Loss: 0.0578, Validation Loss: 0.0577, Generator Loss: 11.9053, Discriminator Loss: 0.3375
Training epoch 269 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.05734909325838089
Training batch 2 / 32
Total batch reconstruction loss: 0.06136859208345413
Training batch 3 / 32
Total batch reconstruction loss: 0.07098688930273056
Training batch 4 / 32
Total batch reconstruction loss: 0.06007570028305054
Training batch 5 / 32
Total batch reconstruction loss: 0.061956167221069336
Training batch 6 / 32
Total batch reconstruction loss: 0.05889216810464859
Training batch 7 / 32
Total batch reconstruction loss: 0.06325813382863998
Training batch 8 / 32
Total batch reconstruction loss: 0.05881121754646301
Training batch 9 / 32
Total batch reconstruction loss: 0.05866215378046036
Training batch 10 / 32
Total batch reconstruction loss: 0.06300297379493713
Training batch 11 / 32
Total batch reconstruction loss: 0.0618322379887104
Training batch 12 / 32
Total batch reconstruction loss: 0.06062700226902962
Training batch 13 / 32
Total batch reconstruction loss: 0.060453206300735474
Training batch 14 / 32
Total batch reconstruction loss: 0.06147333234548569
Training batch 15 / 32
Total batch reconstruction loss: 0.057852938771247864
Training batch 16 / 32
Total batch reconstruction loss: 0.0640856921672821
Training batch 17 / 32
Total batch reconstruction loss: 0.057424820959568024
Training batch 18 / 32
Total batch reconstruction loss: 0.06047895550727844
Training batch 19 / 32
Total batch reconstruction loss: 0.05400741100311279
Training batch 20 / 32
Total batch reconstruction loss: 0.05818630009889603
Training batch 21 / 32
Total batch reconstruction loss: 0.06217970699071884
Training batch 22 / 32
Total batch reconstruction loss: 0.056607600301504135
Training batch 23 / 32
Total batch reconstruction loss: 0.05845331400632858
Training batch 24 / 32
Total batch reconstruction loss: 0.060874760150909424
Training batch 25 / 32
Total batch reconstruction loss: 0.06121384724974632
Training batch 26 / 32
Total batch reconstruction loss: 0.06010180711746216
Training batch 27 / 32
Total batch reconstruction loss: 0.060189470648765564
Training batch 28 / 32
Total batch reconstruction loss: 0.05613647401332855
Training batch 29 / 32
Total batch reconstruction loss: 0.05551552027463913
Training batch 30 / 32
Total batch reconstruction loss: 0.05659974738955498
Training batch 31 / 32
Total batch reconstruction loss: 0.06064264848828316
Training batch 32 / 32
Total batch reconstruction loss: 0.05574529245495796
Epoch [269/500], Train Loss: 0.0584, Validation Loss: 0.0595, Generator Loss: 12.0446, Discriminator Loss: 0.3071
Training epoch 270 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.05774732679128647
Training batch 2 / 32
Total batch reconstruction loss: 0.058148663491010666
Training batch 3 / 32
Total batch reconstruction loss: 0.06364823877811432
Training batch 4 / 32
Total batch reconstruction loss: 0.05843783915042877
Training batch 5 / 32
Total batch reconstruction loss: 0.05648220330476761
Training batch 6 / 32
Total batch reconstruction loss: 0.06139621511101723
Training batch 7 / 32
Total batch reconstruction loss: 0.057032689452171326
Training batch 8 / 32
Total batch reconstruction loss: 0.057456888258457184
Training batch 9 / 32
Total batch reconstruction loss: 0.0602414608001709
Training batch 10 / 32
Total batch reconstruction loss: 0.058855775743722916
Training batch 11 / 32
Total batch reconstruction loss: 0.06245175749063492
Training batch 12 / 32
Total batch reconstruction loss: 0.05976590886712074
Training batch 13 / 32
Total batch reconstruction loss: 0.06088479608297348
Training batch 14 / 32
Total batch reconstruction loss: 0.057761430740356445
Training batch 15 / 32
Total batch reconstruction loss: 0.06096303462982178
Training batch 16 / 32
Total batch reconstruction loss: 0.059899523854255676
Training batch 17 / 32
Total batch reconstruction loss: 0.05899258702993393
Training batch 18 / 32
Total batch reconstruction loss: 0.06260807812213898
Training batch 19 / 32
Total batch reconstruction loss: 0.058056339621543884
Training batch 20 / 32
Total batch reconstruction loss: 0.06250198185443878
Training batch 21 / 32
Total batch reconstruction loss: 0.06076113134622574
Training batch 22 / 32
Total batch reconstruction loss: 0.06508080661296844
Training batch 23 / 32
Total batch reconstruction loss: 0.061483729630708694
Training batch 24 / 32
Total batch reconstruction loss: 0.05741852521896362
Training batch 25 / 32
Total batch reconstruction loss: 0.0583508238196373
Training batch 26 / 32
Total batch reconstruction loss: 0.05504244565963745
Training batch 27 / 32
Total batch reconstruction loss: 0.06082449108362198
Training batch 28 / 32
Total batch reconstruction loss: 0.0584314726293087
Training batch 29 / 32
Total batch reconstruction loss: 0.06040532886981964
Training batch 30 / 32
Total batch reconstruction loss: 0.05664112791419029
Training batch 31 / 32
Total batch reconstruction loss: 0.06124544516205788
Training batch 32 / 32
Total batch reconstruction loss: 0.0655432790517807
Epoch [270/500], Train Loss: 0.0580, Validation Loss: 0.0587, Generator Loss: 12.0460, Discriminator Loss: 0.3009
Training epoch 271 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.05690644308924675
Training batch 2 / 32
Total batch reconstruction loss: 0.060947999358177185
Training batch 3 / 32
Total batch reconstruction loss: 0.06447528302669525
Training batch 4 / 32
Total batch reconstruction loss: 0.05859196558594704
Training batch 5 / 32
Total batch reconstruction loss: 0.058108218014240265
Training batch 6 / 32
Total batch reconstruction loss: 0.05793653801083565
Training batch 7 / 32
Total batch reconstruction loss: 0.06100180745124817
Training batch 8 / 32
Total batch reconstruction loss: 0.05938556417822838
Training batch 9 / 32
Total batch reconstruction loss: 0.0574520044028759
Training batch 10 / 32
Total batch reconstruction loss: 0.061280786991119385
Training batch 11 / 32
Total batch reconstruction loss: 0.05888998880982399
Training batch 12 / 32
Total batch reconstruction loss: 0.0595579668879509
Training batch 13 / 32
Total batch reconstruction loss: 0.06072130799293518
Training batch 14 / 32
Total batch reconstruction loss: 0.05776061862707138
Training batch 15 / 32
Total batch reconstruction loss: 0.06047722324728966
Training batch 16 / 32
Total batch reconstruction loss: 0.06224396824836731
Training batch 17 / 32
Total batch reconstruction loss: 0.06567040085792542
Training batch 18 / 32
Total batch reconstruction loss: 0.06024814769625664
Training batch 19 / 32
Total batch reconstruction loss: 0.05786672234535217
Training batch 20 / 32
Total batch reconstruction loss: 0.05853589251637459
Training batch 21 / 32
Total batch reconstruction loss: 0.06359170377254486
Training batch 22 / 32
Total batch reconstruction loss: 0.05620862543582916
Training batch 23 / 32
Total batch reconstruction loss: 0.057686805725097656
Training batch 24 / 32
Total batch reconstruction loss: 0.06144995242357254
Training batch 25 / 32
Total batch reconstruction loss: 0.05999098718166351
Training batch 26 / 32
Total batch reconstruction loss: 0.05615060031414032
Training batch 27 / 32
Total batch reconstruction loss: 0.06071484461426735
Training batch 28 / 32
Total batch reconstruction loss: 0.06046350672841072
Training batch 29 / 32
Total batch reconstruction loss: 0.05492577701807022
Training batch 30 / 32
Total batch reconstruction loss: 0.06033184379339218
Training batch 31 / 32
Total batch reconstruction loss: 0.058666907250881195
Training batch 32 / 32
Total batch reconstruction loss: 0.057203374803066254
Epoch [271/500], Train Loss: 0.0578, Validation Loss: 0.0616, Generator Loss: 11.9873, Discriminator Loss: 0.3042
Training epoch 272 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.05802135914564133
Training batch 2 / 32
Total batch reconstruction loss: 0.0609879195690155
Training batch 3 / 32
Total batch reconstruction loss: 0.06035810708999634
Training batch 4 / 32
Total batch reconstruction loss: 0.0599246621131897
Training batch 5 / 32
Total batch reconstruction loss: 0.0591123141348362
Training batch 6 / 32
Total batch reconstruction loss: 0.058281559497117996
Training batch 7 / 32
Total batch reconstruction loss: 0.05755697935819626
Training batch 8 / 32
Total batch reconstruction loss: 0.06175912916660309
Training batch 9 / 32
Total batch reconstruction loss: 0.06031293049454689
Training batch 10 / 32
Total batch reconstruction loss: 0.062041349709033966
Training batch 11 / 32
Total batch reconstruction loss: 0.06083207204937935
Training batch 12 / 32
Total batch reconstruction loss: 0.05838511884212494
Training batch 13 / 32
Total batch reconstruction loss: 0.06295270472764969
Training batch 14 / 32
Total batch reconstruction loss: 0.06092771142721176
Training batch 15 / 32
Total batch reconstruction loss: 0.05524677410721779
Training batch 16 / 32
Total batch reconstruction loss: 0.056549765169620514
Training batch 17 / 32
Total batch reconstruction loss: 0.060023073107004166
Training batch 18 / 32
Total batch reconstruction loss: 0.05332602560520172
Training batch 19 / 32
Total batch reconstruction loss: 0.05428333207964897
Training batch 20 / 32
Total batch reconstruction loss: 0.06127716973423958
Training batch 21 / 32
Total batch reconstruction loss: 0.05904681980609894
Training batch 22 / 32
Total batch reconstruction loss: 0.06342759728431702
Training batch 23 / 32
Total batch reconstruction loss: 0.061529941856861115
Training batch 24 / 32
Total batch reconstruction loss: 0.06150434911251068
Training batch 25 / 32
Total batch reconstruction loss: 0.05563928931951523
Training batch 26 / 32
Total batch reconstruction loss: 0.05832698196172714
Training batch 27 / 32
Total batch reconstruction loss: 0.05868441238999367
Training batch 28 / 32
Total batch reconstruction loss: 0.058429621160030365
Training batch 29 / 32
Total batch reconstruction loss: 0.06208505854010582
Training batch 30 / 32
Total batch reconstruction loss: 0.06302084773778915
Training batch 31 / 32
Total batch reconstruction loss: 0.06348419189453125
Training batch 32 / 32
Total batch reconstruction loss: 0.050781577825546265
Epoch [272/500], Train Loss: 0.0577, Validation Loss: 0.0580, Generator Loss: 11.9446, Discriminator Loss: 0.3052
Training epoch 273 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.05598975718021393
Training batch 2 / 32
Total batch reconstruction loss: 0.061402253806591034
Training batch 3 / 32
Total batch reconstruction loss: 0.06002029776573181
Training batch 4 / 32
Total batch reconstruction loss: 0.06367076933383942
Training batch 5 / 32
Total batch reconstruction loss: 0.060149893164634705
Training batch 6 / 32
Total batch reconstruction loss: 0.05954447388648987
Training batch 7 / 32
Total batch reconstruction loss: 0.06005118042230606
Training batch 8 / 32
Total batch reconstruction loss: 0.05813320726156235
Training batch 9 / 32
Total batch reconstruction loss: 0.06154143810272217
Training batch 10 / 32
Total batch reconstruction loss: 0.05965029075741768
Training batch 11 / 32
Total batch reconstruction loss: 0.05931065231561661
Training batch 12 / 32
Total batch reconstruction loss: 0.06271165609359741
Training batch 13 / 32
Total batch reconstruction loss: 0.06623715162277222
Training batch 14 / 32
Total batch reconstruction loss: 0.0567118376493454
Training batch 15 / 32
Total batch reconstruction loss: 0.06109538674354553
Training batch 16 / 32
Total batch reconstruction loss: 0.057267606258392334
Training batch 17 / 32
Total batch reconstruction loss: 0.060265183448791504
Training batch 18 / 32
Total batch reconstruction loss: 0.057661786675453186
Training batch 19 / 32
Total batch reconstruction loss: 0.055556900799274445
Training batch 20 / 32
Total batch reconstruction loss: 0.060299597680568695
Training batch 21 / 32
Total batch reconstruction loss: 0.06131596118211746
Training batch 22 / 32
Total batch reconstruction loss: 0.05800812318921089
Training batch 23 / 32
Total batch reconstruction loss: 0.06280335038900375
Training batch 24 / 32
Total batch reconstruction loss: 0.05676266551017761
Training batch 25 / 32
Total batch reconstruction loss: 0.06032944470643997
Training batch 26 / 32
Total batch reconstruction loss: 0.05616528168320656
Training batch 27 / 32
Total batch reconstruction loss: 0.06366651505231857
Training batch 28 / 32
Total batch reconstruction loss: 0.057434357702732086
Training batch 29 / 32
Total batch reconstruction loss: 0.06363683193922043
Training batch 30 / 32
Total batch reconstruction loss: 0.055028654634952545
Training batch 31 / 32
Total batch reconstruction loss: 0.059223856776952744
Training batch 32 / 32
Total batch reconstruction loss: 0.08869285881519318
Epoch [273/500], Train Loss: 0.0591, Validation Loss: 0.0580, Generator Loss: 12.1964, Discriminator Loss: 0.3134
Training epoch 274 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.057131435722112656
Training batch 2 / 32
Total batch reconstruction loss: 0.06108170002698898
Training batch 3 / 32
Total batch reconstruction loss: 0.06047981232404709
Training batch 4 / 32
Total batch reconstruction loss: 0.06271948665380478
Training batch 5 / 32
Total batch reconstruction loss: 0.0627414882183075
Training batch 6 / 32
Total batch reconstruction loss: 0.05793388932943344
Training batch 7 / 32
Total batch reconstruction loss: 0.061359889805316925
Training batch 8 / 32
Total batch reconstruction loss: 0.05954165756702423
Training batch 9 / 32
Total batch reconstruction loss: 0.055706463754177094
Training batch 10 / 32
Total batch reconstruction loss: 0.06624859571456909
Training batch 11 / 32
Total batch reconstruction loss: 0.05946372449398041
Training batch 12 / 32
Total batch reconstruction loss: 0.06195850670337677
Training batch 13 / 32
Total batch reconstruction loss: 0.06077568978071213
Training batch 14 / 32
Total batch reconstruction loss: 0.05943369120359421
Training batch 15 / 32
Total batch reconstruction loss: 0.05445984750986099
Training batch 16 / 32
Total batch reconstruction loss: 0.06256082653999329
Training batch 17 / 32
Total batch reconstruction loss: 0.05651906132698059
Training batch 18 / 32
Total batch reconstruction loss: 0.06101101636886597
Training batch 19 / 32
Total batch reconstruction loss: 0.062402691692113876
Training batch 20 / 32
Total batch reconstruction loss: 0.057859789580106735
Training batch 21 / 32
Total batch reconstruction loss: 0.058279938995838165
Training batch 22 / 32
Total batch reconstruction loss: 0.0573127306997776
Training batch 23 / 32
Total batch reconstruction loss: 0.06338539719581604
Training batch 24 / 32
Total batch reconstruction loss: 0.05961034446954727
Training batch 25 / 32
Total batch reconstruction loss: 0.05858434736728668
Training batch 26 / 32
Total batch reconstruction loss: 0.06039215624332428
Training batch 27 / 32
Total batch reconstruction loss: 0.06254120171070099
Training batch 28 / 32
Total batch reconstruction loss: 0.05602102726697922
Training batch 29 / 32
Total batch reconstruction loss: 0.06168486550450325
Training batch 30 / 32
Total batch reconstruction loss: 0.05787436291575432
Training batch 31 / 32
Total batch reconstruction loss: 0.06369657814502716
Training batch 32 / 32
Total batch reconstruction loss: 0.05679108202457428
Epoch [274/500], Train Loss: 0.0584, Validation Loss: 0.0578, Generator Loss: 12.0426, Discriminator Loss: 0.3418
Training epoch 275 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.057928480207920074
Training batch 2 / 32
Total batch reconstruction loss: 0.056033626198768616
Training batch 3 / 32
Total batch reconstruction loss: 0.05939388647675514
Training batch 4 / 32
Total batch reconstruction loss: 0.05946986377239227
Training batch 5 / 32
Total batch reconstruction loss: 0.055479951202869415
Training batch 6 / 32
Total batch reconstruction loss: 0.05852309614419937
Training batch 7 / 32
Total batch reconstruction loss: 0.05543738231062889
Training batch 8 / 32
Total batch reconstruction loss: 0.06097579747438431
Training batch 9 / 32
Total batch reconstruction loss: 0.060843247920274734
Training batch 10 / 32
Total batch reconstruction loss: 0.05956117808818817
Training batch 11 / 32
Total batch reconstruction loss: 0.06060820817947388
Training batch 12 / 32
Total batch reconstruction loss: 0.06582601368427277
Training batch 13 / 32
Total batch reconstruction loss: 0.055681392550468445
Training batch 14 / 32
Total batch reconstruction loss: 0.06635012477636337
Training batch 15 / 32
Total batch reconstruction loss: 0.05922182276844978
Training batch 16 / 32
Total batch reconstruction loss: 0.06039769574999809
Training batch 17 / 32
Total batch reconstruction loss: 0.057165734469890594
Training batch 18 / 32
Total batch reconstruction loss: 0.05893031135201454
Training batch 19 / 32
Total batch reconstruction loss: 0.061373502016067505
Training batch 20 / 32
Total batch reconstruction loss: 0.06071494519710541
Training batch 21 / 32
Total batch reconstruction loss: 0.06108175963163376
Training batch 22 / 32
Total batch reconstruction loss: 0.05792263522744179
Training batch 23 / 32
Total batch reconstruction loss: 0.058715205639600754
Training batch 24 / 32
Total batch reconstruction loss: 0.056021641939878464
Training batch 25 / 32
Total batch reconstruction loss: 0.06020817160606384
Training batch 26 / 32
Total batch reconstruction loss: 0.06141389533877373
Training batch 27 / 32
Total batch reconstruction loss: 0.05814623832702637
Training batch 28 / 32
Total batch reconstruction loss: 0.06207316368818283
Training batch 29 / 32
Total batch reconstruction loss: 0.05965372920036316
Training batch 30 / 32
Total batch reconstruction loss: 0.06147575378417969
Training batch 31 / 32
Total batch reconstruction loss: 0.05430427938699722
Training batch 32 / 32
Total batch reconstruction loss: 0.06231974810361862
Epoch [275/500], Train Loss: 0.0577, Validation Loss: 0.0572, Generator Loss: 11.9700, Discriminator Loss: 0.3087
Training epoch 276 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.059753358364105225
Training batch 2 / 32
Total batch reconstruction loss: 0.06185681372880936
Training batch 3 / 32
Total batch reconstruction loss: 0.05699894577264786
Training batch 4 / 32
Total batch reconstruction loss: 0.06377243995666504
Training batch 5 / 32
Total batch reconstruction loss: 0.05701887607574463
Training batch 6 / 32
Total batch reconstruction loss: 0.056995607912540436
Training batch 7 / 32
Total batch reconstruction loss: 0.05607754737138748
Training batch 8 / 32
Total batch reconstruction loss: 0.06018313020467758
Training batch 9 / 32
Total batch reconstruction loss: 0.05717206001281738
Training batch 10 / 32
Total batch reconstruction loss: 0.061957947909832
Training batch 11 / 32
Total batch reconstruction loss: 0.05706476420164108
Training batch 12 / 32
Total batch reconstruction loss: 0.060068897902965546
Training batch 13 / 32
Total batch reconstruction loss: 0.05912153050303459
Training batch 14 / 32
Total batch reconstruction loss: 0.056222524493932724
Training batch 15 / 32
Total batch reconstruction loss: 0.05998921021819115
Training batch 16 / 32
Total batch reconstruction loss: 0.060821209102869034
Training batch 17 / 32
Total batch reconstruction loss: 0.06302797049283981
Training batch 18 / 32
Total batch reconstruction loss: 0.06095552816987038
Training batch 19 / 32
Total batch reconstruction loss: 0.05759003013372421
Training batch 20 / 32
Total batch reconstruction loss: 0.06057927757501602
Training batch 21 / 32
Total batch reconstruction loss: 0.056369319558143616
Training batch 22 / 32
Total batch reconstruction loss: 0.06110156700015068
Training batch 23 / 32
Total batch reconstruction loss: 0.05362663418054581
Training batch 24 / 32
Total batch reconstruction loss: 0.06111781299114227
Training batch 25 / 32
Total batch reconstruction loss: 0.058217279613018036
Training batch 26 / 32
Total batch reconstruction loss: 0.057534195482730865
Training batch 27 / 32
Total batch reconstruction loss: 0.05744495242834091
Training batch 28 / 32
Total batch reconstruction loss: 0.06079970672726631
Training batch 29 / 32
Total batch reconstruction loss: 0.05960608646273613
Training batch 30 / 32
Total batch reconstruction loss: 0.05786660686135292
Training batch 31 / 32
Total batch reconstruction loss: 0.06485720723867416
Training batch 32 / 32
Total batch reconstruction loss: 0.05781280994415283
Epoch [276/500], Train Loss: 0.0576, Validation Loss: 0.0581, Generator Loss: 11.8962, Discriminator Loss: 0.3264
Training epoch 277 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.05562213063240051
Training batch 2 / 32
Total batch reconstruction loss: 0.05606577917933464
Training batch 3 / 32
Total batch reconstruction loss: 0.059032753109931946
Training batch 4 / 32
Total batch reconstruction loss: 0.05727893114089966
Training batch 5 / 32
Total batch reconstruction loss: 0.057746849954128265
Training batch 6 / 32
Total batch reconstruction loss: 0.05740394443273544
Training batch 7 / 32
Total batch reconstruction loss: 0.06322644650936127
Training batch 8 / 32
Total batch reconstruction loss: 0.06498022377490997
Training batch 9 / 32
Total batch reconstruction loss: 0.06556914001703262
Training batch 10 / 32
Total batch reconstruction loss: 0.06018136441707611
Training batch 11 / 32
Total batch reconstruction loss: 0.05776679143309593
Training batch 12 / 32
Total batch reconstruction loss: 0.05844912678003311
Training batch 13 / 32
Total batch reconstruction loss: 0.05694887414574623
Training batch 14 / 32
Total batch reconstruction loss: 0.0624542273581028
Training batch 15 / 32
Total batch reconstruction loss: 0.0604114755988121
Training batch 16 / 32
Total batch reconstruction loss: 0.06606879830360413
Training batch 17 / 32
Total batch reconstruction loss: 0.06328067928552628
Training batch 18 / 32
Total batch reconstruction loss: 0.06547624617815018
Training batch 19 / 32
Total batch reconstruction loss: 0.06138337403535843
Training batch 20 / 32
Total batch reconstruction loss: 0.05942762643098831
Training batch 21 / 32
Total batch reconstruction loss: 0.05953363701701164
Training batch 22 / 32
Total batch reconstruction loss: 0.060178883373737335
Training batch 23 / 32
Total batch reconstruction loss: 0.05659167468547821
Training batch 24 / 32
Total batch reconstruction loss: 0.05739761143922806
Training batch 25 / 32
Total batch reconstruction loss: 0.05597129836678505
Training batch 26 / 32
Total batch reconstruction loss: 0.06344836950302124
Training batch 27 / 32
Total batch reconstruction loss: 0.060799747705459595
Training batch 28 / 32
Total batch reconstruction loss: 0.06049918383359909
Training batch 29 / 32
Total batch reconstruction loss: 0.06186762452125549
Training batch 30 / 32
Total batch reconstruction loss: 0.05822048336267471
Training batch 31 / 32
Total batch reconstruction loss: 0.06317070126533508
Training batch 32 / 32
Total batch reconstruction loss: 0.055493928492069244
Epoch [277/500], Train Loss: 0.0586, Validation Loss: 0.0578, Generator Loss: 12.0843, Discriminator Loss: 0.3210
Training epoch 278 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.06087271869182587
Training batch 2 / 32
Total batch reconstruction loss: 0.06127607077360153
Training batch 3 / 32
Total batch reconstruction loss: 0.06036103889346123
Training batch 4 / 32
Total batch reconstruction loss: 0.06137487664818764
Training batch 5 / 32
Total batch reconstruction loss: 0.0658019483089447
Training batch 6 / 32
Total batch reconstruction loss: 0.060444071888923645
Training batch 7 / 32
Total batch reconstruction loss: 0.06047163903713226
Training batch 8 / 32
Total batch reconstruction loss: 0.05581991374492645
Training batch 9 / 32
Total batch reconstruction loss: 0.05910004302859306
Training batch 10 / 32
Total batch reconstruction loss: 0.056450456380844116
Training batch 11 / 32
Total batch reconstruction loss: 0.06259028613567352
Training batch 12 / 32
Total batch reconstruction loss: 0.06525722146034241
Training batch 13 / 32
Total batch reconstruction loss: 0.05603567883372307
Training batch 14 / 32
Total batch reconstruction loss: 0.05693504214286804
Training batch 15 / 32
Total batch reconstruction loss: 0.056995827704668045
Training batch 16 / 32
Total batch reconstruction loss: 0.06010768562555313
Training batch 17 / 32
Total batch reconstruction loss: 0.05612843483686447
Training batch 18 / 32
Total batch reconstruction loss: 0.060065269470214844
Training batch 19 / 32
Total batch reconstruction loss: 0.06170057877898216
Training batch 20 / 32
Total batch reconstruction loss: 0.05678209662437439
Training batch 21 / 32
Total batch reconstruction loss: 0.0599285289645195
Training batch 22 / 32
Total batch reconstruction loss: 0.05730832368135452
Training batch 23 / 32
Total batch reconstruction loss: 0.0654418021440506
Training batch 24 / 32
Total batch reconstruction loss: 0.05763228237628937
Training batch 25 / 32
Total batch reconstruction loss: 0.057176098227500916
Training batch 26 / 32
Total batch reconstruction loss: 0.05649920552968979
Training batch 27 / 32
Total batch reconstruction loss: 0.06057174503803253
Training batch 28 / 32
Total batch reconstruction loss: 0.05670604109764099
Training batch 29 / 32
Total batch reconstruction loss: 0.06251062452793121
Training batch 30 / 32
Total batch reconstruction loss: 0.05928070843219757
Training batch 31 / 32
Total batch reconstruction loss: 0.06143903732299805
Training batch 32 / 32
Total batch reconstruction loss: 0.05604719743132591
Epoch [278/500], Train Loss: 0.0580, Validation Loss: 0.0586, Generator Loss: 11.9845, Discriminator Loss: 0.3072
Training epoch 279 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.060226745903491974
Training batch 2 / 32
Total batch reconstruction loss: 0.05864661931991577
Training batch 3 / 32
Total batch reconstruction loss: 0.05981803685426712
Training batch 4 / 32
Total batch reconstruction loss: 0.06004077568650246
Training batch 5 / 32
Total batch reconstruction loss: 0.05680304765701294
Training batch 6 / 32
Total batch reconstruction loss: 0.061353739351034164
Training batch 7 / 32
Total batch reconstruction loss: 0.0630090981721878
Training batch 8 / 32
Total batch reconstruction loss: 0.05620955675840378
Training batch 9 / 32
Total batch reconstruction loss: 0.06278474628925323
Training batch 10 / 32
Total batch reconstruction loss: 0.057109810411930084
Training batch 11 / 32
Total batch reconstruction loss: 0.060568712651729584
Training batch 12 / 32
Total batch reconstruction loss: 0.06209491938352585
Training batch 13 / 32
Total batch reconstruction loss: 0.06231335923075676
Training batch 14 / 32
Total batch reconstruction loss: 0.0624706968665123
Training batch 15 / 32
Total batch reconstruction loss: 0.05779877305030823
Training batch 16 / 32
Total batch reconstruction loss: 0.057353824377059937
Training batch 17 / 32
Total batch reconstruction loss: 0.05851035192608833
Training batch 18 / 32
Total batch reconstruction loss: 0.058992888778448105
Training batch 19 / 32
Total batch reconstruction loss: 0.060331813991069794
Training batch 20 / 32
Total batch reconstruction loss: 0.06369535624980927
Training batch 21 / 32
Total batch reconstruction loss: 0.06142555922269821
Training batch 22 / 32
Total batch reconstruction loss: 0.05582110583782196
Training batch 23 / 32
Total batch reconstruction loss: 0.05836232751607895
Training batch 24 / 32
Total batch reconstruction loss: 0.06157072260975838
Training batch 25 / 32
Total batch reconstruction loss: 0.06117662042379379
Training batch 26 / 32
Total batch reconstruction loss: 0.05520062893629074
Training batch 27 / 32
Total batch reconstruction loss: 0.058812618255615234
Training batch 28 / 32
Total batch reconstruction loss: 0.06001009792089462
Training batch 29 / 32
Total batch reconstruction loss: 0.058947496116161346
Training batch 30 / 32
Total batch reconstruction loss: 0.05548703297972679
Training batch 31 / 32
Total batch reconstruction loss: 0.060375504195690155
Training batch 32 / 32
Total batch reconstruction loss: 0.058709703385829926
Epoch [279/500], Train Loss: 0.0583, Validation Loss: 0.0590, Generator Loss: 11.9834, Discriminator Loss: 0.3196
Training epoch 280 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.05871196836233139
Training batch 2 / 32
Total batch reconstruction loss: 0.05939137935638428
Training batch 3 / 32
Total batch reconstruction loss: 0.058632828295230865
Training batch 4 / 32
Total batch reconstruction loss: 0.06768544018268585
Training batch 5 / 32
Total batch reconstruction loss: 0.05568280071020126
Training batch 6 / 32
Total batch reconstruction loss: 0.06221430003643036
Training batch 7 / 32
Total batch reconstruction loss: 0.06034887954592705
Training batch 8 / 32
Total batch reconstruction loss: 0.05933544412255287
Training batch 9 / 32
Total batch reconstruction loss: 0.05713720619678497
Training batch 10 / 32
Total batch reconstruction loss: 0.05508621037006378
Training batch 11 / 32
Total batch reconstruction loss: 0.05676817521452904
Training batch 12 / 32
Total batch reconstruction loss: 0.059476204216480255
Training batch 13 / 32
Total batch reconstruction loss: 0.06282470375299454
Training batch 14 / 32
Total batch reconstruction loss: 0.05770951509475708
Training batch 15 / 32
Total batch reconstruction loss: 0.05706121027469635
Training batch 16 / 32
Total batch reconstruction loss: 0.05798785760998726
Training batch 17 / 32
Total batch reconstruction loss: 0.05623732507228851
Training batch 18 / 32
Total batch reconstruction loss: 0.058294281363487244
Training batch 19 / 32
Total batch reconstruction loss: 0.060687627643346786
Training batch 20 / 32
Total batch reconstruction loss: 0.061166614294052124
Training batch 21 / 32
Total batch reconstruction loss: 0.060718461871147156
Training batch 22 / 32
Total batch reconstruction loss: 0.0603620707988739
Training batch 23 / 32
Total batch reconstruction loss: 0.059937983751297
Training batch 24 / 32
Total batch reconstruction loss: 0.06377220153808594
Training batch 25 / 32
Total batch reconstruction loss: 0.06271275132894516
Training batch 26 / 32
Total batch reconstruction loss: 0.05897873640060425
Training batch 27 / 32
Total batch reconstruction loss: 0.06337208300828934
Training batch 28 / 32
Total batch reconstruction loss: 0.060941316187381744
Training batch 29 / 32
Total batch reconstruction loss: 0.059944529086351395
Training batch 30 / 32
Total batch reconstruction loss: 0.06149037182331085
Training batch 31 / 32
Total batch reconstruction loss: 0.05635005608201027
Training batch 32 / 32
Total batch reconstruction loss: 0.05218704789876938
Epoch [280/500], Train Loss: 0.0582, Validation Loss: 0.0591, Generator Loss: 11.9679, Discriminator Loss: 0.3118
Training epoch 281 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.06284235417842865
Training batch 2 / 32
Total batch reconstruction loss: 0.05919215828180313
Training batch 3 / 32
Total batch reconstruction loss: 0.05986427143216133
Training batch 4 / 32
Total batch reconstruction loss: 0.06393755972385406
Training batch 5 / 32
Total batch reconstruction loss: 0.06192297115921974
Training batch 6 / 32
Total batch reconstruction loss: 0.06243613734841347
Training batch 7 / 32
Total batch reconstruction loss: 0.0539698526263237
Training batch 8 / 32
Total batch reconstruction loss: 0.06055137515068054
Training batch 9 / 32
Total batch reconstruction loss: 0.06144361197948456
Training batch 10 / 32
Total batch reconstruction loss: 0.05969773232936859
Training batch 11 / 32
Total batch reconstruction loss: 0.0600256472826004
Training batch 12 / 32
Total batch reconstruction loss: 0.05648833513259888
Training batch 13 / 32
Total batch reconstruction loss: 0.05657611042261124
Training batch 14 / 32
Total batch reconstruction loss: 0.061347849667072296
Training batch 15 / 32
Total batch reconstruction loss: 0.0569743774831295
Training batch 16 / 32
Total batch reconstruction loss: 0.05926261097192764
Training batch 17 / 32
Total batch reconstruction loss: 0.05717426538467407
Training batch 18 / 32
Total batch reconstruction loss: 0.06202297657728195
Training batch 19 / 32
Total batch reconstruction loss: 0.06057693064212799
Training batch 20 / 32
Total batch reconstruction loss: 0.05858651548624039
Training batch 21 / 32
Total batch reconstruction loss: 0.05997772887349129
Training batch 22 / 32
Total batch reconstruction loss: 0.0572962760925293
Training batch 23 / 32
Total batch reconstruction loss: 0.06079147011041641
Training batch 24 / 32
Total batch reconstruction loss: 0.06120772659778595
Training batch 25 / 32
Total batch reconstruction loss: 0.05922291427850723
Training batch 26 / 32
Total batch reconstruction loss: 0.053936176002025604
Training batch 27 / 32
Total batch reconstruction loss: 0.05639167129993439
Training batch 28 / 32
Total batch reconstruction loss: 0.061999738216400146
Training batch 29 / 32
Total batch reconstruction loss: 0.06136269494891167
Training batch 30 / 32
Total batch reconstruction loss: 0.0629122257232666
Training batch 31 / 32
Total batch reconstruction loss: 0.06408533453941345
Training batch 32 / 32
Total batch reconstruction loss: 0.062275491654872894
Epoch [281/500], Train Loss: 0.0584, Validation Loss: 0.0597, Generator Loss: 12.0535, Discriminator Loss: 0.3056
Training epoch 282 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.059882063418626785
Training batch 2 / 32
Total batch reconstruction loss: 0.05704876407980919
Training batch 3 / 32
Total batch reconstruction loss: 0.058155082166194916
Training batch 4 / 32
Total batch reconstruction loss: 0.05958564579486847
Training batch 5 / 32
Total batch reconstruction loss: 0.06121544539928436
Training batch 6 / 32
Total batch reconstruction loss: 0.05726078897714615
Training batch 7 / 32
Total batch reconstruction loss: 0.06187969818711281
Training batch 8 / 32
Total batch reconstruction loss: 0.060962140560150146
Training batch 9 / 32
Total batch reconstruction loss: 0.05837925896048546
Training batch 10 / 32
Total batch reconstruction loss: 0.0629267543554306
Training batch 11 / 32
Total batch reconstruction loss: 0.05915508419275284
Training batch 12 / 32
Total batch reconstruction loss: 0.055879853665828705
Training batch 13 / 32
Total batch reconstruction loss: 0.06490074098110199
Training batch 14 / 32
Total batch reconstruction loss: 0.06040916219353676
Training batch 15 / 32
Total batch reconstruction loss: 0.0633411854505539
Training batch 16 / 32
Total batch reconstruction loss: 0.05942351743578911
Training batch 17 / 32
Total batch reconstruction loss: 0.05765528976917267
Training batch 18 / 32
Total batch reconstruction loss: 0.0613335445523262
Training batch 19 / 32
Total batch reconstruction loss: 0.058553751558065414
Training batch 20 / 32
Total batch reconstruction loss: 0.06331463158130646
Training batch 21 / 32
Total batch reconstruction loss: 0.06157234311103821
Training batch 22 / 32
Total batch reconstruction loss: 0.06538939476013184
Training batch 23 / 32
Total batch reconstruction loss: 0.06133197620511055
Training batch 24 / 32
Total batch reconstruction loss: 0.05985315889120102
Training batch 25 / 32
Total batch reconstruction loss: 0.05796385183930397
Training batch 26 / 32
Total batch reconstruction loss: 0.058589547872543335
Training batch 27 / 32
Total batch reconstruction loss: 0.05701923742890358
Training batch 28 / 32
Total batch reconstruction loss: 0.056895770132541656
Training batch 29 / 32
Total batch reconstruction loss: 0.05797461420297623
Training batch 30 / 32
Total batch reconstruction loss: 0.05651969090104103
Training batch 31 / 32
Total batch reconstruction loss: 0.051583535969257355
Training batch 32 / 32
Total batch reconstruction loss: 0.05662527680397034
Epoch [282/500], Train Loss: 0.0579, Validation Loss: 0.0573, Generator Loss: 11.9687, Discriminator Loss: 0.3084
Training epoch 283 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.06233232840895653
Training batch 2 / 32
Total batch reconstruction loss: 0.059630852192640305
Training batch 3 / 32
Total batch reconstruction loss: 0.05753440037369728
Training batch 4 / 32
Total batch reconstruction loss: 0.05888592451810837
Training batch 5 / 32
Total batch reconstruction loss: 0.05924314633011818
Training batch 6 / 32
Total batch reconstruction loss: 0.06170674040913582
Training batch 7 / 32
Total batch reconstruction loss: 0.05711788684129715
Training batch 8 / 32
Total batch reconstruction loss: 0.05844816192984581
Training batch 9 / 32
Total batch reconstruction loss: 0.05683658644556999
Training batch 10 / 32
Total batch reconstruction loss: 0.06306954473257065
Training batch 11 / 32
Total batch reconstruction loss: 0.06001834571361542
Training batch 12 / 32
Total batch reconstruction loss: 0.05749579891562462
Training batch 13 / 32
Total batch reconstruction loss: 0.05386226251721382
Training batch 14 / 32
Total batch reconstruction loss: 0.057953789830207825
Training batch 15 / 32
Total batch reconstruction loss: 0.06860466301441193
Training batch 16 / 32
Total batch reconstruction loss: 0.05951996520161629
Training batch 17 / 32
Total batch reconstruction loss: 0.061843693256378174
Training batch 18 / 32
Total batch reconstruction loss: 0.061162788420915604
Training batch 19 / 32
Total batch reconstruction loss: 0.06229209154844284
Training batch 20 / 32
Total batch reconstruction loss: 0.05987708270549774
Training batch 21 / 32
Total batch reconstruction loss: 0.05911289155483246
Training batch 22 / 32
Total batch reconstruction loss: 0.05700593814253807
Training batch 23 / 32
Total batch reconstruction loss: 0.06434153020381927
Training batch 24 / 32
Total batch reconstruction loss: 0.056681226938962936
Training batch 25 / 32
Total batch reconstruction loss: 0.059100303798913956
Training batch 26 / 32
Total batch reconstruction loss: 0.05794698745012283
Training batch 27 / 32
Total batch reconstruction loss: 0.05945386737585068
Training batch 28 / 32
Total batch reconstruction loss: 0.06024063751101494
Training batch 29 / 32
Total batch reconstruction loss: 0.05875560641288757
Training batch 30 / 32
Total batch reconstruction loss: 0.05877171829342842
Training batch 31 / 32
Total batch reconstruction loss: 0.0595686212182045
Training batch 32 / 32
Total batch reconstruction loss: 0.056972041726112366
Epoch [283/500], Train Loss: 0.0580, Validation Loss: 0.0566, Generator Loss: 11.9770, Discriminator Loss: 0.3190
Training epoch 284 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.05826917290687561
Training batch 2 / 32
Total batch reconstruction loss: 0.06227807328104973
Training batch 3 / 32
Total batch reconstruction loss: 0.06069453805685043
Training batch 4 / 32
Total batch reconstruction loss: 0.0613267607986927
Training batch 5 / 32
Total batch reconstruction loss: 0.05876145511865616
Training batch 6 / 32
Total batch reconstruction loss: 0.05734040588140488
Training batch 7 / 32
Total batch reconstruction loss: 0.055653057992458344
Training batch 8 / 32
Total batch reconstruction loss: 0.06347780674695969
Training batch 9 / 32
Total batch reconstruction loss: 0.06248420104384422
Training batch 10 / 32
Total batch reconstruction loss: 0.061654165387153625
Training batch 11 / 32
Total batch reconstruction loss: 0.06372297555208206
Training batch 12 / 32
Total batch reconstruction loss: 0.06461742520332336
Training batch 13 / 32
Total batch reconstruction loss: 0.06307998299598694
Training batch 14 / 32
Total batch reconstruction loss: 0.05904208868741989
Training batch 15 / 32
Total batch reconstruction loss: 0.05943918973207474
Training batch 16 / 32
Total batch reconstruction loss: 0.05920255929231644
Training batch 17 / 32
Total batch reconstruction loss: 0.05846875160932541
Training batch 18 / 32
Total batch reconstruction loss: 0.058835990726947784
Training batch 19 / 32
Total batch reconstruction loss: 0.05536378175020218
Training batch 20 / 32
Total batch reconstruction loss: 0.0571623295545578
Training batch 21 / 32
Total batch reconstruction loss: 0.05710095539689064
Training batch 22 / 32
Total batch reconstruction loss: 0.05835747346282005
Training batch 23 / 32
Total batch reconstruction loss: 0.05843566358089447
Training batch 24 / 32
Total batch reconstruction loss: 0.05612856149673462
Training batch 25 / 32
Total batch reconstruction loss: 0.06282369792461395
Training batch 26 / 32
Total batch reconstruction loss: 0.05524346977472305
Training batch 27 / 32
Total batch reconstruction loss: 0.06277138739824295
Training batch 28 / 32
Total batch reconstruction loss: 0.060178834944963455
Training batch 29 / 32
Total batch reconstruction loss: 0.06276647746562958
Training batch 30 / 32
Total batch reconstruction loss: 0.06083577126264572
Training batch 31 / 32
Total batch reconstruction loss: 0.05949893593788147
Training batch 32 / 32
Total batch reconstruction loss: 0.05297199636697769
Epoch [284/500], Train Loss: 0.0586, Validation Loss: 0.0578, Generator Loss: 11.9837, Discriminator Loss: 0.3330
Training epoch 285 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.05924201011657715
Training batch 2 / 32
Total batch reconstruction loss: 0.06115793436765671
Training batch 3 / 32
Total batch reconstruction loss: 0.058383215218782425
Training batch 4 / 32
Total batch reconstruction loss: 0.0613301545381546
Training batch 5 / 32
Total batch reconstruction loss: 0.0601847879588604
Training batch 6 / 32
Total batch reconstruction loss: 0.05759819597005844
Training batch 7 / 32
Total batch reconstruction loss: 0.05466014891862869
Training batch 8 / 32
Total batch reconstruction loss: 0.06074073165655136
Training batch 9 / 32
Total batch reconstruction loss: 0.05822864919900894
Training batch 10 / 32
Total batch reconstruction loss: 0.05875964090228081
Training batch 11 / 32
Total batch reconstruction loss: 0.05730007588863373
Training batch 12 / 32
Total batch reconstruction loss: 0.058080874383449554
Training batch 13 / 32
Total batch reconstruction loss: 0.06220283731818199
Training batch 14 / 32
Total batch reconstruction loss: 0.061205945909023285
Training batch 15 / 32
Total batch reconstruction loss: 0.05525687709450722
Training batch 16 / 32
Total batch reconstruction loss: 0.05884223431348801
Training batch 17 / 32
Total batch reconstruction loss: 0.05543069168925285
Training batch 18 / 32
Total batch reconstruction loss: 0.058438777923583984
Training batch 19 / 32
Total batch reconstruction loss: 0.06380490958690643
Training batch 20 / 32
Total batch reconstruction loss: 0.0638652965426445
Training batch 21 / 32
Total batch reconstruction loss: 0.057630449533462524
Training batch 22 / 32
Total batch reconstruction loss: 0.05909319221973419
Training batch 23 / 32
Total batch reconstruction loss: 0.0605851374566555
Training batch 24 / 32
Total batch reconstruction loss: 0.06255021691322327
Training batch 25 / 32
Total batch reconstruction loss: 0.0609450489282608
Training batch 26 / 32
Total batch reconstruction loss: 0.060832735151052475
Training batch 27 / 32
Total batch reconstruction loss: 0.059235282242298126
Training batch 28 / 32
Total batch reconstruction loss: 0.05972261726856232
Training batch 29 / 32
Total batch reconstruction loss: 0.06307542324066162
Training batch 30 / 32
Total batch reconstruction loss: 0.05924322456121445
Training batch 31 / 32
Total batch reconstruction loss: 0.06169001758098602
Training batch 32 / 32
Total batch reconstruction loss: 0.05455416068434715
Epoch [285/500], Train Loss: 0.0580, Validation Loss: 0.0583, Generator Loss: 11.9696, Discriminator Loss: 0.3119
Training epoch 286 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.05694344639778137
Training batch 2 / 32
Total batch reconstruction loss: 0.0569695420563221
Training batch 3 / 32
Total batch reconstruction loss: 0.06328374147415161
Training batch 4 / 32
Total batch reconstruction loss: 0.05692943558096886
Training batch 5 / 32
Total batch reconstruction loss: 0.061135806143283844
Training batch 6 / 32
Total batch reconstruction loss: 0.05903344601392746
Training batch 7 / 32
Total batch reconstruction loss: 0.06260497868061066
Training batch 8 / 32
Total batch reconstruction loss: 0.05866479128599167
Training batch 9 / 32
Total batch reconstruction loss: 0.06123042106628418
Training batch 10 / 32
Total batch reconstruction loss: 0.062456779181957245
Training batch 11 / 32
Total batch reconstruction loss: 0.05708971619606018
Training batch 12 / 32
Total batch reconstruction loss: 0.06118268519639969
Training batch 13 / 32
Total batch reconstruction loss: 0.05657487362623215
Training batch 14 / 32
Total batch reconstruction loss: 0.05665532499551773
Training batch 15 / 32
Total batch reconstruction loss: 0.05914944037795067
Training batch 16 / 32
Total batch reconstruction loss: 0.05864847078919411
Training batch 17 / 32
Total batch reconstruction loss: 0.06156148761510849
Training batch 18 / 32
Total batch reconstruction loss: 0.05589964985847473
Training batch 19 / 32
Total batch reconstruction loss: 0.05787023529410362
Training batch 20 / 32
Total batch reconstruction loss: 0.06144537404179573
Training batch 21 / 32
Total batch reconstruction loss: 0.059621360152959824
Training batch 22 / 32
Total batch reconstruction loss: 0.06471484899520874
Training batch 23 / 32
Total batch reconstruction loss: 0.06104876101016998
Training batch 24 / 32
Total batch reconstruction loss: 0.05970517545938492
Training batch 25 / 32
Total batch reconstruction loss: 0.06112530454993248
Training batch 26 / 32
Total batch reconstruction loss: 0.059270769357681274
Training batch 27 / 32
Total batch reconstruction loss: 0.06388603895902634
Training batch 28 / 32
Total batch reconstruction loss: 0.05784556269645691
Training batch 29 / 32
Total batch reconstruction loss: 0.06208137050271034
Training batch 30 / 32
Total batch reconstruction loss: 0.059473589062690735
Training batch 31 / 32
Total batch reconstruction loss: 0.06093001365661621
Training batch 32 / 32
Total batch reconstruction loss: 0.05184050276875496
Epoch [286/500], Train Loss: 0.0579, Validation Loss: 0.0581, Generator Loss: 11.9936, Discriminator Loss: 0.3115
Training epoch 287 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.06164544075727463
Training batch 2 / 32
Total batch reconstruction loss: 0.056839101016521454
Training batch 3 / 32
Total batch reconstruction loss: 0.05860856920480728
Training batch 4 / 32
Total batch reconstruction loss: 0.06206304579973221
Training batch 5 / 32
Total batch reconstruction loss: 0.057708367705345154
Training batch 6 / 32
Total batch reconstruction loss: 0.05784841626882553
Training batch 7 / 32
Total batch reconstruction loss: 0.06367740035057068
Training batch 8 / 32
Total batch reconstruction loss: 0.057271670550107956
Training batch 9 / 32
Total batch reconstruction loss: 0.058357127010822296
Training batch 10 / 32
Total batch reconstruction loss: 0.059384170919656754
Training batch 11 / 32
Total batch reconstruction loss: 0.056465186178684235
Training batch 12 / 32
Total batch reconstruction loss: 0.062319669872522354
Training batch 13 / 32
Total batch reconstruction loss: 0.05628906935453415
Training batch 14 / 32
Total batch reconstruction loss: 0.057302508503198624
Training batch 15 / 32
Total batch reconstruction loss: 0.06030797213315964
Training batch 16 / 32
Total batch reconstruction loss: 0.05702084302902222
Training batch 17 / 32
Total batch reconstruction loss: 0.06010352075099945
Training batch 18 / 32
Total batch reconstruction loss: 0.06126928701996803
Training batch 19 / 32
Total batch reconstruction loss: 0.06024232134222984
Training batch 20 / 32
Total batch reconstruction loss: 0.05905964598059654
Training batch 21 / 32
Total batch reconstruction loss: 0.06674917787313461
Training batch 22 / 32
Total batch reconstruction loss: 0.05974622815847397
Training batch 23 / 32
Total batch reconstruction loss: 0.06082987040281296
Training batch 24 / 32
Total batch reconstruction loss: 0.058322276920080185
Training batch 25 / 32
Total batch reconstruction loss: 0.06271438300609589
Training batch 26 / 32
Total batch reconstruction loss: 0.06108473986387253
Training batch 27 / 32
Total batch reconstruction loss: 0.060873210430145264
Training batch 28 / 32
Total batch reconstruction loss: 0.059902891516685486
Training batch 29 / 32
Total batch reconstruction loss: 0.057189613580703735
Training batch 30 / 32
Total batch reconstruction loss: 0.058219581842422485
Training batch 31 / 32
Total batch reconstruction loss: 0.06223106011748314
Training batch 32 / 32
Total batch reconstruction loss: 0.06231732666492462
Epoch [287/500], Train Loss: 0.0586, Validation Loss: 0.0592, Generator Loss: 12.0335, Discriminator Loss: 0.3064
Training epoch 288 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.056669652462005615
Training batch 2 / 32
Total batch reconstruction loss: 0.060265637934207916
Training batch 3 / 32
Total batch reconstruction loss: 0.05978754907846451
Training batch 4 / 32
Total batch reconstruction loss: 0.05579020082950592
Training batch 5 / 32
Total batch reconstruction loss: 0.058254584670066833
Training batch 6 / 32
Total batch reconstruction loss: 0.06121862679719925
Training batch 7 / 32
Total batch reconstruction loss: 0.06254115700721741
Training batch 8 / 32
Total batch reconstruction loss: 0.057405736297369
Training batch 9 / 32
Total batch reconstruction loss: 0.05869780853390694
Training batch 10 / 32
Total batch reconstruction loss: 0.057087041437625885
Training batch 11 / 32
Total batch reconstruction loss: 0.06227428466081619
Training batch 12 / 32
Total batch reconstruction loss: 0.059577133506536484
Training batch 13 / 32
Total batch reconstruction loss: 0.0608208030462265
Training batch 14 / 32
Total batch reconstruction loss: 0.06179744005203247
Training batch 15 / 32
Total batch reconstruction loss: 0.06030718982219696
Training batch 16 / 32
Total batch reconstruction loss: 0.05944375693798065
Training batch 17 / 32
Total batch reconstruction loss: 0.059084177017211914
Training batch 18 / 32
Total batch reconstruction loss: 0.0590532124042511
Training batch 19 / 32
Total batch reconstruction loss: 0.05669918656349182
Training batch 20 / 32
Total batch reconstruction loss: 0.06334559619426727
Training batch 21 / 32
Total batch reconstruction loss: 0.05519542470574379
Training batch 22 / 32
Total batch reconstruction loss: 0.06314501166343689
Training batch 23 / 32
Total batch reconstruction loss: 0.057678330689668655
Training batch 24 / 32
Total batch reconstruction loss: 0.0625007227063179
Training batch 25 / 32
Total batch reconstruction loss: 0.061904072761535645
Training batch 26 / 32
Total batch reconstruction loss: 0.06058467552065849
Training batch 27 / 32
Total batch reconstruction loss: 0.05913387984037399
Training batch 28 / 32
Total batch reconstruction loss: 0.06309647858142853
Training batch 29 / 32
Total batch reconstruction loss: 0.057611383497714996
Training batch 30 / 32
Total batch reconstruction loss: 0.061386965215206146
Training batch 31 / 32
Total batch reconstruction loss: 0.0604424923658371
Training batch 32 / 32
Total batch reconstruction loss: 0.06893714517354965
Epoch [288/500], Train Loss: 0.0587, Validation Loss: 0.0593, Generator Loss: 12.0785, Discriminator Loss: 0.3185
Training epoch 289 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.06106627732515335
Training batch 2 / 32
Total batch reconstruction loss: 0.05756112560629845
Training batch 3 / 32
Total batch reconstruction loss: 0.06335771083831787
Training batch 4 / 32
Total batch reconstruction loss: 0.05725468695163727
Training batch 5 / 32
Total batch reconstruction loss: 0.05877004191279411
Training batch 6 / 32
Total batch reconstruction loss: 0.05912518501281738
Training batch 7 / 32
Total batch reconstruction loss: 0.0642084926366806
Training batch 8 / 32
Total batch reconstruction loss: 0.057743676006793976
Training batch 9 / 32
Total batch reconstruction loss: 0.061482395976781845
Training batch 10 / 32
Total batch reconstruction loss: 0.06044868752360344
Training batch 11 / 32
Total batch reconstruction loss: 0.05425558611750603
Training batch 12 / 32
Total batch reconstruction loss: 0.060442835092544556
Training batch 13 / 32
Total batch reconstruction loss: 0.05787518247961998
Training batch 14 / 32
Total batch reconstruction loss: 0.061880722641944885
Training batch 15 / 32
Total batch reconstruction loss: 0.06313756853342056
Training batch 16 / 32
Total batch reconstruction loss: 0.0548507384955883
Training batch 17 / 32
Total batch reconstruction loss: 0.05623267590999603
Training batch 18 / 32
Total batch reconstruction loss: 0.05550307035446167
Training batch 19 / 32
Total batch reconstruction loss: 0.059574052691459656
Training batch 20 / 32
Total batch reconstruction loss: 0.05841412395238876
Training batch 21 / 32
Total batch reconstruction loss: 0.06815066933631897
Training batch 22 / 32
Total batch reconstruction loss: 0.05752335488796234
Training batch 23 / 32
Total batch reconstruction loss: 0.056613609194755554
Training batch 24 / 32
Total batch reconstruction loss: 0.058683499693870544
Training batch 25 / 32
Total batch reconstruction loss: 0.055797964334487915
Training batch 26 / 32
Total batch reconstruction loss: 0.06372931599617004
Training batch 27 / 32
Total batch reconstruction loss: 0.057735130190849304
Training batch 28 / 32
Total batch reconstruction loss: 0.05718664824962616
Training batch 29 / 32
Total batch reconstruction loss: 0.05889028683304787
Training batch 30 / 32
Total batch reconstruction loss: 0.0637325644493103
Training batch 31 / 32
Total batch reconstruction loss: 0.0613396093249321
Training batch 32 / 32
Total batch reconstruction loss: 0.055476389825344086
Epoch [289/500], Train Loss: 0.0576, Validation Loss: 0.0583, Generator Loss: 11.9328, Discriminator Loss: 0.3212
Training epoch 290 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.06037137284874916
Training batch 2 / 32
Total batch reconstruction loss: 0.06218430772423744
Training batch 3 / 32
Total batch reconstruction loss: 0.05624657869338989
Training batch 4 / 32
Total batch reconstruction loss: 0.05987180769443512
Training batch 5 / 32
Total batch reconstruction loss: 0.056006669998168945
Training batch 6 / 32
Total batch reconstruction loss: 0.059077709913253784
Training batch 7 / 32
Total batch reconstruction loss: 0.06064962223172188
Training batch 8 / 32
Total batch reconstruction loss: 0.06047604978084564
Training batch 9 / 32
Total batch reconstruction loss: 0.059037383645772934
Training batch 10 / 32
Total batch reconstruction loss: 0.05792708694934845
Training batch 11 / 32
Total batch reconstruction loss: 0.059097856283187866
Training batch 12 / 32
Total batch reconstruction loss: 0.05824873596429825
Training batch 13 / 32
Total batch reconstruction loss: 0.05743029713630676
Training batch 14 / 32
Total batch reconstruction loss: 0.06318899244070053
Training batch 15 / 32
Total batch reconstruction loss: 0.06004456430673599
Training batch 16 / 32
Total batch reconstruction loss: 0.05760449916124344
Training batch 17 / 32
Total batch reconstruction loss: 0.060259487479925156
Training batch 18 / 32
Total batch reconstruction loss: 0.060558952391147614
Training batch 19 / 32
Total batch reconstruction loss: 0.06032475084066391
Training batch 20 / 32
Total batch reconstruction loss: 0.06132752075791359
Training batch 21 / 32
Total batch reconstruction loss: 0.06091860681772232
Training batch 22 / 32
Total batch reconstruction loss: 0.059312060475349426
Training batch 23 / 32
Total batch reconstruction loss: 0.05808401480317116
Training batch 24 / 32
Total batch reconstruction loss: 0.060972996056079865
Training batch 25 / 32
Total batch reconstruction loss: 0.05814312770962715
Training batch 26 / 32
Total batch reconstruction loss: 0.06117900460958481
Training batch 27 / 32
Total batch reconstruction loss: 0.05762319266796112
Training batch 28 / 32
Total batch reconstruction loss: 0.06235019117593765
Training batch 29 / 32
Total batch reconstruction loss: 0.05597984045743942
Training batch 30 / 32
Total batch reconstruction loss: 0.05809009075164795
Training batch 31 / 32
Total batch reconstruction loss: 0.05780636519193649
Training batch 32 / 32
Total batch reconstruction loss: 0.059413567185401917
Epoch [290/500], Train Loss: 0.0578, Validation Loss: 0.0596, Generator Loss: 11.9501, Discriminator Loss: 0.3020
Training epoch 291 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.0636492669582367
Training batch 2 / 32
Total batch reconstruction loss: 0.059935834258794785
Training batch 3 / 32
Total batch reconstruction loss: 0.060568101704120636
Training batch 4 / 32
Total batch reconstruction loss: 0.05736863240599632
Training batch 5 / 32
Total batch reconstruction loss: 0.05845879763364792
Training batch 6 / 32
Total batch reconstruction loss: 0.05690500885248184
Training batch 7 / 32
Total batch reconstruction loss: 0.05579884350299835
Training batch 8 / 32
Total batch reconstruction loss: 0.05963349714875221
Training batch 9 / 32
Total batch reconstruction loss: 0.058046214282512665
Training batch 10 / 32
Total batch reconstruction loss: 0.06515830755233765
Training batch 11 / 32
Total batch reconstruction loss: 0.05711925029754639
Training batch 12 / 32
Total batch reconstruction loss: 0.06161830946803093
Training batch 13 / 32
Total batch reconstruction loss: 0.06080624833703041
Training batch 14 / 32
Total batch reconstruction loss: 0.058435823768377304
Training batch 15 / 32
Total batch reconstruction loss: 0.05662908777594566
Training batch 16 / 32
Total batch reconstruction loss: 0.06322230398654938
Training batch 17 / 32
Total batch reconstruction loss: 0.05832619220018387
Training batch 18 / 32
Total batch reconstruction loss: 0.06119035929441452
Training batch 19 / 32
Total batch reconstruction loss: 0.05794911086559296
Training batch 20 / 32
Total batch reconstruction loss: 0.061900414526462555
Training batch 21 / 32
Total batch reconstruction loss: 0.057192735373973846
Training batch 22 / 32
Total batch reconstruction loss: 0.056020647287368774
Training batch 23 / 32
Total batch reconstruction loss: 0.060679011046886444
Training batch 24 / 32
Total batch reconstruction loss: 0.05794287845492363
Training batch 25 / 32
Total batch reconstruction loss: 0.06011124700307846
Training batch 26 / 32
Total batch reconstruction loss: 0.05597364902496338
Training batch 27 / 32
Total batch reconstruction loss: 0.06079375743865967
Training batch 28 / 32
Total batch reconstruction loss: 0.0581706278026104
Training batch 29 / 32
Total batch reconstruction loss: 0.0655534416437149
Training batch 30 / 32
Total batch reconstruction loss: 0.061905696988105774
Training batch 31 / 32
Total batch reconstruction loss: 0.05572497472167015
Training batch 32 / 32
Total batch reconstruction loss: 0.06713400036096573
Epoch [291/500], Train Loss: 0.0579, Validation Loss: 0.0591, Generator Loss: 12.0015, Discriminator Loss: 0.3202
Training epoch 292 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.06665239483118057
Training batch 2 / 32
Total batch reconstruction loss: 0.06160426139831543
Training batch 3 / 32
Total batch reconstruction loss: 0.05944814532995224
Training batch 4 / 32
Total batch reconstruction loss: 0.062032222747802734
Training batch 5 / 32
Total batch reconstruction loss: 0.06123887002468109
Training batch 6 / 32
Total batch reconstruction loss: 0.05666183680295944
Training batch 7 / 32
Total batch reconstruction loss: 0.05559573695063591
Training batch 8 / 32
Total batch reconstruction loss: 0.06644400954246521
Training batch 9 / 32
Total batch reconstruction loss: 0.06277624517679214
Training batch 10 / 32
Total batch reconstruction loss: 0.060697752982378006
Training batch 11 / 32
Total batch reconstruction loss: 0.0588546097278595
Training batch 12 / 32
Total batch reconstruction loss: 0.06020701676607132
Training batch 13 / 32
Total batch reconstruction loss: 0.058759938925504684
Training batch 14 / 32
Total batch reconstruction loss: 0.0618659071624279
Training batch 15 / 32
Total batch reconstruction loss: 0.05967801809310913
Training batch 16 / 32
Total batch reconstruction loss: 0.062169451266527176
Training batch 17 / 32
Total batch reconstruction loss: 0.0595104917883873
Training batch 18 / 32
Total batch reconstruction loss: 0.06028250604867935
Training batch 19 / 32
Total batch reconstruction loss: 0.06312093883752823
Training batch 20 / 32
Total batch reconstruction loss: 0.05922836810350418
Training batch 21 / 32
Total batch reconstruction loss: 0.06311687082052231
Training batch 22 / 32
Total batch reconstruction loss: 0.06153733283281326
Training batch 23 / 32
Total batch reconstruction loss: 0.05924316123127937
Training batch 24 / 32
Total batch reconstruction loss: 0.06242067739367485
Training batch 25 / 32
Total batch reconstruction loss: 0.05873306840658188
Training batch 26 / 32
Total batch reconstruction loss: 0.0549645721912384
Training batch 27 / 32
Total batch reconstruction loss: 0.05860218405723572
Training batch 28 / 32
Total batch reconstruction loss: 0.0601605623960495
Training batch 29 / 32
Total batch reconstruction loss: 0.0646977424621582
Training batch 30 / 32
Total batch reconstruction loss: 0.061226800084114075
Training batch 31 / 32
Total batch reconstruction loss: 0.06015867739915848
Training batch 32 / 32
Total batch reconstruction loss: 0.06591711193323135
Epoch [292/500], Train Loss: 0.0594, Validation Loss: 0.0570, Generator Loss: 12.2485, Discriminator Loss: 0.3141
Training epoch 293 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.06132475286722183
Training batch 2 / 32
Total batch reconstruction loss: 0.05900118872523308
Training batch 3 / 32
Total batch reconstruction loss: 0.05973176658153534
Training batch 4 / 32
Total batch reconstruction loss: 0.057508111000061035
Training batch 5 / 32
Total batch reconstruction loss: 0.06277088075876236
Training batch 6 / 32
Total batch reconstruction loss: 0.056532151997089386
Training batch 7 / 32
Total batch reconstruction loss: 0.05762503296136856
Training batch 8 / 32
Total batch reconstruction loss: 0.05843652784824371
Training batch 9 / 32
Total batch reconstruction loss: 0.06261608004570007
Training batch 10 / 32
Total batch reconstruction loss: 0.0609402060508728
Training batch 11 / 32
Total batch reconstruction loss: 0.05904364958405495
Training batch 12 / 32
Total batch reconstruction loss: 0.057722680270671844
Training batch 13 / 32
Total batch reconstruction loss: 0.05820152163505554
Training batch 14 / 32
Total batch reconstruction loss: 0.06004781275987625
Training batch 15 / 32
Total batch reconstruction loss: 0.05663476511836052
Training batch 16 / 32
Total batch reconstruction loss: 0.060315631330013275
Training batch 17 / 32
Total batch reconstruction loss: 0.05654880404472351
Training batch 18 / 32
Total batch reconstruction loss: 0.0563649982213974
Training batch 19 / 32
Total batch reconstruction loss: 0.06301021575927734
Training batch 20 / 32
Total batch reconstruction loss: 0.05612567067146301
Training batch 21 / 32
Total batch reconstruction loss: 0.06141584366559982
Training batch 22 / 32
Total batch reconstruction loss: 0.060992296785116196
Training batch 23 / 32
Total batch reconstruction loss: 0.05663057044148445
Training batch 24 / 32
Total batch reconstruction loss: 0.057348355650901794
Training batch 25 / 32
Total batch reconstruction loss: 0.062170978635549545
Training batch 26 / 32
Total batch reconstruction loss: 0.059420786798000336
Training batch 27 / 32
Total batch reconstruction loss: 0.0648006945848465
Training batch 28 / 32
Total batch reconstruction loss: 0.05625995993614197
Training batch 29 / 32
Total batch reconstruction loss: 0.057827431708574295
Training batch 30 / 32
Total batch reconstruction loss: 0.05960262566804886
Training batch 31 / 32
Total batch reconstruction loss: 0.06188288331031799
Training batch 32 / 32
Total batch reconstruction loss: 0.09449212998151779
Epoch [293/500], Train Loss: 0.0586, Validation Loss: 0.0570, Generator Loss: 12.1529, Discriminator Loss: 0.3179
Training epoch 294 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.06298555433750153
Training batch 2 / 32
Total batch reconstruction loss: 0.05993809178471565
Training batch 3 / 32
Total batch reconstruction loss: 0.059225887060165405
Training batch 4 / 32
Total batch reconstruction loss: 0.06295271217823029
Training batch 5 / 32
Total batch reconstruction loss: 0.060083575546741486
Training batch 6 / 32
Total batch reconstruction loss: 0.058446720242500305
Training batch 7 / 32
Total batch reconstruction loss: 0.057687386870384216
Training batch 8 / 32
Total batch reconstruction loss: 0.05654141679406166
Training batch 9 / 32
Total batch reconstruction loss: 0.0581938810646534
Training batch 10 / 32
Total batch reconstruction loss: 0.06016838923096657
Training batch 11 / 32
Total batch reconstruction loss: 0.05702074244618416
Training batch 12 / 32
Total batch reconstruction loss: 0.057979926466941833
Training batch 13 / 32
Total batch reconstruction loss: 0.05846115201711655
Training batch 14 / 32
Total batch reconstruction loss: 0.05714704096317291
Training batch 15 / 32
Total batch reconstruction loss: 0.06089075654745102
Training batch 16 / 32
Total batch reconstruction loss: 0.061024561524391174
Training batch 17 / 32
Total batch reconstruction loss: 0.05881710350513458
Training batch 18 / 32
Total batch reconstruction loss: 0.05529448762536049
Training batch 19 / 32
Total batch reconstruction loss: 0.05826251208782196
Training batch 20 / 32
Total batch reconstruction loss: 0.062241069972515106
Training batch 21 / 32
Total batch reconstruction loss: 0.06132955104112625
Training batch 22 / 32
Total batch reconstruction loss: 0.061057038605213165
Training batch 23 / 32
Total batch reconstruction loss: 0.06380783021450043
Training batch 24 / 32
Total batch reconstruction loss: 0.0593336746096611
Training batch 25 / 32
Total batch reconstruction loss: 0.06595997512340546
Training batch 26 / 32
Total batch reconstruction loss: 0.061398331075906754
Training batch 27 / 32
Total batch reconstruction loss: 0.05747120454907417
Training batch 28 / 32
Total batch reconstruction loss: 0.06182153522968292
Training batch 29 / 32
Total batch reconstruction loss: 0.06135667860507965
Training batch 30 / 32
Total batch reconstruction loss: 0.06168779358267784
Training batch 31 / 32
Total batch reconstruction loss: 0.053868431597948074
Training batch 32 / 32
Total batch reconstruction loss: 0.06599341332912445
Epoch [294/500], Train Loss: 0.0583, Validation Loss: 0.0586, Generator Loss: 12.0548, Discriminator Loss: 0.3207
Training epoch 295 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.059510961174964905
Training batch 2 / 32
Total batch reconstruction loss: 0.0598248653113842
Training batch 3 / 32
Total batch reconstruction loss: 0.059581443667411804
Training batch 4 / 32
Total batch reconstruction loss: 0.062365587800741196
Training batch 5 / 32
Total batch reconstruction loss: 0.06459575891494751
Training batch 6 / 32
Total batch reconstruction loss: 0.05880521237850189
Training batch 7 / 32
Total batch reconstruction loss: 0.05935336649417877
Training batch 8 / 32
Total batch reconstruction loss: 0.06095238775014877
Training batch 9 / 32
Total batch reconstruction loss: 0.06215563416481018
Training batch 10 / 32
Total batch reconstruction loss: 0.06206362694501877
Training batch 11 / 32
Total batch reconstruction loss: 0.06321276724338531
Training batch 12 / 32
Total batch reconstruction loss: 0.05807080119848251
Training batch 13 / 32
Total batch reconstruction loss: 0.0562116801738739
Training batch 14 / 32
Total batch reconstruction loss: 0.06218193098902702
Training batch 15 / 32
Total batch reconstruction loss: 0.06253626197576523
Training batch 16 / 32
Total batch reconstruction loss: 0.06276433914899826
Training batch 17 / 32
Total batch reconstruction loss: 0.054216284304857254
Training batch 18 / 32
Total batch reconstruction loss: 0.058272529393434525
Training batch 19 / 32
Total batch reconstruction loss: 0.05978822708129883
Training batch 20 / 32
Total batch reconstruction loss: 0.061112180352211
Training batch 21 / 32
Total batch reconstruction loss: 0.06064732372760773
Training batch 22 / 32
Total batch reconstruction loss: 0.054682523012161255
Training batch 23 / 32
Total batch reconstruction loss: 0.05812015011906624
Training batch 24 / 32
Total batch reconstruction loss: 0.054958242923021317
Training batch 25 / 32
Total batch reconstruction loss: 0.058968767523765564
Training batch 26 / 32
Total batch reconstruction loss: 0.06075458973646164
Training batch 27 / 32
Total batch reconstruction loss: 0.05760742351412773
Training batch 28 / 32
Total batch reconstruction loss: 0.057771071791648865
Training batch 29 / 32
Total batch reconstruction loss: 0.06244536489248276
Training batch 30 / 32
Total batch reconstruction loss: 0.057879433035850525
Training batch 31 / 32
Total batch reconstruction loss: 0.06220952421426773
Training batch 32 / 32
Total batch reconstruction loss: 0.06731808185577393
Epoch [295/500], Train Loss: 0.0583, Validation Loss: 0.0583, Generator Loss: 12.0784, Discriminator Loss: 0.3169
Training epoch 296 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.060218147933483124
Training batch 2 / 32
Total batch reconstruction loss: 0.06303443014621735
Training batch 3 / 32
Total batch reconstruction loss: 0.06285671144723892
Training batch 4 / 32
Total batch reconstruction loss: 0.06395740807056427
Training batch 5 / 32
Total batch reconstruction loss: 0.06229721009731293
Training batch 6 / 32
Total batch reconstruction loss: 0.060017045587301254
Training batch 7 / 32
Total batch reconstruction loss: 0.059036292135715485
Training batch 8 / 32
Total batch reconstruction loss: 0.06163029745221138
Training batch 9 / 32
Total batch reconstruction loss: 0.058606959879398346
Training batch 10 / 32
Total batch reconstruction loss: 0.059929706156253815
Training batch 11 / 32
Total batch reconstruction loss: 0.059619445353746414
Training batch 12 / 32
Total batch reconstruction loss: 0.056717075407505035
Training batch 13 / 32
Total batch reconstruction loss: 0.0592103935778141
Training batch 14 / 32
Total batch reconstruction loss: 0.059184372425079346
Training batch 15 / 32
Total batch reconstruction loss: 0.0643271654844284
Training batch 16 / 32
Total batch reconstruction loss: 0.06172281876206398
Training batch 17 / 32
Total batch reconstruction loss: 0.05833592265844345
Training batch 18 / 32
Total batch reconstruction loss: 0.05798366665840149
Training batch 19 / 32
Total batch reconstruction loss: 0.06046612560749054
Training batch 20 / 32
Total batch reconstruction loss: 0.057096388190984726
Training batch 21 / 32
Total batch reconstruction loss: 0.06286589056253433
Training batch 22 / 32
Total batch reconstruction loss: 0.06127547845244408
Training batch 23 / 32
Total batch reconstruction loss: 0.055830489844083786
Training batch 24 / 32
Total batch reconstruction loss: 0.05465901643037796
Training batch 25 / 32
Total batch reconstruction loss: 0.056514207273721695
Training batch 26 / 32
Total batch reconstruction loss: 0.06245798617601395
Training batch 27 / 32
Total batch reconstruction loss: 0.06010126322507858
Training batch 28 / 32
Total batch reconstruction loss: 0.05678150802850723
Training batch 29 / 32
Total batch reconstruction loss: 0.057323768734931946
Training batch 30 / 32
Total batch reconstruction loss: 0.062131017446517944
Training batch 31 / 32
Total batch reconstruction loss: 0.061769794672727585
Training batch 32 / 32
Total batch reconstruction loss: 0.054095566272735596
Epoch [296/500], Train Loss: 0.0578, Validation Loss: 0.0571, Generator Loss: 12.0127, Discriminator Loss: 0.3294
Training epoch 297 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.059945590794086456
Training batch 2 / 32
Total batch reconstruction loss: 0.0607207752764225
Training batch 3 / 32
Total batch reconstruction loss: 0.06117361783981323
Training batch 4 / 32
Total batch reconstruction loss: 0.060940220952034
Training batch 5 / 32
Total batch reconstruction loss: 0.055659905076026917
Training batch 6 / 32
Total batch reconstruction loss: 0.05620293319225311
Training batch 7 / 32
Total batch reconstruction loss: 0.061555929481983185
Training batch 8 / 32
Total batch reconstruction loss: 0.06063668057322502
Training batch 9 / 32
Total batch reconstruction loss: 0.06011640280485153
Training batch 10 / 32
Total batch reconstruction loss: 0.061059363186359406
Training batch 11 / 32
Total batch reconstruction loss: 0.05811763554811478
Training batch 12 / 32
Total batch reconstruction loss: 0.061858829110860825
Training batch 13 / 32
Total batch reconstruction loss: 0.059870921075344086
Training batch 14 / 32
Total batch reconstruction loss: 0.06248452514410019
Training batch 15 / 32
Total batch reconstruction loss: 0.061511825770139694
Training batch 16 / 32
Total batch reconstruction loss: 0.05630677938461304
Training batch 17 / 32
Total batch reconstruction loss: 0.05801099166274071
Training batch 18 / 32
Total batch reconstruction loss: 0.06179387867450714
Training batch 19 / 32
Total batch reconstruction loss: 0.05832855403423309
Training batch 20 / 32
Total batch reconstruction loss: 0.05876174941658974
Training batch 21 / 32
Total batch reconstruction loss: 0.06261218339204788
Training batch 22 / 32
Total batch reconstruction loss: 0.06180894002318382
Training batch 23 / 32
Total batch reconstruction loss: 0.057407476007938385
Training batch 24 / 32
Total batch reconstruction loss: 0.05584530532360077
Training batch 25 / 32
Total batch reconstruction loss: 0.05998987331986427
Training batch 26 / 32
Total batch reconstruction loss: 0.05723526328802109
Training batch 27 / 32
Total batch reconstruction loss: 0.057313449680805206
Training batch 28 / 32
Total batch reconstruction loss: 0.059287939220666885
Training batch 29 / 32
Total batch reconstruction loss: 0.06549794971942902
Training batch 30 / 32
Total batch reconstruction loss: 0.0586237832903862
Training batch 31 / 32
Total batch reconstruction loss: 0.06416205316781998
Training batch 32 / 32
Total batch reconstruction loss: 0.05134546011686325
Epoch [297/500], Train Loss: 0.0581, Validation Loss: 0.0578, Generator Loss: 11.9926, Discriminator Loss: 0.2964
Training epoch 298 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.06136985123157501
Training batch 2 / 32
Total batch reconstruction loss: 0.05896243825554848
Training batch 3 / 32
Total batch reconstruction loss: 0.06136135011911392
Training batch 4 / 32
Total batch reconstruction loss: 0.05874870717525482
Training batch 5 / 32
Total batch reconstruction loss: 0.06391309946775436
Training batch 6 / 32
Total batch reconstruction loss: 0.05888703092932701
Training batch 7 / 32
Total batch reconstruction loss: 0.06100538372993469
Training batch 8 / 32
Total batch reconstruction loss: 0.06154875457286835
Training batch 9 / 32
Total batch reconstruction loss: 0.05904325470328331
Training batch 10 / 32
Total batch reconstruction loss: 0.05759533494710922
Training batch 11 / 32
Total batch reconstruction loss: 0.05857042968273163
Training batch 12 / 32
Total batch reconstruction loss: 0.057588353753089905
Training batch 13 / 32
Total batch reconstruction loss: 0.05908653885126114
Training batch 14 / 32
Total batch reconstruction loss: 0.061107248067855835
Training batch 15 / 32
Total batch reconstruction loss: 0.05651810020208359
Training batch 16 / 32
Total batch reconstruction loss: 0.05985849350690842
Training batch 17 / 32
Total batch reconstruction loss: 0.060039274394512177
Training batch 18 / 32
Total batch reconstruction loss: 0.06576819717884064
Training batch 19 / 32
Total batch reconstruction loss: 0.05837319791316986
Training batch 20 / 32
Total batch reconstruction loss: 0.05723006650805473
Training batch 21 / 32
Total batch reconstruction loss: 0.06265883147716522
Training batch 22 / 32
Total batch reconstruction loss: 0.05897243693470955
Training batch 23 / 32
Total batch reconstruction loss: 0.059799738228321075
Training batch 24 / 32
Total batch reconstruction loss: 0.05981994792819023
Training batch 25 / 32
Total batch reconstruction loss: 0.06121579185128212
Training batch 26 / 32
Total batch reconstruction loss: 0.0588289313018322
Training batch 27 / 32
Total batch reconstruction loss: 0.06048689782619476
Training batch 28 / 32
Total batch reconstruction loss: 0.057999152690172195
Training batch 29 / 32
Total batch reconstruction loss: 0.056253377348184586
Training batch 30 / 32
Total batch reconstruction loss: 0.057982854545116425
Training batch 31 / 32
Total batch reconstruction loss: 0.0689452514052391
Training batch 32 / 32
Total batch reconstruction loss: 0.05414295941591263
Epoch [298/500], Train Loss: 0.0588, Validation Loss: 0.0573, Generator Loss: 12.0319, Discriminator Loss: 0.3109
Training epoch 299 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.055157579481601715
Training batch 2 / 32
Total batch reconstruction loss: 0.059784308075904846
Training batch 3 / 32
Total batch reconstruction loss: 0.06033124402165413
Training batch 4 / 32
Total batch reconstruction loss: 0.05625356733798981
Training batch 5 / 32
Total batch reconstruction loss: 0.06404244899749756
Training batch 6 / 32
Total batch reconstruction loss: 0.06612128019332886
Training batch 7 / 32
Total batch reconstruction loss: 0.054172150790691376
Training batch 8 / 32
Total batch reconstruction loss: 0.05933067202568054
Training batch 9 / 32
Total batch reconstruction loss: 0.05867878347635269
Training batch 10 / 32
Total batch reconstruction loss: 0.057075224816799164
Training batch 11 / 32
Total batch reconstruction loss: 0.05806155875325203
Training batch 12 / 32
Total batch reconstruction loss: 0.05737055093050003
Training batch 13 / 32
Total batch reconstruction loss: 0.06016986817121506
Training batch 14 / 32
Total batch reconstruction loss: 0.05996980145573616
Training batch 15 / 32
Total batch reconstruction loss: 0.061999402940273285
Training batch 16 / 32
Total batch reconstruction loss: 0.06246691197156906
Training batch 17 / 32
Total batch reconstruction loss: 0.05536520481109619
Training batch 18 / 32
Total batch reconstruction loss: 0.0643639862537384
Training batch 19 / 32
Total batch reconstruction loss: 0.06238255649805069
Training batch 20 / 32
Total batch reconstruction loss: 0.06316687911748886
Training batch 21 / 32
Total batch reconstruction loss: 0.06192048639059067
Training batch 22 / 32
Total batch reconstruction loss: 0.05973019078373909
Training batch 23 / 32
Total batch reconstruction loss: 0.05851859971880913
Training batch 24 / 32
Total batch reconstruction loss: 0.05878473073244095
Training batch 25 / 32
Total batch reconstruction loss: 0.05409557372331619
Training batch 26 / 32
Total batch reconstruction loss: 0.059848882257938385
Training batch 27 / 32
Total batch reconstruction loss: 0.061780817806720734
Training batch 28 / 32
Total batch reconstruction loss: 0.054329629987478256
Training batch 29 / 32
Total batch reconstruction loss: 0.05457504093647003
Training batch 30 / 32
Total batch reconstruction loss: 0.05420476198196411
Training batch 31 / 32
Total batch reconstruction loss: 0.05824671685695648
Training batch 32 / 32
Total batch reconstruction loss: 0.05735307186841965
Epoch [299/500], Train Loss: 0.0574, Validation Loss: 0.0583, Generator Loss: 11.8898, Discriminator Loss: 0.2954
Training epoch 300 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.058803804218769073
Training batch 2 / 32
Total batch reconstruction loss: 0.05766797810792923
Training batch 3 / 32
Total batch reconstruction loss: 0.06212413311004639
Training batch 4 / 32
Total batch reconstruction loss: 0.05770564079284668
Training batch 5 / 32
Total batch reconstruction loss: 0.05810917913913727
Training batch 6 / 32
Total batch reconstruction loss: 0.06816339492797852
Training batch 7 / 32
Total batch reconstruction loss: 0.05869873985648155
Training batch 8 / 32
Total batch reconstruction loss: 0.061047784984111786
Training batch 9 / 32
Total batch reconstruction loss: 0.05801665782928467
Training batch 10 / 32
Total batch reconstruction loss: 0.059758566319942474
Training batch 11 / 32
Total batch reconstruction loss: 0.05611360818147659
Training batch 12 / 32
Total batch reconstruction loss: 0.06059829890727997
Training batch 13 / 32
Total batch reconstruction loss: 0.05716514214873314
Training batch 14 / 32
Total batch reconstruction loss: 0.0638950914144516
Training batch 15 / 32
Total batch reconstruction loss: 0.05839350447058678
Training batch 16 / 32
Total batch reconstruction loss: 0.05903803929686546
Training batch 17 / 32
Total batch reconstruction loss: 0.055737413465976715
Training batch 18 / 32
Total batch reconstruction loss: 0.05784003064036369
Training batch 19 / 32
Total batch reconstruction loss: 0.0576607882976532
Training batch 20 / 32
Total batch reconstruction loss: 0.061271101236343384
Training batch 21 / 32
Total batch reconstruction loss: 0.06332030892372131
Training batch 22 / 32
Total batch reconstruction loss: 0.05568080395460129
Training batch 23 / 32
Total batch reconstruction loss: 0.05844923481345177
Training batch 24 / 32
Total batch reconstruction loss: 0.062484968453645706
Training batch 25 / 32
Total batch reconstruction loss: 0.05599117651581764
Training batch 26 / 32
Total batch reconstruction loss: 0.05933795124292374
Training batch 27 / 32
Total batch reconstruction loss: 0.060842812061309814
Training batch 28 / 32
Total batch reconstruction loss: 0.0595087930560112
Training batch 29 / 32
Total batch reconstruction loss: 0.05483187362551689
Training batch 30 / 32
Total batch reconstruction loss: 0.06124367192387581
Training batch 31 / 32
Total batch reconstruction loss: 0.060971274971961975
Training batch 32 / 32
Total batch reconstruction loss: 0.056764934211969376
Epoch [300/500], Train Loss: 0.0578, Validation Loss: 0.0584, Generator Loss: 11.9329, Discriminator Loss: 0.3178
Training epoch 301 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.0568401962518692
Training batch 2 / 32
Total batch reconstruction loss: 0.05930112674832344
Training batch 3 / 32
Total batch reconstruction loss: 0.060966454446315765
Training batch 4 / 32
Total batch reconstruction loss: 0.060995735228061676
Training batch 5 / 32
Total batch reconstruction loss: 0.06038142368197441
Training batch 6 / 32
Total batch reconstruction loss: 0.057309817522764206
Training batch 7 / 32
Total batch reconstruction loss: 0.05737707018852234
Training batch 8 / 32
Total batch reconstruction loss: 0.06159908324480057
Training batch 9 / 32
Total batch reconstruction loss: 0.059604279696941376
Training batch 10 / 32
Total batch reconstruction loss: 0.0652836263179779
Training batch 11 / 32
Total batch reconstruction loss: 0.06731127202510834
Training batch 12 / 32
Total batch reconstruction loss: 0.059582967311143875
Training batch 13 / 32
Total batch reconstruction loss: 0.06149815395474434
Training batch 14 / 32
Total batch reconstruction loss: 0.05782632902264595
Training batch 15 / 32
Total batch reconstruction loss: 0.05790041759610176
Training batch 16 / 32
Total batch reconstruction loss: 0.060523852705955505
Training batch 17 / 32
Total batch reconstruction loss: 0.059482768177986145
Training batch 18 / 32
Total batch reconstruction loss: 0.05945326387882233
Training batch 19 / 32
Total batch reconstruction loss: 0.06120391935110092
Training batch 20 / 32
Total batch reconstruction loss: 0.06063390523195267
Training batch 21 / 32
Total batch reconstruction loss: 0.05720293149352074
Training batch 22 / 32
Total batch reconstruction loss: 0.058163516223430634
Training batch 23 / 32
Total batch reconstruction loss: 0.06235392764210701
Training batch 24 / 32
Total batch reconstruction loss: 0.0577370822429657
Training batch 25 / 32
Total batch reconstruction loss: 0.05906955525279045
Training batch 26 / 32
Total batch reconstruction loss: 0.05685404688119888
Training batch 27 / 32
Total batch reconstruction loss: 0.05960916727781296
Training batch 28 / 32
Total batch reconstruction loss: 0.06033296138048172
Training batch 29 / 32
Total batch reconstruction loss: 0.05835118144750595
Training batch 30 / 32
Total batch reconstruction loss: 0.067112997174263
Training batch 31 / 32
Total batch reconstruction loss: 0.06060875952243805
Training batch 32 / 32
Total batch reconstruction loss: 0.05310804396867752
Epoch [301/500], Train Loss: 0.0579, Validation Loss: 0.0578, Generator Loss: 12.0386, Discriminator Loss: 0.3162
Training epoch 302 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.062462348490953445
Training batch 2 / 32
Total batch reconstruction loss: 0.06130485609173775
Training batch 3 / 32
Total batch reconstruction loss: 0.05826834589242935
Training batch 4 / 32
Total batch reconstruction loss: 0.057167161256074905
Training batch 5 / 32
Total batch reconstruction loss: 0.05703403800725937
Training batch 6 / 32
Total batch reconstruction loss: 0.05792129784822464
Training batch 7 / 32
Total batch reconstruction loss: 0.058370042592287064
Training batch 8 / 32
Total batch reconstruction loss: 0.060123324394226074
Training batch 9 / 32
Total batch reconstruction loss: 0.059197716414928436
Training batch 10 / 32
Total batch reconstruction loss: 0.062355414032936096
Training batch 11 / 32
Total batch reconstruction loss: 0.062142737209796906
Training batch 12 / 32
Total batch reconstruction loss: 0.06177772954106331
Training batch 13 / 32
Total batch reconstruction loss: 0.058765292167663574
Training batch 14 / 32
Total batch reconstruction loss: 0.06498098373413086
Training batch 15 / 32
Total batch reconstruction loss: 0.061743125319480896
Training batch 16 / 32
Total batch reconstruction loss: 0.059245724231004715
Training batch 17 / 32
Total batch reconstruction loss: 0.0574229434132576
Training batch 18 / 32
Total batch reconstruction loss: 0.0587601438164711
Training batch 19 / 32
Total batch reconstruction loss: 0.05516576021909714
Training batch 20 / 32
Total batch reconstruction loss: 0.06079708784818649
Training batch 21 / 32
Total batch reconstruction loss: 0.057835653424263
Training batch 22 / 32
Total batch reconstruction loss: 0.06359878182411194
Training batch 23 / 32
Total batch reconstruction loss: 0.05781981348991394
Training batch 24 / 32
Total batch reconstruction loss: 0.06097135320305824
Training batch 25 / 32
Total batch reconstruction loss: 0.05664314329624176
Training batch 26 / 32
Total batch reconstruction loss: 0.05958592891693115
Training batch 27 / 32
Total batch reconstruction loss: 0.059819407761096954
Training batch 28 / 32
Total batch reconstruction loss: 0.05602622777223587
Training batch 29 / 32
Total batch reconstruction loss: 0.05715261399745941
Training batch 30 / 32
Total batch reconstruction loss: 0.06029170751571655
Training batch 31 / 32
Total batch reconstruction loss: 0.05504029244184494
Training batch 32 / 32
Total batch reconstruction loss: 0.05480637773871422
Epoch [302/500], Train Loss: 0.0576, Validation Loss: 0.0585, Generator Loss: 11.9037, Discriminator Loss: 0.3207
Training epoch 303 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.06027444824576378
Training batch 2 / 32
Total batch reconstruction loss: 0.05936059355735779
Training batch 3 / 32
Total batch reconstruction loss: 0.05814366787672043
Training batch 4 / 32
Total batch reconstruction loss: 0.06204365938901901
Training batch 5 / 32
Total batch reconstruction loss: 0.059262000024318695
Training batch 6 / 32
Total batch reconstruction loss: 0.059737369418144226
Training batch 7 / 32
Total batch reconstruction loss: 0.05896730720996857
Training batch 8 / 32
Total batch reconstruction loss: 0.05769231170415878
Training batch 9 / 32
Total batch reconstruction loss: 0.06120598316192627
Training batch 10 / 32
Total batch reconstruction loss: 0.06076066941022873
Training batch 11 / 32
Total batch reconstruction loss: 0.06223475933074951
Training batch 12 / 32
Total batch reconstruction loss: 0.058667197823524475
Training batch 13 / 32
Total batch reconstruction loss: 0.05929326266050339
Training batch 14 / 32
Total batch reconstruction loss: 0.06133511662483215
Training batch 15 / 32
Total batch reconstruction loss: 0.06430487334728241
Training batch 16 / 32
Total batch reconstruction loss: 0.059711307287216187
Training batch 17 / 32
Total batch reconstruction loss: 0.05811591446399689
Training batch 18 / 32
Total batch reconstruction loss: 0.05724326893687248
Training batch 19 / 32
Total batch reconstruction loss: 0.06101664900779724
Training batch 20 / 32
Total batch reconstruction loss: 0.05896858125925064
Training batch 21 / 32
Total batch reconstruction loss: 0.05652604624629021
Training batch 22 / 32
Total batch reconstruction loss: 0.057877667248249054
Training batch 23 / 32
Total batch reconstruction loss: 0.057572200894355774
Training batch 24 / 32
Total batch reconstruction loss: 0.058164507150650024
Training batch 25 / 32
Total batch reconstruction loss: 0.057989899069070816
Training batch 26 / 32
Total batch reconstruction loss: 0.05925245210528374
Training batch 27 / 32
Total batch reconstruction loss: 0.06106743589043617
Training batch 28 / 32
Total batch reconstruction loss: 0.05780934542417526
Training batch 29 / 32
Total batch reconstruction loss: 0.05718504264950752
Training batch 30 / 32
Total batch reconstruction loss: 0.062272198498249054
Training batch 31 / 32
Total batch reconstruction loss: 0.0602044016122818
Training batch 32 / 32
Total batch reconstruction loss: 0.04573797434568405
Epoch [303/500], Train Loss: 0.0575, Validation Loss: 0.0575, Generator Loss: 11.8816, Discriminator Loss: 0.3152
Training epoch 304 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.05939498543739319
Training batch 2 / 32
Total batch reconstruction loss: 0.062330394983291626
Training batch 3 / 32
Total batch reconstruction loss: 0.06261536478996277
Training batch 4 / 32
Total batch reconstruction loss: 0.05952819436788559
Training batch 5 / 32
Total batch reconstruction loss: 0.062499646097421646
Training batch 6 / 32
Total batch reconstruction loss: 0.056735046207904816
Training batch 7 / 32
Total batch reconstruction loss: 0.058656781911849976
Training batch 8 / 32
Total batch reconstruction loss: 0.05675281584262848
Training batch 9 / 32
Total batch reconstruction loss: 0.05453126132488251
Training batch 10 / 32
Total batch reconstruction loss: 0.061107464134693146
Training batch 11 / 32
Total batch reconstruction loss: 0.060267865657806396
Training batch 12 / 32
Total batch reconstruction loss: 0.059118837118148804
Training batch 13 / 32
Total batch reconstruction loss: 0.05952676013112068
Training batch 14 / 32
Total batch reconstruction loss: 0.06277766823768616
Training batch 15 / 32
Total batch reconstruction loss: 0.061236221343278885
Training batch 16 / 32
Total batch reconstruction loss: 0.062378622591495514
Training batch 17 / 32
Total batch reconstruction loss: 0.05680584907531738
Training batch 18 / 32
Total batch reconstruction loss: 0.05901658162474632
Training batch 19 / 32
Total batch reconstruction loss: 0.05665181949734688
Training batch 20 / 32
Total batch reconstruction loss: 0.05917362496256828
Training batch 21 / 32
Total batch reconstruction loss: 0.0600520595908165
Training batch 22 / 32
Total batch reconstruction loss: 0.06183192878961563
Training batch 23 / 32
Total batch reconstruction loss: 0.055864203721284866
Training batch 24 / 32
Total batch reconstruction loss: 0.06166493147611618
Training batch 25 / 32
Total batch reconstruction loss: 0.06189967319369316
Training batch 26 / 32
Total batch reconstruction loss: 0.05457822233438492
Training batch 27 / 32
Total batch reconstruction loss: 0.05913824588060379
Training batch 28 / 32
Total batch reconstruction loss: 0.05598588287830353
Training batch 29 / 32
Total batch reconstruction loss: 0.05786031112074852
Training batch 30 / 32
Total batch reconstruction loss: 0.05994349718093872
Training batch 31 / 32
Total batch reconstruction loss: 0.05837350711226463
Training batch 32 / 32
Total batch reconstruction loss: 0.05143657326698303
Epoch [304/500], Train Loss: 0.0575, Validation Loss: 0.0589, Generator Loss: 11.8832, Discriminator Loss: 0.3158
Training epoch 305 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.05702044069766998
Training batch 2 / 32
Total batch reconstruction loss: 0.055490247905254364
Training batch 3 / 32
Total batch reconstruction loss: 0.061000652611255646
Training batch 4 / 32
Total batch reconstruction loss: 0.05854387581348419
Training batch 5 / 32
Total batch reconstruction loss: 0.05766081437468529
Training batch 6 / 32
Total batch reconstruction loss: 0.06149636209011078
Training batch 7 / 32
Total batch reconstruction loss: 0.06025725603103638
Training batch 8 / 32
Total batch reconstruction loss: 0.060272667557001114
Training batch 9 / 32
Total batch reconstruction loss: 0.06086844950914383
Training batch 10 / 32
Total batch reconstruction loss: 0.06265988200902939
Training batch 11 / 32
Total batch reconstruction loss: 0.054950051009655
Training batch 12 / 32
Total batch reconstruction loss: 0.0621296651661396
Training batch 13 / 32
Total batch reconstruction loss: 0.06136090308427811
Training batch 14 / 32
Total batch reconstruction loss: 0.06087125092744827
Training batch 15 / 32
Total batch reconstruction loss: 0.05502844601869583
Training batch 16 / 32
Total batch reconstruction loss: 0.05900512635707855
Training batch 17 / 32
Total batch reconstruction loss: 0.05837658792734146
Training batch 18 / 32
Total batch reconstruction loss: 0.05934900790452957
Training batch 19 / 32
Total batch reconstruction loss: 0.057525452226400375
Training batch 20 / 32
Total batch reconstruction loss: 0.05616704374551773
Training batch 21 / 32
Total batch reconstruction loss: 0.0649951845407486
Training batch 22 / 32
Total batch reconstruction loss: 0.06190219521522522
Training batch 23 / 32
Total batch reconstruction loss: 0.05975779891014099
Training batch 24 / 32
Total batch reconstruction loss: 0.0619385689496994
Training batch 25 / 32
Total batch reconstruction loss: 0.06094564497470856
Training batch 26 / 32
Total batch reconstruction loss: 0.06375852227210999
Training batch 27 / 32
Total batch reconstruction loss: 0.05980575084686279
Training batch 28 / 32
Total batch reconstruction loss: 0.05751163512468338
Training batch 29 / 32
Total batch reconstruction loss: 0.056906379759311676
Training batch 30 / 32
Total batch reconstruction loss: 0.0580572709441185
Training batch 31 / 32
Total batch reconstruction loss: 0.05604848265647888
Training batch 32 / 32
Total batch reconstruction loss: 0.06510588526725769
Epoch [305/500], Train Loss: 0.0583, Validation Loss: 0.0581, Generator Loss: 11.9842, Discriminator Loss: 0.3163
Training epoch 306 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.05641167610883713
Training batch 2 / 32
Total batch reconstruction loss: 0.05823340266942978
Training batch 3 / 32
Total batch reconstruction loss: 0.05808630958199501
Training batch 4 / 32
Total batch reconstruction loss: 0.05608413368463516
Training batch 5 / 32
Total batch reconstruction loss: 0.061395809054374695
Training batch 6 / 32
Total batch reconstruction loss: 0.0591094009578228
Training batch 7 / 32
Total batch reconstruction loss: 0.0563410222530365
Training batch 8 / 32
Total batch reconstruction loss: 0.057902947068214417
Training batch 9 / 32
Total batch reconstruction loss: 0.06080225855112076
Training batch 10 / 32
Total batch reconstruction loss: 0.06323312222957611
Training batch 11 / 32
Total batch reconstruction loss: 0.05819214880466461
Training batch 12 / 32
Total batch reconstruction loss: 0.05776313692331314
Training batch 13 / 32
Total batch reconstruction loss: 0.05983589589595795
Training batch 14 / 32
Total batch reconstruction loss: 0.05652747303247452
Training batch 15 / 32
Total batch reconstruction loss: 0.0621437206864357
Training batch 16 / 32
Total batch reconstruction loss: 0.05571543052792549
Training batch 17 / 32
Total batch reconstruction loss: 0.05733150243759155
Training batch 18 / 32
Total batch reconstruction loss: 0.0615999810397625
Training batch 19 / 32
Total batch reconstruction loss: 0.05828756093978882
Training batch 20 / 32
Total batch reconstruction loss: 0.06226806342601776
Training batch 21 / 32
Total batch reconstruction loss: 0.05890563502907753
Training batch 22 / 32
Total batch reconstruction loss: 0.059060417115688324
Training batch 23 / 32
Total batch reconstruction loss: 0.06212374567985535
Training batch 24 / 32
Total batch reconstruction loss: 0.05925287306308746
Training batch 25 / 32
Total batch reconstruction loss: 0.06030716374516487
Training batch 26 / 32
Total batch reconstruction loss: 0.06624913215637207
Training batch 27 / 32
Total batch reconstruction loss: 0.05718231946229935
Training batch 28 / 32
Total batch reconstruction loss: 0.05588182806968689
Training batch 29 / 32
Total batch reconstruction loss: 0.058478228747844696
Training batch 30 / 32
Total batch reconstruction loss: 0.056267622858285904
Training batch 31 / 32
Total batch reconstruction loss: 0.06388498842716217
Training batch 32 / 32
Total batch reconstruction loss: 0.06043912470340729
Epoch [306/500], Train Loss: 0.0572, Validation Loss: 0.0584, Generator Loss: 11.9095, Discriminator Loss: 0.3211
Training epoch 307 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.05924005061388016
Training batch 2 / 32
Total batch reconstruction loss: 0.05622922256588936
Training batch 3 / 32
Total batch reconstruction loss: 0.05610010027885437
Training batch 4 / 32
Total batch reconstruction loss: 0.061797648668289185
Training batch 5 / 32
Total batch reconstruction loss: 0.05885925889015198
Training batch 6 / 32
Total batch reconstruction loss: 0.05527506768703461
Training batch 7 / 32
Total batch reconstruction loss: 0.06063006818294525
Training batch 8 / 32
Total batch reconstruction loss: 0.058439258486032486
Training batch 9 / 32
Total batch reconstruction loss: 0.058471836149692535
Training batch 10 / 32
Total batch reconstruction loss: 0.05577463656663895
Training batch 11 / 32
Total batch reconstruction loss: 0.05910447612404823
Training batch 12 / 32
Total batch reconstruction loss: 0.059729114174842834
Training batch 13 / 32
Total batch reconstruction loss: 0.06050776690244675
Training batch 14 / 32
Total batch reconstruction loss: 0.06085936725139618
Training batch 15 / 32
Total batch reconstruction loss: 0.06115442141890526
Training batch 16 / 32
Total batch reconstruction loss: 0.05883915722370148
Training batch 17 / 32
Total batch reconstruction loss: 0.05847906321287155
Training batch 18 / 32
Total batch reconstruction loss: 0.06134827435016632
Training batch 19 / 32
Total batch reconstruction loss: 0.061312951147556305
Training batch 20 / 32
Total batch reconstruction loss: 0.05777767673134804
Training batch 21 / 32
Total batch reconstruction loss: 0.061306171119213104
Training batch 22 / 32
Total batch reconstruction loss: 0.059916578233242035
Training batch 23 / 32
Total batch reconstruction loss: 0.06129217892885208
Training batch 24 / 32
Total batch reconstruction loss: 0.05986950173974037
Training batch 25 / 32
Total batch reconstruction loss: 0.05465453863143921
Training batch 26 / 32
Total batch reconstruction loss: 0.05933289974927902
Training batch 27 / 32
Total batch reconstruction loss: 0.05780576169490814
Training batch 28 / 32
Total batch reconstruction loss: 0.06490691006183624
Training batch 29 / 32
Total batch reconstruction loss: 0.05574043095111847
Training batch 30 / 32
Total batch reconstruction loss: 0.05949239060282707
Training batch 31 / 32
Total batch reconstruction loss: 0.05959829315543175
Training batch 32 / 32
Total batch reconstruction loss: 0.05304493382573128
Epoch [307/500], Train Loss: 0.0574, Validation Loss: 0.0575, Generator Loss: 11.8691, Discriminator Loss: 0.3020
Training epoch 308 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.057190701365470886
Training batch 2 / 32
Total batch reconstruction loss: 0.061634741723537445
Training batch 3 / 32
Total batch reconstruction loss: 0.06189114600419998
Training batch 4 / 32
Total batch reconstruction loss: 0.05604788661003113
Training batch 5 / 32
Total batch reconstruction loss: 0.06232475861907005
Training batch 6 / 32
Total batch reconstruction loss: 0.05949411541223526
Training batch 7 / 32
Total batch reconstruction loss: 0.06172589212656021
Training batch 8 / 32
Total batch reconstruction loss: 0.05834903568029404
Training batch 9 / 32
Total batch reconstruction loss: 0.05897878482937813
Training batch 10 / 32
Total batch reconstruction loss: 0.05674564838409424
Training batch 11 / 32
Total batch reconstruction loss: 0.05806097760796547
Training batch 12 / 32
Total batch reconstruction loss: 0.05709201097488403
Training batch 13 / 32
Total batch reconstruction loss: 0.06066719442605972
Training batch 14 / 32
Total batch reconstruction loss: 0.0567261278629303
Training batch 15 / 32
Total batch reconstruction loss: 0.0565943717956543
Training batch 16 / 32
Total batch reconstruction loss: 0.057468630373477936
Training batch 17 / 32
Total batch reconstruction loss: 0.06406812369823456
Training batch 18 / 32
Total batch reconstruction loss: 0.05869092792272568
Training batch 19 / 32
Total batch reconstruction loss: 0.05746246874332428
Training batch 20 / 32
Total batch reconstruction loss: 0.06374739110469818
Training batch 21 / 32
Total batch reconstruction loss: 0.06273806095123291
Training batch 22 / 32
Total batch reconstruction loss: 0.05762588232755661
Training batch 23 / 32
Total batch reconstruction loss: 0.06095290929079056
Training batch 24 / 32
Total batch reconstruction loss: 0.056812990456819534
Training batch 25 / 32
Total batch reconstruction loss: 0.05814481899142265
Training batch 26 / 32
Total batch reconstruction loss: 0.05796617269515991
Training batch 27 / 32
Total batch reconstruction loss: 0.06374545395374298
Training batch 28 / 32
Total batch reconstruction loss: 0.06346577405929565
Training batch 29 / 32
Total batch reconstruction loss: 0.060368143022060394
Training batch 30 / 32
Total batch reconstruction loss: 0.055996306240558624
Training batch 31 / 32
Total batch reconstruction loss: 0.05702996999025345
Training batch 32 / 32
Total batch reconstruction loss: 0.0588320717215538
Epoch [308/500], Train Loss: 0.0578, Validation Loss: 0.0588, Generator Loss: 11.9382, Discriminator Loss: 0.3139
Training epoch 309 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.05707082897424698
Training batch 2 / 32
Total batch reconstruction loss: 0.06468409299850464
Training batch 3 / 32
Total batch reconstruction loss: 0.06034231185913086
Training batch 4 / 32
Total batch reconstruction loss: 0.06711024791002274
Training batch 5 / 32
Total batch reconstruction loss: 0.055490776896476746
Training batch 6 / 32
Total batch reconstruction loss: 0.06639277935028076
Training batch 7 / 32
Total batch reconstruction loss: 0.0590691864490509
Training batch 8 / 32
Total batch reconstruction loss: 0.059219732880592346
Training batch 9 / 32
Total batch reconstruction loss: 0.06382720917463303
Training batch 10 / 32
Total batch reconstruction loss: 0.060783591121435165
Training batch 11 / 32
Total batch reconstruction loss: 0.06375698745250702
Training batch 12 / 32
Total batch reconstruction loss: 0.059846337884664536
Training batch 13 / 32
Total batch reconstruction loss: 0.05990512669086456
Training batch 14 / 32
Total batch reconstruction loss: 0.055612049996852875
Training batch 15 / 32
Total batch reconstruction loss: 0.05771522596478462
Training batch 16 / 32
Total batch reconstruction loss: 0.05962773412466049
Training batch 17 / 32
Total batch reconstruction loss: 0.05780502408742905
Training batch 18 / 32
Total batch reconstruction loss: 0.06001882255077362
Training batch 19 / 32
Total batch reconstruction loss: 0.05985371768474579
Training batch 20 / 32
Total batch reconstruction loss: 0.05704307556152344
Training batch 21 / 32
Total batch reconstruction loss: 0.05787127837538719
Training batch 22 / 32
Total batch reconstruction loss: 0.05737840011715889
Training batch 23 / 32
Total batch reconstruction loss: 0.05815115198493004
Training batch 24 / 32
Total batch reconstruction loss: 0.05961929261684418
Training batch 25 / 32
Total batch reconstruction loss: 0.05723438784480095
Training batch 26 / 32
Total batch reconstruction loss: 0.053667761385440826
Training batch 27 / 32
Total batch reconstruction loss: 0.05745059251785278
Training batch 28 / 32
Total batch reconstruction loss: 0.06075694039463997
Training batch 29 / 32
Total batch reconstruction loss: 0.05814656242728233
Training batch 30 / 32
Total batch reconstruction loss: 0.06009671092033386
Training batch 31 / 32
Total batch reconstruction loss: 0.060431189835071564
Training batch 32 / 32
Total batch reconstruction loss: 0.055527083575725555
Epoch [309/500], Train Loss: 0.0578, Validation Loss: 0.0581, Generator Loss: 11.9568, Discriminator Loss: 0.3108
Training epoch 310 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.0587836317718029
Training batch 2 / 32
Total batch reconstruction loss: 0.054229676723480225
Training batch 3 / 32
Total batch reconstruction loss: 0.05670159310102463
Training batch 4 / 32
Total batch reconstruction loss: 0.05614966154098511
Training batch 5 / 32
Total batch reconstruction loss: 0.059142984449863434
Training batch 6 / 32
Total batch reconstruction loss: 0.06157046556472778
Training batch 7 / 32
Total batch reconstruction loss: 0.062443822622299194
Training batch 8 / 32
Total batch reconstruction loss: 0.06092372536659241
Training batch 9 / 32
Total batch reconstruction loss: 0.05427001416683197
Training batch 10 / 32
Total batch reconstruction loss: 0.06160425394773483
Training batch 11 / 32
Total batch reconstruction loss: 0.06044119596481323
Training batch 12 / 32
Total batch reconstruction loss: 0.059567973017692566
Training batch 13 / 32
Total batch reconstruction loss: 0.05866871401667595
Training batch 14 / 32
Total batch reconstruction loss: 0.05626092478632927
Training batch 15 / 32
Total batch reconstruction loss: 0.061403825879096985
Training batch 16 / 32
Total batch reconstruction loss: 0.05445634946227074
Training batch 17 / 32
Total batch reconstruction loss: 0.0630493089556694
Training batch 18 / 32
Total batch reconstruction loss: 0.060073163360357285
Training batch 19 / 32
Total batch reconstruction loss: 0.058894697576761246
Training batch 20 / 32
Total batch reconstruction loss: 0.06189936399459839
Training batch 21 / 32
Total batch reconstruction loss: 0.05834250897169113
Training batch 22 / 32
Total batch reconstruction loss: 0.057153478264808655
Training batch 23 / 32
Total batch reconstruction loss: 0.05849982053041458
Training batch 24 / 32
Total batch reconstruction loss: 0.05871807411313057
Training batch 25 / 32
Total batch reconstruction loss: 0.055325575172901154
Training batch 26 / 32
Total batch reconstruction loss: 0.068764328956604
Training batch 27 / 32
Total batch reconstruction loss: 0.05643908679485321
Training batch 28 / 32
Total batch reconstruction loss: 0.059435926377773285
Training batch 29 / 32
Total batch reconstruction loss: 0.06098530441522598
Training batch 30 / 32
Total batch reconstruction loss: 0.061679206788539886
Training batch 31 / 32
Total batch reconstruction loss: 0.06188088282942772
Training batch 32 / 32
Total batch reconstruction loss: 0.05254436656832695
Epoch [310/500], Train Loss: 0.0576, Validation Loss: 0.0574, Generator Loss: 11.8850, Discriminator Loss: 0.3172
Training epoch 311 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.05701425299048424
Training batch 2 / 32
Total batch reconstruction loss: 0.05954917520284653
Training batch 3 / 32
Total batch reconstruction loss: 0.05565432459115982
Training batch 4 / 32
Total batch reconstruction loss: 0.0620536133646965
Training batch 5 / 32
Total batch reconstruction loss: 0.06166520714759827
Training batch 6 / 32
Total batch reconstruction loss: 0.05769813805818558
Training batch 7 / 32
Total batch reconstruction loss: 0.059682197868824005
Training batch 8 / 32
Total batch reconstruction loss: 0.05964372307062149
Training batch 9 / 32
Total batch reconstruction loss: 0.05900580435991287
Training batch 10 / 32
Total batch reconstruction loss: 0.057932548224925995
Training batch 11 / 32
Total batch reconstruction loss: 0.06060048192739487
Training batch 12 / 32
Total batch reconstruction loss: 0.06085137650370598
Training batch 13 / 32
Total batch reconstruction loss: 0.06071190536022186
Training batch 14 / 32
Total batch reconstruction loss: 0.05658788979053497
Training batch 15 / 32
Total batch reconstruction loss: 0.06162169948220253
Training batch 16 / 32
Total batch reconstruction loss: 0.05758494883775711
Training batch 17 / 32
Total batch reconstruction loss: 0.05809197202324867
Training batch 18 / 32
Total batch reconstruction loss: 0.06062322109937668
Training batch 19 / 32
Total batch reconstruction loss: 0.05855714529752731
Training batch 20 / 32
Total batch reconstruction loss: 0.05958303436636925
Training batch 21 / 32
Total batch reconstruction loss: 0.060062095522880554
Training batch 22 / 32
Total batch reconstruction loss: 0.057319968938827515
Training batch 23 / 32
Total batch reconstruction loss: 0.05786024034023285
Training batch 24 / 32
Total batch reconstruction loss: 0.06258903443813324
Training batch 25 / 32
Total batch reconstruction loss: 0.057639818638563156
Training batch 26 / 32
Total batch reconstruction loss: 0.05905129015445709
Training batch 27 / 32
Total batch reconstruction loss: 0.06050585210323334
Training batch 28 / 32
Total batch reconstruction loss: 0.06168916076421738
Training batch 29 / 32
Total batch reconstruction loss: 0.05576271563768387
Training batch 30 / 32
Total batch reconstruction loss: 0.06005307286977768
Training batch 31 / 32
Total batch reconstruction loss: 0.0574224591255188
Training batch 32 / 32
Total batch reconstruction loss: 0.10383869707584381
Epoch [311/500], Train Loss: 0.0588, Validation Loss: 0.0576, Generator Loss: 12.1771, Discriminator Loss: 0.3247
Training epoch 312 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.05804048851132393
Training batch 2 / 32
Total batch reconstruction loss: 0.05713408440351486
Training batch 3 / 32
Total batch reconstruction loss: 0.05584526062011719
Training batch 4 / 32
Total batch reconstruction loss: 0.060695067048072815
Training batch 5 / 32
Total batch reconstruction loss: 0.05723949894309044
Training batch 6 / 32
Total batch reconstruction loss: 0.06119588762521744
Training batch 7 / 32
Total batch reconstruction loss: 0.058918632566928864
Training batch 8 / 32
Total batch reconstruction loss: 0.05815181881189346
Training batch 9 / 32
Total batch reconstruction loss: 0.058756425976753235
Training batch 10 / 32
Total batch reconstruction loss: 0.06512511521577835
Training batch 11 / 32
Total batch reconstruction loss: 0.05965302139520645
Training batch 12 / 32
Total batch reconstruction loss: 0.061208683997392654
Training batch 13 / 32
Total batch reconstruction loss: 0.05679689720273018
Training batch 14 / 32
Total batch reconstruction loss: 0.06056346744298935
Training batch 15 / 32
Total batch reconstruction loss: 0.06114467978477478
Training batch 16 / 32
Total batch reconstruction loss: 0.06306906044483185
Training batch 17 / 32
Total batch reconstruction loss: 0.05801547318696976
Training batch 18 / 32
Total batch reconstruction loss: 0.05771758034825325
Training batch 19 / 32
Total batch reconstruction loss: 0.0604243166744709
Training batch 20 / 32
Total batch reconstruction loss: 0.055229298770427704
Training batch 21 / 32
Total batch reconstruction loss: 0.057585209608078
Training batch 22 / 32
Total batch reconstruction loss: 0.05735884606838226
Training batch 23 / 32
Total batch reconstruction loss: 0.06256458163261414
Training batch 24 / 32
Total batch reconstruction loss: 0.06191140413284302
Training batch 25 / 32
Total batch reconstruction loss: 0.057319276034832
Training batch 26 / 32
Total batch reconstruction loss: 0.05874214321374893
Training batch 27 / 32
Total batch reconstruction loss: 0.06580477207899094
Training batch 28 / 32
Total batch reconstruction loss: 0.05993804335594177
Training batch 29 / 32
Total batch reconstruction loss: 0.053711675107479095
Training batch 30 / 32
Total batch reconstruction loss: 0.05765869468450546
Training batch 31 / 32
Total batch reconstruction loss: 0.058706995099782944
Training batch 32 / 32
Total batch reconstruction loss: 0.07915555685758591
Epoch [312/500], Train Loss: 0.0583, Validation Loss: 0.0577, Generator Loss: 12.0330, Discriminator Loss: 0.3262
Training epoch 313 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.06321091949939728
Training batch 2 / 32
Total batch reconstruction loss: 0.05672247335314751
Training batch 3 / 32
Total batch reconstruction loss: 0.057070739567279816
Training batch 4 / 32
Total batch reconstruction loss: 0.055861033499240875
Training batch 5 / 32
Total batch reconstruction loss: 0.06363873928785324
Training batch 6 / 32
Total batch reconstruction loss: 0.05774039402604103
Training batch 7 / 32
Total batch reconstruction loss: 0.061890341341495514
Training batch 8 / 32
Total batch reconstruction loss: 0.05847370997071266
Training batch 9 / 32
Total batch reconstruction loss: 0.06496644020080566
Training batch 10 / 32
Total batch reconstruction loss: 0.06085069850087166
Training batch 11 / 32
Total batch reconstruction loss: 0.059495799243450165
Training batch 12 / 32
Total batch reconstruction loss: 0.05926006659865379
Training batch 13 / 32
Total batch reconstruction loss: 0.056959837675094604
Training batch 14 / 32
Total batch reconstruction loss: 0.06426501274108887
Training batch 15 / 32
Total batch reconstruction loss: 0.06398102641105652
Training batch 16 / 32
Total batch reconstruction loss: 0.06081476807594299
Training batch 17 / 32
Total batch reconstruction loss: 0.0603431798517704
Training batch 18 / 32
Total batch reconstruction loss: 0.05493176355957985
Training batch 19 / 32
Total batch reconstruction loss: 0.060322441160678864
Training batch 20 / 32
Total batch reconstruction loss: 0.05789630487561226
Training batch 21 / 32
Total batch reconstruction loss: 0.062139470130205154
Training batch 22 / 32
Total batch reconstruction loss: 0.05780966579914093
Training batch 23 / 32
Total batch reconstruction loss: 0.05695705860853195
Training batch 24 / 32
Total batch reconstruction loss: 0.06198924034833908
Training batch 25 / 32
Total batch reconstruction loss: 0.057452987879514694
Training batch 26 / 32
Total batch reconstruction loss: 0.05997210741043091
Training batch 27 / 32
Total batch reconstruction loss: 0.05488947406411171
Training batch 28 / 32
Total batch reconstruction loss: 0.05965607613325119
Training batch 29 / 32
Total batch reconstruction loss: 0.05961476266384125
Training batch 30 / 32
Total batch reconstruction loss: 0.06252320110797882
Training batch 31 / 32
Total batch reconstruction loss: 0.059079915285110474
Training batch 32 / 32
Total batch reconstruction loss: 0.0758291631937027
Epoch [313/500], Train Loss: 0.0588, Validation Loss: 0.0597, Generator Loss: 12.1204, Discriminator Loss: 0.3059
Training epoch 314 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.06312749534845352
Training batch 2 / 32
Total batch reconstruction loss: 0.06635765731334686
Training batch 3 / 32
Total batch reconstruction loss: 0.058163899928331375
Training batch 4 / 32
Total batch reconstruction loss: 0.05896957218647003
Training batch 5 / 32
Total batch reconstruction loss: 0.06218237057328224
Training batch 6 / 32
Total batch reconstruction loss: 0.058285657316446304
Training batch 7 / 32
Total batch reconstruction loss: 0.05792655795812607
Training batch 8 / 32
Total batch reconstruction loss: 0.06091337651014328
Training batch 9 / 32
Total batch reconstruction loss: 0.06105806678533554
Training batch 10 / 32
Total batch reconstruction loss: 0.05708477273583412
Training batch 11 / 32
Total batch reconstruction loss: 0.058565445244312286
Training batch 12 / 32
Total batch reconstruction loss: 0.056889548897743225
Training batch 13 / 32
Total batch reconstruction loss: 0.05726705119013786
Training batch 14 / 32
Total batch reconstruction loss: 0.05998948961496353
Training batch 15 / 32
Total batch reconstruction loss: 0.057228587567806244
Training batch 16 / 32
Total batch reconstruction loss: 0.06405334174633026
Training batch 17 / 32
Total batch reconstruction loss: 0.05590685456991196
Training batch 18 / 32
Total batch reconstruction loss: 0.05978094041347504
Training batch 19 / 32
Total batch reconstruction loss: 0.0645103007555008
Training batch 20 / 32
Total batch reconstruction loss: 0.061417356133461
Training batch 21 / 32
Total batch reconstruction loss: 0.06033393368124962
Training batch 22 / 32
Total batch reconstruction loss: 0.06020788103342056
Training batch 23 / 32
Total batch reconstruction loss: 0.06262893229722977
Training batch 24 / 32
Total batch reconstruction loss: 0.062052737921476364
Training batch 25 / 32
Total batch reconstruction loss: 0.057656411081552505
Training batch 26 / 32
Total batch reconstruction loss: 0.060118768364191055
Training batch 27 / 32
Total batch reconstruction loss: 0.053379517048597336
Training batch 28 / 32
Total batch reconstruction loss: 0.05954650044441223
Training batch 29 / 32
Total batch reconstruction loss: 0.05966585874557495
Training batch 30 / 32
Total batch reconstruction loss: 0.061300650238990784
Training batch 31 / 32
Total batch reconstruction loss: 0.057133715599775314
Training batch 32 / 32
Total batch reconstruction loss: 0.04705610126256943
Epoch [314/500], Train Loss: 0.0580, Validation Loss: 0.0565, Generator Loss: 11.9474, Discriminator Loss: 0.3190
Training epoch 315 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.05566449463367462
Training batch 2 / 32
Total batch reconstruction loss: 0.06164960190653801
Training batch 3 / 32
Total batch reconstruction loss: 0.06322943419218063
Training batch 4 / 32
Total batch reconstruction loss: 0.06426741182804108
Training batch 5 / 32
Total batch reconstruction loss: 0.05924413353204727
Training batch 6 / 32
Total batch reconstruction loss: 0.05883067101240158
Training batch 7 / 32
Total batch reconstruction loss: 0.058354418724775314
Training batch 8 / 32
Total batch reconstruction loss: 0.057550735771656036
Training batch 9 / 32
Total batch reconstruction loss: 0.0574353002011776
Training batch 10 / 32
Total batch reconstruction loss: 0.056960027664899826
Training batch 11 / 32
Total batch reconstruction loss: 0.060271866619586945
Training batch 12 / 32
Total batch reconstruction loss: 0.05622429773211479
Training batch 13 / 32
Total batch reconstruction loss: 0.05803662911057472
Training batch 14 / 32
Total batch reconstruction loss: 0.061117447912693024
Training batch 15 / 32
Total batch reconstruction loss: 0.05943109840154648
Training batch 16 / 32
Total batch reconstruction loss: 0.05697353556752205
Training batch 17 / 32
Total batch reconstruction loss: 0.05785481631755829
Training batch 18 / 32
Total batch reconstruction loss: 0.05737072229385376
Training batch 19 / 32
Total batch reconstruction loss: 0.05595274269580841
Training batch 20 / 32
Total batch reconstruction loss: 0.056077904999256134
Training batch 21 / 32
Total batch reconstruction loss: 0.0670156255364418
Training batch 22 / 32
Total batch reconstruction loss: 0.05363702028989792
Training batch 23 / 32
Total batch reconstruction loss: 0.06086857616901398
Training batch 24 / 32
Total batch reconstruction loss: 0.054966822266578674
Training batch 25 / 32
Total batch reconstruction loss: 0.05548979341983795
Training batch 26 / 32
Total batch reconstruction loss: 0.06138300895690918
Training batch 27 / 32
Total batch reconstruction loss: 0.05782371014356613
Training batch 28 / 32
Total batch reconstruction loss: 0.06130163371562958
Training batch 29 / 32
Total batch reconstruction loss: 0.057108014822006226
Training batch 30 / 32
Total batch reconstruction loss: 0.05937087535858154
Training batch 31 / 32
Total batch reconstruction loss: 0.06728842854499817
Training batch 32 / 32
Total batch reconstruction loss: 0.05867047235369682
Epoch [315/500], Train Loss: 0.0571, Validation Loss: 0.0575, Generator Loss: 11.8686, Discriminator Loss: 0.3075
Training epoch 316 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.06374096870422363
Training batch 2 / 32
Total batch reconstruction loss: 0.06296508759260178
Training batch 3 / 32
Total batch reconstruction loss: 0.0695902407169342
Training batch 4 / 32
Total batch reconstruction loss: 0.060265686362981796
Training batch 5 / 32
Total batch reconstruction loss: 0.06061634048819542
Training batch 6 / 32
Total batch reconstruction loss: 0.06221853196620941
Training batch 7 / 32
Total batch reconstruction loss: 0.06002579256892204
Training batch 8 / 32
Total batch reconstruction loss: 0.059126026928424835
Training batch 9 / 32
Total batch reconstruction loss: 0.06039241701364517
Training batch 10 / 32
Total batch reconstruction loss: 0.058269158005714417
Training batch 11 / 32
Total batch reconstruction loss: 0.05641422048211098
Training batch 12 / 32
Total batch reconstruction loss: 0.061988528817892075
Training batch 13 / 32
Total batch reconstruction loss: 0.05874846130609512
Training batch 14 / 32
Total batch reconstruction loss: 0.060756832361221313
Training batch 15 / 32
Total batch reconstruction loss: 0.05935104191303253
Training batch 16 / 32
Total batch reconstruction loss: 0.060759734362363815
Training batch 17 / 32
Total batch reconstruction loss: 0.059715427458286285
Training batch 18 / 32
Total batch reconstruction loss: 0.057483915239572525
Training batch 19 / 32
Total batch reconstruction loss: 0.05975683778524399
Training batch 20 / 32
Total batch reconstruction loss: 0.056767188012599945
Training batch 21 / 32
Total batch reconstruction loss: 0.058070287108421326
Training batch 22 / 32
Total batch reconstruction loss: 0.056286729872226715
Training batch 23 / 32
Total batch reconstruction loss: 0.05788315832614899
Training batch 24 / 32
Total batch reconstruction loss: 0.05914013832807541
Training batch 25 / 32
Total batch reconstruction loss: 0.06108235567808151
Training batch 26 / 32
Total batch reconstruction loss: 0.05932782217860222
Training batch 27 / 32
Total batch reconstruction loss: 0.0585186593234539
Training batch 28 / 32
Total batch reconstruction loss: 0.05520234629511833
Training batch 29 / 32
Total batch reconstruction loss: 0.05743572488427162
Training batch 30 / 32
Total batch reconstruction loss: 0.05768343061208725
Training batch 31 / 32
Total batch reconstruction loss: 0.05893918126821518
Training batch 32 / 32
Total batch reconstruction loss: 0.051614902913570404
Epoch [316/500], Train Loss: 0.0578, Validation Loss: 0.0586, Generator Loss: 11.9461, Discriminator Loss: 0.3139
Training epoch 317 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.06102216616272926
Training batch 2 / 32
Total batch reconstruction loss: 0.053554467856884
Training batch 3 / 32
Total batch reconstruction loss: 0.06171290576457977
Training batch 4 / 32
Total batch reconstruction loss: 0.0582573339343071
Training batch 5 / 32
Total batch reconstruction loss: 0.058915406465530396
Training batch 6 / 32
Total batch reconstruction loss: 0.054984237998723984
Training batch 7 / 32
Total batch reconstruction loss: 0.05996299535036087
Training batch 8 / 32
Total batch reconstruction loss: 0.06216953694820404
Training batch 9 / 32
Total batch reconstruction loss: 0.0650552362203598
Training batch 10 / 32
Total batch reconstruction loss: 0.06063513085246086
Training batch 11 / 32
Total batch reconstruction loss: 0.06081535667181015
Training batch 12 / 32
Total batch reconstruction loss: 0.05827793851494789
Training batch 13 / 32
Total batch reconstruction loss: 0.05861629545688629
Training batch 14 / 32
Total batch reconstruction loss: 0.0584561750292778
Training batch 15 / 32
Total batch reconstruction loss: 0.05648300051689148
Training batch 16 / 32
Total batch reconstruction loss: 0.05902368575334549
Training batch 17 / 32
Total batch reconstruction loss: 0.06350958347320557
Training batch 18 / 32
Total batch reconstruction loss: 0.060976386070251465
Training batch 19 / 32
Total batch reconstruction loss: 0.058578528463840485
Training batch 20 / 32
Total batch reconstruction loss: 0.05939694494009018
Training batch 21 / 32
Total batch reconstruction loss: 0.05990191549062729
Training batch 22 / 32
Total batch reconstruction loss: 0.058514609932899475
Training batch 23 / 32
Total batch reconstruction loss: 0.058137211948633194
Training batch 24 / 32
Total batch reconstruction loss: 0.06085474044084549
Training batch 25 / 32
Total batch reconstruction loss: 0.05977007746696472
Training batch 26 / 32
Total batch reconstruction loss: 0.05720149353146553
Training batch 27 / 32
Total batch reconstruction loss: 0.05977735295891762
Training batch 28 / 32
Total batch reconstruction loss: 0.05490896850824356
Training batch 29 / 32
Total batch reconstruction loss: 0.05649010092020035
Training batch 30 / 32
Total batch reconstruction loss: 0.07043088972568512
Training batch 31 / 32
Total batch reconstruction loss: 0.05757508799433708
Training batch 32 / 32
Total batch reconstruction loss: 0.05935503542423248
Epoch [317/500], Train Loss: 0.0579, Validation Loss: 0.0573, Generator Loss: 11.9621, Discriminator Loss: 0.3221
Training epoch 318 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.06116831302642822
Training batch 2 / 32
Total batch reconstruction loss: 0.06373774260282516
Training batch 3 / 32
Total batch reconstruction loss: 0.06566321104764938
Training batch 4 / 32
Total batch reconstruction loss: 0.059677548706531525
Training batch 5 / 32
Total batch reconstruction loss: 0.05652552843093872
Training batch 6 / 32
Total batch reconstruction loss: 0.058540791273117065
Training batch 7 / 32
Total batch reconstruction loss: 0.06196841970086098
Training batch 8 / 32
Total batch reconstruction loss: 0.05434587597846985
Training batch 9 / 32
Total batch reconstruction loss: 0.06229589134454727
Training batch 10 / 32
Total batch reconstruction loss: 0.05747946724295616
Training batch 11 / 32
Total batch reconstruction loss: 0.0648101270198822
Training batch 12 / 32
Total batch reconstruction loss: 0.06133583188056946
Training batch 13 / 32
Total batch reconstruction loss: 0.05272737890481949
Training batch 14 / 32
Total batch reconstruction loss: 0.056643545627593994
Training batch 15 / 32
Total batch reconstruction loss: 0.05923588201403618
Training batch 16 / 32
Total batch reconstruction loss: 0.055349115282297134
Training batch 17 / 32
Total batch reconstruction loss: 0.05572447553277016
Training batch 18 / 32
Total batch reconstruction loss: 0.06196601688861847
Training batch 19 / 32
Total batch reconstruction loss: 0.05854183807969093
Training batch 20 / 32
Total batch reconstruction loss: 0.0589526891708374
Training batch 21 / 32
Total batch reconstruction loss: 0.05718977004289627
Training batch 22 / 32
Total batch reconstruction loss: 0.05751731991767883
Training batch 23 / 32
Total batch reconstruction loss: 0.06126845255494118
Training batch 24 / 32
Total batch reconstruction loss: 0.062264032661914825
Training batch 25 / 32
Total batch reconstruction loss: 0.058005016297101974
Training batch 26 / 32
Total batch reconstruction loss: 0.061487145721912384
Training batch 27 / 32
Total batch reconstruction loss: 0.060186877846717834
Training batch 28 / 32
Total batch reconstruction loss: 0.060100898146629333
Training batch 29 / 32
Total batch reconstruction loss: 0.06003006920218468
Training batch 30 / 32
Total batch reconstruction loss: 0.05553893372416496
Training batch 31 / 32
Total batch reconstruction loss: 0.05935169756412506
Training batch 32 / 32
Total batch reconstruction loss: 0.047629211097955704
Epoch [318/500], Train Loss: 0.0575, Validation Loss: 0.0595, Generator Loss: 11.8653, Discriminator Loss: 0.3142
Training epoch 319 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.0593559667468071
Training batch 2 / 32
Total batch reconstruction loss: 0.05470116436481476
Training batch 3 / 32
Total batch reconstruction loss: 0.06572294980287552
Training batch 4 / 32
Total batch reconstruction loss: 0.05873134732246399
Training batch 5 / 32
Total batch reconstruction loss: 0.062285132706165314
Training batch 6 / 32
Total batch reconstruction loss: 0.06136171147227287
Training batch 7 / 32
Total batch reconstruction loss: 0.05806972458958626
Training batch 8 / 32
Total batch reconstruction loss: 0.06016752868890762
Training batch 9 / 32
Total batch reconstruction loss: 0.06487351655960083
Training batch 10 / 32
Total batch reconstruction loss: 0.06084984540939331
Training batch 11 / 32
Total batch reconstruction loss: 0.06422790139913559
Training batch 12 / 32
Total batch reconstruction loss: 0.05878608673810959
Training batch 13 / 32
Total batch reconstruction loss: 0.05610586702823639
Training batch 14 / 32
Total batch reconstruction loss: 0.059908099472522736
Training batch 15 / 32
Total batch reconstruction loss: 0.06695534288883209
Training batch 16 / 32
Total batch reconstruction loss: 0.058748453855514526
Training batch 17 / 32
Total batch reconstruction loss: 0.057793647050857544
Training batch 18 / 32
Total batch reconstruction loss: 0.06120165064930916
Training batch 19 / 32
Total batch reconstruction loss: 0.05742388218641281
Training batch 20 / 32
Total batch reconstruction loss: 0.057535335421562195
Training batch 21 / 32
Total batch reconstruction loss: 0.056624263525009155
Training batch 22 / 32
Total batch reconstruction loss: 0.05629556626081467
Training batch 23 / 32
Total batch reconstruction loss: 0.056080132722854614
Training batch 24 / 32
Total batch reconstruction loss: 0.058232709765434265
Training batch 25 / 32
Total batch reconstruction loss: 0.06491100788116455
Training batch 26 / 32
Total batch reconstruction loss: 0.06404969096183777
Training batch 27 / 32
Total batch reconstruction loss: 0.05543018504977226
Training batch 28 / 32
Total batch reconstruction loss: 0.05678722634911537
Training batch 29 / 32
Total batch reconstruction loss: 0.061926402151584625
Training batch 30 / 32
Total batch reconstruction loss: 0.0598607063293457
Training batch 31 / 32
Total batch reconstruction loss: 0.05838262289762497
Training batch 32 / 32
Total batch reconstruction loss: 0.05877019464969635
Epoch [319/500], Train Loss: 0.0582, Validation Loss: 0.0571, Generator Loss: 12.0221, Discriminator Loss: 0.3086
Training epoch 320 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.059982553124427795
Training batch 2 / 32
Total batch reconstruction loss: 0.058240629732608795
Training batch 3 / 32
Total batch reconstruction loss: 0.06354978680610657
Training batch 4 / 32
Total batch reconstruction loss: 0.05690648406744003
Training batch 5 / 32
Total batch reconstruction loss: 0.0567653551697731
Training batch 6 / 32
Total batch reconstruction loss: 0.06000685691833496
Training batch 7 / 32
Total batch reconstruction loss: 0.058654338121414185
Training batch 8 / 32
Total batch reconstruction loss: 0.054281219840049744
Training batch 9 / 32
Total batch reconstruction loss: 0.058318957686424255
Training batch 10 / 32
Total batch reconstruction loss: 0.05927656590938568
Training batch 11 / 32
Total batch reconstruction loss: 0.057877857238054276
Training batch 12 / 32
Total batch reconstruction loss: 0.05999232828617096
Training batch 13 / 32
Total batch reconstruction loss: 0.06161334738135338
Training batch 14 / 32
Total batch reconstruction loss: 0.055071793496608734
Training batch 15 / 32
Total batch reconstruction loss: 0.05932282656431198
Training batch 16 / 32
Total batch reconstruction loss: 0.05861874669790268
Training batch 17 / 32
Total batch reconstruction loss: 0.055425092577934265
Training batch 18 / 32
Total batch reconstruction loss: 0.061151742935180664
Training batch 19 / 32
Total batch reconstruction loss: 0.06299956142902374
Training batch 20 / 32
Total batch reconstruction loss: 0.058024100959300995
Training batch 21 / 32
Total batch reconstruction loss: 0.057020433247089386
Training batch 22 / 32
Total batch reconstruction loss: 0.057747188955545425
Training batch 23 / 32
Total batch reconstruction loss: 0.057111550122499466
Training batch 24 / 32
Total batch reconstruction loss: 0.060444511473178864
Training batch 25 / 32
Total batch reconstruction loss: 0.05873071029782295
Training batch 26 / 32
Total batch reconstruction loss: 0.06336522102355957
Training batch 27 / 32
Total batch reconstruction loss: 0.05952732264995575
Training batch 28 / 32
Total batch reconstruction loss: 0.0641290545463562
Training batch 29 / 32
Total batch reconstruction loss: 0.06021688133478165
Training batch 30 / 32
Total batch reconstruction loss: 0.058693233877420425
Training batch 31 / 32
Total batch reconstruction loss: 0.05968451872467995
Training batch 32 / 32
Total batch reconstruction loss: 0.05602263659238815
Epoch [320/500], Train Loss: 0.0572, Validation Loss: 0.0575, Generator Loss: 11.8769, Discriminator Loss: 0.3141
Training epoch 321 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.05855150893330574
Training batch 2 / 32
Total batch reconstruction loss: 0.054861754179000854
Training batch 3 / 32
Total batch reconstruction loss: 0.06137774512171745
Training batch 4 / 32
Total batch reconstruction loss: 0.05761493369936943
Training batch 5 / 32
Total batch reconstruction loss: 0.059921279549598694
Training batch 6 / 32
Total batch reconstruction loss: 0.05640473961830139
Training batch 7 / 32
Total batch reconstruction loss: 0.05738046020269394
Training batch 8 / 32
Total batch reconstruction loss: 0.0579310841858387
Training batch 9 / 32
Total batch reconstruction loss: 0.05911944806575775
Training batch 10 / 32
Total batch reconstruction loss: 0.058528900146484375
Training batch 11 / 32
Total batch reconstruction loss: 0.06363844126462936
Training batch 12 / 32
Total batch reconstruction loss: 0.06092396751046181
Training batch 13 / 32
Total batch reconstruction loss: 0.06142350286245346
Training batch 14 / 32
Total batch reconstruction loss: 0.055830031633377075
Training batch 15 / 32
Total batch reconstruction loss: 0.06315883249044418
Training batch 16 / 32
Total batch reconstruction loss: 0.05797334015369415
Training batch 17 / 32
Total batch reconstruction loss: 0.058782223612070084
Training batch 18 / 32
Total batch reconstruction loss: 0.05889275297522545
Training batch 19 / 32
Total batch reconstruction loss: 0.061052896082401276
Training batch 20 / 32
Total batch reconstruction loss: 0.058854926377534866
Training batch 21 / 32
Total batch reconstruction loss: 0.057152941823005676
Training batch 22 / 32
Total batch reconstruction loss: 0.06067943572998047
Training batch 23 / 32
Total batch reconstruction loss: 0.05766908451914787
Training batch 24 / 32
Total batch reconstruction loss: 0.05880960077047348
Training batch 25 / 32
Total batch reconstruction loss: 0.06104858219623566
Training batch 26 / 32
Total batch reconstruction loss: 0.062060583382844925
Training batch 27 / 32
Total batch reconstruction loss: 0.058457136154174805
Training batch 28 / 32
Total batch reconstruction loss: 0.057032596319913864
Training batch 29 / 32
Total batch reconstruction loss: 0.05975760519504547
Training batch 30 / 32
Total batch reconstruction loss: 0.06060681492090225
Training batch 31 / 32
Total batch reconstruction loss: 0.06048658490180969
Training batch 32 / 32
Total batch reconstruction loss: 0.05787035822868347
Epoch [321/500], Train Loss: 0.0574, Validation Loss: 0.0577, Generator Loss: 11.9123, Discriminator Loss: 0.3099
Training epoch 322 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.05817856639623642
Training batch 2 / 32
Total batch reconstruction loss: 0.05853522568941116
Training batch 3 / 32
Total batch reconstruction loss: 0.06046360731124878
Training batch 4 / 32
Total batch reconstruction loss: 0.05587375909090042
Training batch 5 / 32
Total batch reconstruction loss: 0.05653436481952667
Training batch 6 / 32
Total batch reconstruction loss: 0.05627525597810745
Training batch 7 / 32
Total batch reconstruction loss: 0.05914938822388649
Training batch 8 / 32
Total batch reconstruction loss: 0.05576968565583229
Training batch 9 / 32
Total batch reconstruction loss: 0.06579110026359558
Training batch 10 / 32
Total batch reconstruction loss: 0.06261604279279709
Training batch 11 / 32
Total batch reconstruction loss: 0.05786026641726494
Training batch 12 / 32
Total batch reconstruction loss: 0.06159783899784088
Training batch 13 / 32
Total batch reconstruction loss: 0.05937936156988144
Training batch 14 / 32
Total batch reconstruction loss: 0.056631892919540405
Training batch 15 / 32
Total batch reconstruction loss: 0.05694332718849182
Training batch 16 / 32
Total batch reconstruction loss: 0.058813437819480896
Training batch 17 / 32
Total batch reconstruction loss: 0.0609864816069603
Training batch 18 / 32
Total batch reconstruction loss: 0.059060849249362946
Training batch 19 / 32
Total batch reconstruction loss: 0.054116539657115936
Training batch 20 / 32
Total batch reconstruction loss: 0.0590989887714386
Training batch 21 / 32
Total batch reconstruction loss: 0.058340977877378464
Training batch 22 / 32
Total batch reconstruction loss: 0.057698316872119904
Training batch 23 / 32
Total batch reconstruction loss: 0.06178221479058266
Training batch 24 / 32
Total batch reconstruction loss: 0.06345957517623901
Training batch 25 / 32
Total batch reconstruction loss: 0.06242863088846207
Training batch 26 / 32
Total batch reconstruction loss: 0.0559963695704937
Training batch 27 / 32
Total batch reconstruction loss: 0.05766282230615616
Training batch 28 / 32
Total batch reconstruction loss: 0.05794564634561539
Training batch 29 / 32
Total batch reconstruction loss: 0.06290814280509949
Training batch 30 / 32
Total batch reconstruction loss: 0.05909282714128494
Training batch 31 / 32
Total batch reconstruction loss: 0.0584186427295208
Training batch 32 / 32
Total batch reconstruction loss: 0.0731959193944931
Epoch [322/500], Train Loss: 0.0578, Validation Loss: 0.0575, Generator Loss: 11.9573, Discriminator Loss: 0.3155
Training epoch 323 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.058654919266700745
Training batch 2 / 32
Total batch reconstruction loss: 0.06127999350428581
Training batch 3 / 32
Total batch reconstruction loss: 0.059498973190784454
Training batch 4 / 32
Total batch reconstruction loss: 0.06523621082305908
Training batch 5 / 32
Total batch reconstruction loss: 0.05702447146177292
Training batch 6 / 32
Total batch reconstruction loss: 0.05612249672412872
Training batch 7 / 32
Total batch reconstruction loss: 0.05480370298027992
Training batch 8 / 32
Total batch reconstruction loss: 0.06132712587714195
Training batch 9 / 32
Total batch reconstruction loss: 0.061203911900520325
Training batch 10 / 32
Total batch reconstruction loss: 0.06880812346935272
Training batch 11 / 32
Total batch reconstruction loss: 0.05689799413084984
Training batch 12 / 32
Total batch reconstruction loss: 0.062399670481681824
Training batch 13 / 32
Total batch reconstruction loss: 0.06181224435567856
Training batch 14 / 32
Total batch reconstruction loss: 0.06484900414943695
Training batch 15 / 32
Total batch reconstruction loss: 0.06001066416501999
Training batch 16 / 32
Total batch reconstruction loss: 0.06095219403505325
Training batch 17 / 32
Total batch reconstruction loss: 0.05987122654914856
Training batch 18 / 32
Total batch reconstruction loss: 0.06311804801225662
Training batch 19 / 32
Total batch reconstruction loss: 0.057712167501449585
Training batch 20 / 32
Total batch reconstruction loss: 0.06063194200396538
Training batch 21 / 32
Total batch reconstruction loss: 0.05991936847567558
Training batch 22 / 32
Total batch reconstruction loss: 0.05645816773176193
Training batch 23 / 32
Total batch reconstruction loss: 0.05479871854186058
Training batch 24 / 32
Total batch reconstruction loss: 0.06103897839784622
Training batch 25 / 32
Total batch reconstruction loss: 0.06026748940348625
Training batch 26 / 32
Total batch reconstruction loss: 0.06200433149933815
Training batch 27 / 32
Total batch reconstruction loss: 0.05875714495778084
Training batch 28 / 32
Total batch reconstruction loss: 0.054552678018808365
Training batch 29 / 32
Total batch reconstruction loss: 0.057291269302368164
Training batch 30 / 32
Total batch reconstruction loss: 0.054639022797346115
Training batch 31 / 32
Total batch reconstruction loss: 0.05943378061056137
Training batch 32 / 32
Total batch reconstruction loss: 0.06901951134204865
Epoch [323/500], Train Loss: 0.0584, Validation Loss: 0.0599, Generator Loss: 12.0606, Discriminator Loss: 0.3393
Training epoch 324 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.056856438517570496
Training batch 2 / 32
Total batch reconstruction loss: 0.0633135437965393
Training batch 3 / 32
Total batch reconstruction loss: 0.059070393443107605
Training batch 4 / 32
Total batch reconstruction loss: 0.06280345469713211
Training batch 5 / 32
Total batch reconstruction loss: 0.06243790686130524
Training batch 6 / 32
Total batch reconstruction loss: 0.059793490916490555
Training batch 7 / 32
Total batch reconstruction loss: 0.062145039439201355
Training batch 8 / 32
Total batch reconstruction loss: 0.06195206195116043
Training batch 9 / 32
Total batch reconstruction loss: 0.056987062096595764
Training batch 10 / 32
Total batch reconstruction loss: 0.061824239790439606
Training batch 11 / 32
Total batch reconstruction loss: 0.05767250806093216
Training batch 12 / 32
Total batch reconstruction loss: 0.06230453774333
Training batch 13 / 32
Total batch reconstruction loss: 0.056000471115112305
Training batch 14 / 32
Total batch reconstruction loss: 0.058571986854076385
Training batch 15 / 32
Total batch reconstruction loss: 0.05652201920747757
Training batch 16 / 32
Total batch reconstruction loss: 0.06278637051582336
Training batch 17 / 32
Total batch reconstruction loss: 0.06365793198347092
Training batch 18 / 32
Total batch reconstruction loss: 0.05992305651307106
Training batch 19 / 32
Total batch reconstruction loss: 0.05963227525353432
Training batch 20 / 32
Total batch reconstruction loss: 0.0590919628739357
Training batch 21 / 32
Total batch reconstruction loss: 0.0584607794880867
Training batch 22 / 32
Total batch reconstruction loss: 0.0558786578476429
Training batch 23 / 32
Total batch reconstruction loss: 0.05652995407581329
Training batch 24 / 32
Total batch reconstruction loss: 0.06379137188196182
Training batch 25 / 32
Total batch reconstruction loss: 0.05967691168189049
Training batch 26 / 32
Total batch reconstruction loss: 0.06086799129843712
Training batch 27 / 32
Total batch reconstruction loss: 0.05524079501628876
Training batch 28 / 32
Total batch reconstruction loss: 0.05975443497300148
Training batch 29 / 32
Total batch reconstruction loss: 0.056856341660022736
Training batch 30 / 32
Total batch reconstruction loss: 0.06152227148413658
Training batch 31 / 32
Total batch reconstruction loss: 0.057400062680244446
Training batch 32 / 32
Total batch reconstruction loss: 0.047236889600753784
Epoch [324/500], Train Loss: 0.0575, Validation Loss: 0.0572, Generator Loss: 11.9187, Discriminator Loss: 0.3219
Training epoch 325 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.06048660725355148
Training batch 2 / 32
Total batch reconstruction loss: 0.05998605117201805
Training batch 3 / 32
Total batch reconstruction loss: 0.057497233152389526
Training batch 4 / 32
Total batch reconstruction loss: 0.05764778330922127
Training batch 5 / 32
Total batch reconstruction loss: 0.059595756232738495
Training batch 6 / 32
Total batch reconstruction loss: 0.0656733587384224
Training batch 7 / 32
Total batch reconstruction loss: 0.05655645206570625
Training batch 8 / 32
Total batch reconstruction loss: 0.06140705943107605
Training batch 9 / 32
Total batch reconstruction loss: 0.05990859121084213
Training batch 10 / 32
Total batch reconstruction loss: 0.06377717107534409
Training batch 11 / 32
Total batch reconstruction loss: 0.062276504933834076
Training batch 12 / 32
Total batch reconstruction loss: 0.05795985087752342
Training batch 13 / 32
Total batch reconstruction loss: 0.061801016330718994
Training batch 14 / 32
Total batch reconstruction loss: 0.055999405682086945
Training batch 15 / 32
Total batch reconstruction loss: 0.05535196140408516
Training batch 16 / 32
Total batch reconstruction loss: 0.06057696044445038
Training batch 17 / 32
Total batch reconstruction loss: 0.057514261454343796
Training batch 18 / 32
Total batch reconstruction loss: 0.054315246641635895
Training batch 19 / 32
Total batch reconstruction loss: 0.05452119559049606
Training batch 20 / 32
Total batch reconstruction loss: 0.060229718685150146
Training batch 21 / 32
Total batch reconstruction loss: 0.05867549031972885
Training batch 22 / 32
Total batch reconstruction loss: 0.06317497789859772
Training batch 23 / 32
Total batch reconstruction loss: 0.05664355307817459
Training batch 24 / 32
Total batch reconstruction loss: 0.05866982042789459
Training batch 25 / 32
Total batch reconstruction loss: 0.054487571120262146
Training batch 26 / 32
Total batch reconstruction loss: 0.05646880716085434
Training batch 27 / 32
Total batch reconstruction loss: 0.06079556792974472
Training batch 28 / 32
Total batch reconstruction loss: 0.0574164092540741
Training batch 29 / 32
Total batch reconstruction loss: 0.05731668695807457
Training batch 30 / 32
Total batch reconstruction loss: 0.0589958056807518
Training batch 31 / 32
Total batch reconstruction loss: 0.06210719794034958
Training batch 32 / 32
Total batch reconstruction loss: 0.05270133912563324
Epoch [325/500], Train Loss: 0.0570, Validation Loss: 0.0576, Generator Loss: 11.8341, Discriminator Loss: 0.2947
Training epoch 326 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.05682778358459473
Training batch 2 / 32
Total batch reconstruction loss: 0.0637645423412323
Training batch 3 / 32
Total batch reconstruction loss: 0.06367789208889008
Training batch 4 / 32
Total batch reconstruction loss: 0.05926087498664856
Training batch 5 / 32
Total batch reconstruction loss: 0.05833324044942856
Training batch 6 / 32
Total batch reconstruction loss: 0.05880860984325409
Training batch 7 / 32
Total batch reconstruction loss: 0.05923464894294739
Training batch 8 / 32
Total batch reconstruction loss: 0.05829128623008728
Training batch 9 / 32
Total batch reconstruction loss: 0.06064995378255844
Training batch 10 / 32
Total batch reconstruction loss: 0.0633653923869133
Training batch 11 / 32
Total batch reconstruction loss: 0.06393947452306747
Training batch 12 / 32
Total batch reconstruction loss: 0.05820852145552635
Training batch 13 / 32
Total batch reconstruction loss: 0.06088573485612869
Training batch 14 / 32
Total batch reconstruction loss: 0.05633535236120224
Training batch 15 / 32
Total batch reconstruction loss: 0.05675917863845825
Training batch 16 / 32
Total batch reconstruction loss: 0.057501062750816345
Training batch 17 / 32
Total batch reconstruction loss: 0.06212884187698364
Training batch 18 / 32
Total batch reconstruction loss: 0.05478638410568237
Training batch 19 / 32
Total batch reconstruction loss: 0.06033419072628021
Training batch 20 / 32
Total batch reconstruction loss: 0.05902475863695145
Training batch 21 / 32
Total batch reconstruction loss: 0.060521289706230164
Training batch 22 / 32
Total batch reconstruction loss: 0.0575222373008728
Training batch 23 / 32
Total batch reconstruction loss: 0.055119588971138
Training batch 24 / 32
Total batch reconstruction loss: 0.05873464047908783
Training batch 25 / 32
Total batch reconstruction loss: 0.05898434296250343
Training batch 26 / 32
Total batch reconstruction loss: 0.062220726162195206
Training batch 27 / 32
Total batch reconstruction loss: 0.0592883825302124
Training batch 28 / 32
Total batch reconstruction loss: 0.06299850344657898
Training batch 29 / 32
Total batch reconstruction loss: 0.06679844856262207
Training batch 30 / 32
Total batch reconstruction loss: 0.06232014298439026
Training batch 31 / 32
Total batch reconstruction loss: 0.05719589442014694
Training batch 32 / 32
Total batch reconstruction loss: 0.07268527895212173
Epoch [326/500], Train Loss: 0.0586, Validation Loss: 0.0570, Generator Loss: 12.1086, Discriminator Loss: 0.3159
Training epoch 327 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.05771021172404289
Training batch 2 / 32
Total batch reconstruction loss: 0.061960745602846146
Training batch 3 / 32
Total batch reconstruction loss: 0.05950310826301575
Training batch 4 / 32
Total batch reconstruction loss: 0.05829862877726555
Training batch 5 / 32
Total batch reconstruction loss: 0.060694802552461624
Training batch 6 / 32
Total batch reconstruction loss: 0.05908997356891632
Training batch 7 / 32
Total batch reconstruction loss: 0.06583824753761292
Training batch 8 / 32
Total batch reconstruction loss: 0.06467306613922119
Training batch 9 / 32
Total batch reconstruction loss: 0.05608556419610977
Training batch 10 / 32
Total batch reconstruction loss: 0.0579865388572216
Training batch 11 / 32
Total batch reconstruction loss: 0.058852419257164
Training batch 12 / 32
Total batch reconstruction loss: 0.05318932235240936
Training batch 13 / 32
Total batch reconstruction loss: 0.06368100643157959
Training batch 14 / 32
Total batch reconstruction loss: 0.0624149888753891
Training batch 15 / 32
Total batch reconstruction loss: 0.058389417827129364
Training batch 16 / 32
Total batch reconstruction loss: 0.061262212693691254
Training batch 17 / 32
Total batch reconstruction loss: 0.058930687606334686
Training batch 18 / 32
Total batch reconstruction loss: 0.056764326989650726
Training batch 19 / 32
Total batch reconstruction loss: 0.056867703795433044
Training batch 20 / 32
Total batch reconstruction loss: 0.0596560463309288
Training batch 21 / 32
Total batch reconstruction loss: 0.060388997197151184
Training batch 22 / 32
Total batch reconstruction loss: 0.05954962968826294
Training batch 23 / 32
Total batch reconstruction loss: 0.05797094106674194
Training batch 24 / 32
Total batch reconstruction loss: 0.056825462728738785
Training batch 25 / 32
Total batch reconstruction loss: 0.060983289033174515
Training batch 26 / 32
Total batch reconstruction loss: 0.05609538406133652
Training batch 27 / 32
Total batch reconstruction loss: 0.05664531886577606
Training batch 28 / 32
Total batch reconstruction loss: 0.055652011185884476
Training batch 29 / 32
Total batch reconstruction loss: 0.0562766008079052
Training batch 30 / 32
Total batch reconstruction loss: 0.05838896334171295
Training batch 31 / 32
Total batch reconstruction loss: 0.05788798630237579
Training batch 32 / 32
Total batch reconstruction loss: 0.10035570710897446
Epoch [327/500], Train Loss: 0.0586, Validation Loss: 0.0581, Generator Loss: 12.1224, Discriminator Loss: 0.3274
Training epoch 328 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.06193595752120018
Training batch 2 / 32
Total batch reconstruction loss: 0.05943744629621506
Training batch 3 / 32
Total batch reconstruction loss: 0.059880178421735764
Training batch 4 / 32
Total batch reconstruction loss: 0.06124962866306305
Training batch 5 / 32
Total batch reconstruction loss: 0.06110674515366554
Training batch 6 / 32
Total batch reconstruction loss: 0.06121832877397537
Training batch 7 / 32
Total batch reconstruction loss: 0.06141030043363571
Training batch 8 / 32
Total batch reconstruction loss: 0.058680981397628784
Training batch 9 / 32
Total batch reconstruction loss: 0.06472547352313995
Training batch 10 / 32
Total batch reconstruction loss: 0.059065286070108414
Training batch 11 / 32
Total batch reconstruction loss: 0.05468515306711197
Training batch 12 / 32
Total batch reconstruction loss: 0.06142810732126236
Training batch 13 / 32
Total batch reconstruction loss: 0.06252047419548035
Training batch 14 / 32
Total batch reconstruction loss: 0.05933352932333946
Training batch 15 / 32
Total batch reconstruction loss: 0.05805060267448425
Training batch 16 / 32
Total batch reconstruction loss: 0.06308668106794357
Training batch 17 / 32
Total batch reconstruction loss: 0.05638660490512848
Training batch 18 / 32
Total batch reconstruction loss: 0.05994900315999985
Training batch 19 / 32
Total batch reconstruction loss: 0.05541069433093071
Training batch 20 / 32
Total batch reconstruction loss: 0.056859880685806274
Training batch 21 / 32
Total batch reconstruction loss: 0.062340907752513885
Training batch 22 / 32
Total batch reconstruction loss: 0.05739952623844147
Training batch 23 / 32
Total batch reconstruction loss: 0.06049317121505737
Training batch 24 / 32
Total batch reconstruction loss: 0.061107270419597626
Training batch 25 / 32
Total batch reconstruction loss: 0.056799355894327164
Training batch 26 / 32
Total batch reconstruction loss: 0.05733582377433777
Training batch 27 / 32
Total batch reconstruction loss: 0.058889877051115036
Training batch 28 / 32
Total batch reconstruction loss: 0.05361476540565491
Training batch 29 / 32
Total batch reconstruction loss: 0.054898880422115326
Training batch 30 / 32
Total batch reconstruction loss: 0.05797000229358673
Training batch 31 / 32
Total batch reconstruction loss: 0.05773456394672394
Training batch 32 / 32
Total batch reconstruction loss: 0.07760034501552582
Epoch [328/500], Train Loss: 0.0578, Validation Loss: 0.0593, Generator Loss: 12.0233, Discriminator Loss: 0.3179
Training epoch 329 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.05961137264966965
Training batch 2 / 32
Total batch reconstruction loss: 0.06026267260313034
Training batch 3 / 32
Total batch reconstruction loss: 0.05685040354728699
Training batch 4 / 32
Total batch reconstruction loss: 0.06068964675068855
Training batch 5 / 32
Total batch reconstruction loss: 0.057587139308452606
Training batch 6 / 32
Total batch reconstruction loss: 0.059535570442676544
Training batch 7 / 32
Total batch reconstruction loss: 0.05892375484108925
Training batch 8 / 32
Total batch reconstruction loss: 0.054996564984321594
Training batch 9 / 32
Total batch reconstruction loss: 0.06147252768278122
Training batch 10 / 32
Total batch reconstruction loss: 0.05816124379634857
Training batch 11 / 32
Total batch reconstruction loss: 0.057151246815919876
Training batch 12 / 32
Total batch reconstruction loss: 0.05486632138490677
Training batch 13 / 32
Total batch reconstruction loss: 0.057340461760759354
Training batch 14 / 32
Total batch reconstruction loss: 0.05727668106555939
Training batch 15 / 32
Total batch reconstruction loss: 0.061648670583963394
Training batch 16 / 32
Total batch reconstruction loss: 0.05809547007083893
Training batch 17 / 32
Total batch reconstruction loss: 0.06114096939563751
Training batch 18 / 32
Total batch reconstruction loss: 0.055881597101688385
Training batch 19 / 32
Total batch reconstruction loss: 0.06244443729519844
Training batch 20 / 32
Total batch reconstruction loss: 0.06638722121715546
Training batch 21 / 32
Total batch reconstruction loss: 0.05516062676906586
Training batch 22 / 32
Total batch reconstruction loss: 0.056899502873420715
Training batch 23 / 32
Total batch reconstruction loss: 0.06006837636232376
Training batch 24 / 32
Total batch reconstruction loss: 0.06713011860847473
Training batch 25 / 32
Total batch reconstruction loss: 0.06524476408958435
Training batch 26 / 32
Total batch reconstruction loss: 0.057523295283317566
Training batch 27 / 32
Total batch reconstruction loss: 0.06089058518409729
Training batch 28 / 32
Total batch reconstruction loss: 0.05685340613126755
Training batch 29 / 32
Total batch reconstruction loss: 0.057879768311977386
Training batch 30 / 32
Total batch reconstruction loss: 0.05972279608249664
Training batch 31 / 32
Total batch reconstruction loss: 0.055620282888412476
Training batch 32 / 32
Total batch reconstruction loss: 0.059518810361623764
Epoch [329/500], Train Loss: 0.0574, Validation Loss: 0.0577, Generator Loss: 11.9008, Discriminator Loss: 0.3183
Training epoch 330 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.05989707633852959
Training batch 2 / 32
Total batch reconstruction loss: 0.058116137981414795
Training batch 3 / 32
Total batch reconstruction loss: 0.05678777024149895
Training batch 4 / 32
Total batch reconstruction loss: 0.057887136936187744
Training batch 5 / 32
Total batch reconstruction loss: 0.056883230805397034
Training batch 6 / 32
Total batch reconstruction loss: 0.06106570363044739
Training batch 7 / 32
Total batch reconstruction loss: 0.059543244540691376
Training batch 8 / 32
Total batch reconstruction loss: 0.058897651731967926
Training batch 9 / 32
Total batch reconstruction loss: 0.057604387402534485
Training batch 10 / 32
Total batch reconstruction loss: 0.058799296617507935
Training batch 11 / 32
Total batch reconstruction loss: 0.05846511572599411
Training batch 12 / 32
Total batch reconstruction loss: 0.056116823107004166
Training batch 13 / 32
Total batch reconstruction loss: 0.0608222596347332
Training batch 14 / 32
Total batch reconstruction loss: 0.058989256620407104
Training batch 15 / 32
Total batch reconstruction loss: 0.058675624430179596
Training batch 16 / 32
Total batch reconstruction loss: 0.058851297944784164
Training batch 17 / 32
Total batch reconstruction loss: 0.056386083364486694
Training batch 18 / 32
Total batch reconstruction loss: 0.05705289542675018
Training batch 19 / 32
Total batch reconstruction loss: 0.05621878802776337
Training batch 20 / 32
Total batch reconstruction loss: 0.06157698109745979
Training batch 21 / 32
Total batch reconstruction loss: 0.05672278255224228
Training batch 22 / 32
Total batch reconstruction loss: 0.056229978799819946
Training batch 23 / 32
Total batch reconstruction loss: 0.061501018702983856
Training batch 24 / 32
Total batch reconstruction loss: 0.06123855337500572
Training batch 25 / 32
Total batch reconstruction loss: 0.0626191645860672
Training batch 26 / 32
Total batch reconstruction loss: 0.06247183680534363
Training batch 27 / 32
Total batch reconstruction loss: 0.056583601981401443
Training batch 28 / 32
Total batch reconstruction loss: 0.06057198345661163
Training batch 29 / 32
Total batch reconstruction loss: 0.0590936541557312
Training batch 30 / 32
Total batch reconstruction loss: 0.05671299248933792
Training batch 31 / 32
Total batch reconstruction loss: 0.059734225273132324
Training batch 32 / 32
Total batch reconstruction loss: 0.08841419219970703
Epoch [330/500], Train Loss: 0.0579, Validation Loss: 0.0567, Generator Loss: 12.0003, Discriminator Loss: 0.3317
Training epoch 331 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.059512604027986526
Training batch 2 / 32
Total batch reconstruction loss: 0.05539487302303314
Training batch 3 / 32
Total batch reconstruction loss: 0.05867023020982742
Training batch 4 / 32
Total batch reconstruction loss: 0.05500706285238266
Training batch 5 / 32
Total batch reconstruction loss: 0.05768799036741257
Training batch 6 / 32
Total batch reconstruction loss: 0.05989489704370499
Training batch 7 / 32
Total batch reconstruction loss: 0.05909772962331772
Training batch 8 / 32
Total batch reconstruction loss: 0.06141588091850281
Training batch 9 / 32
Total batch reconstruction loss: 0.05696260556578636
Training batch 10 / 32
Total batch reconstruction loss: 0.0553167387843132
Training batch 11 / 32
Total batch reconstruction loss: 0.06458039581775665
Training batch 12 / 32
Total batch reconstruction loss: 0.05980442464351654
Training batch 13 / 32
Total batch reconstruction loss: 0.05629445239901543
Training batch 14 / 32
Total batch reconstruction loss: 0.06356239318847656
Training batch 15 / 32
Total batch reconstruction loss: 0.05668126791715622
Training batch 16 / 32
Total batch reconstruction loss: 0.06336845457553864
Training batch 17 / 32
Total batch reconstruction loss: 0.06350115686655045
Training batch 18 / 32
Total batch reconstruction loss: 0.05783841758966446
Training batch 19 / 32
Total batch reconstruction loss: 0.05890321359038353
Training batch 20 / 32
Total batch reconstruction loss: 0.059536807239055634
Training batch 21 / 32
Total batch reconstruction loss: 0.061265528202056885
Training batch 22 / 32
Total batch reconstruction loss: 0.05848207324743271
Training batch 23 / 32
Total batch reconstruction loss: 0.05716259405016899
Training batch 24 / 32
Total batch reconstruction loss: 0.058196283876895905
Training batch 25 / 32
Total batch reconstruction loss: 0.05577787011861801
Training batch 26 / 32
Total batch reconstruction loss: 0.06424184143543243
Training batch 27 / 32
Total batch reconstruction loss: 0.05580891668796539
Training batch 28 / 32
Total batch reconstruction loss: 0.057127390056848526
Training batch 29 / 32
Total batch reconstruction loss: 0.061163242906332016
Training batch 30 / 32
Total batch reconstruction loss: 0.05618049204349518
Training batch 31 / 32
Total batch reconstruction loss: 0.05489838868379593
Training batch 32 / 32
Total batch reconstruction loss: 0.05111893638968468
Epoch [331/500], Train Loss: 0.0568, Validation Loss: 0.0576, Generator Loss: 11.7867, Discriminator Loss: 0.3152
Training epoch 332 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.05617181956768036
Training batch 2 / 32
Total batch reconstruction loss: 0.060843463987112045
Training batch 3 / 32
Total batch reconstruction loss: 0.05982498824596405
Training batch 4 / 32
Total batch reconstruction loss: 0.05623998865485191
Training batch 5 / 32
Total batch reconstruction loss: 0.05981481820344925
Training batch 6 / 32
Total batch reconstruction loss: 0.060482703149318695
Training batch 7 / 32
Total batch reconstruction loss: 0.06276581436395645
Training batch 8 / 32
Total batch reconstruction loss: 0.05867766961455345
Training batch 9 / 32
Total batch reconstruction loss: 0.060224976390600204
Training batch 10 / 32
Total batch reconstruction loss: 0.0629354864358902
Training batch 11 / 32
Total batch reconstruction loss: 0.06099342927336693
Training batch 12 / 32
Total batch reconstruction loss: 0.055556170642375946
Training batch 13 / 32
Total batch reconstruction loss: 0.05544756352901459
Training batch 14 / 32
Total batch reconstruction loss: 0.056300222873687744
Training batch 15 / 32
Total batch reconstruction loss: 0.060191404074430466
Training batch 16 / 32
Total batch reconstruction loss: 0.0589488185942173
Training batch 17 / 32
Total batch reconstruction loss: 0.06005013734102249
Training batch 18 / 32
Total batch reconstruction loss: 0.05819252133369446
Training batch 19 / 32
Total batch reconstruction loss: 0.054513897746801376
Training batch 20 / 32
Total batch reconstruction loss: 0.056485503911972046
Training batch 21 / 32
Total batch reconstruction loss: 0.0594501718878746
Training batch 22 / 32
Total batch reconstruction loss: 0.06475580483675003
Training batch 23 / 32
Total batch reconstruction loss: 0.05728485435247421
Training batch 24 / 32
Total batch reconstruction loss: 0.05918654054403305
Training batch 25 / 32
Total batch reconstruction loss: 0.06189746409654617
Training batch 26 / 32
Total batch reconstruction loss: 0.058643169701099396
Training batch 27 / 32
Total batch reconstruction loss: 0.062026046216487885
Training batch 28 / 32
Total batch reconstruction loss: 0.05570444092154503
Training batch 29 / 32
Total batch reconstruction loss: 0.06359073519706726
Training batch 30 / 32
Total batch reconstruction loss: 0.05896849185228348
Training batch 31 / 32
Total batch reconstruction loss: 0.06021203473210335
Training batch 32 / 32
Total batch reconstruction loss: 0.07377910614013672
Epoch [332/500], Train Loss: 0.0581, Validation Loss: 0.0621, Generator Loss: 12.0031, Discriminator Loss: 0.3180
Training epoch 333 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.058126769959926605
Training batch 2 / 32
Total batch reconstruction loss: 0.0587235689163208
Training batch 3 / 32
Total batch reconstruction loss: 0.055130988359451294
Training batch 4 / 32
Total batch reconstruction loss: 0.06054176390171051
Training batch 5 / 32
Total batch reconstruction loss: 0.05747660994529724
Training batch 6 / 32
Total batch reconstruction loss: 0.06342913955450058
Training batch 7 / 32
Total batch reconstruction loss: 0.0605119913816452
Training batch 8 / 32
Total batch reconstruction loss: 0.0562138557434082
Training batch 9 / 32
Total batch reconstruction loss: 0.06023654714226723
Training batch 10 / 32
Total batch reconstruction loss: 0.06020677089691162
Training batch 11 / 32
Total batch reconstruction loss: 0.057093627750873566
Training batch 12 / 32
Total batch reconstruction loss: 0.05821484327316284
Training batch 13 / 32
Total batch reconstruction loss: 0.05987373739480972
Training batch 14 / 32
Total batch reconstruction loss: 0.05697501823306084
Training batch 15 / 32
Total batch reconstruction loss: 0.06326824426651001
Training batch 16 / 32
Total batch reconstruction loss: 0.06212911754846573
Training batch 17 / 32
Total batch reconstruction loss: 0.060794681310653687
Training batch 18 / 32
Total batch reconstruction loss: 0.05927484855055809
Training batch 19 / 32
Total batch reconstruction loss: 0.05982790142297745
Training batch 20 / 32
Total batch reconstruction loss: 0.0617845356464386
Training batch 21 / 32
Total batch reconstruction loss: 0.056725651025772095
Training batch 22 / 32
Total batch reconstruction loss: 0.0583161860704422
Training batch 23 / 32
Total batch reconstruction loss: 0.05678531527519226
Training batch 24 / 32
Total batch reconstruction loss: 0.06046455353498459
Training batch 25 / 32
Total batch reconstruction loss: 0.056455932557582855
Training batch 26 / 32
Total batch reconstruction loss: 0.05624812841415405
Training batch 27 / 32
Total batch reconstruction loss: 0.06098337471485138
Training batch 28 / 32
Total batch reconstruction loss: 0.06782226264476776
Training batch 29 / 32
Total batch reconstruction loss: 0.05583588778972626
Training batch 30 / 32
Total batch reconstruction loss: 0.06298511475324631
Training batch 31 / 32
Total batch reconstruction loss: 0.055229317396879196
Training batch 32 / 32
Total batch reconstruction loss: 0.06295263767242432
Epoch [333/500], Train Loss: 0.0578, Validation Loss: 0.0571, Generator Loss: 11.9507, Discriminator Loss: 0.3107
Training epoch 334 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.06054776906967163
Training batch 2 / 32
Total batch reconstruction loss: 0.058884818106889725
Training batch 3 / 32
Total batch reconstruction loss: 0.05859113112092018
Training batch 4 / 32
Total batch reconstruction loss: 0.058636732399463654
Training batch 5 / 32
Total batch reconstruction loss: 0.05748911574482918
Training batch 6 / 32
Total batch reconstruction loss: 0.062365222722291946
Training batch 7 / 32
Total batch reconstruction loss: 0.057488322257995605
Training batch 8 / 32
Total batch reconstruction loss: 0.059530265629291534
Training batch 9 / 32
Total batch reconstruction loss: 0.05885930359363556
Training batch 10 / 32
Total batch reconstruction loss: 0.056688617914915085
Training batch 11 / 32
Total batch reconstruction loss: 0.0606466680765152
Training batch 12 / 32
Total batch reconstruction loss: 0.059211090207099915
Training batch 13 / 32
Total batch reconstruction loss: 0.05705156549811363
Training batch 14 / 32
Total batch reconstruction loss: 0.054954707622528076
Training batch 15 / 32
Total batch reconstruction loss: 0.05855743587017059
Training batch 16 / 32
Total batch reconstruction loss: 0.05754821002483368
Training batch 17 / 32
Total batch reconstruction loss: 0.06071029230952263
Training batch 18 / 32
Total batch reconstruction loss: 0.056655872613191605
Training batch 19 / 32
Total batch reconstruction loss: 0.05549916625022888
Training batch 20 / 32
Total batch reconstruction loss: 0.05739182233810425
Training batch 21 / 32
Total batch reconstruction loss: 0.06099802255630493
Training batch 22 / 32
Total batch reconstruction loss: 0.0685833990573883
Training batch 23 / 32
Total batch reconstruction loss: 0.057648446410894394
Training batch 24 / 32
Total batch reconstruction loss: 0.06211983039975166
Training batch 25 / 32
Total batch reconstruction loss: 0.05733122304081917
Training batch 26 / 32
Total batch reconstruction loss: 0.057529255747795105
Training batch 27 / 32
Total batch reconstruction loss: 0.059773970395326614
Training batch 28 / 32
Total batch reconstruction loss: 0.05850939452648163
Training batch 29 / 32
Total batch reconstruction loss: 0.0572688952088356
Training batch 30 / 32
Total batch reconstruction loss: 0.056212909519672394
Training batch 31 / 32
Total batch reconstruction loss: 0.059541162103414536
Training batch 32 / 32
Total batch reconstruction loss: 0.048515353351831436
Epoch [334/500], Train Loss: 0.0566, Validation Loss: 0.0580, Generator Loss: 11.7602, Discriminator Loss: 0.3236
Training epoch 335 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.05607115849852562
Training batch 2 / 32
Total batch reconstruction loss: 0.05575428158044815
Training batch 3 / 32
Total batch reconstruction loss: 0.06026211008429527
Training batch 4 / 32
Total batch reconstruction loss: 0.06097681447863579
Training batch 5 / 32
Total batch reconstruction loss: 0.060402631759643555
Training batch 6 / 32
Total batch reconstruction loss: 0.05580903962254524
Training batch 7 / 32
Total batch reconstruction loss: 0.06278444826602936
Training batch 8 / 32
Total batch reconstruction loss: 0.06082507222890854
Training batch 9 / 32
Total batch reconstruction loss: 0.05472419783473015
Training batch 10 / 32
Total batch reconstruction loss: 0.05874048173427582
Training batch 11 / 32
Total batch reconstruction loss: 0.0601532980799675
Training batch 12 / 32
Total batch reconstruction loss: 0.05876198038458824
Training batch 13 / 32
Total batch reconstruction loss: 0.06213763728737831
Training batch 14 / 32
Total batch reconstruction loss: 0.0588788278400898
Training batch 15 / 32
Total batch reconstruction loss: 0.056128136813640594
Training batch 16 / 32
Total batch reconstruction loss: 0.06053801253437996
Training batch 17 / 32
Total batch reconstruction loss: 0.06394308060407639
Training batch 18 / 32
Total batch reconstruction loss: 0.05615495145320892
Training batch 19 / 32
Total batch reconstruction loss: 0.05859540402889252
Training batch 20 / 32
Total batch reconstruction loss: 0.059574708342552185
Training batch 21 / 32
Total batch reconstruction loss: 0.06075447052717209
Training batch 22 / 32
Total batch reconstruction loss: 0.055132392793893814
Training batch 23 / 32
Total batch reconstruction loss: 0.06372401118278503
Training batch 24 / 32
Total batch reconstruction loss: 0.057846374809741974
Training batch 25 / 32
Total batch reconstruction loss: 0.06104232743382454
Training batch 26 / 32
Total batch reconstruction loss: 0.06315730512142181
Training batch 27 / 32
Total batch reconstruction loss: 0.05533831566572189
Training batch 28 / 32
Total batch reconstruction loss: 0.05946960300207138
Training batch 29 / 32
Total batch reconstruction loss: 0.06511779129505157
Training batch 30 / 32
Total batch reconstruction loss: 0.05652698874473572
Training batch 31 / 32
Total batch reconstruction loss: 0.057360950857400894
Training batch 32 / 32
Total batch reconstruction loss: 0.0474366769194603
Epoch [335/500], Train Loss: 0.0572, Validation Loss: 0.0562, Generator Loss: 11.8447, Discriminator Loss: 0.3194
Training epoch 336 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.06194981187582016
Training batch 2 / 32
Total batch reconstruction loss: 0.055924102663993835
Training batch 3 / 32
Total batch reconstruction loss: 0.054564766585826874
Training batch 4 / 32
Total batch reconstruction loss: 0.060188177973032
Training batch 5 / 32
Total batch reconstruction loss: 0.057945616543293
Training batch 6 / 32
Total batch reconstruction loss: 0.05889769271016121
Training batch 7 / 32
Total batch reconstruction loss: 0.060429200530052185
Training batch 8 / 32
Total batch reconstruction loss: 0.05480596423149109
Training batch 9 / 32
Total batch reconstruction loss: 0.059629812836647034
Training batch 10 / 32
Total batch reconstruction loss: 0.0590628981590271
Training batch 11 / 32
Total batch reconstruction loss: 0.055723562836647034
Training batch 12 / 32
Total batch reconstruction loss: 0.06100216880440712
Training batch 13 / 32
Total batch reconstruction loss: 0.056605711579322815
Training batch 14 / 32
Total batch reconstruction loss: 0.05981821194291115
Training batch 15 / 32
Total batch reconstruction loss: 0.05973683297634125
Training batch 16 / 32
Total batch reconstruction loss: 0.05928076431155205
Training batch 17 / 32
Total batch reconstruction loss: 0.05751354247331619
Training batch 18 / 32
Total batch reconstruction loss: 0.062124598771333694
Training batch 19 / 32
Total batch reconstruction loss: 0.0627489686012268
Training batch 20 / 32
Total batch reconstruction loss: 0.05866006389260292
Training batch 21 / 32
Total batch reconstruction loss: 0.05701268091797829
Training batch 22 / 32
Total batch reconstruction loss: 0.059347473084926605
Training batch 23 / 32
Total batch reconstruction loss: 0.05852649360895157
Training batch 24 / 32
Total batch reconstruction loss: 0.06029285863041878
Training batch 25 / 32
Total batch reconstruction loss: 0.058584924787282944
Training batch 26 / 32
Total batch reconstruction loss: 0.06156621500849724
Training batch 27 / 32
Total batch reconstruction loss: 0.05908637121319771
Training batch 28 / 32
Total batch reconstruction loss: 0.06453738361597061
Training batch 29 / 32
Total batch reconstruction loss: 0.057519033551216125
Training batch 30 / 32
Total batch reconstruction loss: 0.058291733264923096
Training batch 31 / 32
Total batch reconstruction loss: 0.056882187724113464
Training batch 32 / 32
Total batch reconstruction loss: 0.062388572841882706
Epoch [336/500], Train Loss: 0.0575, Validation Loss: 0.0590, Generator Loss: 11.8948, Discriminator Loss: 0.3114
Training epoch 337 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.05294329300522804
Training batch 2 / 32
Total batch reconstruction loss: 0.06316984444856644
Training batch 3 / 32
Total batch reconstruction loss: 0.05956781655550003
Training batch 4 / 32
Total batch reconstruction loss: 0.058509912341833115
Training batch 5 / 32
Total batch reconstruction loss: 0.059904344379901886
Training batch 6 / 32
Total batch reconstruction loss: 0.06261590868234634
Training batch 7 / 32
Total batch reconstruction loss: 0.05585792660713196
Training batch 8 / 32
Total batch reconstruction loss: 0.05713239312171936
Training batch 9 / 32
Total batch reconstruction loss: 0.0572967492043972
Training batch 10 / 32
Total batch reconstruction loss: 0.057941049337387085
Training batch 11 / 32
Total batch reconstruction loss: 0.0549444854259491
Training batch 12 / 32
Total batch reconstruction loss: 0.06500238925218582
Training batch 13 / 32
Total batch reconstruction loss: 0.05896966904401779
Training batch 14 / 32
Total batch reconstruction loss: 0.06032892316579819
Training batch 15 / 32
Total batch reconstruction loss: 0.056427061557769775
Training batch 16 / 32
Total batch reconstruction loss: 0.05732503533363342
Training batch 17 / 32
Total batch reconstruction loss: 0.05684804916381836
Training batch 18 / 32
Total batch reconstruction loss: 0.0580049566924572
Training batch 19 / 32
Total batch reconstruction loss: 0.06136583536863327
Training batch 20 / 32
Total batch reconstruction loss: 0.05699664726853371
Training batch 21 / 32
Total batch reconstruction loss: 0.05737384036183357
Training batch 22 / 32
Total batch reconstruction loss: 0.05982401967048645
Training batch 23 / 32
Total batch reconstruction loss: 0.060100194066762924
Training batch 24 / 32
Total batch reconstruction loss: 0.060035839676856995
Training batch 25 / 32
Total batch reconstruction loss: 0.06437848508358002
Training batch 26 / 32
Total batch reconstruction loss: 0.05777169018983841
Training batch 27 / 32
Total batch reconstruction loss: 0.060702286660671234
Training batch 28 / 32
Total batch reconstruction loss: 0.06524783372879028
Training batch 29 / 32
Total batch reconstruction loss: 0.05922728031873703
Training batch 30 / 32
Total batch reconstruction loss: 0.05648351088166237
Training batch 31 / 32
Total batch reconstruction loss: 0.05972057208418846
Training batch 32 / 32
Total batch reconstruction loss: 0.05232977122068405
Epoch [337/500], Train Loss: 0.0572, Validation Loss: 0.0585, Generator Loss: 11.8347, Discriminator Loss: 0.3397
Training epoch 338 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.056317038834095
Training batch 2 / 32
Total batch reconstruction loss: 0.05677308142185211
Training batch 3 / 32
Total batch reconstruction loss: 0.057704806327819824
Training batch 4 / 32
Total batch reconstruction loss: 0.060171790421009064
Training batch 5 / 32
Total batch reconstruction loss: 0.059963956475257874
Training batch 6 / 32
Total batch reconstruction loss: 0.06464900076389313
Training batch 7 / 32
Total batch reconstruction loss: 0.062305137515068054
Training batch 8 / 32
Total batch reconstruction loss: 0.06278353929519653
Training batch 9 / 32
Total batch reconstruction loss: 0.06254696100950241
Training batch 10 / 32
Total batch reconstruction loss: 0.058684252202510834
Training batch 11 / 32
Total batch reconstruction loss: 0.05327846482396126
Training batch 12 / 32
Total batch reconstruction loss: 0.05754273384809494
Training batch 13 / 32
Total batch reconstruction loss: 0.0577721931040287
Training batch 14 / 32
Total batch reconstruction loss: 0.05762185901403427
Training batch 15 / 32
Total batch reconstruction loss: 0.05880818888545036
Training batch 16 / 32
Total batch reconstruction loss: 0.05838681757450104
Training batch 17 / 32
Total batch reconstruction loss: 0.0606725700199604
Training batch 18 / 32
Total batch reconstruction loss: 0.06021023541688919
Training batch 19 / 32
Total batch reconstruction loss: 0.06038712337613106
Training batch 20 / 32
Total batch reconstruction loss: 0.057552166283130646
Training batch 21 / 32
Total batch reconstruction loss: 0.06220217049121857
Training batch 22 / 32
Total batch reconstruction loss: 0.06094599515199661
Training batch 23 / 32
Total batch reconstruction loss: 0.05548004433512688
Training batch 24 / 32
Total batch reconstruction loss: 0.05861074477434158
Training batch 25 / 32
Total batch reconstruction loss: 0.06096339225769043
Training batch 26 / 32
Total batch reconstruction loss: 0.05608758702874184
Training batch 27 / 32
Total batch reconstruction loss: 0.06042654812335968
Training batch 28 / 32
Total batch reconstruction loss: 0.06275515258312225
Training batch 29 / 32
Total batch reconstruction loss: 0.058362554758787155
Training batch 30 / 32
Total batch reconstruction loss: 0.05542602390050888
Training batch 31 / 32
Total batch reconstruction loss: 0.06254653632640839
Training batch 32 / 32
Total batch reconstruction loss: 0.06405961513519287
Epoch [338/500], Train Loss: 0.0581, Validation Loss: 0.0585, Generator Loss: 11.9639, Discriminator Loss: 0.3047
Training epoch 339 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.061167579144239426
Training batch 2 / 32
Total batch reconstruction loss: 0.05584557354450226
Training batch 3 / 32
Total batch reconstruction loss: 0.06177175045013428
Training batch 4 / 32
Total batch reconstruction loss: 0.06330645084381104
Training batch 5 / 32
Total batch reconstruction loss: 0.059497665613889694
Training batch 6 / 32
Total batch reconstruction loss: 0.06293155997991562
Training batch 7 / 32
Total batch reconstruction loss: 0.05739732086658478
Training batch 8 / 32
Total batch reconstruction loss: 0.05773726850748062
Training batch 9 / 32
Total batch reconstruction loss: 0.06274662911891937
Training batch 10 / 32
Total batch reconstruction loss: 0.059505756944417953
Training batch 11 / 32
Total batch reconstruction loss: 0.057553455233573914
Training batch 12 / 32
Total batch reconstruction loss: 0.05949315428733826
Training batch 13 / 32
Total batch reconstruction loss: 0.06072229892015457
Training batch 14 / 32
Total batch reconstruction loss: 0.058133549988269806
Training batch 15 / 32
Total batch reconstruction loss: 0.06052383407950401
Training batch 16 / 32
Total batch reconstruction loss: 0.05794962868094444
Training batch 17 / 32
Total batch reconstruction loss: 0.05917632579803467
Training batch 18 / 32
Total batch reconstruction loss: 0.058337293565273285
Training batch 19 / 32
Total batch reconstruction loss: 0.05939885601401329
Training batch 20 / 32
Total batch reconstruction loss: 0.055791936814785004
Training batch 21 / 32
Total batch reconstruction loss: 0.05684421956539154
Training batch 22 / 32
Total batch reconstruction loss: 0.056427791714668274
Training batch 23 / 32
Total batch reconstruction loss: 0.05972479283809662
Training batch 24 / 32
Total batch reconstruction loss: 0.05985269695520401
Training batch 25 / 32
Total batch reconstruction loss: 0.06386004388332367
Training batch 26 / 32
Total batch reconstruction loss: 0.05790374428033829
Training batch 27 / 32
Total batch reconstruction loss: 0.05819077789783478
Training batch 28 / 32
Total batch reconstruction loss: 0.059482935816049576
Training batch 29 / 32
Total batch reconstruction loss: 0.05927719920873642
Training batch 30 / 32
Total batch reconstruction loss: 0.05949438363313675
Training batch 31 / 32
Total batch reconstruction loss: 0.06070011109113693
Training batch 32 / 32
Total batch reconstruction loss: 0.04927169904112816
Epoch [339/500], Train Loss: 0.0574, Validation Loss: 0.0588, Generator Loss: 11.8963, Discriminator Loss: 0.2909
Training epoch 340 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.05764234811067581
Training batch 2 / 32
Total batch reconstruction loss: 0.06049264222383499
Training batch 3 / 32
Total batch reconstruction loss: 0.05906992405653
Training batch 4 / 32
Total batch reconstruction loss: 0.057378239929676056
Training batch 5 / 32
Total batch reconstruction loss: 0.05766546353697777
Training batch 6 / 32
Total batch reconstruction loss: 0.061829641461372375
Training batch 7 / 32
Total batch reconstruction loss: 0.05651337280869484
Training batch 8 / 32
Total batch reconstruction loss: 0.057425469160079956
Training batch 9 / 32
Total batch reconstruction loss: 0.054161056876182556
Training batch 10 / 32
Total batch reconstruction loss: 0.061675067991018295
Training batch 11 / 32
Total batch reconstruction loss: 0.06293900310993195
Training batch 12 / 32
Total batch reconstruction loss: 0.06014145165681839
Training batch 13 / 32
Total batch reconstruction loss: 0.057553134858608246
Training batch 14 / 32
Total batch reconstruction loss: 0.059402499347925186
Training batch 15 / 32
Total batch reconstruction loss: 0.056204184889793396
Training batch 16 / 32
Total batch reconstruction loss: 0.05985386669635773
Training batch 17 / 32
Total batch reconstruction loss: 0.05916999280452728
Training batch 18 / 32
Total batch reconstruction loss: 0.06042727082967758
Training batch 19 / 32
Total batch reconstruction loss: 0.06072135269641876
Training batch 20 / 32
Total batch reconstruction loss: 0.06972428411245346
Training batch 21 / 32
Total batch reconstruction loss: 0.061214715242385864
Training batch 22 / 32
Total batch reconstruction loss: 0.05865439027547836
Training batch 23 / 32
Total batch reconstruction loss: 0.06077836826443672
Training batch 24 / 32
Total batch reconstruction loss: 0.05705094709992409
Training batch 25 / 32
Total batch reconstruction loss: 0.05765385553240776
Training batch 26 / 32
Total batch reconstruction loss: 0.05603896453976631
Training batch 27 / 32
Total batch reconstruction loss: 0.057415273040533066
Training batch 28 / 32
Total batch reconstruction loss: 0.05937603488564491
Training batch 29 / 32
Total batch reconstruction loss: 0.0628109946846962
Training batch 30 / 32
Total batch reconstruction loss: 0.05773378908634186
Training batch 31 / 32
Total batch reconstruction loss: 0.0559757724404335
Training batch 32 / 32
Total batch reconstruction loss: 0.046819813549518585
Epoch [340/500], Train Loss: 0.0570, Validation Loss: 0.0575, Generator Loss: 11.8267, Discriminator Loss: 0.3217
Training epoch 341 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.06033886969089508
Training batch 2 / 32
Total batch reconstruction loss: 0.05825059115886688
Training batch 3 / 32
Total batch reconstruction loss: 0.06313115358352661
Training batch 4 / 32
Total batch reconstruction loss: 0.06137147173285484
Training batch 5 / 32
Total batch reconstruction loss: 0.0592728853225708
Training batch 6 / 32
Total batch reconstruction loss: 0.05602189898490906
Training batch 7 / 32
Total batch reconstruction loss: 0.06152235344052315
Training batch 8 / 32
Total batch reconstruction loss: 0.06165185570716858
Training batch 9 / 32
Total batch reconstruction loss: 0.0570799857378006
Training batch 10 / 32
Total batch reconstruction loss: 0.05413554236292839
Training batch 11 / 32
Total batch reconstruction loss: 0.05784980207681656
Training batch 12 / 32
Total batch reconstruction loss: 0.055921174585819244
Training batch 13 / 32
Total batch reconstruction loss: 0.05970989912748337
Training batch 14 / 32
Total batch reconstruction loss: 0.05588672310113907
Training batch 15 / 32
Total batch reconstruction loss: 0.05668869987130165
Training batch 16 / 32
Total batch reconstruction loss: 0.05651175230741501
Training batch 17 / 32
Total batch reconstruction loss: 0.056670770049095154
Training batch 18 / 32
Total batch reconstruction loss: 0.06111250817775726
Training batch 19 / 32
Total batch reconstruction loss: 0.05810049548745155
Training batch 20 / 32
Total batch reconstruction loss: 0.05809490755200386
Training batch 21 / 32
Total batch reconstruction loss: 0.05757244676351547
Training batch 22 / 32
Total batch reconstruction loss: 0.05970471352338791
Training batch 23 / 32
Total batch reconstruction loss: 0.059767842292785645
Training batch 24 / 32
Total batch reconstruction loss: 0.06639052927494049
Training batch 25 / 32
Total batch reconstruction loss: 0.06204430013895035
Training batch 26 / 32
Total batch reconstruction loss: 0.06059420853853226
Training batch 27 / 32
Total batch reconstruction loss: 0.060967084020376205
Training batch 28 / 32
Total batch reconstruction loss: 0.06296426802873611
Training batch 29 / 32
Total batch reconstruction loss: 0.059255100786685944
Training batch 30 / 32
Total batch reconstruction loss: 0.06110943853855133
Training batch 31 / 32
Total batch reconstruction loss: 0.05843084305524826
Training batch 32 / 32
Total batch reconstruction loss: 0.04592258483171463
Epoch [341/500], Train Loss: 0.0573, Validation Loss: 0.0586, Generator Loss: 11.8478, Discriminator Loss: 0.3185
Training epoch 342 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.05923233553767204
Training batch 2 / 32
Total batch reconstruction loss: 0.057556718587875366
Training batch 3 / 32
Total batch reconstruction loss: 0.06081657111644745
Training batch 4 / 32
Total batch reconstruction loss: 0.059959933161735535
Training batch 5 / 32
Total batch reconstruction loss: 0.06042668968439102
Training batch 6 / 32
Total batch reconstruction loss: 0.06101285666227341
Training batch 7 / 32
Total batch reconstruction loss: 0.053636956959962845
Training batch 8 / 32
Total batch reconstruction loss: 0.05772630125284195
Training batch 9 / 32
Total batch reconstruction loss: 0.05632339417934418
Training batch 10 / 32
Total batch reconstruction loss: 0.05836484953761101
Training batch 11 / 32
Total batch reconstruction loss: 0.06106209009885788
Training batch 12 / 32
Total batch reconstruction loss: 0.05918305367231369
Training batch 13 / 32
Total batch reconstruction loss: 0.057380691170692444
Training batch 14 / 32
Total batch reconstruction loss: 0.05530986934900284
Training batch 15 / 32
Total batch reconstruction loss: 0.05856957286596298
Training batch 16 / 32
Total batch reconstruction loss: 0.06495417654514313
Training batch 17 / 32
Total batch reconstruction loss: 0.05591610074043274
Training batch 18 / 32
Total batch reconstruction loss: 0.06165684759616852
Training batch 19 / 32
Total batch reconstruction loss: 0.061362188309431076
Training batch 20 / 32
Total batch reconstruction loss: 0.059870317578315735
Training batch 21 / 32
Total batch reconstruction loss: 0.06026880815625191
Training batch 22 / 32
Total batch reconstruction loss: 0.0588616281747818
Training batch 23 / 32
Total batch reconstruction loss: 0.05628449097275734
Training batch 24 / 32
Total batch reconstruction loss: 0.0634693130850792
Training batch 25 / 32
Total batch reconstruction loss: 0.05786576867103577
Training batch 26 / 32
Total batch reconstruction loss: 0.05698198825120926
Training batch 27 / 32
Total batch reconstruction loss: 0.060849059373140335
Training batch 28 / 32
Total batch reconstruction loss: 0.06046471372246742
Training batch 29 / 32
Total batch reconstruction loss: 0.06557293981313705
Training batch 30 / 32
Total batch reconstruction loss: 0.05902806669473648
Training batch 31 / 32
Total batch reconstruction loss: 0.057833775877952576
Training batch 32 / 32
Total batch reconstruction loss: 0.056056901812553406
Epoch [342/500], Train Loss: 0.0574, Validation Loss: 0.0567, Generator Loss: 11.9128, Discriminator Loss: 0.3112
Training epoch 343 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.05742783099412918
Training batch 2 / 32
Total batch reconstruction loss: 0.05790382996201515
Training batch 3 / 32
Total batch reconstruction loss: 0.0619690865278244
Training batch 4 / 32
Total batch reconstruction loss: 0.05830690264701843
Training batch 5 / 32
Total batch reconstruction loss: 0.056270889937877655
Training batch 6 / 32
Total batch reconstruction loss: 0.059438735246658325
Training batch 7 / 32
Total batch reconstruction loss: 0.05948691815137863
Training batch 8 / 32
Total batch reconstruction loss: 0.056590404361486435
Training batch 9 / 32
Total batch reconstruction loss: 0.05995833873748779
Training batch 10 / 32
Total batch reconstruction loss: 0.061486855149269104
Training batch 11 / 32
Total batch reconstruction loss: 0.0571964792907238
Training batch 12 / 32
Total batch reconstruction loss: 0.05902617797255516
Training batch 13 / 32
Total batch reconstruction loss: 0.059833068400621414
Training batch 14 / 32
Total batch reconstruction loss: 0.06162622570991516
Training batch 15 / 32
Total batch reconstruction loss: 0.06183381751179695
Training batch 16 / 32
Total batch reconstruction loss: 0.060522496700286865
Training batch 17 / 32
Total batch reconstruction loss: 0.059973374009132385
Training batch 18 / 32
Total batch reconstruction loss: 0.05738382413983345
Training batch 19 / 32
Total batch reconstruction loss: 0.056957222521305084
Training batch 20 / 32
Total batch reconstruction loss: 0.06054051220417023
Training batch 21 / 32
Total batch reconstruction loss: 0.05852029472589493
Training batch 22 / 32
Total batch reconstruction loss: 0.05987173691391945
Training batch 23 / 32
Total batch reconstruction loss: 0.062365271151065826
Training batch 24 / 32
Total batch reconstruction loss: 0.05760587006807327
Training batch 25 / 32
Total batch reconstruction loss: 0.05548220872879028
Training batch 26 / 32
Total batch reconstruction loss: 0.05948740243911743
Training batch 27 / 32
Total batch reconstruction loss: 0.059238433837890625
Training batch 28 / 32
Total batch reconstruction loss: 0.057105496525764465
Training batch 29 / 32
Total batch reconstruction loss: 0.059011030942201614
Training batch 30 / 32
Total batch reconstruction loss: 0.057185977697372437
Training batch 31 / 32
Total batch reconstruction loss: 0.061194390058517456
Training batch 32 / 32
Total batch reconstruction loss: 0.06432464718818665
Epoch [343/500], Train Loss: 0.0574, Validation Loss: 0.0585, Generator Loss: 11.9231, Discriminator Loss: 0.3070
Training epoch 344 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.057375334203243256
Training batch 2 / 32
Total batch reconstruction loss: 0.05723255127668381
Training batch 3 / 32
Total batch reconstruction loss: 0.06211990863084793
Training batch 4 / 32
Total batch reconstruction loss: 0.06610138714313507
Training batch 5 / 32
Total batch reconstruction loss: 0.05762752145528793
Training batch 6 / 32
Total batch reconstruction loss: 0.06041760742664337
Training batch 7 / 32
Total batch reconstruction loss: 0.05770206078886986
Training batch 8 / 32
Total batch reconstruction loss: 0.05973385274410248
Training batch 9 / 32
Total batch reconstruction loss: 0.058406759053468704
Training batch 10 / 32
Total batch reconstruction loss: 0.05466879904270172
Training batch 11 / 32
Total batch reconstruction loss: 0.0598822683095932
Training batch 12 / 32
Total batch reconstruction loss: 0.061329435557127
Training batch 13 / 32
Total batch reconstruction loss: 0.059455208480358124
Training batch 14 / 32
Total batch reconstruction loss: 0.05555306747555733
Training batch 15 / 32
Total batch reconstruction loss: 0.0625038594007492
Training batch 16 / 32
Total batch reconstruction loss: 0.059088096022605896
Training batch 17 / 32
Total batch reconstruction loss: 0.05781160295009613
Training batch 18 / 32
Total batch reconstruction loss: 0.059865910559892654
Training batch 19 / 32
Total batch reconstruction loss: 0.058579087257385254
Training batch 20 / 32
Total batch reconstruction loss: 0.059856120496988297
Training batch 21 / 32
Total batch reconstruction loss: 0.06029064953327179
Training batch 22 / 32
Total batch reconstruction loss: 0.05556374788284302
Training batch 23 / 32
Total batch reconstruction loss: 0.059391237795352936
Training batch 24 / 32
Total batch reconstruction loss: 0.058183059096336365
Training batch 25 / 32
Total batch reconstruction loss: 0.05996260419487953
Training batch 26 / 32
Total batch reconstruction loss: 0.059017278254032135
Training batch 27 / 32
Total batch reconstruction loss: 0.05788155645132065
Training batch 28 / 32
Total batch reconstruction loss: 0.06811832636594772
Training batch 29 / 32
Total batch reconstruction loss: 0.057375941425561905
Training batch 30 / 32
Total batch reconstruction loss: 0.06048591434955597
Training batch 31 / 32
Total batch reconstruction loss: 0.058835260570049286
Training batch 32 / 32
Total batch reconstruction loss: 0.0701790452003479
Epoch [344/500], Train Loss: 0.0583, Validation Loss: 0.0589, Generator Loss: 12.0091, Discriminator Loss: 0.3133
Training epoch 345 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.05798421800136566
Training batch 2 / 32
Total batch reconstruction loss: 0.0603061318397522
Training batch 3 / 32
Total batch reconstruction loss: 0.05714147165417671
Training batch 4 / 32
Total batch reconstruction loss: 0.06275786459445953
Training batch 5 / 32
Total batch reconstruction loss: 0.060516536235809326
Training batch 6 / 32
Total batch reconstruction loss: 0.06339269131422043
Training batch 7 / 32
Total batch reconstruction loss: 0.05623500049114227
Training batch 8 / 32
Total batch reconstruction loss: 0.05662655457854271
Training batch 9 / 32
Total batch reconstruction loss: 0.06203571707010269
Training batch 10 / 32
Total batch reconstruction loss: 0.060524433851242065
Training batch 11 / 32
Total batch reconstruction loss: 0.05928684026002884
Training batch 12 / 32
Total batch reconstruction loss: 0.060510970652103424
Training batch 13 / 32
Total batch reconstruction loss: 0.05500490963459015
Training batch 14 / 32
Total batch reconstruction loss: 0.06405360251665115
Training batch 15 / 32
Total batch reconstruction loss: 0.05654900521039963
Training batch 16 / 32
Total batch reconstruction loss: 0.06121364235877991
Training batch 17 / 32
Total batch reconstruction loss: 0.05935663357377052
Training batch 18 / 32
Total batch reconstruction loss: 0.05742068588733673
Training batch 19 / 32
Total batch reconstruction loss: 0.056367941200733185
Training batch 20 / 32
Total batch reconstruction loss: 0.061806440353393555
Training batch 21 / 32
Total batch reconstruction loss: 0.06520166993141174
Training batch 22 / 32
Total batch reconstruction loss: 0.057871945202350616
Training batch 23 / 32
Total batch reconstruction loss: 0.05669492855668068
Training batch 24 / 32
Total batch reconstruction loss: 0.05627761781215668
Training batch 25 / 32
Total batch reconstruction loss: 0.059531524777412415
Training batch 26 / 32
Total batch reconstruction loss: 0.05630609765648842
Training batch 27 / 32
Total batch reconstruction loss: 0.059380244463682175
Training batch 28 / 32
Total batch reconstruction loss: 0.0577995702624321
Training batch 29 / 32
Total batch reconstruction loss: 0.06079069897532463
Training batch 30 / 32
Total batch reconstruction loss: 0.05654428154230118
Training batch 31 / 32
Total batch reconstruction loss: 0.0570576936006546
Training batch 32 / 32
Total batch reconstruction loss: 0.051881272345781326
Epoch [345/500], Train Loss: 0.0573, Validation Loss: 0.0573, Generator Loss: 11.8438, Discriminator Loss: 0.3223
Training epoch 346 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.055930204689502716
Training batch 2 / 32
Total batch reconstruction loss: 0.06098451465368271
Training batch 3 / 32
Total batch reconstruction loss: 0.061063945293426514
Training batch 4 / 32
Total batch reconstruction loss: 0.05908811464905739
Training batch 5 / 32
Total batch reconstruction loss: 0.060750097036361694
Training batch 6 / 32
Total batch reconstruction loss: 0.057374048978090286
Training batch 7 / 32
Total batch reconstruction loss: 0.05717012286186218
Training batch 8 / 32
Total batch reconstruction loss: 0.056593477725982666
Training batch 9 / 32
Total batch reconstruction loss: 0.060968901962041855
Training batch 10 / 32
Total batch reconstruction loss: 0.05633167549967766
Training batch 11 / 32
Total batch reconstruction loss: 0.056813329458236694
Training batch 12 / 32
Total batch reconstruction loss: 0.06053612381219864
Training batch 13 / 32
Total batch reconstruction loss: 0.05863010883331299
Training batch 14 / 32
Total batch reconstruction loss: 0.05835626274347305
Training batch 15 / 32
Total batch reconstruction loss: 0.06185462325811386
Training batch 16 / 32
Total batch reconstruction loss: 0.0574285164475441
Training batch 17 / 32
Total batch reconstruction loss: 0.05760607123374939
Training batch 18 / 32
Total batch reconstruction loss: 0.05924157798290253
Training batch 19 / 32
Total batch reconstruction loss: 0.0597878061234951
Training batch 20 / 32
Total batch reconstruction loss: 0.056068845093250275
Training batch 21 / 32
Total batch reconstruction loss: 0.05846945941448212
Training batch 22 / 32
Total batch reconstruction loss: 0.05557858943939209
Training batch 23 / 32
Total batch reconstruction loss: 0.06230657547712326
Training batch 24 / 32
Total batch reconstruction loss: 0.05940687656402588
Training batch 25 / 32
Total batch reconstruction loss: 0.06127127632498741
Training batch 26 / 32
Total batch reconstruction loss: 0.05894564837217331
Training batch 27 / 32
Total batch reconstruction loss: 0.06224118173122406
Training batch 28 / 32
Total batch reconstruction loss: 0.05813855677843094
Training batch 29 / 32
Total batch reconstruction loss: 0.059259314090013504
Training batch 30 / 32
Total batch reconstruction loss: 0.059324637055397034
Training batch 31 / 32
Total batch reconstruction loss: 0.06094986945390701
Training batch 32 / 32
Total batch reconstruction loss: 0.06312431395053864
Epoch [346/500], Train Loss: 0.0576, Validation Loss: 0.0579, Generator Loss: 11.8951, Discriminator Loss: 0.3164
Training epoch 347 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.058668479323387146
Training batch 2 / 32
Total batch reconstruction loss: 0.06323955208063126
Training batch 3 / 32
Total batch reconstruction loss: 0.059175532311201096
Training batch 4 / 32
Total batch reconstruction loss: 0.05951221287250519
Training batch 5 / 32
Total batch reconstruction loss: 0.05865906551480293
Training batch 6 / 32
Total batch reconstruction loss: 0.06258799135684967
Training batch 7 / 32
Total batch reconstruction loss: 0.05945749580860138
Training batch 8 / 32
Total batch reconstruction loss: 0.06304720789194107
Training batch 9 / 32
Total batch reconstruction loss: 0.06241108104586601
Training batch 10 / 32
Total batch reconstruction loss: 0.057689398527145386
Training batch 11 / 32
Total batch reconstruction loss: 0.05782613158226013
Training batch 12 / 32
Total batch reconstruction loss: 0.06112324818968773
Training batch 13 / 32
Total batch reconstruction loss: 0.05585097894072533
Training batch 14 / 32
Total batch reconstruction loss: 0.0585959255695343
Training batch 15 / 32
Total batch reconstruction loss: 0.05866111069917679
Training batch 16 / 32
Total batch reconstruction loss: 0.05729830637574196
Training batch 17 / 32
Total batch reconstruction loss: 0.05631554126739502
Training batch 18 / 32
Total batch reconstruction loss: 0.058766186237335205
Training batch 19 / 32
Total batch reconstruction loss: 0.05914590135216713
Training batch 20 / 32
Total batch reconstruction loss: 0.05745084211230278
Training batch 21 / 32
Total batch reconstruction loss: 0.061486173421144485
Training batch 22 / 32
Total batch reconstruction loss: 0.059656597673892975
Training batch 23 / 32
Total batch reconstruction loss: 0.0573563314974308
Training batch 24 / 32
Total batch reconstruction loss: 0.057766497135162354
Training batch 25 / 32
Total batch reconstruction loss: 0.059470005333423615
Training batch 26 / 32
Total batch reconstruction loss: 0.056842342019081116
Training batch 27 / 32
Total batch reconstruction loss: 0.05967190861701965
Training batch 28 / 32
Total batch reconstruction loss: 0.061083391308784485
Training batch 29 / 32
Total batch reconstruction loss: 0.05661843717098236
Training batch 30 / 32
Total batch reconstruction loss: 0.05702988803386688
Training batch 31 / 32
Total batch reconstruction loss: 0.05857561156153679
Training batch 32 / 32
Total batch reconstruction loss: 0.05935592204332352
Epoch [347/500], Train Loss: 0.0575, Validation Loss: 0.0574, Generator Loss: 11.8878, Discriminator Loss: 0.3110
Training epoch 348 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.056833989918231964
Training batch 2 / 32
Total batch reconstruction loss: 0.0619928240776062
Training batch 3 / 32
Total batch reconstruction loss: 0.061101242899894714
Training batch 4 / 32
Total batch reconstruction loss: 0.0567687451839447
Training batch 5 / 32
Total batch reconstruction loss: 0.06044255197048187
Training batch 6 / 32
Total batch reconstruction loss: 0.06195267289876938
Training batch 7 / 32
Total batch reconstruction loss: 0.0632597878575325
Training batch 8 / 32
Total batch reconstruction loss: 0.061331070959568024
Training batch 9 / 32
Total batch reconstruction loss: 0.05763343721628189
Training batch 10 / 32
Total batch reconstruction loss: 0.05832850933074951
Training batch 11 / 32
Total batch reconstruction loss: 0.058039579540491104
Training batch 12 / 32
Total batch reconstruction loss: 0.05564896762371063
Training batch 13 / 32
Total batch reconstruction loss: 0.054778240621089935
Training batch 14 / 32
Total batch reconstruction loss: 0.05883828550577164
Training batch 15 / 32
Total batch reconstruction loss: 0.05669180303812027
Training batch 16 / 32
Total batch reconstruction loss: 0.05729912593960762
Training batch 17 / 32
Total batch reconstruction loss: 0.06183618679642677
Training batch 18 / 32
Total batch reconstruction loss: 0.060863230377435684
Training batch 19 / 32
Total batch reconstruction loss: 0.055568598210811615
Training batch 20 / 32
Total batch reconstruction loss: 0.05883464217185974
Training batch 21 / 32
Total batch reconstruction loss: 0.059682607650756836
Training batch 22 / 32
Total batch reconstruction loss: 0.06195804476737976
Training batch 23 / 32
Total batch reconstruction loss: 0.05550394952297211
Training batch 24 / 32
Total batch reconstruction loss: 0.05731523036956787
Training batch 25 / 32
Total batch reconstruction loss: 0.05890780687332153
Training batch 26 / 32
Total batch reconstruction loss: 0.05865319073200226
Training batch 27 / 32
Total batch reconstruction loss: 0.059430159628391266
Training batch 28 / 32
Total batch reconstruction loss: 0.06185527145862579
Training batch 29 / 32
Total batch reconstruction loss: 0.05773645639419556
Training batch 30 / 32
Total batch reconstruction loss: 0.05924815684556961
Training batch 31 / 32
Total batch reconstruction loss: 0.05729440599679947
Training batch 32 / 32
Total batch reconstruction loss: 0.05479227751493454
Epoch [348/500], Train Loss: 0.0572, Validation Loss: 0.0575, Generator Loss: 11.8086, Discriminator Loss: 0.3352
Training epoch 349 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.05419941991567612
Training batch 2 / 32
Total batch reconstruction loss: 0.057785000652074814
Training batch 3 / 32
Total batch reconstruction loss: 0.05587771534919739
Training batch 4 / 32
Total batch reconstruction loss: 0.05844832956790924
Training batch 5 / 32
Total batch reconstruction loss: 0.05836215615272522
Training batch 6 / 32
Total batch reconstruction loss: 0.05931016057729721
Training batch 7 / 32
Total batch reconstruction loss: 0.058992184698581696
Training batch 8 / 32
Total batch reconstruction loss: 0.054947055876255035
Training batch 9 / 32
Total batch reconstruction loss: 0.06365855038166046
Training batch 10 / 32
Total batch reconstruction loss: 0.06075996160507202
Training batch 11 / 32
Total batch reconstruction loss: 0.0584956593811512
Training batch 12 / 32
Total batch reconstruction loss: 0.06009363755583763
Training batch 13 / 32
Total batch reconstruction loss: 0.059034790843725204
Training batch 14 / 32
Total batch reconstruction loss: 0.061766885221004486
Training batch 15 / 32
Total batch reconstruction loss: 0.061256226152181625
Training batch 16 / 32
Total batch reconstruction loss: 0.06095645949244499
Training batch 17 / 32
Total batch reconstruction loss: 0.0630699098110199
Training batch 18 / 32
Total batch reconstruction loss: 0.061537180095911026
Training batch 19 / 32
Total batch reconstruction loss: 0.06036766618490219
Training batch 20 / 32
Total batch reconstruction loss: 0.05870671570301056
Training batch 21 / 32
Total batch reconstruction loss: 0.058434296399354935
Training batch 22 / 32
Total batch reconstruction loss: 0.0575190968811512
Training batch 23 / 32
Total batch reconstruction loss: 0.06092599779367447
Training batch 24 / 32
Total batch reconstruction loss: 0.05995487421751022
Training batch 25 / 32
Total batch reconstruction loss: 0.056585684418678284
Training batch 26 / 32
Total batch reconstruction loss: 0.06406263262033463
Training batch 27 / 32
Total batch reconstruction loss: 0.057564523071050644
Training batch 28 / 32
Total batch reconstruction loss: 0.05693306028842926
Training batch 29 / 32
Total batch reconstruction loss: 0.058725833892822266
Training batch 30 / 32
Total batch reconstruction loss: 0.060338228940963745
Training batch 31 / 32
Total batch reconstruction loss: 0.05729730427265167
Training batch 32 / 32
Total batch reconstruction loss: 0.05818529427051544
Epoch [349/500], Train Loss: 0.0574, Validation Loss: 0.0599, Generator Loss: 11.9174, Discriminator Loss: 0.2968
Training epoch 350 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.0577070526778698
Training batch 2 / 32
Total batch reconstruction loss: 0.059050872921943665
Training batch 3 / 32
Total batch reconstruction loss: 0.05682572349905968
Training batch 4 / 32
Total batch reconstruction loss: 0.05551882088184357
Training batch 5 / 32
Total batch reconstruction loss: 0.05626779422163963
Training batch 6 / 32
Total batch reconstruction loss: 0.05768067017197609
Training batch 7 / 32
Total batch reconstruction loss: 0.06238093972206116
Training batch 8 / 32
Total batch reconstruction loss: 0.05893451347947121
Training batch 9 / 32
Total batch reconstruction loss: 0.0600086934864521
Training batch 10 / 32
Total batch reconstruction loss: 0.05847075581550598
Training batch 11 / 32
Total batch reconstruction loss: 0.0611138790845871
Training batch 12 / 32
Total batch reconstruction loss: 0.05790061503648758
Training batch 13 / 32
Total batch reconstruction loss: 0.058589234948158264
Training batch 14 / 32
Total batch reconstruction loss: 0.05867084860801697
Training batch 15 / 32
Total batch reconstruction loss: 0.05806023254990578
Training batch 16 / 32
Total batch reconstruction loss: 0.05792408436536789
Training batch 17 / 32
Total batch reconstruction loss: 0.05515657737851143
Training batch 18 / 32
Total batch reconstruction loss: 0.05938826501369476
Training batch 19 / 32
Total batch reconstruction loss: 0.06358294188976288
Training batch 20 / 32
Total batch reconstruction loss: 0.06049728021025658
Training batch 21 / 32
Total batch reconstruction loss: 0.05936383828520775
Training batch 22 / 32
Total batch reconstruction loss: 0.05891171097755432
Training batch 23 / 32
Total batch reconstruction loss: 0.061849817633628845
Training batch 24 / 32
Total batch reconstruction loss: 0.0607166588306427
Training batch 25 / 32
Total batch reconstruction loss: 0.06075713410973549
Training batch 26 / 32
Total batch reconstruction loss: 0.05963027477264404
Training batch 27 / 32
Total batch reconstruction loss: 0.05850692093372345
Training batch 28 / 32
Total batch reconstruction loss: 0.056377094238996506
Training batch 29 / 32
Total batch reconstruction loss: 0.05797925963997841
Training batch 30 / 32
Total batch reconstruction loss: 0.06105099990963936
Training batch 31 / 32
Total batch reconstruction loss: 0.061648111790418625
Training batch 32 / 32
Total batch reconstruction loss: 0.05540885031223297
Epoch [350/500], Train Loss: 0.0574, Validation Loss: 0.0573, Generator Loss: 11.8566, Discriminator Loss: 0.3084
Training epoch 351 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.057662930339574814
Training batch 2 / 32
Total batch reconstruction loss: 0.058666445314884186
Training batch 3 / 32
Total batch reconstruction loss: 0.058093853294849396
Training batch 4 / 32
Total batch reconstruction loss: 0.05609077960252762
Training batch 5 / 32
Total batch reconstruction loss: 0.06046091765165329
Training batch 6 / 32
Total batch reconstruction loss: 0.05682552978396416
Training batch 7 / 32
Total batch reconstruction loss: 0.05918267369270325
Training batch 8 / 32
Total batch reconstruction loss: 0.05885936692357063
Training batch 9 / 32
Total batch reconstruction loss: 0.0636921375989914
Training batch 10 / 32
Total batch reconstruction loss: 0.056652188301086426
Training batch 11 / 32
Total batch reconstruction loss: 0.05804417282342911
Training batch 12 / 32
Total batch reconstruction loss: 0.054586153477430344
Training batch 13 / 32
Total batch reconstruction loss: 0.05781412869691849
Training batch 14 / 32
Total batch reconstruction loss: 0.060032621026039124
Training batch 15 / 32
Total batch reconstruction loss: 0.05404932424426079
Training batch 16 / 32
Total batch reconstruction loss: 0.057002954185009
Training batch 17 / 32
Total batch reconstruction loss: 0.0555562786757946
Training batch 18 / 32
Total batch reconstruction loss: 0.06204627454280853
Training batch 19 / 32
Total batch reconstruction loss: 0.05532311275601387
Training batch 20 / 32
Total batch reconstruction loss: 0.06581000983715057
Training batch 21 / 32
Total batch reconstruction loss: 0.060053132474422455
Training batch 22 / 32
Total batch reconstruction loss: 0.06045081466436386
Training batch 23 / 32
Total batch reconstruction loss: 0.05978870391845703
Training batch 24 / 32
Total batch reconstruction loss: 0.061433568596839905
Training batch 25 / 32
Total batch reconstruction loss: 0.06116374209523201
Training batch 26 / 32
Total batch reconstruction loss: 0.06271866708993912
Training batch 27 / 32
Total batch reconstruction loss: 0.058541204780340195
Training batch 28 / 32
Total batch reconstruction loss: 0.05643807351589203
Training batch 29 / 32
Total batch reconstruction loss: 0.0607076957821846
Training batch 30 / 32
Total batch reconstruction loss: 0.058265335857868195
Training batch 31 / 32
Total batch reconstruction loss: 0.06508868932723999
Training batch 32 / 32
Total batch reconstruction loss: 0.0545777752995491
Epoch [351/500], Train Loss: 0.0573, Validation Loss: 0.0568, Generator Loss: 11.8557, Discriminator Loss: 0.3186
Training epoch 352 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.05698292329907417
Training batch 2 / 32
Total batch reconstruction loss: 0.05726202577352524
Training batch 3 / 32
Total batch reconstruction loss: 0.056675031781196594
Training batch 4 / 32
Total batch reconstruction loss: 0.06047358363866806
Training batch 5 / 32
Total batch reconstruction loss: 0.05590086430311203
Training batch 6 / 32
Total batch reconstruction loss: 0.06142398715019226
Training batch 7 / 32
Total batch reconstruction loss: 0.05775848776102066
Training batch 8 / 32
Total batch reconstruction loss: 0.05542607605457306
Training batch 9 / 32
Total batch reconstruction loss: 0.06341622769832611
Training batch 10 / 32
Total batch reconstruction loss: 0.060510434210300446
Training batch 11 / 32
Total batch reconstruction loss: 0.059106118977069855
Training batch 12 / 32
Total batch reconstruction loss: 0.059420231729745865
Training batch 13 / 32
Total batch reconstruction loss: 0.06207667291164398
Training batch 14 / 32
Total batch reconstruction loss: 0.05805402994155884
Training batch 15 / 32
Total batch reconstruction loss: 0.05868624523282051
Training batch 16 / 32
Total batch reconstruction loss: 0.05942497402429581
Training batch 17 / 32
Total batch reconstruction loss: 0.05988983064889908
Training batch 18 / 32
Total batch reconstruction loss: 0.06009650602936745
Training batch 19 / 32
Total batch reconstruction loss: 0.06146026402711868
Training batch 20 / 32
Total batch reconstruction loss: 0.05374404788017273
Training batch 21 / 32
Total batch reconstruction loss: 0.05836133658885956
Training batch 22 / 32
Total batch reconstruction loss: 0.05691642314195633
Training batch 23 / 32
Total batch reconstruction loss: 0.06653409451246262
Training batch 24 / 32
Total batch reconstruction loss: 0.06068682670593262
Training batch 25 / 32
Total batch reconstruction loss: 0.05697003751993179
Training batch 26 / 32
Total batch reconstruction loss: 0.06280416250228882
Training batch 27 / 32
Total batch reconstruction loss: 0.06016291677951813
Training batch 28 / 32
Total batch reconstruction loss: 0.05710992217063904
Training batch 29 / 32
Total batch reconstruction loss: 0.05958705395460129
Training batch 30 / 32
Total batch reconstruction loss: 0.059844642877578735
Training batch 31 / 32
Total batch reconstruction loss: 0.05377298966050148
Training batch 32 / 32
Total batch reconstruction loss: 0.06248854845762253
Epoch [352/500], Train Loss: 0.0573, Validation Loss: 0.0598, Generator Loss: 11.8954, Discriminator Loss: 0.3189
Training epoch 353 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.05529420077800751
Training batch 2 / 32
Total batch reconstruction loss: 0.05932486057281494
Training batch 3 / 32
Total batch reconstruction loss: 0.06038471311330795
Training batch 4 / 32
Total batch reconstruction loss: 0.06347653269767761
Training batch 5 / 32
Total batch reconstruction loss: 0.060381487011909485
Training batch 6 / 32
Total batch reconstruction loss: 0.062782421708107
Training batch 7 / 32
Total batch reconstruction loss: 0.0629015862941742
Training batch 8 / 32
Total batch reconstruction loss: 0.05829402059316635
Training batch 9 / 32
Total batch reconstruction loss: 0.05789497122168541
Training batch 10 / 32
Total batch reconstruction loss: 0.06079966202378273
Training batch 11 / 32
Total batch reconstruction loss: 0.061095066368579865
Training batch 12 / 32
Total batch reconstruction loss: 0.06645826250314713
Training batch 13 / 32
Total batch reconstruction loss: 0.057959236204624176
Training batch 14 / 32
Total batch reconstruction loss: 0.05631515383720398
Training batch 15 / 32
Total batch reconstruction loss: 0.05884256586432457
Training batch 16 / 32
Total batch reconstruction loss: 0.05625542998313904
Training batch 17 / 32
Total batch reconstruction loss: 0.059853680431842804
Training batch 18 / 32
Total batch reconstruction loss: 0.05673181638121605
Training batch 19 / 32
Total batch reconstruction loss: 0.06488354504108429
Training batch 20 / 32
Total batch reconstruction loss: 0.05413617566227913
Training batch 21 / 32
Total batch reconstruction loss: 0.05743284523487091
Training batch 22 / 32
Total batch reconstruction loss: 0.05662303790450096
Training batch 23 / 32
Total batch reconstruction loss: 0.058245062828063965
Training batch 24 / 32
Total batch reconstruction loss: 0.06356209516525269
Training batch 25 / 32
Total batch reconstruction loss: 0.06005461886525154
Training batch 26 / 32
Total batch reconstruction loss: 0.06306737661361694
Training batch 27 / 32
Total batch reconstruction loss: 0.06079212576150894
Training batch 28 / 32
Total batch reconstruction loss: 0.05886620283126831
Training batch 29 / 32
Total batch reconstruction loss: 0.06617075204849243
Training batch 30 / 32
Total batch reconstruction loss: 0.05811621993780136
Training batch 31 / 32
Total batch reconstruction loss: 0.06022818014025688
Training batch 32 / 32
Total batch reconstruction loss: 0.07067826390266418
Epoch [353/500], Train Loss: 0.0583, Validation Loss: 0.0581, Generator Loss: 12.1165, Discriminator Loss: 0.3191
Training epoch 354 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.059505969285964966
Training batch 2 / 32
Total batch reconstruction loss: 0.06159927323460579
Training batch 3 / 32
Total batch reconstruction loss: 0.05552038550376892
Training batch 4 / 32
Total batch reconstruction loss: 0.06000132858753204
Training batch 5 / 32
Total batch reconstruction loss: 0.05910083279013634
Training batch 6 / 32
Total batch reconstruction loss: 0.053290534764528275
Training batch 7 / 32
Total batch reconstruction loss: 0.05766155943274498
Training batch 8 / 32
Total batch reconstruction loss: 0.05705832690000534
Training batch 9 / 32
Total batch reconstruction loss: 0.05999583750963211
Training batch 10 / 32
Total batch reconstruction loss: 0.06581448018550873
Training batch 11 / 32
Total batch reconstruction loss: 0.06384552270174026
Training batch 12 / 32
Total batch reconstruction loss: 0.06075599789619446
Training batch 13 / 32
Total batch reconstruction loss: 0.060222476720809937
Training batch 14 / 32
Total batch reconstruction loss: 0.05690208077430725
Training batch 15 / 32
Total batch reconstruction loss: 0.06047558784484863
Training batch 16 / 32
Total batch reconstruction loss: 0.05484319478273392
Training batch 17 / 32
Total batch reconstruction loss: 0.06213796138763428
Training batch 18 / 32
Total batch reconstruction loss: 0.05648981034755707
Training batch 19 / 32
Total batch reconstruction loss: 0.05956236273050308
Training batch 20 / 32
Total batch reconstruction loss: 0.059840112924575806
Training batch 21 / 32
Total batch reconstruction loss: 0.05926574021577835
Training batch 22 / 32
Total batch reconstruction loss: 0.0644690990447998
Training batch 23 / 32
Total batch reconstruction loss: 0.05895879492163658
Training batch 24 / 32
Total batch reconstruction loss: 0.06366820633411407
Training batch 25 / 32
Total batch reconstruction loss: 0.058544009923934937
Training batch 26 / 32
Total batch reconstruction loss: 0.05893222615122795
Training batch 27 / 32
Total batch reconstruction loss: 0.05581764504313469
Training batch 28 / 32
Total batch reconstruction loss: 0.05522100254893303
Training batch 29 / 32
Total batch reconstruction loss: 0.06060507521033287
Training batch 30 / 32
Total batch reconstruction loss: 0.05918728560209274
Training batch 31 / 32
Total batch reconstruction loss: 0.0628100261092186
Training batch 32 / 32
Total batch reconstruction loss: 0.058284275233745575
Epoch [354/500], Train Loss: 0.0577, Validation Loss: 0.0580, Generator Loss: 11.9415, Discriminator Loss: 0.3214
Training epoch 355 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.06088101863861084
Training batch 2 / 32
Total batch reconstruction loss: 0.061554595828056335
Training batch 3 / 32
Total batch reconstruction loss: 0.056233000010252
Training batch 4 / 32
Total batch reconstruction loss: 0.0619317963719368
Training batch 5 / 32
Total batch reconstruction loss: 0.05710544437170029
Training batch 6 / 32
Total batch reconstruction loss: 0.05650002881884575
Training batch 7 / 32
Total batch reconstruction loss: 0.0552440769970417
Training batch 8 / 32
Total batch reconstruction loss: 0.06537333875894547
Training batch 9 / 32
Total batch reconstruction loss: 0.061390068382024765
Training batch 10 / 32
Total batch reconstruction loss: 0.056533634662628174
Training batch 11 / 32
Total batch reconstruction loss: 0.06070689111948013
Training batch 12 / 32
Total batch reconstruction loss: 0.05590835213661194
Training batch 13 / 32
Total batch reconstruction loss: 0.06045141816139221
Training batch 14 / 32
Total batch reconstruction loss: 0.06263598799705505
Training batch 15 / 32
Total batch reconstruction loss: 0.05768334120512009
Training batch 16 / 32
Total batch reconstruction loss: 0.05926273390650749
Training batch 17 / 32
Total batch reconstruction loss: 0.05920501798391342
Training batch 18 / 32
Total batch reconstruction loss: 0.05837869271636009
Training batch 19 / 32
Total batch reconstruction loss: 0.06183452904224396
Training batch 20 / 32
Total batch reconstruction loss: 0.05755624920129776
Training batch 21 / 32
Total batch reconstruction loss: 0.06401906162500381
Training batch 22 / 32
Total batch reconstruction loss: 0.059011369943618774
Training batch 23 / 32
Total batch reconstruction loss: 0.05677339807152748
Training batch 24 / 32
Total batch reconstruction loss: 0.05454135686159134
Training batch 25 / 32
Total batch reconstruction loss: 0.05638226121664047
Training batch 26 / 32
Total batch reconstruction loss: 0.059248536825180054
Training batch 27 / 32
Total batch reconstruction loss: 0.05983401834964752
Training batch 28 / 32
Total batch reconstruction loss: 0.055007874965667725
Training batch 29 / 32
Total batch reconstruction loss: 0.05888374149799347
Training batch 30 / 32
Total batch reconstruction loss: 0.06289077550172806
Training batch 31 / 32
Total batch reconstruction loss: 0.05892428010702133
Training batch 32 / 32
Total batch reconstruction loss: 0.04559891298413277
Epoch [355/500], Train Loss: 0.0570, Validation Loss: 0.0569, Generator Loss: 11.8148, Discriminator Loss: 0.3042
Training epoch 356 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.06103382632136345
Training batch 2 / 32
Total batch reconstruction loss: 0.06112731248140335
Training batch 3 / 32
Total batch reconstruction loss: 0.05616386607289314
Training batch 4 / 32
Total batch reconstruction loss: 0.060225509107112885
Training batch 5 / 32
Total batch reconstruction loss: 0.05892196297645569
Training batch 6 / 32
Total batch reconstruction loss: 0.06407066434621811
Training batch 7 / 32
Total batch reconstruction loss: 0.06029018387198448
Training batch 8 / 32
Total batch reconstruction loss: 0.05724635347723961
Training batch 9 / 32
Total batch reconstruction loss: 0.055066272616386414
Training batch 10 / 32
Total batch reconstruction loss: 0.06089671701192856
Training batch 11 / 32
Total batch reconstruction loss: 0.05390126630663872
Training batch 12 / 32
Total batch reconstruction loss: 0.06459750980138779
Training batch 13 / 32
Total batch reconstruction loss: 0.058001019060611725
Training batch 14 / 32
Total batch reconstruction loss: 0.055629514157772064
Training batch 15 / 32
Total batch reconstruction loss: 0.060782983899116516
Training batch 16 / 32
Total batch reconstruction loss: 0.060485661029815674
Training batch 17 / 32
Total batch reconstruction loss: 0.057479433715343475
Training batch 18 / 32
Total batch reconstruction loss: 0.0606088750064373
Training batch 19 / 32
Total batch reconstruction loss: 0.05654808506369591
Training batch 20 / 32
Total batch reconstruction loss: 0.05688600242137909
Training batch 21 / 32
Total batch reconstruction loss: 0.05992799252271652
Training batch 22 / 32
Total batch reconstruction loss: 0.060185257345438004
Training batch 23 / 32
Total batch reconstruction loss: 0.05843678116798401
Training batch 24 / 32
Total batch reconstruction loss: 0.06369926780462265
Training batch 25 / 32
Total batch reconstruction loss: 0.057915449142456055
Training batch 26 / 32
Total batch reconstruction loss: 0.05662662163376808
Training batch 27 / 32
Total batch reconstruction loss: 0.05722617357969284
Training batch 28 / 32
Total batch reconstruction loss: 0.060556113719940186
Training batch 29 / 32
Total batch reconstruction loss: 0.060246437788009644
Training batch 30 / 32
Total batch reconstruction loss: 0.056610748171806335
Training batch 31 / 32
Total batch reconstruction loss: 0.05984034389257431
Training batch 32 / 32
Total batch reconstruction loss: 0.04386497661471367
Epoch [356/500], Train Loss: 0.0571, Validation Loss: 0.0562, Generator Loss: 11.7987, Discriminator Loss: 0.2963
Training epoch 357 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.05772341042757034
Training batch 2 / 32
Total batch reconstruction loss: 0.05442807823419571
Training batch 3 / 32
Total batch reconstruction loss: 0.059818439185619354
Training batch 4 / 32
Total batch reconstruction loss: 0.06355364620685577
Training batch 5 / 32
Total batch reconstruction loss: 0.059516336768865585
Training batch 6 / 32
Total batch reconstruction loss: 0.06534764170646667
Training batch 7 / 32
Total batch reconstruction loss: 0.05870382487773895
Training batch 8 / 32
Total batch reconstruction loss: 0.05891154706478119
Training batch 9 / 32
Total batch reconstruction loss: 0.0570480152964592
Training batch 10 / 32
Total batch reconstruction loss: 0.05543573200702667
Training batch 11 / 32
Total batch reconstruction loss: 0.05952073261141777
Training batch 12 / 32
Total batch reconstruction loss: 0.05842333287000656
Training batch 13 / 32
Total batch reconstruction loss: 0.05950449779629707
Training batch 14 / 32
Total batch reconstruction loss: 0.05636833235621452
Training batch 15 / 32
Total batch reconstruction loss: 0.05811317265033722
Training batch 16 / 32
Total batch reconstruction loss: 0.05661589652299881
Training batch 17 / 32
Total batch reconstruction loss: 0.061033666133880615
Training batch 18 / 32
Total batch reconstruction loss: 0.05962628871202469
Training batch 19 / 32
Total batch reconstruction loss: 0.055069491267204285
Training batch 20 / 32
Total batch reconstruction loss: 0.06053999811410904
Training batch 21 / 32
Total batch reconstruction loss: 0.05901607125997543
Training batch 22 / 32
Total batch reconstruction loss: 0.05739413574337959
Training batch 23 / 32
Total batch reconstruction loss: 0.05947515368461609
Training batch 24 / 32
Total batch reconstruction loss: 0.055844746530056
Training batch 25 / 32
Total batch reconstruction loss: 0.061264585703611374
Training batch 26 / 32
Total batch reconstruction loss: 0.06026831269264221
Training batch 27 / 32
Total batch reconstruction loss: 0.05708475410938263
Training batch 28 / 32
Total batch reconstruction loss: 0.05865337699651718
Training batch 29 / 32
Total batch reconstruction loss: 0.05923819541931152
Training batch 30 / 32
Total batch reconstruction loss: 0.062284089624881744
Training batch 31 / 32
Total batch reconstruction loss: 0.05765099823474884
Training batch 32 / 32
Total batch reconstruction loss: 0.09036888927221298
Epoch [357/500], Train Loss: 0.0583, Validation Loss: 0.0577, Generator Loss: 12.0231, Discriminator Loss: 0.3257
Training epoch 358 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.05878554284572601
Training batch 2 / 32
Total batch reconstruction loss: 0.05642567574977875
Training batch 3 / 32
Total batch reconstruction loss: 0.05767937749624252
Training batch 4 / 32
Total batch reconstruction loss: 0.061472028493881226
Training batch 5 / 32
Total batch reconstruction loss: 0.05729592218995094
Training batch 6 / 32
Total batch reconstruction loss: 0.05510410666465759
Training batch 7 / 32
Total batch reconstruction loss: 0.06316430121660233
Training batch 8 / 32
Total batch reconstruction loss: 0.06290888041257858
Training batch 9 / 32
Total batch reconstruction loss: 0.061053358018398285
Training batch 10 / 32
Total batch reconstruction loss: 0.05626969784498215
Training batch 11 / 32
Total batch reconstruction loss: 0.05919787287712097
Training batch 12 / 32
Total batch reconstruction loss: 0.05754336714744568
Training batch 13 / 32
Total batch reconstruction loss: 0.052949607372283936
Training batch 14 / 32
Total batch reconstruction loss: 0.06210615485906601
Training batch 15 / 32
Total batch reconstruction loss: 0.06044934317469597
Training batch 16 / 32
Total batch reconstruction loss: 0.057560328394174576
Training batch 17 / 32
Total batch reconstruction loss: 0.060120679438114166
Training batch 18 / 32
Total batch reconstruction loss: 0.05972592160105705
Training batch 19 / 32
Total batch reconstruction loss: 0.05814947932958603
Training batch 20 / 32
Total batch reconstruction loss: 0.0616605207324028
Training batch 21 / 32
Total batch reconstruction loss: 0.059201668947935104
Training batch 22 / 32
Total batch reconstruction loss: 0.06153927743434906
Training batch 23 / 32
Total batch reconstruction loss: 0.057007014751434326
Training batch 24 / 32
Total batch reconstruction loss: 0.05967634916305542
Training batch 25 / 32
Total batch reconstruction loss: 0.05712292343378067
Training batch 26 / 32
Total batch reconstruction loss: 0.058291245251894
Training batch 27 / 32
Total batch reconstruction loss: 0.05351895093917847
Training batch 28 / 32
Total batch reconstruction loss: 0.05939171090722084
Training batch 29 / 32
Total batch reconstruction loss: 0.06327851116657257
Training batch 30 / 32
Total batch reconstruction loss: 0.05659043416380882
Training batch 31 / 32
Total batch reconstruction loss: 0.06333015859127045
Training batch 32 / 32
Total batch reconstruction loss: 0.0588526651263237
Epoch [358/500], Train Loss: 0.0575, Validation Loss: 0.0577, Generator Loss: 11.8719, Discriminator Loss: 0.3029
Training epoch 359 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.06326784938573837
Training batch 2 / 32
Total batch reconstruction loss: 0.060042399913072586
Training batch 3 / 32
Total batch reconstruction loss: 0.0666179209947586
Training batch 4 / 32
Total batch reconstruction loss: 0.058137886226177216
Training batch 5 / 32
Total batch reconstruction loss: 0.06084812805056572
Training batch 6 / 32
Total batch reconstruction loss: 0.05940666422247887
Training batch 7 / 32
Total batch reconstruction loss: 0.05580741912126541
Training batch 8 / 32
Total batch reconstruction loss: 0.06162601336836815
Training batch 9 / 32
Total batch reconstruction loss: 0.06258383393287659
Training batch 10 / 32
Total batch reconstruction loss: 0.0560106560587883
Training batch 11 / 32
Total batch reconstruction loss: 0.058042190968990326
Training batch 12 / 32
Total batch reconstruction loss: 0.05517534911632538
Training batch 13 / 32
Total batch reconstruction loss: 0.06036699563264847
Training batch 14 / 32
Total batch reconstruction loss: 0.0592123344540596
Training batch 15 / 32
Total batch reconstruction loss: 0.05374782532453537
Training batch 16 / 32
Total batch reconstruction loss: 0.055243127048015594
Training batch 17 / 32
Total batch reconstruction loss: 0.05573601275682449
Training batch 18 / 32
Total batch reconstruction loss: 0.060407064855098724
Training batch 19 / 32
Total batch reconstruction loss: 0.06034442037343979
Training batch 20 / 32
Total batch reconstruction loss: 0.05805177986621857
Training batch 21 / 32
Total batch reconstruction loss: 0.06016315519809723
Training batch 22 / 32
Total batch reconstruction loss: 0.0588870570063591
Training batch 23 / 32
Total batch reconstruction loss: 0.06695352494716644
Training batch 24 / 32
Total batch reconstruction loss: 0.06034160777926445
Training batch 25 / 32
Total batch reconstruction loss: 0.05668294429779053
Training batch 26 / 32
Total batch reconstruction loss: 0.05520695820450783
Training batch 27 / 32
Total batch reconstruction loss: 0.06345994770526886
Training batch 28 / 32
Total batch reconstruction loss: 0.0601922869682312
Training batch 29 / 32
Total batch reconstruction loss: 0.053724028170108795
Training batch 30 / 32
Total batch reconstruction loss: 0.06329723447561264
Training batch 31 / 32
Total batch reconstruction loss: 0.057388171553611755
Training batch 32 / 32
Total batch reconstruction loss: 0.049335308372974396
Epoch [359/500], Train Loss: 0.0572, Validation Loss: 0.0569, Generator Loss: 11.8607, Discriminator Loss: 0.3177
Training epoch 360 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.056759417057037354
Training batch 2 / 32
Total batch reconstruction loss: 0.056997790932655334
Training batch 3 / 32
Total batch reconstruction loss: 0.06173330545425415
Training batch 4 / 32
Total batch reconstruction loss: 0.061488956212997437
Training batch 5 / 32
Total batch reconstruction loss: 0.05758317559957504
Training batch 6 / 32
Total batch reconstruction loss: 0.0589224174618721
Training batch 7 / 32
Total batch reconstruction loss: 0.05894836038351059
Training batch 8 / 32
Total batch reconstruction loss: 0.06466574966907501
Training batch 9 / 32
Total batch reconstruction loss: 0.06216428056359291
Training batch 10 / 32
Total batch reconstruction loss: 0.0573834553360939
Training batch 11 / 32
Total batch reconstruction loss: 0.0651669055223465
Training batch 12 / 32
Total batch reconstruction loss: 0.057553574442863464
Training batch 13 / 32
Total batch reconstruction loss: 0.05991239845752716
Training batch 14 / 32
Total batch reconstruction loss: 0.0566910058259964
Training batch 15 / 32
Total batch reconstruction loss: 0.05824747309088707
Training batch 16 / 32
Total batch reconstruction loss: 0.057898592203855515
Training batch 17 / 32
Total batch reconstruction loss: 0.058873146772384644
Training batch 18 / 32
Total batch reconstruction loss: 0.058742791414260864
Training batch 19 / 32
Total batch reconstruction loss: 0.059034816920757294
Training batch 20 / 32
Total batch reconstruction loss: 0.05854461342096329
Training batch 21 / 32
Total batch reconstruction loss: 0.06423810124397278
Training batch 22 / 32
Total batch reconstruction loss: 0.05973055213689804
Training batch 23 / 32
Total batch reconstruction loss: 0.05815154314041138
Training batch 24 / 32
Total batch reconstruction loss: 0.05952656269073486
Training batch 25 / 32
Total batch reconstruction loss: 0.05312797427177429
Training batch 26 / 32
Total batch reconstruction loss: 0.06013896316289902
Training batch 27 / 32
Total batch reconstruction loss: 0.05775929242372513
Training batch 28 / 32
Total batch reconstruction loss: 0.05742010474205017
Training batch 29 / 32
Total batch reconstruction loss: 0.06153644248843193
Training batch 30 / 32
Total batch reconstruction loss: 0.058256905525922775
Training batch 31 / 32
Total batch reconstruction loss: 0.05494757741689682
Training batch 32 / 32
Total batch reconstruction loss: 0.043843820691108704
Epoch [360/500], Train Loss: 0.0570, Validation Loss: 0.0625, Generator Loss: 11.7830, Discriminator Loss: 0.3313
Training epoch 361 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.05704120174050331
Training batch 2 / 32
Total batch reconstruction loss: 0.05791617929935455
Training batch 3 / 32
Total batch reconstruction loss: 0.060421813279390335
Training batch 4 / 32
Total batch reconstruction loss: 0.05982755869626999
Training batch 5 / 32
Total batch reconstruction loss: 0.05559026822447777
Training batch 6 / 32
Total batch reconstruction loss: 0.05954533815383911
Training batch 7 / 32
Total batch reconstruction loss: 0.0604962520301342
Training batch 8 / 32
Total batch reconstruction loss: 0.060987815260887146
Training batch 9 / 32
Total batch reconstruction loss: 0.06170117110013962
Training batch 10 / 32
Total batch reconstruction loss: 0.05768786370754242
Training batch 11 / 32
Total batch reconstruction loss: 0.059680476784706116
Training batch 12 / 32
Total batch reconstruction loss: 0.05828168988227844
Training batch 13 / 32
Total batch reconstruction loss: 0.06389440596103668
Training batch 14 / 32
Total batch reconstruction loss: 0.0566086545586586
Training batch 15 / 32
Total batch reconstruction loss: 0.06241345778107643
Training batch 16 / 32
Total batch reconstruction loss: 0.059492018073797226
Training batch 17 / 32
Total batch reconstruction loss: 0.057339146733284
Training batch 18 / 32
Total batch reconstruction loss: 0.06258025765419006
Training batch 19 / 32
Total batch reconstruction loss: 0.05943458899855614
Training batch 20 / 32
Total batch reconstruction loss: 0.05744054913520813
Training batch 21 / 32
Total batch reconstruction loss: 0.05802097171545029
Training batch 22 / 32
Total batch reconstruction loss: 0.05812663584947586
Training batch 23 / 32
Total batch reconstruction loss: 0.05957801640033722
Training batch 24 / 32
Total batch reconstruction loss: 0.0588417612016201
Training batch 25 / 32
Total batch reconstruction loss: 0.0591280534863472
Training batch 26 / 32
Total batch reconstruction loss: 0.05989580228924751
Training batch 27 / 32
Total batch reconstruction loss: 0.05730029568076134
Training batch 28 / 32
Total batch reconstruction loss: 0.06247897446155548
Training batch 29 / 32
Total batch reconstruction loss: 0.054660897701978683
Training batch 30 / 32
Total batch reconstruction loss: 0.06020385026931763
Training batch 31 / 32
Total batch reconstruction loss: 0.059108853340148926
Training batch 32 / 32
Total batch reconstruction loss: 0.06310408562421799
Epoch [361/500], Train Loss: 0.0573, Validation Loss: 0.0591, Generator Loss: 11.9413, Discriminator Loss: 0.3082
Training epoch 362 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.05622507259249687
Training batch 2 / 32
Total batch reconstruction loss: 0.060068096965551376
Training batch 3 / 32
Total batch reconstruction loss: 0.05688529089093208
Training batch 4 / 32
Total batch reconstruction loss: 0.0637345165014267
Training batch 5 / 32
Total batch reconstruction loss: 0.058968864381313324
Training batch 6 / 32
Total batch reconstruction loss: 0.06066737696528435
Training batch 7 / 32
Total batch reconstruction loss: 0.0582759715616703
Training batch 8 / 32
Total batch reconstruction loss: 0.05940989777445793
Training batch 9 / 32
Total batch reconstruction loss: 0.05767359584569931
Training batch 10 / 32
Total batch reconstruction loss: 0.06470099836587906
Training batch 11 / 32
Total batch reconstruction loss: 0.06637217104434967
Training batch 12 / 32
Total batch reconstruction loss: 0.058317914605140686
Training batch 13 / 32
Total batch reconstruction loss: 0.06099756807088852
Training batch 14 / 32
Total batch reconstruction loss: 0.05875777825713158
Training batch 15 / 32
Total batch reconstruction loss: 0.05883307754993439
Training batch 16 / 32
Total batch reconstruction loss: 0.056824054569005966
Training batch 17 / 32
Total batch reconstruction loss: 0.057659633457660675
Training batch 18 / 32
Total batch reconstruction loss: 0.05788007751107216
Training batch 19 / 32
Total batch reconstruction loss: 0.05776597559452057
Training batch 20 / 32
Total batch reconstruction loss: 0.05541802942752838
Training batch 21 / 32
Total batch reconstruction loss: 0.05540238693356514
Training batch 22 / 32
Total batch reconstruction loss: 0.058652713894844055
Training batch 23 / 32
Total batch reconstruction loss: 0.056639526039361954
Training batch 24 / 32
Total batch reconstruction loss: 0.05592817813158035
Training batch 25 / 32
Total batch reconstruction loss: 0.057882748544216156
Training batch 26 / 32
Total batch reconstruction loss: 0.05541439354419708
Training batch 27 / 32
Total batch reconstruction loss: 0.05946102365851402
Training batch 28 / 32
Total batch reconstruction loss: 0.06127277389168739
Training batch 29 / 32
Total batch reconstruction loss: 0.05814157798886299
Training batch 30 / 32
Total batch reconstruction loss: 0.05823558196425438
Training batch 31 / 32
Total batch reconstruction loss: 0.06038990616798401
Training batch 32 / 32
Total batch reconstruction loss: 0.09158317744731903
Epoch [362/500], Train Loss: 0.0577, Validation Loss: 0.0565, Generator Loss: 12.0380, Discriminator Loss: 0.3078
Training epoch 363 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.06360411643981934
Training batch 2 / 32
Total batch reconstruction loss: 0.0577501580119133
Training batch 3 / 32
Total batch reconstruction loss: 0.06595416367053986
Training batch 4 / 32
Total batch reconstruction loss: 0.05655740201473236
Training batch 5 / 32
Total batch reconstruction loss: 0.06280285865068436
Training batch 6 / 32
Total batch reconstruction loss: 0.05999627336859703
Training batch 7 / 32
Total batch reconstruction loss: 0.0549222007393837
Training batch 8 / 32
Total batch reconstruction loss: 0.05906166508793831
Training batch 9 / 32
Total batch reconstruction loss: 0.05871704965829849
Training batch 10 / 32
Total batch reconstruction loss: 0.055167246609926224
Training batch 11 / 32
Total batch reconstruction loss: 0.05484490096569061
Training batch 12 / 32
Total batch reconstruction loss: 0.05427546799182892
Training batch 13 / 32
Total batch reconstruction loss: 0.0586729496717453
Training batch 14 / 32
Total batch reconstruction loss: 0.068743497133255
Training batch 15 / 32
Total batch reconstruction loss: 0.05321085453033447
Training batch 16 / 32
Total batch reconstruction loss: 0.059169355779886246
Training batch 17 / 32
Total batch reconstruction loss: 0.058851514011621475
Training batch 18 / 32
Total batch reconstruction loss: 0.05768828094005585
Training batch 19 / 32
Total batch reconstruction loss: 0.05698375403881073
Training batch 20 / 32
Total batch reconstruction loss: 0.058566078543663025
Training batch 21 / 32
Total batch reconstruction loss: 0.058712154626846313
Training batch 22 / 32
Total batch reconstruction loss: 0.060121677815914154
Training batch 23 / 32
Total batch reconstruction loss: 0.06034418195486069
Training batch 24 / 32
Total batch reconstruction loss: 0.055674631148576736
Training batch 25 / 32
Total batch reconstruction loss: 0.0571623295545578
Training batch 26 / 32
Total batch reconstruction loss: 0.06376098096370697
Training batch 27 / 32
Total batch reconstruction loss: 0.058017048984766006
Training batch 28 / 32
Total batch reconstruction loss: 0.05909138172864914
Training batch 29 / 32
Total batch reconstruction loss: 0.05824323743581772
Training batch 30 / 32
Total batch reconstruction loss: 0.060778796672821045
Training batch 31 / 32
Total batch reconstruction loss: 0.056876301765441895
Training batch 32 / 32
Total batch reconstruction loss: 0.05706693232059479
Epoch [363/500], Train Loss: 0.0569, Validation Loss: 0.0577, Generator Loss: 11.8168, Discriminator Loss: 0.3347
Training epoch 364 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.055506519973278046
Training batch 2 / 32
Total batch reconstruction loss: 0.05686822906136513
Training batch 3 / 32
Total batch reconstruction loss: 0.05822710692882538
Training batch 4 / 32
Total batch reconstruction loss: 0.05670168623328209
Training batch 5 / 32
Total batch reconstruction loss: 0.056218020617961884
Training batch 6 / 32
Total batch reconstruction loss: 0.05797137692570686
Training batch 7 / 32
Total batch reconstruction loss: 0.05974549800157547
Training batch 8 / 32
Total batch reconstruction loss: 0.05763077735900879
Training batch 9 / 32
Total batch reconstruction loss: 0.05861455947160721
Training batch 10 / 32
Total batch reconstruction loss: 0.06298360228538513
Training batch 11 / 32
Total batch reconstruction loss: 0.06494317948818207
Training batch 12 / 32
Total batch reconstruction loss: 0.06262289732694626
Training batch 13 / 32
Total batch reconstruction loss: 0.05864117667078972
Training batch 14 / 32
Total batch reconstruction loss: 0.05873928219079971
Training batch 15 / 32
Total batch reconstruction loss: 0.05833348631858826
Training batch 16 / 32
Total batch reconstruction loss: 0.05644148588180542
Training batch 17 / 32
Total batch reconstruction loss: 0.06053250655531883
Training batch 18 / 32
Total batch reconstruction loss: 0.05687866359949112
Training batch 19 / 32
Total batch reconstruction loss: 0.05331266671419144
Training batch 20 / 32
Total batch reconstruction loss: 0.05763964727520943
Training batch 21 / 32
Total batch reconstruction loss: 0.0558679923415184
Training batch 22 / 32
Total batch reconstruction loss: 0.053714312613010406
Training batch 23 / 32
Total batch reconstruction loss: 0.05651122331619263
Training batch 24 / 32
Total batch reconstruction loss: 0.056709252297878265
Training batch 25 / 32
Total batch reconstruction loss: 0.06900176405906677
Training batch 26 / 32
Total batch reconstruction loss: 0.061521079391241074
Training batch 27 / 32
Total batch reconstruction loss: 0.061769284307956696
Training batch 28 / 32
Total batch reconstruction loss: 0.05692754685878754
Training batch 29 / 32
Total batch reconstruction loss: 0.0600275993347168
Training batch 30 / 32
Total batch reconstruction loss: 0.06094422936439514
Training batch 31 / 32
Total batch reconstruction loss: 0.06013795733451843
Training batch 32 / 32
Total batch reconstruction loss: 0.06429314613342285
Epoch [364/500], Train Loss: 0.0572, Validation Loss: 0.0574, Generator Loss: 11.8559, Discriminator Loss: 0.3160
Training epoch 365 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.061877503991127014
Training batch 2 / 32
Total batch reconstruction loss: 0.062547467648983
Training batch 3 / 32
Total batch reconstruction loss: 0.059109315276145935
Training batch 4 / 32
Total batch reconstruction loss: 0.05944552272558212
Training batch 5 / 32
Total batch reconstruction loss: 0.05920204520225525
Training batch 6 / 32
Total batch reconstruction loss: 0.056736722588539124
Training batch 7 / 32
Total batch reconstruction loss: 0.05521571636199951
Training batch 8 / 32
Total batch reconstruction loss: 0.05940094590187073
Training batch 9 / 32
Total batch reconstruction loss: 0.056459277868270874
Training batch 10 / 32
Total batch reconstruction loss: 0.06831741333007812
Training batch 11 / 32
Total batch reconstruction loss: 0.059727586805820465
Training batch 12 / 32
Total batch reconstruction loss: 0.05809184908866882
Training batch 13 / 32
Total batch reconstruction loss: 0.06030580773949623
Training batch 14 / 32
Total batch reconstruction loss: 0.057062648236751556
Training batch 15 / 32
Total batch reconstruction loss: 0.058121852576732635
Training batch 16 / 32
Total batch reconstruction loss: 0.06088457256555557
Training batch 17 / 32
Total batch reconstruction loss: 0.06174043193459511
Training batch 18 / 32
Total batch reconstruction loss: 0.05584251135587692
Training batch 19 / 32
Total batch reconstruction loss: 0.057417601346969604
Training batch 20 / 32
Total batch reconstruction loss: 0.05784578621387482
Training batch 21 / 32
Total batch reconstruction loss: 0.05634522810578346
Training batch 22 / 32
Total batch reconstruction loss: 0.055247340351343155
Training batch 23 / 32
Total batch reconstruction loss: 0.05765175819396973
Training batch 24 / 32
Total batch reconstruction loss: 0.05618530139327049
Training batch 25 / 32
Total batch reconstruction loss: 0.059036336839199066
Training batch 26 / 32
Total batch reconstruction loss: 0.056225769221782684
Training batch 27 / 32
Total batch reconstruction loss: 0.0580470934510231
Training batch 28 / 32
Total batch reconstruction loss: 0.058269429951906204
Training batch 29 / 32
Total batch reconstruction loss: 0.05722038447856903
Training batch 30 / 32
Total batch reconstruction loss: 0.055645763874053955
Training batch 31 / 32
Total batch reconstruction loss: 0.06303956359624863
Training batch 32 / 32
Total batch reconstruction loss: 0.049162428826093674
Epoch [365/500], Train Loss: 0.0566, Validation Loss: 0.0586, Generator Loss: 11.7311, Discriminator Loss: 0.3365
Training epoch 366 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.06228114664554596
Training batch 2 / 32
Total batch reconstruction loss: 0.05581396073102951
Training batch 3 / 32
Total batch reconstruction loss: 0.06009141355752945
Training batch 4 / 32
Total batch reconstruction loss: 0.05636085197329521
Training batch 5 / 32
Total batch reconstruction loss: 0.0615953654050827
Training batch 6 / 32
Total batch reconstruction loss: 0.05933753028512001
Training batch 7 / 32
Total batch reconstruction loss: 0.06154748052358627
Training batch 8 / 32
Total batch reconstruction loss: 0.0573979914188385
Training batch 9 / 32
Total batch reconstruction loss: 0.0552455373108387
Training batch 10 / 32
Total batch reconstruction loss: 0.0575479194521904
Training batch 11 / 32
Total batch reconstruction loss: 0.053822942078113556
Training batch 12 / 32
Total batch reconstruction loss: 0.06308712065219879
Training batch 13 / 32
Total batch reconstruction loss: 0.05962527170777321
Training batch 14 / 32
Total batch reconstruction loss: 0.05271367356181145
Training batch 15 / 32
Total batch reconstruction loss: 0.06099690869450569
Training batch 16 / 32
Total batch reconstruction loss: 0.05900377780199051
Training batch 17 / 32
Total batch reconstruction loss: 0.0574522465467453
Training batch 18 / 32
Total batch reconstruction loss: 0.06254181265830994
Training batch 19 / 32
Total batch reconstruction loss: 0.056142084300518036
Training batch 20 / 32
Total batch reconstruction loss: 0.05814580246806145
Training batch 21 / 32
Total batch reconstruction loss: 0.05924506112933159
Training batch 22 / 32
Total batch reconstruction loss: 0.060144513845443726
Training batch 23 / 32
Total batch reconstruction loss: 0.058281391859054565
Training batch 24 / 32
Total batch reconstruction loss: 0.06177088990807533
Training batch 25 / 32
Total batch reconstruction loss: 0.062307290732860565
Training batch 26 / 32
Total batch reconstruction loss: 0.06160986050963402
Training batch 27 / 32
Total batch reconstruction loss: 0.059818245470523834
Training batch 28 / 32
Total batch reconstruction loss: 0.059105779975652695
Training batch 29 / 32
Total batch reconstruction loss: 0.06191377714276314
Training batch 30 / 32
Total batch reconstruction loss: 0.05640403553843498
Training batch 31 / 32
Total batch reconstruction loss: 0.05759269744157791
Training batch 32 / 32
Total batch reconstruction loss: 0.04902946949005127
Epoch [366/500], Train Loss: 0.0571, Validation Loss: 0.0590, Generator Loss: 11.8058, Discriminator Loss: 0.3200
Training epoch 367 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.056081876158714294
Training batch 2 / 32
Total batch reconstruction loss: 0.061102960258722305
Training batch 3 / 32
Total batch reconstruction loss: 0.06195465475320816
Training batch 4 / 32
Total batch reconstruction loss: 0.06396841257810593
Training batch 5 / 32
Total batch reconstruction loss: 0.0605887770652771
Training batch 6 / 32
Total batch reconstruction loss: 0.0652361735701561
Training batch 7 / 32
Total batch reconstruction loss: 0.06403891742229462
Training batch 8 / 32
Total batch reconstruction loss: 0.057657793164253235
Training batch 9 / 32
Total batch reconstruction loss: 0.06083512678742409
Training batch 10 / 32
Total batch reconstruction loss: 0.06170661747455597
Training batch 11 / 32
Total batch reconstruction loss: 0.05931684747338295
Training batch 12 / 32
Total batch reconstruction loss: 0.05850820243358612
Training batch 13 / 32
Total batch reconstruction loss: 0.06064676493406296
Training batch 14 / 32
Total batch reconstruction loss: 0.056796178221702576
Training batch 15 / 32
Total batch reconstruction loss: 0.059829022735357285
Training batch 16 / 32
Total batch reconstruction loss: 0.056985653936862946
Training batch 17 / 32
Total batch reconstruction loss: 0.05450185388326645
Training batch 18 / 32
Total batch reconstruction loss: 0.05754489451646805
Training batch 19 / 32
Total batch reconstruction loss: 0.05960674583911896
Training batch 20 / 32
Total batch reconstruction loss: 0.05969417095184326
Training batch 21 / 32
Total batch reconstruction loss: 0.0621531717479229
Training batch 22 / 32
Total batch reconstruction loss: 0.05711796134710312
Training batch 23 / 32
Total batch reconstruction loss: 0.05996676906943321
Training batch 24 / 32
Total batch reconstruction loss: 0.059424810111522675
Training batch 25 / 32
Total batch reconstruction loss: 0.05722012370824814
Training batch 26 / 32
Total batch reconstruction loss: 0.05901592969894409
Training batch 27 / 32
Total batch reconstruction loss: 0.056040599942207336
Training batch 28 / 32
Total batch reconstruction loss: 0.057637035846710205
Training batch 29 / 32
Total batch reconstruction loss: 0.05618790537118912
Training batch 30 / 32
Total batch reconstruction loss: 0.0659542828798294
Training batch 31 / 32
Total batch reconstruction loss: 0.056076906621456146
Training batch 32 / 32
Total batch reconstruction loss: 0.051276933401823044
Epoch [367/500], Train Loss: 0.0577, Validation Loss: 0.0576, Generator Loss: 11.9186, Discriminator Loss: 0.3035
Training epoch 368 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.06097051873803139
Training batch 2 / 32
Total batch reconstruction loss: 0.05836699903011322
Training batch 3 / 32
Total batch reconstruction loss: 0.05949973315000534
Training batch 4 / 32
Total batch reconstruction loss: 0.059048086404800415
Training batch 5 / 32
Total batch reconstruction loss: 0.058779142796993256
Training batch 6 / 32
Total batch reconstruction loss: 0.05906417593359947
Training batch 7 / 32
Total batch reconstruction loss: 0.056190695613622665
Training batch 8 / 32
Total batch reconstruction loss: 0.06162770092487335
Training batch 9 / 32
Total batch reconstruction loss: 0.06620793044567108
Training batch 10 / 32
Total batch reconstruction loss: 0.056464359164237976
Training batch 11 / 32
Total batch reconstruction loss: 0.054528333246707916
Training batch 12 / 32
Total batch reconstruction loss: 0.056054744869470596
Training batch 13 / 32
Total batch reconstruction loss: 0.058752190321683884
Training batch 14 / 32
Total batch reconstruction loss: 0.064146988093853
Training batch 15 / 32
Total batch reconstruction loss: 0.058922067284584045
Training batch 16 / 32
Total batch reconstruction loss: 0.058884650468826294
Training batch 17 / 32
Total batch reconstruction loss: 0.05911015719175339
Training batch 18 / 32
Total batch reconstruction loss: 0.056661058217287064
Training batch 19 / 32
Total batch reconstruction loss: 0.06010638177394867
Training batch 20 / 32
Total batch reconstruction loss: 0.061266377568244934
Training batch 21 / 32
Total batch reconstruction loss: 0.059248097240924835
Training batch 22 / 32
Total batch reconstruction loss: 0.06213061138987541
Training batch 23 / 32
Total batch reconstruction loss: 0.06029900163412094
Training batch 24 / 32
Total batch reconstruction loss: 0.056785859167575836
Training batch 25 / 32
Total batch reconstruction loss: 0.056489914655685425
Training batch 26 / 32
Total batch reconstruction loss: 0.05473078787326813
Training batch 27 / 32
Total batch reconstruction loss: 0.054215144366025925
Training batch 28 / 32
Total batch reconstruction loss: 0.057097263634204865
Training batch 29 / 32
Total batch reconstruction loss: 0.059956666082143784
Training batch 30 / 32
Total batch reconstruction loss: 0.05688406154513359
Training batch 31 / 32
Total batch reconstruction loss: 0.060007356107234955
Training batch 32 / 32
Total batch reconstruction loss: 0.04939591884613037
Epoch [368/500], Train Loss: 0.0567, Validation Loss: 0.0578, Generator Loss: 11.7726, Discriminator Loss: 0.3093
Training epoch 369 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.05847954377532005
Training batch 2 / 32
Total batch reconstruction loss: 0.05730757117271423
Training batch 3 / 32
Total batch reconstruction loss: 0.05844539776444435
Training batch 4 / 32
Total batch reconstruction loss: 0.06441813707351685
Training batch 5 / 32
Total batch reconstruction loss: 0.057762980461120605
Training batch 6 / 32
Total batch reconstruction loss: 0.0583290234208107
Training batch 7 / 32
Total batch reconstruction loss: 0.05717586353421211
Training batch 8 / 32
Total batch reconstruction loss: 0.06303156912326813
Training batch 9 / 32
Total batch reconstruction loss: 0.0595402866601944
Training batch 10 / 32
Total batch reconstruction loss: 0.061276018619537354
Training batch 11 / 32
Total batch reconstruction loss: 0.05612032487988472
Training batch 12 / 32
Total batch reconstruction loss: 0.057648710906505585
Training batch 13 / 32
Total batch reconstruction loss: 0.058039791882038116
Training batch 14 / 32
Total batch reconstruction loss: 0.05621495470404625
Training batch 15 / 32
Total batch reconstruction loss: 0.05778053402900696
Training batch 16 / 32
Total batch reconstruction loss: 0.06023677438497543
Training batch 17 / 32
Total batch reconstruction loss: 0.05605274811387062
Training batch 18 / 32
Total batch reconstruction loss: 0.052839167416095734
Training batch 19 / 32
Total batch reconstruction loss: 0.0569419227540493
Training batch 20 / 32
Total batch reconstruction loss: 0.05991799384355545
Training batch 21 / 32
Total batch reconstruction loss: 0.0588284507393837
Training batch 22 / 32
Total batch reconstruction loss: 0.06773776561021805
Training batch 23 / 32
Total batch reconstruction loss: 0.061515361070632935
Training batch 24 / 32
Total batch reconstruction loss: 0.056603480130434036
Training batch 25 / 32
Total batch reconstruction loss: 0.06222780793905258
Training batch 26 / 32
Total batch reconstruction loss: 0.05716020613908768
Training batch 27 / 32
Total batch reconstruction loss: 0.061169758439064026
Training batch 28 / 32
Total batch reconstruction loss: 0.057402387261390686
Training batch 29 / 32
Total batch reconstruction loss: 0.05727113038301468
Training batch 30 / 32
Total batch reconstruction loss: 0.05928859859704971
Training batch 31 / 32
Total batch reconstruction loss: 0.05655984580516815
Training batch 32 / 32
Total batch reconstruction loss: 0.05258609354496002
Epoch [369/500], Train Loss: 0.0567, Validation Loss: 0.0561, Generator Loss: 11.7916, Discriminator Loss: 0.3193
Training epoch 370 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.06063997745513916
Training batch 2 / 32
Total batch reconstruction loss: 0.05713622272014618
Training batch 3 / 32
Total batch reconstruction loss: 0.05490582436323166
Training batch 4 / 32
Total batch reconstruction loss: 0.06173563748598099
Training batch 5 / 32
Total batch reconstruction loss: 0.05969787761569023
Training batch 6 / 32
Total batch reconstruction loss: 0.05622479319572449
Training batch 7 / 32
Total batch reconstruction loss: 0.05996467173099518
Training batch 8 / 32
Total batch reconstruction loss: 0.06409505009651184
Training batch 9 / 32
Total batch reconstruction loss: 0.05845651030540466
Training batch 10 / 32
Total batch reconstruction loss: 0.05646070837974548
Training batch 11 / 32
Total batch reconstruction loss: 0.05651112645864487
Training batch 12 / 32
Total batch reconstruction loss: 0.058335840702056885
Training batch 13 / 32
Total batch reconstruction loss: 0.0539160780608654
Training batch 14 / 32
Total batch reconstruction loss: 0.05931752920150757
Training batch 15 / 32
Total batch reconstruction loss: 0.06061107665300369
Training batch 16 / 32
Total batch reconstruction loss: 0.058372996747493744
Training batch 17 / 32
Total batch reconstruction loss: 0.05830533057451248
Training batch 18 / 32
Total batch reconstruction loss: 0.05951152741909027
Training batch 19 / 32
Total batch reconstruction loss: 0.0606837272644043
Training batch 20 / 32
Total batch reconstruction loss: 0.05836611986160278
Training batch 21 / 32
Total batch reconstruction loss: 0.05728106573224068
Training batch 22 / 32
Total batch reconstruction loss: 0.05618591606616974
Training batch 23 / 32
Total batch reconstruction loss: 0.05905173718929291
Training batch 24 / 32
Total batch reconstruction loss: 0.060507632791996
Training batch 25 / 32
Total batch reconstruction loss: 0.05645034462213516
Training batch 26 / 32
Total batch reconstruction loss: 0.062256500124931335
Training batch 27 / 32
Total batch reconstruction loss: 0.060398124158382416
Training batch 28 / 32
Total batch reconstruction loss: 0.05820412188768387
Training batch 29 / 32
Total batch reconstruction loss: 0.06167834624648094
Training batch 30 / 32
Total batch reconstruction loss: 0.057122327387332916
Training batch 31 / 32
Total batch reconstruction loss: 0.05823127180337906
Training batch 32 / 32
Total batch reconstruction loss: 0.051340844482183456
Epoch [370/500], Train Loss: 0.0568, Validation Loss: 0.0564, Generator Loss: 11.7758, Discriminator Loss: 0.3028
Training epoch 371 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.06044731289148331
Training batch 2 / 32
Total batch reconstruction loss: 0.055391162633895874
Training batch 3 / 32
Total batch reconstruction loss: 0.059855442494153976
Training batch 4 / 32
Total batch reconstruction loss: 0.059649981558322906
Training batch 5 / 32
Total batch reconstruction loss: 0.0552792064845562
Training batch 6 / 32
Total batch reconstruction loss: 0.06578797101974487
Training batch 7 / 32
Total batch reconstruction loss: 0.058288030326366425
Training batch 8 / 32
Total batch reconstruction loss: 0.05826187878847122
Training batch 9 / 32
Total batch reconstruction loss: 0.056828729808330536
Training batch 10 / 32
Total batch reconstruction loss: 0.05998411029577255
Training batch 11 / 32
Total batch reconstruction loss: 0.055194929242134094
Training batch 12 / 32
Total batch reconstruction loss: 0.059940826147794724
Training batch 13 / 32
Total batch reconstruction loss: 0.05566280335187912
Training batch 14 / 32
Total batch reconstruction loss: 0.06091364100575447
Training batch 15 / 32
Total batch reconstruction loss: 0.06177618354558945
Training batch 16 / 32
Total batch reconstruction loss: 0.056425999850034714
Training batch 17 / 32
Total batch reconstruction loss: 0.05883295461535454
Training batch 18 / 32
Total batch reconstruction loss: 0.05648333206772804
Training batch 19 / 32
Total batch reconstruction loss: 0.05912946164608002
Training batch 20 / 32
Total batch reconstruction loss: 0.06066529452800751
Training batch 21 / 32
Total batch reconstruction loss: 0.056858647614717484
Training batch 22 / 32
Total batch reconstruction loss: 0.05839860439300537
Training batch 23 / 32
Total batch reconstruction loss: 0.062501460313797
Training batch 24 / 32
Total batch reconstruction loss: 0.06213627755641937
Training batch 25 / 32
Total batch reconstruction loss: 0.0607907772064209
Training batch 26 / 32
Total batch reconstruction loss: 0.056499794125556946
Training batch 27 / 32
Total batch reconstruction loss: 0.05682012438774109
Training batch 28 / 32
Total batch reconstruction loss: 0.057935647666454315
Training batch 29 / 32
Total batch reconstruction loss: 0.05732753872871399
Training batch 30 / 32
Total batch reconstruction loss: 0.056439898908138275
Training batch 31 / 32
Total batch reconstruction loss: 0.05736447870731354
Training batch 32 / 32
Total batch reconstruction loss: 0.10119467228651047
Epoch [371/500], Train Loss: 0.0581, Validation Loss: 0.0566, Generator Loss: 12.0662, Discriminator Loss: 0.3123
Training epoch 372 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.05667634308338165
Training batch 2 / 32
Total batch reconstruction loss: 0.05950361117720604
Training batch 3 / 32
Total batch reconstruction loss: 0.05972106754779816
Training batch 4 / 32
Total batch reconstruction loss: 0.057596590369939804
Training batch 5 / 32
Total batch reconstruction loss: 0.05972845479846001
Training batch 6 / 32
Total batch reconstruction loss: 0.0631674975156784
Training batch 7 / 32
Total batch reconstruction loss: 0.06333027780056
Training batch 8 / 32
Total batch reconstruction loss: 0.05893324315547943
Training batch 9 / 32
Total batch reconstruction loss: 0.06020914018154144
Training batch 10 / 32
Total batch reconstruction loss: 0.060292549431324005
Training batch 11 / 32
Total batch reconstruction loss: 0.059881821274757385
Training batch 12 / 32
Total batch reconstruction loss: 0.06322304904460907
Training batch 13 / 32
Total batch reconstruction loss: 0.056367963552474976
Training batch 14 / 32
Total batch reconstruction loss: 0.05481422320008278
Training batch 15 / 32
Total batch reconstruction loss: 0.05518614500761032
Training batch 16 / 32
Total batch reconstruction loss: 0.0586012601852417
Training batch 17 / 32
Total batch reconstruction loss: 0.054836004972457886
Training batch 18 / 32
Total batch reconstruction loss: 0.06092320382595062
Training batch 19 / 32
Total batch reconstruction loss: 0.05866440013051033
Training batch 20 / 32
Total batch reconstruction loss: 0.05947059392929077
Training batch 21 / 32
Total batch reconstruction loss: 0.05918324738740921
Training batch 22 / 32
Total batch reconstruction loss: 0.058859728276729584
Training batch 23 / 32
Total batch reconstruction loss: 0.056481823325157166
Training batch 24 / 32
Total batch reconstruction loss: 0.05710245296359062
Training batch 25 / 32
Total batch reconstruction loss: 0.060090746730566025
Training batch 26 / 32
Total batch reconstruction loss: 0.05929034948348999
Training batch 27 / 32
Total batch reconstruction loss: 0.05634601414203644
Training batch 28 / 32
Total batch reconstruction loss: 0.06309936195611954
Training batch 29 / 32
Total batch reconstruction loss: 0.05409170687198639
Training batch 30 / 32
Total batch reconstruction loss: 0.05652894452214241
Training batch 31 / 32
Total batch reconstruction loss: 0.06015177071094513
Training batch 32 / 32
Total batch reconstruction loss: 0.05450751632452011
Epoch [372/500], Train Loss: 0.0567, Validation Loss: 0.0566, Generator Loss: 11.7959, Discriminator Loss: 0.3199
Training epoch 373 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.05566798150539398
Training batch 2 / 32
Total batch reconstruction loss: 0.05466679483652115
Training batch 3 / 32
Total batch reconstruction loss: 0.06397251039743423
Training batch 4 / 32
Total batch reconstruction loss: 0.05888976901769638
Training batch 5 / 32
Total batch reconstruction loss: 0.06033775955438614
Training batch 6 / 32
Total batch reconstruction loss: 0.05837799608707428
Training batch 7 / 32
Total batch reconstruction loss: 0.06193359196186066
Training batch 8 / 32
Total batch reconstruction loss: 0.0574125312268734
Training batch 9 / 32
Total batch reconstruction loss: 0.058507040143013
Training batch 10 / 32
Total batch reconstruction loss: 0.058350808918476105
Training batch 11 / 32
Total batch reconstruction loss: 0.056399792432785034
Training batch 12 / 32
Total batch reconstruction loss: 0.0537915974855423
Training batch 13 / 32
Total batch reconstruction loss: 0.06026095151901245
Training batch 14 / 32
Total batch reconstruction loss: 0.05753573775291443
Training batch 15 / 32
Total batch reconstruction loss: 0.061123382300138474
Training batch 16 / 32
Total batch reconstruction loss: 0.05778689682483673
Training batch 17 / 32
Total batch reconstruction loss: 0.06087245047092438
Training batch 18 / 32
Total batch reconstruction loss: 0.05804162472486496
Training batch 19 / 32
Total batch reconstruction loss: 0.057842716574668884
Training batch 20 / 32
Total batch reconstruction loss: 0.05903369188308716
Training batch 21 / 32
Total batch reconstruction loss: 0.054082758724689484
Training batch 22 / 32
Total batch reconstruction loss: 0.06411012262105942
Training batch 23 / 32
Total batch reconstruction loss: 0.05688076466321945
Training batch 24 / 32
Total batch reconstruction loss: 0.05584514141082764
Training batch 25 / 32
Total batch reconstruction loss: 0.06638558954000473
Training batch 26 / 32
Total batch reconstruction loss: 0.06387005746364594
Training batch 27 / 32
Total batch reconstruction loss: 0.057834915816783905
Training batch 28 / 32
Total batch reconstruction loss: 0.060442522168159485
Training batch 29 / 32
Total batch reconstruction loss: 0.05999557301402092
Training batch 30 / 32
Total batch reconstruction loss: 0.05604711174964905
Training batch 31 / 32
Total batch reconstruction loss: 0.057276174426078796
Training batch 32 / 32
Total batch reconstruction loss: 0.06198796257376671
Epoch [373/500], Train Loss: 0.0568, Validation Loss: 0.0570, Generator Loss: 11.8593, Discriminator Loss: 0.3049
Training epoch 374 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.05625472962856293
Training batch 2 / 32
Total batch reconstruction loss: 0.055959105491638184
Training batch 3 / 32
Total batch reconstruction loss: 0.058759573847055435
Training batch 4 / 32
Total batch reconstruction loss: 0.055236443877220154
Training batch 5 / 32
Total batch reconstruction loss: 0.062602698802948
Training batch 6 / 32
Total batch reconstruction loss: 0.053746215999126434
Training batch 7 / 32
Total batch reconstruction loss: 0.06088133901357651
Training batch 8 / 32
Total batch reconstruction loss: 0.06095528602600098
Training batch 9 / 32
Total batch reconstruction loss: 0.061108626425266266
Training batch 10 / 32
Total batch reconstruction loss: 0.06167832016944885
Training batch 11 / 32
Total batch reconstruction loss: 0.05932377278804779
Training batch 12 / 32
Total batch reconstruction loss: 0.053460493683815
Training batch 13 / 32
Total batch reconstruction loss: 0.056093983352184296
Training batch 14 / 32
Total batch reconstruction loss: 0.06173792481422424
Training batch 15 / 32
Total batch reconstruction loss: 0.06489552557468414
Training batch 16 / 32
Total batch reconstruction loss: 0.06354930996894836
Training batch 17 / 32
Total batch reconstruction loss: 0.06052818149328232
Training batch 18 / 32
Total batch reconstruction loss: 0.0612882561981678
Training batch 19 / 32
Total batch reconstruction loss: 0.06003592908382416
Training batch 20 / 32
Total batch reconstruction loss: 0.06248270347714424
Training batch 21 / 32
Total batch reconstruction loss: 0.06075119227170944
Training batch 22 / 32
Total batch reconstruction loss: 0.0548202283680439
Training batch 23 / 32
Total batch reconstruction loss: 0.056683026254177094
Training batch 24 / 32
Total batch reconstruction loss: 0.06295084953308105
Training batch 25 / 32
Total batch reconstruction loss: 0.06118033826351166
Training batch 26 / 32
Total batch reconstruction loss: 0.059446364641189575
Training batch 27 / 32
Total batch reconstruction loss: 0.06062904745340347
Training batch 28 / 32
Total batch reconstruction loss: 0.05825132131576538
Training batch 29 / 32
Total batch reconstruction loss: 0.05913873016834259
Training batch 30 / 32
Total batch reconstruction loss: 0.05983888730406761
Training batch 31 / 32
Total batch reconstruction loss: 0.05620037019252777
Training batch 32 / 32
Total batch reconstruction loss: 0.05025605112314224
Epoch [374/500], Train Loss: 0.0573, Validation Loss: 0.0573, Generator Loss: 11.8813, Discriminator Loss: 0.3216
Training epoch 375 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.06361117959022522
Training batch 2 / 32
Total batch reconstruction loss: 0.06429099291563034
Training batch 3 / 32
Total batch reconstruction loss: 0.0582699254155159
Training batch 4 / 32
Total batch reconstruction loss: 0.05690409243106842
Training batch 5 / 32
Total batch reconstruction loss: 0.0599997453391552
Training batch 6 / 32
Total batch reconstruction loss: 0.05686128884553909
Training batch 7 / 32
Total batch reconstruction loss: 0.055289581418037415
Training batch 8 / 32
Total batch reconstruction loss: 0.057621248066425323
Training batch 9 / 32
Total batch reconstruction loss: 0.06314334273338318
Training batch 10 / 32
Total batch reconstruction loss: 0.05714280158281326
Training batch 11 / 32
Total batch reconstruction loss: 0.05732027068734169
Training batch 12 / 32
Total batch reconstruction loss: 0.061916232109069824
Training batch 13 / 32
Total batch reconstruction loss: 0.06050214171409607
Training batch 14 / 32
Total batch reconstruction loss: 0.05742039531469345
Training batch 15 / 32
Total batch reconstruction loss: 0.059364743530750275
Training batch 16 / 32
Total batch reconstruction loss: 0.05855048447847366
Training batch 17 / 32
Total batch reconstruction loss: 0.060137927532196045
Training batch 18 / 32
Total batch reconstruction loss: 0.0605962797999382
Training batch 19 / 32
Total batch reconstruction loss: 0.05590229481458664
Training batch 20 / 32
Total batch reconstruction loss: 0.05683543160557747
Training batch 21 / 32
Total batch reconstruction loss: 0.054187845438718796
Training batch 22 / 32
Total batch reconstruction loss: 0.05940530449151993
Training batch 23 / 32
Total batch reconstruction loss: 0.06039339303970337
Training batch 24 / 32
Total batch reconstruction loss: 0.061708807945251465
Training batch 25 / 32
Total batch reconstruction loss: 0.05497158318758011
Training batch 26 / 32
Total batch reconstruction loss: 0.058863624930381775
Training batch 27 / 32
Total batch reconstruction loss: 0.06112782657146454
Training batch 28 / 32
Total batch reconstruction loss: 0.05552578717470169
Training batch 29 / 32
Total batch reconstruction loss: 0.054673969745635986
Training batch 30 / 32
Total batch reconstruction loss: 0.06045335531234741
Training batch 31 / 32
Total batch reconstruction loss: 0.05752657353878021
Training batch 32 / 32
Total batch reconstruction loss: 0.05075716972351074
Epoch [375/500], Train Loss: 0.0566, Validation Loss: 0.0577, Generator Loss: 11.7695, Discriminator Loss: 0.3048
Training epoch 376 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.05689171701669693
Training batch 2 / 32
Total batch reconstruction loss: 0.061879441142082214
Training batch 3 / 32
Total batch reconstruction loss: 0.05758306011557579
Training batch 4 / 32
Total batch reconstruction loss: 0.06565956771373749
Training batch 5 / 32
Total batch reconstruction loss: 0.062019988894462585
Training batch 6 / 32
Total batch reconstruction loss: 0.06012497842311859
Training batch 7 / 32
Total batch reconstruction loss: 0.059814706444740295
Training batch 8 / 32
Total batch reconstruction loss: 0.05721241980791092
Training batch 9 / 32
Total batch reconstruction loss: 0.05975152552127838
Training batch 10 / 32
Total batch reconstruction loss: 0.05840851739048958
Training batch 11 / 32
Total batch reconstruction loss: 0.05982556194067001
Training batch 12 / 32
Total batch reconstruction loss: 0.057753413915634155
Training batch 13 / 32
Total batch reconstruction loss: 0.05754583328962326
Training batch 14 / 32
Total batch reconstruction loss: 0.059308942407369614
Training batch 15 / 32
Total batch reconstruction loss: 0.05812689661979675
Training batch 16 / 32
Total batch reconstruction loss: 0.0556773766875267
Training batch 17 / 32
Total batch reconstruction loss: 0.05782264843583107
Training batch 18 / 32
Total batch reconstruction loss: 0.059403978288173676
Training batch 19 / 32
Total batch reconstruction loss: 0.0560569167137146
Training batch 20 / 32
Total batch reconstruction loss: 0.05978654697537422
Training batch 21 / 32
Total batch reconstruction loss: 0.057590510696172714
Training batch 22 / 32
Total batch reconstruction loss: 0.05755557864904404
Training batch 23 / 32
Total batch reconstruction loss: 0.053981490433216095
Training batch 24 / 32
Total batch reconstruction loss: 0.05880147963762283
Training batch 25 / 32
Total batch reconstruction loss: 0.056747570633888245
Training batch 26 / 32
Total batch reconstruction loss: 0.05990055203437805
Training batch 27 / 32
Total batch reconstruction loss: 0.05752984434366226
Training batch 28 / 32
Total batch reconstruction loss: 0.05521189048886299
Training batch 29 / 32
Total batch reconstruction loss: 0.061979394406080246
Training batch 30 / 32
Total batch reconstruction loss: 0.059673771262168884
Training batch 31 / 32
Total batch reconstruction loss: 0.058280833065509796
Training batch 32 / 32
Total batch reconstruction loss: 0.051766663789749146
Epoch [376/500], Train Loss: 0.0566, Validation Loss: 0.0578, Generator Loss: 11.7515, Discriminator Loss: 0.3170
Training epoch 377 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.05889194458723068
Training batch 2 / 32
Total batch reconstruction loss: 0.05611049011349678
Training batch 3 / 32
Total batch reconstruction loss: 0.058570656925439835
Training batch 4 / 32
Total batch reconstruction loss: 0.05797974020242691
Training batch 5 / 32
Total batch reconstruction loss: 0.06256227940320969
Training batch 6 / 32
Total batch reconstruction loss: 0.05939818173646927
Training batch 7 / 32
Total batch reconstruction loss: 0.05756862089037895
Training batch 8 / 32
Total batch reconstruction loss: 0.059004172682762146
Training batch 9 / 32
Total batch reconstruction loss: 0.05524497479200363
Training batch 10 / 32
Total batch reconstruction loss: 0.059836238622665405
Training batch 11 / 32
Total batch reconstruction loss: 0.06100814789533615
Training batch 12 / 32
Total batch reconstruction loss: 0.05978775769472122
Training batch 13 / 32
Total batch reconstruction loss: 0.05964784324169159
Training batch 14 / 32
Total batch reconstruction loss: 0.05929924547672272
Training batch 15 / 32
Total batch reconstruction loss: 0.061037421226501465
Training batch 16 / 32
Total batch reconstruction loss: 0.05970517545938492
Training batch 17 / 32
Total batch reconstruction loss: 0.05721743032336235
Training batch 18 / 32
Total batch reconstruction loss: 0.06050870195031166
Training batch 19 / 32
Total batch reconstruction loss: 0.061928890645504
Training batch 20 / 32
Total batch reconstruction loss: 0.05601150169968605
Training batch 21 / 32
Total batch reconstruction loss: 0.056319378316402435
Training batch 22 / 32
Total batch reconstruction loss: 0.057522520422935486
Training batch 23 / 32
Total batch reconstruction loss: 0.058012865483760834
Training batch 24 / 32
Total batch reconstruction loss: 0.061511166393756866
Training batch 25 / 32
Total batch reconstruction loss: 0.05770523473620415
Training batch 26 / 32
Total batch reconstruction loss: 0.05674799159169197
Training batch 27 / 32
Total batch reconstruction loss: 0.05860140174627304
Training batch 28 / 32
Total batch reconstruction loss: 0.059896960854530334
Training batch 29 / 32
Total batch reconstruction loss: 0.06031397357583046
Training batch 30 / 32
Total batch reconstruction loss: 0.05852942168712616
Training batch 31 / 32
Total batch reconstruction loss: 0.06120893359184265
Training batch 32 / 32
Total batch reconstruction loss: 0.04808982461690903
Epoch [377/500], Train Loss: 0.0569, Validation Loss: 0.0568, Generator Loss: 11.7870, Discriminator Loss: 0.3192
Training epoch 378 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.061248622834682465
Training batch 2 / 32
Total batch reconstruction loss: 0.05957026779651642
Training batch 3 / 32
Total batch reconstruction loss: 0.05777202546596527
Training batch 4 / 32
Total batch reconstruction loss: 0.059195294976234436
Training batch 5 / 32
Total batch reconstruction loss: 0.057739295065402985
Training batch 6 / 32
Total batch reconstruction loss: 0.058275509625673294
Training batch 7 / 32
Total batch reconstruction loss: 0.06431005895137787
Training batch 8 / 32
Total batch reconstruction loss: 0.06039278581738472
Training batch 9 / 32
Total batch reconstruction loss: 0.06039748713374138
Training batch 10 / 32
Total batch reconstruction loss: 0.056323301047086716
Training batch 11 / 32
Total batch reconstruction loss: 0.05535890534520149
Training batch 12 / 32
Total batch reconstruction loss: 0.05552876740694046
Training batch 13 / 32
Total batch reconstruction loss: 0.05784665048122406
Training batch 14 / 32
Total batch reconstruction loss: 0.05851949378848076
Training batch 15 / 32
Total batch reconstruction loss: 0.060481660068035126
Training batch 16 / 32
Total batch reconstruction loss: 0.05630744993686676
Training batch 17 / 32
Total batch reconstruction loss: 0.06215120851993561
Training batch 18 / 32
Total batch reconstruction loss: 0.05988343060016632
Training batch 19 / 32
Total batch reconstruction loss: 0.058237068355083466
Training batch 20 / 32
Total batch reconstruction loss: 0.05889533460140228
Training batch 21 / 32
Total batch reconstruction loss: 0.05766347795724869
Training batch 22 / 32
Total batch reconstruction loss: 0.05833297222852707
Training batch 23 / 32
Total batch reconstruction loss: 0.06123083829879761
Training batch 24 / 32
Total batch reconstruction loss: 0.056839581578969955
Training batch 25 / 32
Total batch reconstruction loss: 0.058745212852954865
Training batch 26 / 32
Total batch reconstruction loss: 0.055799368768930435
Training batch 27 / 32
Total batch reconstruction loss: 0.055780891329050064
Training batch 28 / 32
Total batch reconstruction loss: 0.059681326150894165
Training batch 29 / 32
Total batch reconstruction loss: 0.06095645949244499
Training batch 30 / 32
Total batch reconstruction loss: 0.05886393040418625
Training batch 31 / 32
Total batch reconstruction loss: 0.05524985119700432
Training batch 32 / 32
Total batch reconstruction loss: 0.05073867738246918
Epoch [378/500], Train Loss: 0.0566, Validation Loss: 0.0575, Generator Loss: 11.7497, Discriminator Loss: 0.3083
Training epoch 379 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.05966398864984512
Training batch 2 / 32
Total batch reconstruction loss: 0.058140188455581665
Training batch 3 / 32
Total batch reconstruction loss: 0.0543396957218647
Training batch 4 / 32
Total batch reconstruction loss: 0.06040273234248161
Training batch 5 / 32
Total batch reconstruction loss: 0.05747362971305847
Training batch 6 / 32
Total batch reconstruction loss: 0.055943239480257034
Training batch 7 / 32
Total batch reconstruction loss: 0.05657622963190079
Training batch 8 / 32
Total batch reconstruction loss: 0.0605156347155571
Training batch 9 / 32
Total batch reconstruction loss: 0.05760018527507782
Training batch 10 / 32
Total batch reconstruction loss: 0.05733897536993027
Training batch 11 / 32
Total batch reconstruction loss: 0.06158086657524109
Training batch 12 / 32
Total batch reconstruction loss: 0.06134495139122009
Training batch 13 / 32
Total batch reconstruction loss: 0.05313871055841446
Training batch 14 / 32
Total batch reconstruction loss: 0.056345026940107346
Training batch 15 / 32
Total batch reconstruction loss: 0.059627972543239594
Training batch 16 / 32
Total batch reconstruction loss: 0.05663280934095383
Training batch 17 / 32
Total batch reconstruction loss: 0.05762133374810219
Training batch 18 / 32
Total batch reconstruction loss: 0.06078080087900162
Training batch 19 / 32
Total batch reconstruction loss: 0.0675954595208168
Training batch 20 / 32
Total batch reconstruction loss: 0.05772535502910614
Training batch 21 / 32
Total batch reconstruction loss: 0.06042103469371796
Training batch 22 / 32
Total batch reconstruction loss: 0.06113827973604202
Training batch 23 / 32
Total batch reconstruction loss: 0.06090220436453819
Training batch 24 / 32
Total batch reconstruction loss: 0.05737743154168129
Training batch 25 / 32
Total batch reconstruction loss: 0.05861999839544296
Training batch 26 / 32
Total batch reconstruction loss: 0.054918114095926285
Training batch 27 / 32
Total batch reconstruction loss: 0.0593477226793766
Training batch 28 / 32
Total batch reconstruction loss: 0.05472662299871445
Training batch 29 / 32
Total batch reconstruction loss: 0.06212414801120758
Training batch 30 / 32
Total batch reconstruction loss: 0.06241936981678009
Training batch 31 / 32
Total batch reconstruction loss: 0.059781573712825775
Training batch 32 / 32
Total batch reconstruction loss: 0.049539778381586075
Epoch [379/500], Train Loss: 0.0568, Validation Loss: 0.0566, Generator Loss: 11.7633, Discriminator Loss: 0.3194
Training epoch 380 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.05720048025250435
Training batch 2 / 32
Total batch reconstruction loss: 0.057786114513874054
Training batch 3 / 32
Total batch reconstruction loss: 0.05992552638053894
Training batch 4 / 32
Total batch reconstruction loss: 0.05843522399663925
Training batch 5 / 32
Total batch reconstruction loss: 0.05598905310034752
Training batch 6 / 32
Total batch reconstruction loss: 0.05566368252038956
Training batch 7 / 32
Total batch reconstruction loss: 0.062136147171258926
Training batch 8 / 32
Total batch reconstruction loss: 0.056269291788339615
Training batch 9 / 32
Total batch reconstruction loss: 0.06104845181107521
Training batch 10 / 32
Total batch reconstruction loss: 0.0581531897187233
Training batch 11 / 32
Total batch reconstruction loss: 0.0658349096775055
Training batch 12 / 32
Total batch reconstruction loss: 0.058675576001405716
Training batch 13 / 32
Total batch reconstruction loss: 0.05750181898474693
Training batch 14 / 32
Total batch reconstruction loss: 0.05684969574213028
Training batch 15 / 32
Total batch reconstruction loss: 0.06009548157453537
Training batch 16 / 32
Total batch reconstruction loss: 0.05966544896364212
Training batch 17 / 32
Total batch reconstruction loss: 0.061025723814964294
Training batch 18 / 32
Total batch reconstruction loss: 0.05950205773115158
Training batch 19 / 32
Total batch reconstruction loss: 0.05641905218362808
Training batch 20 / 32
Total batch reconstruction loss: 0.059539057314395905
Training batch 21 / 32
Total batch reconstruction loss: 0.05627373605966568
Training batch 22 / 32
Total batch reconstruction loss: 0.05915975570678711
Training batch 23 / 32
Total batch reconstruction loss: 0.05835843086242676
Training batch 24 / 32
Total batch reconstruction loss: 0.05403108894824982
Training batch 25 / 32
Total batch reconstruction loss: 0.05639078468084335
Training batch 26 / 32
Total batch reconstruction loss: 0.059339094907045364
Training batch 27 / 32
Total batch reconstruction loss: 0.06079889088869095
Training batch 28 / 32
Total batch reconstruction loss: 0.05933765321969986
Training batch 29 / 32
Total batch reconstruction loss: 0.06357317417860031
Training batch 30 / 32
Total batch reconstruction loss: 0.058943428099155426
Training batch 31 / 32
Total batch reconstruction loss: 0.059082064777612686
Training batch 32 / 32
Total batch reconstruction loss: 0.060952238738536835
Epoch [380/500], Train Loss: 0.0570, Validation Loss: 0.0569, Generator Loss: 11.8319, Discriminator Loss: 0.3294
Training epoch 381 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.058543357998132706
Training batch 2 / 32
Total batch reconstruction loss: 0.05579829588532448
Training batch 3 / 32
Total batch reconstruction loss: 0.057584330439567566
Training batch 4 / 32
Total batch reconstruction loss: 0.05626155436038971
Training batch 5 / 32
Total batch reconstruction loss: 0.06220182776451111
Training batch 6 / 32
Total batch reconstruction loss: 0.057774029672145844
Training batch 7 / 32
Total batch reconstruction loss: 0.061341993510723114
Training batch 8 / 32
Total batch reconstruction loss: 0.06409816443920135
Training batch 9 / 32
Total batch reconstruction loss: 0.056535013020038605
Training batch 10 / 32
Total batch reconstruction loss: 0.06361621618270874
Training batch 11 / 32
Total batch reconstruction loss: 0.0591464564204216
Training batch 12 / 32
Total batch reconstruction loss: 0.05872337520122528
Training batch 13 / 32
Total batch reconstruction loss: 0.05808638036251068
Training batch 14 / 32
Total batch reconstruction loss: 0.05513095110654831
Training batch 15 / 32
Total batch reconstruction loss: 0.057359300553798676
Training batch 16 / 32
Total batch reconstruction loss: 0.05415390431880951
Training batch 17 / 32
Total batch reconstruction loss: 0.05805143713951111
Training batch 18 / 32
Total batch reconstruction loss: 0.061242930591106415
Training batch 19 / 32
Total batch reconstruction loss: 0.060098446905612946
Training batch 20 / 32
Total batch reconstruction loss: 0.05248251184821129
Training batch 21 / 32
Total batch reconstruction loss: 0.0615629181265831
Training batch 22 / 32
Total batch reconstruction loss: 0.06063827872276306
Training batch 23 / 32
Total batch reconstruction loss: 0.05793222784996033
Training batch 24 / 32
Total batch reconstruction loss: 0.0610598623752594
Training batch 25 / 32
Total batch reconstruction loss: 0.05875071883201599
Training batch 26 / 32
Total batch reconstruction loss: 0.057218439877033234
Training batch 27 / 32
Total batch reconstruction loss: 0.06074497848749161
Training batch 28 / 32
Total batch reconstruction loss: 0.054309628903865814
Training batch 29 / 32
Total batch reconstruction loss: 0.059438712894916534
Training batch 30 / 32
Total batch reconstruction loss: 0.06508520990610123
Training batch 31 / 32
Total batch reconstruction loss: 0.061781950294971466
Training batch 32 / 32
Total batch reconstruction loss: 0.05825117975473404
Epoch [381/500], Train Loss: 0.0570, Validation Loss: 0.0572, Generator Loss: 11.8543, Discriminator Loss: 0.3105
Training epoch 382 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.06204589083790779
Training batch 2 / 32
Total batch reconstruction loss: 0.05621476098895073
Training batch 3 / 32
Total batch reconstruction loss: 0.057850778102874756
Training batch 4 / 32
Total batch reconstruction loss: 0.0605015829205513
Training batch 5 / 32
Total batch reconstruction loss: 0.05758170038461685
Training batch 6 / 32
Total batch reconstruction loss: 0.056468188762664795
Training batch 7 / 32
Total batch reconstruction loss: 0.05830741673707962
Training batch 8 / 32
Total batch reconstruction loss: 0.05513780564069748
Training batch 9 / 32
Total batch reconstruction loss: 0.06118986755609512
Training batch 10 / 32
Total batch reconstruction loss: 0.06029428541660309
Training batch 11 / 32
Total batch reconstruction loss: 0.06220624968409538
Training batch 12 / 32
Total batch reconstruction loss: 0.05687378719449043
Training batch 13 / 32
Total batch reconstruction loss: 0.061028048396110535
Training batch 14 / 32
Total batch reconstruction loss: 0.055210355669260025
Training batch 15 / 32
Total batch reconstruction loss: 0.057819463312625885
Training batch 16 / 32
Total batch reconstruction loss: 0.062246885150671005
Training batch 17 / 32
Total batch reconstruction loss: 0.06137208640575409
Training batch 18 / 32
Total batch reconstruction loss: 0.05962666496634483
Training batch 19 / 32
Total batch reconstruction loss: 0.06286831200122833
Training batch 20 / 32
Total batch reconstruction loss: 0.05601302534341812
Training batch 21 / 32
Total batch reconstruction loss: 0.05637521296739578
Training batch 22 / 32
Total batch reconstruction loss: 0.05815232917666435
Training batch 23 / 32
Total batch reconstruction loss: 0.05243748798966408
Training batch 24 / 32
Total batch reconstruction loss: 0.05880843847990036
Training batch 25 / 32
Total batch reconstruction loss: 0.05800829827785492
Training batch 26 / 32
Total batch reconstruction loss: 0.058935798704624176
Training batch 27 / 32
Total batch reconstruction loss: 0.06315137445926666
Training batch 28 / 32
Total batch reconstruction loss: 0.060892503708601
Training batch 29 / 32
Total batch reconstruction loss: 0.056546956300735474
Training batch 30 / 32
Total batch reconstruction loss: 0.06075096130371094
Training batch 31 / 32
Total batch reconstruction loss: 0.060020562261343
Training batch 32 / 32
Total batch reconstruction loss: 0.06367096304893494
Epoch [382/500], Train Loss: 0.0574, Validation Loss: 0.0582, Generator Loss: 11.8682, Discriminator Loss: 0.3248
Training epoch 383 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.06173486262559891
Training batch 2 / 32
Total batch reconstruction loss: 0.05600232630968094
Training batch 3 / 32
Total batch reconstruction loss: 0.058402322232723236
Training batch 4 / 32
Total batch reconstruction loss: 0.06317274272441864
Training batch 5 / 32
Total batch reconstruction loss: 0.06320734322071075
Training batch 6 / 32
Total batch reconstruction loss: 0.057027965784072876
Training batch 7 / 32
Total batch reconstruction loss: 0.056348733603954315
Training batch 8 / 32
Total batch reconstruction loss: 0.062161024659872055
Training batch 9 / 32
Total batch reconstruction loss: 0.0589575469493866
Training batch 10 / 32
Total batch reconstruction loss: 0.057578183710575104
Training batch 11 / 32
Total batch reconstruction loss: 0.05760825425386429
Training batch 12 / 32
Total batch reconstruction loss: 0.05950605124235153
Training batch 13 / 32
Total batch reconstruction loss: 0.0563356876373291
Training batch 14 / 32
Total batch reconstruction loss: 0.058757081627845764
Training batch 15 / 32
Total batch reconstruction loss: 0.06196948513388634
Training batch 16 / 32
Total batch reconstruction loss: 0.06134382262825966
Training batch 17 / 32
Total batch reconstruction loss: 0.059214282780885696
Training batch 18 / 32
Total batch reconstruction loss: 0.06450533866882324
Training batch 19 / 32
Total batch reconstruction loss: 0.058006756007671356
Training batch 20 / 32
Total batch reconstruction loss: 0.05842382088303566
Training batch 21 / 32
Total batch reconstruction loss: 0.05725385993719101
Training batch 22 / 32
Total batch reconstruction loss: 0.05725740268826485
Training batch 23 / 32
Total batch reconstruction loss: 0.06054294854402542
Training batch 24 / 32
Total batch reconstruction loss: 0.05583526939153671
Training batch 25 / 32
Total batch reconstruction loss: 0.0547594353556633
Training batch 26 / 32
Total batch reconstruction loss: 0.05495062470436096
Training batch 27 / 32
Total batch reconstruction loss: 0.05826530605554581
Training batch 28 / 32
Total batch reconstruction loss: 0.06088437885046005
Training batch 29 / 32
Total batch reconstruction loss: 0.060425251722335815
Training batch 30 / 32
Total batch reconstruction loss: 0.056543171405792236
Training batch 31 / 32
Total batch reconstruction loss: 0.05897601321339607
Training batch 32 / 32
Total batch reconstruction loss: 0.05643685534596443
Epoch [383/500], Train Loss: 0.0570, Validation Loss: 0.0576, Generator Loss: 11.8292, Discriminator Loss: 0.3241
Training epoch 384 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.05866321176290512
Training batch 2 / 32
Total batch reconstruction loss: 0.06005866080522537
Training batch 3 / 32
Total batch reconstruction loss: 0.06353598833084106
Training batch 4 / 32
Total batch reconstruction loss: 0.05671880766749382
Training batch 5 / 32
Total batch reconstruction loss: 0.062182407826185226
Training batch 6 / 32
Total batch reconstruction loss: 0.053421735763549805
Training batch 7 / 32
Total batch reconstruction loss: 0.056408170610666275
Training batch 8 / 32
Total batch reconstruction loss: 0.0639292299747467
Training batch 9 / 32
Total batch reconstruction loss: 0.05889175832271576
Training batch 10 / 32
Total batch reconstruction loss: 0.05603426694869995
Training batch 11 / 32
Total batch reconstruction loss: 0.06024429202079773
Training batch 12 / 32
Total batch reconstruction loss: 0.057035043835639954
Training batch 13 / 32
Total batch reconstruction loss: 0.056494783610105515
Training batch 14 / 32
Total batch reconstruction loss: 0.05558941513299942
Training batch 15 / 32
Total batch reconstruction loss: 0.058320075273513794
Training batch 16 / 32
Total batch reconstruction loss: 0.05726449936628342
Training batch 17 / 32
Total batch reconstruction loss: 0.05991336703300476
Training batch 18 / 32
Total batch reconstruction loss: 0.06684297323226929
Training batch 19 / 32
Total batch reconstruction loss: 0.058653414249420166
Training batch 20 / 32
Total batch reconstruction loss: 0.05953943729400635
Training batch 21 / 32
Total batch reconstruction loss: 0.06120028719305992
Training batch 22 / 32
Total batch reconstruction loss: 0.05329476669430733
Training batch 23 / 32
Total batch reconstruction loss: 0.05913197994232178
Training batch 24 / 32
Total batch reconstruction loss: 0.05931198224425316
Training batch 25 / 32
Total batch reconstruction loss: 0.05757390335202217
Training batch 26 / 32
Total batch reconstruction loss: 0.056289851665496826
Training batch 27 / 32
Total batch reconstruction loss: 0.059591133147478104
Training batch 28 / 32
Total batch reconstruction loss: 0.05663428455591202
Training batch 29 / 32
Total batch reconstruction loss: 0.059194885194301605
Training batch 30 / 32
Total batch reconstruction loss: 0.05853454768657684
Training batch 31 / 32
Total batch reconstruction loss: 0.06084287166595459
Training batch 32 / 32
Total batch reconstruction loss: 0.05511104315519333
Epoch [384/500], Train Loss: 0.0569, Validation Loss: 0.0574, Generator Loss: 11.7992, Discriminator Loss: 0.3134
Training epoch 385 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.058009661734104156
Training batch 2 / 32
Total batch reconstruction loss: 0.057198576629161835
Training batch 3 / 32
Total batch reconstruction loss: 0.060454633086919785
Training batch 4 / 32
Total batch reconstruction loss: 0.06427513062953949
Training batch 5 / 32
Total batch reconstruction loss: 0.060169562697410583
Training batch 6 / 32
Total batch reconstruction loss: 0.057385437190532684
Training batch 7 / 32
Total batch reconstruction loss: 0.059337638318538666
Training batch 8 / 32
Total batch reconstruction loss: 0.06186939775943756
Training batch 9 / 32
Total batch reconstruction loss: 0.06013021618127823
Training batch 10 / 32
Total batch reconstruction loss: 0.059658750891685486
Training batch 11 / 32
Total batch reconstruction loss: 0.061286747455596924
Training batch 12 / 32
Total batch reconstruction loss: 0.06500078737735748
Training batch 13 / 32
Total batch reconstruction loss: 0.05631616711616516
Training batch 14 / 32
Total batch reconstruction loss: 0.05570700764656067
Training batch 15 / 32
Total batch reconstruction loss: 0.056564148515462875
Training batch 16 / 32
Total batch reconstruction loss: 0.05973384529352188
Training batch 17 / 32
Total batch reconstruction loss: 0.05475740134716034
Training batch 18 / 32
Total batch reconstruction loss: 0.06247107684612274
Training batch 19 / 32
Total batch reconstruction loss: 0.06321044266223907
Training batch 20 / 32
Total batch reconstruction loss: 0.062016282230615616
Training batch 21 / 32
Total batch reconstruction loss: 0.05954299122095108
Training batch 22 / 32
Total batch reconstruction loss: 0.06042737513780594
Training batch 23 / 32
Total batch reconstruction loss: 0.05731871724128723
Training batch 24 / 32
Total batch reconstruction loss: 0.06075245887041092
Training batch 25 / 32
Total batch reconstruction loss: 0.06077485531568527
Training batch 26 / 32
Total batch reconstruction loss: 0.057807937264442444
Training batch 27 / 32
Total batch reconstruction loss: 0.056372322142124176
Training batch 28 / 32
Total batch reconstruction loss: 0.055337093770504
Training batch 29 / 32
Total batch reconstruction loss: 0.06079353019595146
Training batch 30 / 32
Total batch reconstruction loss: 0.05590236186981201
Training batch 31 / 32
Total batch reconstruction loss: 0.05647723004221916
Training batch 32 / 32
Total batch reconstruction loss: 0.07395462691783905
Epoch [385/500], Train Loss: 0.0580, Validation Loss: 0.0566, Generator Loss: 12.0130, Discriminator Loss: 0.3124
Training epoch 386 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.05850616842508316
Training batch 2 / 32
Total batch reconstruction loss: 0.05710539594292641
Training batch 3 / 32
Total batch reconstruction loss: 0.060073934495449066
Training batch 4 / 32
Total batch reconstruction loss: 0.06364931911230087
Training batch 5 / 32
Total batch reconstruction loss: 0.06112793833017349
Training batch 6 / 32
Total batch reconstruction loss: 0.05722740665078163
Training batch 7 / 32
Total batch reconstruction loss: 0.06044546887278557
Training batch 8 / 32
Total batch reconstruction loss: 0.06108244135975838
Training batch 9 / 32
Total batch reconstruction loss: 0.059638164937496185
Training batch 10 / 32
Total batch reconstruction loss: 0.05928301066160202
Training batch 11 / 32
Total batch reconstruction loss: 0.06093320995569229
Training batch 12 / 32
Total batch reconstruction loss: 0.056408874690532684
Training batch 13 / 32
Total batch reconstruction loss: 0.05716930329799652
Training batch 14 / 32
Total batch reconstruction loss: 0.058817096054553986
Training batch 15 / 32
Total batch reconstruction loss: 0.05671557039022446
Training batch 16 / 32
Total batch reconstruction loss: 0.06158313900232315
Training batch 17 / 32
Total batch reconstruction loss: 0.05467008799314499
Training batch 18 / 32
Total batch reconstruction loss: 0.05842939019203186
Training batch 19 / 32
Total batch reconstruction loss: 0.055939994752407074
Training batch 20 / 32
Total batch reconstruction loss: 0.05905325710773468
Training batch 21 / 32
Total batch reconstruction loss: 0.06094789505004883
Training batch 22 / 32
Total batch reconstruction loss: 0.05918430536985397
Training batch 23 / 32
Total batch reconstruction loss: 0.057046979665756226
Training batch 24 / 32
Total batch reconstruction loss: 0.057992834597826004
Training batch 25 / 32
Total batch reconstruction loss: 0.06317063421010971
Training batch 26 / 32
Total batch reconstruction loss: 0.0567590668797493
Training batch 27 / 32
Total batch reconstruction loss: 0.05593764781951904
Training batch 28 / 32
Total batch reconstruction loss: 0.06118465214967728
Training batch 29 / 32
Total batch reconstruction loss: 0.05680138245224953
Training batch 30 / 32
Total batch reconstruction loss: 0.05864230915904045
Training batch 31 / 32
Total batch reconstruction loss: 0.059896040707826614
Training batch 32 / 32
Total batch reconstruction loss: 0.04948948696255684
Epoch [386/500], Train Loss: 0.0567, Validation Loss: 0.0562, Generator Loss: 11.7881, Discriminator Loss: 0.3144
Training epoch 387 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.062040068209171295
Training batch 2 / 32
Total batch reconstruction loss: 0.059758756309747696
Training batch 3 / 32
Total batch reconstruction loss: 0.05609723925590515
Training batch 4 / 32
Total batch reconstruction loss: 0.0609523206949234
Training batch 5 / 32
Total batch reconstruction loss: 0.055322352796792984
Training batch 6 / 32
Total batch reconstruction loss: 0.06035786122083664
Training batch 7 / 32
Total batch reconstruction loss: 0.05676686391234398
Training batch 8 / 32
Total batch reconstruction loss: 0.05983423814177513
Training batch 9 / 32
Total batch reconstruction loss: 0.05745518207550049
Training batch 10 / 32
Total batch reconstruction loss: 0.059972021728754044
Training batch 11 / 32
Total batch reconstruction loss: 0.06395575404167175
Training batch 12 / 32
Total batch reconstruction loss: 0.06113724038004875
Training batch 13 / 32
Total batch reconstruction loss: 0.05829280614852905
Training batch 14 / 32
Total batch reconstruction loss: 0.05752860754728317
Training batch 15 / 32
Total batch reconstruction loss: 0.06020351126790047
Training batch 16 / 32
Total batch reconstruction loss: 0.06020699441432953
Training batch 17 / 32
Total batch reconstruction loss: 0.06024789810180664
Training batch 18 / 32
Total batch reconstruction loss: 0.05706214904785156
Training batch 19 / 32
Total batch reconstruction loss: 0.05878856033086777
Training batch 20 / 32
Total batch reconstruction loss: 0.05590633302927017
Training batch 21 / 32
Total batch reconstruction loss: 0.05827818810939789
Training batch 22 / 32
Total batch reconstruction loss: 0.0637209415435791
Training batch 23 / 32
Total batch reconstruction loss: 0.05773976445198059
Training batch 24 / 32
Total batch reconstruction loss: 0.059958308935165405
Training batch 25 / 32
Total batch reconstruction loss: 0.05612098425626755
Training batch 26 / 32
Total batch reconstruction loss: 0.05356782302260399
Training batch 27 / 32
Total batch reconstruction loss: 0.0586540512740612
Training batch 28 / 32
Total batch reconstruction loss: 0.05924290418624878
Training batch 29 / 32
Total batch reconstruction loss: 0.05418894439935684
Training batch 30 / 32
Total batch reconstruction loss: 0.056363340467214584
Training batch 31 / 32
Total batch reconstruction loss: 0.058560457080602646
Training batch 32 / 32
Total batch reconstruction loss: 0.059625960886478424
Epoch [387/500], Train Loss: 0.0567, Validation Loss: 0.0575, Generator Loss: 11.8082, Discriminator Loss: 0.3128
Training epoch 388 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.05812013894319534
Training batch 2 / 32
Total batch reconstruction loss: 0.058390893042087555
Training batch 3 / 32
Total batch reconstruction loss: 0.06390485912561417
Training batch 4 / 32
Total batch reconstruction loss: 0.054857827723026276
Training batch 5 / 32
Total batch reconstruction loss: 0.06138350069522858
Training batch 6 / 32
Total batch reconstruction loss: 0.056933581829071045
Training batch 7 / 32
Total batch reconstruction loss: 0.059129901230335236
Training batch 8 / 32
Total batch reconstruction loss: 0.055692024528980255
Training batch 9 / 32
Total batch reconstruction loss: 0.06491823494434357
Training batch 10 / 32
Total batch reconstruction loss: 0.05855896323919296
Training batch 11 / 32
Total batch reconstruction loss: 0.05935358256101608
Training batch 12 / 32
Total batch reconstruction loss: 0.05949879437685013
Training batch 13 / 32
Total batch reconstruction loss: 0.05722009390592575
Training batch 14 / 32
Total batch reconstruction loss: 0.05641430616378784
Training batch 15 / 32
Total batch reconstruction loss: 0.05658732354640961
Training batch 16 / 32
Total batch reconstruction loss: 0.06268361955881119
Training batch 17 / 32
Total batch reconstruction loss: 0.05772834271192551
Training batch 18 / 32
Total batch reconstruction loss: 0.05830587446689606
Training batch 19 / 32
Total batch reconstruction loss: 0.05737849324941635
Training batch 20 / 32
Total batch reconstruction loss: 0.057154640555381775
Training batch 21 / 32
Total batch reconstruction loss: 0.057370658963918686
Training batch 22 / 32
Total batch reconstruction loss: 0.06288126856088638
Training batch 23 / 32
Total batch reconstruction loss: 0.05732809379696846
Training batch 24 / 32
Total batch reconstruction loss: 0.05544251576066017
Training batch 25 / 32
Total batch reconstruction loss: 0.05422013998031616
Training batch 26 / 32
Total batch reconstruction loss: 0.06460987031459808
Training batch 27 / 32
Total batch reconstruction loss: 0.05977633595466614
Training batch 28 / 32
Total batch reconstruction loss: 0.05685042962431908
Training batch 29 / 32
Total batch reconstruction loss: 0.061092834919691086
Training batch 30 / 32
Total batch reconstruction loss: 0.057078778743743896
Training batch 31 / 32
Total batch reconstruction loss: 0.05878065526485443
Training batch 32 / 32
Total batch reconstruction loss: 0.05436408519744873
Epoch [388/500], Train Loss: 0.0568, Validation Loss: 0.0596, Generator Loss: 11.7854, Discriminator Loss: 0.3078
Training epoch 389 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.05866151675581932
Training batch 2 / 32
Total batch reconstruction loss: 0.06838023662567139
Training batch 3 / 32
Total batch reconstruction loss: 0.05777953192591667
Training batch 4 / 32
Total batch reconstruction loss: 0.06461220979690552
Training batch 5 / 32
Total batch reconstruction loss: 0.05881037190556526
Training batch 6 / 32
Total batch reconstruction loss: 0.058496516197919846
Training batch 7 / 32
Total batch reconstruction loss: 0.06263984739780426
Training batch 8 / 32
Total batch reconstruction loss: 0.06104683876037598
Training batch 9 / 32
Total batch reconstruction loss: 0.059616051614284515
Training batch 10 / 32
Total batch reconstruction loss: 0.05859259143471718
Training batch 11 / 32
Total batch reconstruction loss: 0.05800916254520416
Training batch 12 / 32
Total batch reconstruction loss: 0.05956028401851654
Training batch 13 / 32
Total batch reconstruction loss: 0.058359358459711075
Training batch 14 / 32
Total batch reconstruction loss: 0.06110473722219467
Training batch 15 / 32
Total batch reconstruction loss: 0.06293046474456787
Training batch 16 / 32
Total batch reconstruction loss: 0.05842018872499466
Training batch 17 / 32
Total batch reconstruction loss: 0.05849476903676987
Training batch 18 / 32
Total batch reconstruction loss: 0.057717032730579376
Training batch 19 / 32
Total batch reconstruction loss: 0.06182129681110382
Training batch 20 / 32
Total batch reconstruction loss: 0.05788522586226463
Training batch 21 / 32
Total batch reconstruction loss: 0.05813934653997421
Training batch 22 / 32
Total batch reconstruction loss: 0.0608207993209362
Training batch 23 / 32
Total batch reconstruction loss: 0.0632079541683197
Training batch 24 / 32
Total batch reconstruction loss: 0.05778903514146805
Training batch 25 / 32
Total batch reconstruction loss: 0.05355614051222801
Training batch 26 / 32
Total batch reconstruction loss: 0.058074843138456345
Training batch 27 / 32
Total batch reconstruction loss: 0.06266064196825027
Training batch 28 / 32
Total batch reconstruction loss: 0.05357550084590912
Training batch 29 / 32
Total batch reconstruction loss: 0.06065463274717331
Training batch 30 / 32
Total batch reconstruction loss: 0.057864103466272354
Training batch 31 / 32
Total batch reconstruction loss: 0.058081287890672684
Training batch 32 / 32
Total batch reconstruction loss: 0.05716097354888916
Epoch [389/500], Train Loss: 0.0578, Validation Loss: 0.0581, Generator Loss: 11.9667, Discriminator Loss: 0.3235
Training epoch 390 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.06220123916864395
Training batch 2 / 32
Total batch reconstruction loss: 0.059471115469932556
Training batch 3 / 32
Total batch reconstruction loss: 0.0592380166053772
Training batch 4 / 32
Total batch reconstruction loss: 0.0668725073337555
Training batch 5 / 32
Total batch reconstruction loss: 0.05873279273509979
Training batch 6 / 32
Total batch reconstruction loss: 0.05881711095571518
Training batch 7 / 32
Total batch reconstruction loss: 0.05622822418808937
Training batch 8 / 32
Total batch reconstruction loss: 0.056816183030605316
Training batch 9 / 32
Total batch reconstruction loss: 0.05799365043640137
Training batch 10 / 32
Total batch reconstruction loss: 0.06138993799686432
Training batch 11 / 32
Total batch reconstruction loss: 0.057077862322330475
Training batch 12 / 32
Total batch reconstruction loss: 0.05936527997255325
Training batch 13 / 32
Total batch reconstruction loss: 0.054811619222164154
Training batch 14 / 32
Total batch reconstruction loss: 0.056926924735307693
Training batch 15 / 32
Total batch reconstruction loss: 0.05592730641365051
Training batch 16 / 32
Total batch reconstruction loss: 0.05504905804991722
Training batch 17 / 32
Total batch reconstruction loss: 0.06148204207420349
Training batch 18 / 32
Total batch reconstruction loss: 0.0643194243311882
Training batch 19 / 32
Total batch reconstruction loss: 0.05616603046655655
Training batch 20 / 32
Total batch reconstruction loss: 0.05893964320421219
Training batch 21 / 32
Total batch reconstruction loss: 0.06365595757961273
Training batch 22 / 32
Total batch reconstruction loss: 0.05705321952700615
Training batch 23 / 32
Total batch reconstruction loss: 0.057595930993556976
Training batch 24 / 32
Total batch reconstruction loss: 0.05961719900369644
Training batch 25 / 32
Total batch reconstruction loss: 0.055901214480400085
Training batch 26 / 32
Total batch reconstruction loss: 0.06085733324289322
Training batch 27 / 32
Total batch reconstruction loss: 0.05518009886145592
Training batch 28 / 32
Total batch reconstruction loss: 0.0580480694770813
Training batch 29 / 32
Total batch reconstruction loss: 0.06125464290380478
Training batch 30 / 32
Total batch reconstruction loss: 0.05616620182991028
Training batch 31 / 32
Total batch reconstruction loss: 0.05972152575850487
Training batch 32 / 32
Total batch reconstruction loss: 0.09540291130542755
Epoch [390/500], Train Loss: 0.0580, Validation Loss: 0.0575, Generator Loss: 12.0459, Discriminator Loss: 0.3379
Training epoch 391 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.06166747957468033
Training batch 2 / 32
Total batch reconstruction loss: 0.06093906983733177
Training batch 3 / 32
Total batch reconstruction loss: 0.06282417476177216
Training batch 4 / 32
Total batch reconstruction loss: 0.0641181468963623
Training batch 5 / 32
Total batch reconstruction loss: 0.06353580206632614
Training batch 6 / 32
Total batch reconstruction loss: 0.06198250874876976
Training batch 7 / 32
Total batch reconstruction loss: 0.05577465146780014
Training batch 8 / 32
Total batch reconstruction loss: 0.06036286801099777
Training batch 9 / 32
Total batch reconstruction loss: 0.05798598378896713
Training batch 10 / 32
Total batch reconstruction loss: 0.05942796543240547
Training batch 11 / 32
Total batch reconstruction loss: 0.057596996426582336
Training batch 12 / 32
Total batch reconstruction loss: 0.06343181431293488
Training batch 13 / 32
Total batch reconstruction loss: 0.05548831820487976
Training batch 14 / 32
Total batch reconstruction loss: 0.05926359444856644
Training batch 15 / 32
Total batch reconstruction loss: 0.05820707976818085
Training batch 16 / 32
Total batch reconstruction loss: 0.057172201573848724
Training batch 17 / 32
Total batch reconstruction loss: 0.05570516735315323
Training batch 18 / 32
Total batch reconstruction loss: 0.057789914309978485
Training batch 19 / 32
Total batch reconstruction loss: 0.057798732072114944
Training batch 20 / 32
Total batch reconstruction loss: 0.056223221123218536
Training batch 21 / 32
Total batch reconstruction loss: 0.06299731135368347
Training batch 22 / 32
Total batch reconstruction loss: 0.060201458632946014
Training batch 23 / 32
Total batch reconstruction loss: 0.05962637811899185
Training batch 24 / 32
Total batch reconstruction loss: 0.06035434454679489
Training batch 25 / 32
Total batch reconstruction loss: 0.054695501923561096
Training batch 26 / 32
Total batch reconstruction loss: 0.05874348431825638
Training batch 27 / 32
Total batch reconstruction loss: 0.0595218688249588
Training batch 28 / 32
Total batch reconstruction loss: 0.05833827704191208
Training batch 29 / 32
Total batch reconstruction loss: 0.059766195714473724
Training batch 30 / 32
Total batch reconstruction loss: 0.055290624499320984
Training batch 31 / 32
Total batch reconstruction loss: 0.055932074785232544
Training batch 32 / 32
Total batch reconstruction loss: 0.050637319684028625
Epoch [391/500], Train Loss: 0.0574, Validation Loss: 0.0565, Generator Loss: 11.8577, Discriminator Loss: 0.2944
Training epoch 392 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.05926183611154556
Training batch 2 / 32
Total batch reconstruction loss: 0.062068089842796326
Training batch 3 / 32
Total batch reconstruction loss: 0.05729784816503525
Training batch 4 / 32
Total batch reconstruction loss: 0.06156473979353905
Training batch 5 / 32
Total batch reconstruction loss: 0.06548963487148285
Training batch 6 / 32
Total batch reconstruction loss: 0.05720294266939163
Training batch 7 / 32
Total batch reconstruction loss: 0.059476956725120544
Training batch 8 / 32
Total batch reconstruction loss: 0.05849236994981766
Training batch 9 / 32
Total batch reconstruction loss: 0.05802314728498459
Training batch 10 / 32
Total batch reconstruction loss: 0.05813027173280716
Training batch 11 / 32
Total batch reconstruction loss: 0.06221641227602959
Training batch 12 / 32
Total batch reconstruction loss: 0.05734976381063461
Training batch 13 / 32
Total batch reconstruction loss: 0.05604782700538635
Training batch 14 / 32
Total batch reconstruction loss: 0.056094177067279816
Training batch 15 / 32
Total batch reconstruction loss: 0.053937990218400955
Training batch 16 / 32
Total batch reconstruction loss: 0.057417601346969604
Training batch 17 / 32
Total batch reconstruction loss: 0.0565270371735096
Training batch 18 / 32
Total batch reconstruction loss: 0.058866698294878006
Training batch 19 / 32
Total batch reconstruction loss: 0.05751150846481323
Training batch 20 / 32
Total batch reconstruction loss: 0.05547868460416794
Training batch 21 / 32
Total batch reconstruction loss: 0.05664545297622681
Training batch 22 / 32
Total batch reconstruction loss: 0.06048789620399475
Training batch 23 / 32
Total batch reconstruction loss: 0.05986640602350235
Training batch 24 / 32
Total batch reconstruction loss: 0.05897967517375946
Training batch 25 / 32
Total batch reconstruction loss: 0.062389202415943146
Training batch 26 / 32
Total batch reconstruction loss: 0.06174241006374359
Training batch 27 / 32
Total batch reconstruction loss: 0.05879407748579979
Training batch 28 / 32
Total batch reconstruction loss: 0.060448918491601944
Training batch 29 / 32
Total batch reconstruction loss: 0.05936726927757263
Training batch 30 / 32
Total batch reconstruction loss: 0.05624336749315262
Training batch 31 / 32
Total batch reconstruction loss: 0.054749857634305954
Training batch 32 / 32
Total batch reconstruction loss: 0.06252135336399078
Epoch [392/500], Train Loss: 0.0566, Validation Loss: 0.0582, Generator Loss: 11.8218, Discriminator Loss: 0.3160
Training epoch 393 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.0578492097556591
Training batch 2 / 32
Total batch reconstruction loss: 0.05744529515504837
Training batch 3 / 32
Total batch reconstruction loss: 0.06219188868999481
Training batch 4 / 32
Total batch reconstruction loss: 0.059072598814964294
Training batch 5 / 32
Total batch reconstruction loss: 0.058787811547517776
Training batch 6 / 32
Total batch reconstruction loss: 0.05849315971136093
Training batch 7 / 32
Total batch reconstruction loss: 0.05789018049836159
Training batch 8 / 32
Total batch reconstruction loss: 0.05922380089759827
Training batch 9 / 32
Total batch reconstruction loss: 0.057579152286052704
Training batch 10 / 32
Total batch reconstruction loss: 0.060297928750514984
Training batch 11 / 32
Total batch reconstruction loss: 0.0563092976808548
Training batch 12 / 32
Total batch reconstruction loss: 0.0551338754594326
Training batch 13 / 32
Total batch reconstruction loss: 0.05670071765780449
Training batch 14 / 32
Total batch reconstruction loss: 0.062441401183605194
Training batch 15 / 32
Total batch reconstruction loss: 0.05516771599650383
Training batch 16 / 32
Total batch reconstruction loss: 0.0626249834895134
Training batch 17 / 32
Total batch reconstruction loss: 0.059694886207580566
Training batch 18 / 32
Total batch reconstruction loss: 0.06085779145359993
Training batch 19 / 32
Total batch reconstruction loss: 0.0579020157456398
Training batch 20 / 32
Total batch reconstruction loss: 0.05953458696603775
Training batch 21 / 32
Total batch reconstruction loss: 0.05839093402028084
Training batch 22 / 32
Total batch reconstruction loss: 0.055570583790540695
Training batch 23 / 32
Total batch reconstruction loss: 0.06264042854309082
Training batch 24 / 32
Total batch reconstruction loss: 0.05666346102952957
Training batch 25 / 32
Total batch reconstruction loss: 0.058899298310279846
Training batch 26 / 32
Total batch reconstruction loss: 0.06388426572084427
Training batch 27 / 32
Total batch reconstruction loss: 0.06597040593624115
Training batch 28 / 32
Total batch reconstruction loss: 0.06088212877511978
Training batch 29 / 32
Total batch reconstruction loss: 0.06042071059346199
Training batch 30 / 32
Total batch reconstruction loss: 0.058839261531829834
Training batch 31 / 32
Total batch reconstruction loss: 0.0604344978928566
Training batch 32 / 32
Total batch reconstruction loss: 0.0696595162153244
Epoch [393/500], Train Loss: 0.0579, Validation Loss: 0.0567, Generator Loss: 11.9939, Discriminator Loss: 0.3141
Training epoch 394 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.060521017760038376
Training batch 2 / 32
Total batch reconstruction loss: 0.057577576488256454
Training batch 3 / 32
Total batch reconstruction loss: 0.0545281320810318
Training batch 4 / 32
Total batch reconstruction loss: 0.05719876289367676
Training batch 5 / 32
Total batch reconstruction loss: 0.059217505156993866
Training batch 6 / 32
Total batch reconstruction loss: 0.059732552617788315
Training batch 7 / 32
Total batch reconstruction loss: 0.055021386593580246
Training batch 8 / 32
Total batch reconstruction loss: 0.05623753368854523
Training batch 9 / 32
Total batch reconstruction loss: 0.05581032484769821
Training batch 10 / 32
Total batch reconstruction loss: 0.056799374520778656
Training batch 11 / 32
Total batch reconstruction loss: 0.06096304580569267
Training batch 12 / 32
Total batch reconstruction loss: 0.062238529324531555
Training batch 13 / 32
Total batch reconstruction loss: 0.05738721787929535
Training batch 14 / 32
Total batch reconstruction loss: 0.058668140321969986
Training batch 15 / 32
Total batch reconstruction loss: 0.06384719908237457
Training batch 16 / 32
Total batch reconstruction loss: 0.05655449628829956
Training batch 17 / 32
Total batch reconstruction loss: 0.060283951461315155
Training batch 18 / 32
Total batch reconstruction loss: 0.059000205248594284
Training batch 19 / 32
Total batch reconstruction loss: 0.05906405672430992
Training batch 20 / 32
Total batch reconstruction loss: 0.06161855533719063
Training batch 21 / 32
Total batch reconstruction loss: 0.0572885125875473
Training batch 22 / 32
Total batch reconstruction loss: 0.061086587607860565
Training batch 23 / 32
Total batch reconstruction loss: 0.059077657759189606
Training batch 24 / 32
Total batch reconstruction loss: 0.05854149907827377
Training batch 25 / 32
Total batch reconstruction loss: 0.0628271996974945
Training batch 26 / 32
Total batch reconstruction loss: 0.06411843001842499
Training batch 27 / 32
Total batch reconstruction loss: 0.05573081225156784
Training batch 28 / 32
Total batch reconstruction loss: 0.056193508207798004
Training batch 29 / 32
Total batch reconstruction loss: 0.0562884621322155
Training batch 30 / 32
Total batch reconstruction loss: 0.05119332671165466
Training batch 31 / 32
Total batch reconstruction loss: 0.05829046666622162
Training batch 32 / 32
Total batch reconstruction loss: 0.0751955509185791
Epoch [394/500], Train Loss: 0.0572, Validation Loss: 0.0598, Generator Loss: 11.8619, Discriminator Loss: 0.3301
Training epoch 395 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.058197297155857086
Training batch 2 / 32
Total batch reconstruction loss: 0.06347449123859406
Training batch 3 / 32
Total batch reconstruction loss: 0.06137411296367645
Training batch 4 / 32
Total batch reconstruction loss: 0.061976317316293716
Training batch 5 / 32
Total batch reconstruction loss: 0.05740339681506157
Training batch 6 / 32
Total batch reconstruction loss: 0.056460313498973846
Training batch 7 / 32
Total batch reconstruction loss: 0.05801400914788246
Training batch 8 / 32
Total batch reconstruction loss: 0.059057608246803284
Training batch 9 / 32
Total batch reconstruction loss: 0.05989745631814003
Training batch 10 / 32
Total batch reconstruction loss: 0.05755404382944107
Training batch 11 / 32
Total batch reconstruction loss: 0.06328300386667252
Training batch 12 / 32
Total batch reconstruction loss: 0.05928958207368851
Training batch 13 / 32
Total batch reconstruction loss: 0.056105829775333405
Training batch 14 / 32
Total batch reconstruction loss: 0.0629798099398613
Training batch 15 / 32
Total batch reconstruction loss: 0.05751931667327881
Training batch 16 / 32
Total batch reconstruction loss: 0.05484040081501007
Training batch 17 / 32
Total batch reconstruction loss: 0.057823680341243744
Training batch 18 / 32
Total batch reconstruction loss: 0.05838647112250328
Training batch 19 / 32
Total batch reconstruction loss: 0.05738244950771332
Training batch 20 / 32
Total batch reconstruction loss: 0.05697179585695267
Training batch 21 / 32
Total batch reconstruction loss: 0.05745654180645943
Training batch 22 / 32
Total batch reconstruction loss: 0.06338223814964294
Training batch 23 / 32
Total batch reconstruction loss: 0.06664809584617615
Training batch 24 / 32
Total batch reconstruction loss: 0.057309649884700775
Training batch 25 / 32
Total batch reconstruction loss: 0.0610034242272377
Training batch 26 / 32
Total batch reconstruction loss: 0.06110718473792076
Training batch 27 / 32
Total batch reconstruction loss: 0.059627335518598557
Training batch 28 / 32
Total batch reconstruction loss: 0.06066112220287323
Training batch 29 / 32
Total batch reconstruction loss: 0.05534576624631882
Training batch 30 / 32
Total batch reconstruction loss: 0.05666392296552658
Training batch 31 / 32
Total batch reconstruction loss: 0.05997354909777641
Training batch 32 / 32
Total batch reconstruction loss: 0.0500115230679512
Epoch [395/500], Train Loss: 0.0572, Validation Loss: 0.0566, Generator Loss: 11.8640, Discriminator Loss: 0.3118
Training epoch 396 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.05958157777786255
Training batch 2 / 32
Total batch reconstruction loss: 0.0577850304543972
Training batch 3 / 32
Total batch reconstruction loss: 0.06512641161680222
Training batch 4 / 32
Total batch reconstruction loss: 0.06092064455151558
Training batch 5 / 32
Total batch reconstruction loss: 0.05845794454216957
Training batch 6 / 32
Total batch reconstruction loss: 0.05909588187932968
Training batch 7 / 32
Total batch reconstruction loss: 0.05828314274549484
Training batch 8 / 32
Total batch reconstruction loss: 0.05992702767252922
Training batch 9 / 32
Total batch reconstruction loss: 0.0580664724111557
Training batch 10 / 32
Total batch reconstruction loss: 0.05928594991564751
Training batch 11 / 32
Total batch reconstruction loss: 0.05943680182099342
Training batch 12 / 32
Total batch reconstruction loss: 0.060029707849025726
Training batch 13 / 32
Total batch reconstruction loss: 0.0572621151804924
Training batch 14 / 32
Total batch reconstruction loss: 0.06129930913448334
Training batch 15 / 32
Total batch reconstruction loss: 0.05640270560979843
Training batch 16 / 32
Total batch reconstruction loss: 0.06366202235221863
Training batch 17 / 32
Total batch reconstruction loss: 0.0616539902985096
Training batch 18 / 32
Total batch reconstruction loss: 0.059272799640893936
Training batch 19 / 32
Total batch reconstruction loss: 0.05673009902238846
Training batch 20 / 32
Total batch reconstruction loss: 0.05928002670407295
Training batch 21 / 32
Total batch reconstruction loss: 0.05936111882328987
Training batch 22 / 32
Total batch reconstruction loss: 0.05632008612155914
Training batch 23 / 32
Total batch reconstruction loss: 0.06098543852567673
Training batch 24 / 32
Total batch reconstruction loss: 0.06252070516347885
Training batch 25 / 32
Total batch reconstruction loss: 0.05836830288171768
Training batch 26 / 32
Total batch reconstruction loss: 0.057220667600631714
Training batch 27 / 32
Total batch reconstruction loss: 0.05948197469115257
Training batch 28 / 32
Total batch reconstruction loss: 0.05624121055006981
Training batch 29 / 32
Total batch reconstruction loss: 0.05440168082714081
Training batch 30 / 32
Total batch reconstruction loss: 0.058955587446689606
Training batch 31 / 32
Total batch reconstruction loss: 0.054976239800453186
Training batch 32 / 32
Total batch reconstruction loss: 0.07548775523900986
Epoch [396/500], Train Loss: 0.0577, Validation Loss: 0.0566, Generator Loss: 11.9740, Discriminator Loss: 0.3317
Training epoch 397 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.057220444083213806
Training batch 2 / 32
Total batch reconstruction loss: 0.05896759778261185
Training batch 3 / 32
Total batch reconstruction loss: 0.05831682309508324
Training batch 4 / 32
Total batch reconstruction loss: 0.06204906105995178
Training batch 5 / 32
Total batch reconstruction loss: 0.057178087532520294
Training batch 6 / 32
Total batch reconstruction loss: 0.06424540281295776
Training batch 7 / 32
Total batch reconstruction loss: 0.06302626430988312
Training batch 8 / 32
Total batch reconstruction loss: 0.056379303336143494
Training batch 9 / 32
Total batch reconstruction loss: 0.05992123857140541
Training batch 10 / 32
Total batch reconstruction loss: 0.061916932463645935
Training batch 11 / 32
Total batch reconstruction loss: 0.05810694023966789
Training batch 12 / 32
Total batch reconstruction loss: 0.058939509093761444
Training batch 13 / 32
Total batch reconstruction loss: 0.060176316648721695
Training batch 14 / 32
Total batch reconstruction loss: 0.0605471134185791
Training batch 15 / 32
Total batch reconstruction loss: 0.05921580642461777
Training batch 16 / 32
Total batch reconstruction loss: 0.05895863100886345
Training batch 17 / 32
Total batch reconstruction loss: 0.05842936038970947
Training batch 18 / 32
Total batch reconstruction loss: 0.0547698438167572
Training batch 19 / 32
Total batch reconstruction loss: 0.06125800684094429
Training batch 20 / 32
Total batch reconstruction loss: 0.061681441962718964
Training batch 21 / 32
Total batch reconstruction loss: 0.05774573236703873
Training batch 22 / 32
Total batch reconstruction loss: 0.05430830270051956
Training batch 23 / 32
Total batch reconstruction loss: 0.062436215579509735
Training batch 24 / 32
Total batch reconstruction loss: 0.058641865849494934
Training batch 25 / 32
Total batch reconstruction loss: 0.05565162003040314
Training batch 26 / 32
Total batch reconstruction loss: 0.05606776475906372
Training batch 27 / 32
Total batch reconstruction loss: 0.05473990738391876
Training batch 28 / 32
Total batch reconstruction loss: 0.056928664445877075
Training batch 29 / 32
Total batch reconstruction loss: 0.06259629130363464
Training batch 30 / 32
Total batch reconstruction loss: 0.06131402775645256
Training batch 31 / 32
Total batch reconstruction loss: 0.06332049518823624
Training batch 32 / 32
Total batch reconstruction loss: 0.047394827008247375
Epoch [397/500], Train Loss: 0.0568, Validation Loss: 0.0580, Generator Loss: 11.8358, Discriminator Loss: 0.3106
Training epoch 398 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.053347185254096985
Training batch 2 / 32
Total batch reconstruction loss: 0.05618207901716232
Training batch 3 / 32
Total batch reconstruction loss: 0.05751346796751022
Training batch 4 / 32
Total batch reconstruction loss: 0.06265856325626373
Training batch 5 / 32
Total batch reconstruction loss: 0.05784635245800018
Training batch 6 / 32
Total batch reconstruction loss: 0.05646923929452896
Training batch 7 / 32
Total batch reconstruction loss: 0.05768788605928421
Training batch 8 / 32
Total batch reconstruction loss: 0.058212120085954666
Training batch 9 / 32
Total batch reconstruction loss: 0.05763119459152222
Training batch 10 / 32
Total batch reconstruction loss: 0.060293104499578476
Training batch 11 / 32
Total batch reconstruction loss: 0.057946495711803436
Training batch 12 / 32
Total batch reconstruction loss: 0.06055082380771637
Training batch 13 / 32
Total batch reconstruction loss: 0.053831301629543304
Training batch 14 / 32
Total batch reconstruction loss: 0.060186851769685745
Training batch 15 / 32
Total batch reconstruction loss: 0.056241706013679504
Training batch 16 / 32
Total batch reconstruction loss: 0.061877600848674774
Training batch 17 / 32
Total batch reconstruction loss: 0.0551743321120739
Training batch 18 / 32
Total batch reconstruction loss: 0.06218375638127327
Training batch 19 / 32
Total batch reconstruction loss: 0.058077991008758545
Training batch 20 / 32
Total batch reconstruction loss: 0.05912448465824127
Training batch 21 / 32
Total batch reconstruction loss: 0.05638985335826874
Training batch 22 / 32
Total batch reconstruction loss: 0.05655822902917862
Training batch 23 / 32
Total batch reconstruction loss: 0.06257537007331848
Training batch 24 / 32
Total batch reconstruction loss: 0.06373712420463562
Training batch 25 / 32
Total batch reconstruction loss: 0.05961528420448303
Training batch 26 / 32
Total batch reconstruction loss: 0.05402711406350136
Training batch 27 / 32
Total batch reconstruction loss: 0.05772366002202034
Training batch 28 / 32
Total batch reconstruction loss: 0.06312514841556549
Training batch 29 / 32
Total batch reconstruction loss: 0.06034589931368828
Training batch 30 / 32
Total batch reconstruction loss: 0.05750943347811699
Training batch 31 / 32
Total batch reconstruction loss: 0.05961420014500618
Training batch 32 / 32
Total batch reconstruction loss: 0.05164497718214989
Epoch [398/500], Train Loss: 0.0564, Validation Loss: 0.0579, Generator Loss: 11.7275, Discriminator Loss: 0.3173
Training epoch 399 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.060387250036001205
Training batch 2 / 32
Total batch reconstruction loss: 0.05509311705827713
Training batch 3 / 32
Total batch reconstruction loss: 0.06164829060435295
Training batch 4 / 32
Total batch reconstruction loss: 0.05796128883957863
Training batch 5 / 32
Total batch reconstruction loss: 0.056938011199235916
Training batch 6 / 32
Total batch reconstruction loss: 0.053971074521541595
Training batch 7 / 32
Total batch reconstruction loss: 0.05588763207197189
Training batch 8 / 32
Total batch reconstruction loss: 0.05544598400592804
Training batch 9 / 32
Total batch reconstruction loss: 0.057686299085617065
Training batch 10 / 32
Total batch reconstruction loss: 0.05594831332564354
Training batch 11 / 32
Total batch reconstruction loss: 0.06015753000974655
Training batch 12 / 32
Total batch reconstruction loss: 0.05748198926448822
Training batch 13 / 32
Total batch reconstruction loss: 0.05835002288222313
Training batch 14 / 32
Total batch reconstruction loss: 0.06490496546030045
Training batch 15 / 32
Total batch reconstruction loss: 0.0588315948843956
Training batch 16 / 32
Total batch reconstruction loss: 0.05696405470371246
Training batch 17 / 32
Total batch reconstruction loss: 0.06119333207607269
Training batch 18 / 32
Total batch reconstruction loss: 0.05708770453929901
Training batch 19 / 32
Total batch reconstruction loss: 0.059506844729185104
Training batch 20 / 32
Total batch reconstruction loss: 0.05887771397829056
Training batch 21 / 32
Total batch reconstruction loss: 0.06135077029466629
Training batch 22 / 32
Total batch reconstruction loss: 0.06046212837100029
Training batch 23 / 32
Total batch reconstruction loss: 0.059466343373060226
Training batch 24 / 32
Total batch reconstruction loss: 0.056414924561977386
Training batch 25 / 32
Total batch reconstruction loss: 0.06294875591993332
Training batch 26 / 32
Total batch reconstruction loss: 0.05707893520593643
Training batch 27 / 32
Total batch reconstruction loss: 0.06072323024272919
Training batch 28 / 32
Total batch reconstruction loss: 0.05417220667004585
Training batch 29 / 32
Total batch reconstruction loss: 0.060191068798303604
Training batch 30 / 32
Total batch reconstruction loss: 0.0580601766705513
Training batch 31 / 32
Total batch reconstruction loss: 0.05709873139858246
Training batch 32 / 32
Total batch reconstruction loss: 0.06269820779561996
Epoch [399/500], Train Loss: 0.0568, Validation Loss: 0.0567, Generator Loss: 11.7897, Discriminator Loss: 0.3100
Training epoch 400 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.055819321423769
Training batch 2 / 32
Total batch reconstruction loss: 0.058570168912410736
Training batch 3 / 32
Total batch reconstruction loss: 0.06169995293021202
Training batch 4 / 32
Total batch reconstruction loss: 0.057388462126255035
Training batch 5 / 32
Total batch reconstruction loss: 0.05671405792236328
Training batch 6 / 32
Total batch reconstruction loss: 0.05871277302503586
Training batch 7 / 32
Total batch reconstruction loss: 0.053652435541152954
Training batch 8 / 32
Total batch reconstruction loss: 0.06260184198617935
Training batch 9 / 32
Total batch reconstruction loss: 0.05912376940250397
Training batch 10 / 32
Total batch reconstruction loss: 0.056575365364551544
Training batch 11 / 32
Total batch reconstruction loss: 0.06201016530394554
Training batch 12 / 32
Total batch reconstruction loss: 0.05844266712665558
Training batch 13 / 32
Total batch reconstruction loss: 0.0629030242562294
Training batch 14 / 32
Total batch reconstruction loss: 0.06513715535402298
Training batch 15 / 32
Total batch reconstruction loss: 0.05636902153491974
Training batch 16 / 32
Total batch reconstruction loss: 0.0561249814927578
Training batch 17 / 32
Total batch reconstruction loss: 0.05703166872262955
Training batch 18 / 32
Total batch reconstruction loss: 0.05894310027360916
Training batch 19 / 32
Total batch reconstruction loss: 0.057973019778728485
Training batch 20 / 32
Total batch reconstruction loss: 0.05973096564412117
Training batch 21 / 32
Total batch reconstruction loss: 0.05465415120124817
Training batch 22 / 32
Total batch reconstruction loss: 0.056482478976249695
Training batch 23 / 32
Total batch reconstruction loss: 0.05901411920785904
Training batch 24 / 32
Total batch reconstruction loss: 0.05687183141708374
Training batch 25 / 32
Total batch reconstruction loss: 0.06264006346464157
Training batch 26 / 32
Total batch reconstruction loss: 0.056612249463796616
Training batch 27 / 32
Total batch reconstruction loss: 0.060093630105257034
Training batch 28 / 32
Total batch reconstruction loss: 0.056803084909915924
Training batch 29 / 32
Total batch reconstruction loss: 0.05675876513123512
Training batch 30 / 32
Total batch reconstruction loss: 0.05935688316822052
Training batch 31 / 32
Total batch reconstruction loss: 0.060746632516384125
Training batch 32 / 32
Total batch reconstruction loss: 0.0649847686290741
Epoch [400/500], Train Loss: 0.0570, Validation Loss: 0.0569, Generator Loss: 11.8242, Discriminator Loss: 0.3091
Training epoch 401 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.05709453672170639
Training batch 2 / 32
Total batch reconstruction loss: 0.05971473827958107
Training batch 3 / 32
Total batch reconstruction loss: 0.061977263540029526
Training batch 4 / 32
Total batch reconstruction loss: 0.05808884650468826
Training batch 5 / 32
Total batch reconstruction loss: 0.06255920231342316
Training batch 6 / 32
Total batch reconstruction loss: 0.05790980905294418
Training batch 7 / 32
Total batch reconstruction loss: 0.05959053710103035
Training batch 8 / 32
Total batch reconstruction loss: 0.05926026403903961
Training batch 9 / 32
Total batch reconstruction loss: 0.05996073782444
Training batch 10 / 32
Total batch reconstruction loss: 0.06144987791776657
Training batch 11 / 32
Total batch reconstruction loss: 0.061464592814445496
Training batch 12 / 32
Total batch reconstruction loss: 0.05744782090187073
Training batch 13 / 32
Total batch reconstruction loss: 0.05986296385526657
Training batch 14 / 32
Total batch reconstruction loss: 0.06197080388665199
Training batch 15 / 32
Total batch reconstruction loss: 0.060666680335998535
Training batch 16 / 32
Total batch reconstruction loss: 0.060862891376018524
Training batch 17 / 32
Total batch reconstruction loss: 0.05995497852563858
Training batch 18 / 32
Total batch reconstruction loss: 0.05958593636751175
Training batch 19 / 32
Total batch reconstruction loss: 0.05566583573818207
Training batch 20 / 32
Total batch reconstruction loss: 0.0625912994146347
Training batch 21 / 32
Total batch reconstruction loss: 0.06097692996263504
Training batch 22 / 32
Total batch reconstruction loss: 0.054156266152858734
Training batch 23 / 32
Total batch reconstruction loss: 0.0540718249976635
Training batch 24 / 32
Total batch reconstruction loss: 0.05757995322346687
Training batch 25 / 32
Total batch reconstruction loss: 0.05613293498754501
Training batch 26 / 32
Total batch reconstruction loss: 0.06211257725954056
Training batch 27 / 32
Total batch reconstruction loss: 0.05551696568727493
Training batch 28 / 32
Total batch reconstruction loss: 0.05420056730508804
Training batch 29 / 32
Total batch reconstruction loss: 0.062172915786504745
Training batch 30 / 32
Total batch reconstruction loss: 0.059612758457660675
Training batch 31 / 32
Total batch reconstruction loss: 0.05788808315992355
Training batch 32 / 32
Total batch reconstruction loss: 0.06401340663433075
Epoch [401/500], Train Loss: 0.0571, Validation Loss: 0.0569, Generator Loss: 11.9235, Discriminator Loss: 0.3115
Training epoch 402 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.05989474058151245
Training batch 2 / 32
Total batch reconstruction loss: 0.0611020028591156
Training batch 3 / 32
Total batch reconstruction loss: 0.06500180065631866
Training batch 4 / 32
Total batch reconstruction loss: 0.06269250810146332
Training batch 5 / 32
Total batch reconstruction loss: 0.06182461604475975
Training batch 6 / 32
Total batch reconstruction loss: 0.05785507708787918
Training batch 7 / 32
Total batch reconstruction loss: 0.058593474328517914
Training batch 8 / 32
Total batch reconstruction loss: 0.05863386392593384
Training batch 9 / 32
Total batch reconstruction loss: 0.05881381407380104
Training batch 10 / 32
Total batch reconstruction loss: 0.06044548749923706
Training batch 11 / 32
Total batch reconstruction loss: 0.06146346032619476
Training batch 12 / 32
Total batch reconstruction loss: 0.05980931222438812
Training batch 13 / 32
Total batch reconstruction loss: 0.05761437863111496
Training batch 14 / 32
Total batch reconstruction loss: 0.06078111752867699
Training batch 15 / 32
Total batch reconstruction loss: 0.059816472232341766
Training batch 16 / 32
Total batch reconstruction loss: 0.0604105107486248
Training batch 17 / 32
Total batch reconstruction loss: 0.05625241622328758
Training batch 18 / 32
Total batch reconstruction loss: 0.054503485560417175
Training batch 19 / 32
Total batch reconstruction loss: 0.059248507022857666
Training batch 20 / 32
Total batch reconstruction loss: 0.05426990985870361
Training batch 21 / 32
Total batch reconstruction loss: 0.06375635415315628
Training batch 22 / 32
Total batch reconstruction loss: 0.057767823338508606
Training batch 23 / 32
Total batch reconstruction loss: 0.058426305651664734
Training batch 24 / 32
Total batch reconstruction loss: 0.05711941421031952
Training batch 25 / 32
Total batch reconstruction loss: 0.05751272290945053
Training batch 26 / 32
Total batch reconstruction loss: 0.05840644985437393
Training batch 27 / 32
Total batch reconstruction loss: 0.05913002789020538
Training batch 28 / 32
Total batch reconstruction loss: 0.05395083874464035
Training batch 29 / 32
Total batch reconstruction loss: 0.056994128972291946
Training batch 30 / 32
Total batch reconstruction loss: 0.06161266565322876
Training batch 31 / 32
Total batch reconstruction loss: 0.05047687515616417
Training batch 32 / 32
Total batch reconstruction loss: 0.05263814330101013
Epoch [402/500], Train Loss: 0.0567, Validation Loss: 0.0564, Generator Loss: 11.7992, Discriminator Loss: 0.3126
Training epoch 403 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.05693553388118744
Training batch 2 / 32
Total batch reconstruction loss: 0.05553452670574188
Training batch 3 / 32
Total batch reconstruction loss: 0.057205189019441605
Training batch 4 / 32
Total batch reconstruction loss: 0.05492934584617615
Training batch 5 / 32
Total batch reconstruction loss: 0.054939769208431244
Training batch 6 / 32
Total batch reconstruction loss: 0.05513907968997955
Training batch 7 / 32
Total batch reconstruction loss: 0.05789116397500038
Training batch 8 / 32
Total batch reconstruction loss: 0.06367915868759155
Training batch 9 / 32
Total batch reconstruction loss: 0.05740901082754135
Training batch 10 / 32
Total batch reconstruction loss: 0.05589045584201813
Training batch 11 / 32
Total batch reconstruction loss: 0.06343862414360046
Training batch 12 / 32
Total batch reconstruction loss: 0.060651760548353195
Training batch 13 / 32
Total batch reconstruction loss: 0.05802910029888153
Training batch 14 / 32
Total batch reconstruction loss: 0.05956694483757019
Training batch 15 / 32
Total batch reconstruction loss: 0.06220472604036331
Training batch 16 / 32
Total batch reconstruction loss: 0.060101814568042755
Training batch 17 / 32
Total batch reconstruction loss: 0.059715233743190765
Training batch 18 / 32
Total batch reconstruction loss: 0.06519488990306854
Training batch 19 / 32
Total batch reconstruction loss: 0.055299725383520126
Training batch 20 / 32
Total batch reconstruction loss: 0.057074397802352905
Training batch 21 / 32
Total batch reconstruction loss: 0.05742768198251724
Training batch 22 / 32
Total batch reconstruction loss: 0.06022631749510765
Training batch 23 / 32
Total batch reconstruction loss: 0.05658678710460663
Training batch 24 / 32
Total batch reconstruction loss: 0.06007350981235504
Training batch 25 / 32
Total batch reconstruction loss: 0.05707115679979324
Training batch 26 / 32
Total batch reconstruction loss: 0.05492231249809265
Training batch 27 / 32
Total batch reconstruction loss: 0.05699533969163895
Training batch 28 / 32
Total batch reconstruction loss: 0.05789138004183769
Training batch 29 / 32
Total batch reconstruction loss: 0.05414394289255142
Training batch 30 / 32
Total batch reconstruction loss: 0.056487832218408585
Training batch 31 / 32
Total batch reconstruction loss: 0.060781098902225494
Training batch 32 / 32
Total batch reconstruction loss: 0.047770220786333084
Epoch [403/500], Train Loss: 0.0559, Validation Loss: 0.0592, Generator Loss: 11.6435, Discriminator Loss: 0.3181
Training epoch 404 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.05597376823425293
Training batch 2 / 32
Total batch reconstruction loss: 0.05542198196053505
Training batch 3 / 32
Total batch reconstruction loss: 0.057897016406059265
Training batch 4 / 32
Total batch reconstruction loss: 0.059178538620471954
Training batch 5 / 32
Total batch reconstruction loss: 0.056627124547958374
Training batch 6 / 32
Total batch reconstruction loss: 0.05645596981048584
Training batch 7 / 32
Total batch reconstruction loss: 0.05892518162727356
Training batch 8 / 32
Total batch reconstruction loss: 0.05774855613708496
Training batch 9 / 32
Total batch reconstruction loss: 0.05932216718792915
Training batch 10 / 32
Total batch reconstruction loss: 0.05798419937491417
Training batch 11 / 32
Total batch reconstruction loss: 0.0615716278553009
Training batch 12 / 32
Total batch reconstruction loss: 0.05779999494552612
Training batch 13 / 32
Total batch reconstruction loss: 0.061160165816545486
Training batch 14 / 32
Total batch reconstruction loss: 0.06145656853914261
Training batch 15 / 32
Total batch reconstruction loss: 0.05851142853498459
Training batch 16 / 32
Total batch reconstruction loss: 0.06053338199853897
Training batch 17 / 32
Total batch reconstruction loss: 0.06022810563445091
Training batch 18 / 32
Total batch reconstruction loss: 0.060392752289772034
Training batch 19 / 32
Total batch reconstruction loss: 0.057398755103349686
Training batch 20 / 32
Total batch reconstruction loss: 0.06218276545405388
Training batch 21 / 32
Total batch reconstruction loss: 0.05561499670147896
Training batch 22 / 32
Total batch reconstruction loss: 0.05583333224058151
Training batch 23 / 32
Total batch reconstruction loss: 0.05788850784301758
Training batch 24 / 32
Total batch reconstruction loss: 0.058900244534015656
Training batch 25 / 32
Total batch reconstruction loss: 0.06041156128048897
Training batch 26 / 32
Total batch reconstruction loss: 0.06279823184013367
Training batch 27 / 32
Total batch reconstruction loss: 0.05968902260065079
Training batch 28 / 32
Total batch reconstruction loss: 0.05618263781070709
Training batch 29 / 32
Total batch reconstruction loss: 0.060522254556417465
Training batch 30 / 32
Total batch reconstruction loss: 0.06060313433408737
Training batch 31 / 32
Total batch reconstruction loss: 0.05396762117743492
Training batch 32 / 32
Total batch reconstruction loss: 0.0600651353597641
Epoch [404/500], Train Loss: 0.0572, Validation Loss: 0.0575, Generator Loss: 11.8152, Discriminator Loss: 0.3205
Training epoch 405 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.06020288169384003
Training batch 2 / 32
Total batch reconstruction loss: 0.06023257225751877
Training batch 3 / 32
Total batch reconstruction loss: 0.06246472895145416
Training batch 4 / 32
Total batch reconstruction loss: 0.06004905328154564
Training batch 5 / 32
Total batch reconstruction loss: 0.061354734003543854
Training batch 6 / 32
Total batch reconstruction loss: 0.062162842601537704
Training batch 7 / 32
Total batch reconstruction loss: 0.06229826807975769
Training batch 8 / 32
Total batch reconstruction loss: 0.05580104887485504
Training batch 9 / 32
Total batch reconstruction loss: 0.06348438560962677
Training batch 10 / 32
Total batch reconstruction loss: 0.05739464610815048
Training batch 11 / 32
Total batch reconstruction loss: 0.06306202709674835
Training batch 12 / 32
Total batch reconstruction loss: 0.057066697627305984
Training batch 13 / 32
Total batch reconstruction loss: 0.056150637567043304
Training batch 14 / 32
Total batch reconstruction loss: 0.06292442977428436
Training batch 15 / 32
Total batch reconstruction loss: 0.05473827198147774
Training batch 16 / 32
Total batch reconstruction loss: 0.06362821906805038
Training batch 17 / 32
Total batch reconstruction loss: 0.061297111213207245
Training batch 18 / 32
Total batch reconstruction loss: 0.06206583231687546
Training batch 19 / 32
Total batch reconstruction loss: 0.05874999612569809
Training batch 20 / 32
Total batch reconstruction loss: 0.05593154579401016
Training batch 21 / 32
Total batch reconstruction loss: 0.06000582501292229
Training batch 22 / 32
Total batch reconstruction loss: 0.06042681261897087
Training batch 23 / 32
Total batch reconstruction loss: 0.05668244883418083
Training batch 24 / 32
Total batch reconstruction loss: 0.05634981393814087
Training batch 25 / 32
Total batch reconstruction loss: 0.057771895080804825
Training batch 26 / 32
Total batch reconstruction loss: 0.05797377973794937
Training batch 27 / 32
Total batch reconstruction loss: 0.06538327038288116
Training batch 28 / 32
Total batch reconstruction loss: 0.054377712309360504
Training batch 29 / 32
Total batch reconstruction loss: 0.05889434367418289
Training batch 30 / 32
Total batch reconstruction loss: 0.06325087696313858
Training batch 31 / 32
Total batch reconstruction loss: 0.05475574731826782
Training batch 32 / 32
Total batch reconstruction loss: 0.05177575349807739
Epoch [405/500], Train Loss: 0.0578, Validation Loss: 0.0573, Generator Loss: 11.9381, Discriminator Loss: 0.3132
Training epoch 406 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.062110841274261475
Training batch 2 / 32
Total batch reconstruction loss: 0.058023907244205475
Training batch 3 / 32
Total batch reconstruction loss: 0.061261408030986786
Training batch 4 / 32
Total batch reconstruction loss: 0.05970809608697891
Training batch 5 / 32
Total batch reconstruction loss: 0.060250572860240936
Training batch 6 / 32
Total batch reconstruction loss: 0.055003587156534195
Training batch 7 / 32
Total batch reconstruction loss: 0.0597204864025116
Training batch 8 / 32
Total batch reconstruction loss: 0.05859213322401047
Training batch 9 / 32
Total batch reconstruction loss: 0.060394834727048874
Training batch 10 / 32
Total batch reconstruction loss: 0.06243055313825607
Training batch 11 / 32
Total batch reconstruction loss: 0.05818914622068405
Training batch 12 / 32
Total batch reconstruction loss: 0.06088149547576904
Training batch 13 / 32
Total batch reconstruction loss: 0.05667855590581894
Training batch 14 / 32
Total batch reconstruction loss: 0.05982627719640732
Training batch 15 / 32
Total batch reconstruction loss: 0.06378684192895889
Training batch 16 / 32
Total batch reconstruction loss: 0.053540464490652084
Training batch 17 / 32
Total batch reconstruction loss: 0.05770207569003105
Training batch 18 / 32
Total batch reconstruction loss: 0.05958682671189308
Training batch 19 / 32
Total batch reconstruction loss: 0.062310591340065
Training batch 20 / 32
Total batch reconstruction loss: 0.05684134364128113
Training batch 21 / 32
Total batch reconstruction loss: 0.06757186353206635
Training batch 22 / 32
Total batch reconstruction loss: 0.058165762573480606
Training batch 23 / 32
Total batch reconstruction loss: 0.05836828052997589
Training batch 24 / 32
Total batch reconstruction loss: 0.06048082560300827
Training batch 25 / 32
Total batch reconstruction loss: 0.05551560968160629
Training batch 26 / 32
Total batch reconstruction loss: 0.05440933257341385
Training batch 27 / 32
Total batch reconstruction loss: 0.05765456706285477
Training batch 28 / 32
Total batch reconstruction loss: 0.06059837341308594
Training batch 29 / 32
Total batch reconstruction loss: 0.0593201145529747
Training batch 30 / 32
Total batch reconstruction loss: 0.058775369077920914
Training batch 31 / 32
Total batch reconstruction loss: 0.05459287762641907
Training batch 32 / 32
Total batch reconstruction loss: 0.046800319105386734
Epoch [406/500], Train Loss: 0.0568, Validation Loss: 0.0564, Generator Loss: 11.8058, Discriminator Loss: 0.3218
Training epoch 407 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.05840549245476723
Training batch 2 / 32
Total batch reconstruction loss: 0.057237789034843445
Training batch 3 / 32
Total batch reconstruction loss: 0.05960605666041374
Training batch 4 / 32
Total batch reconstruction loss: 0.059216395020484924
Training batch 5 / 32
Total batch reconstruction loss: 0.05813239887356758
Training batch 6 / 32
Total batch reconstruction loss: 0.057856831699609756
Training batch 7 / 32
Total batch reconstruction loss: 0.058278221637010574
Training batch 8 / 32
Total batch reconstruction loss: 0.056576844304800034
Training batch 9 / 32
Total batch reconstruction loss: 0.06512613594532013
Training batch 10 / 32
Total batch reconstruction loss: 0.06046105921268463
Training batch 11 / 32
Total batch reconstruction loss: 0.057664088904857635
Training batch 12 / 32
Total batch reconstruction loss: 0.061097122728824615
Training batch 13 / 32
Total batch reconstruction loss: 0.059318821877241135
Training batch 14 / 32
Total batch reconstruction loss: 0.05747806280851364
Training batch 15 / 32
Total batch reconstruction loss: 0.056800514459609985
Training batch 16 / 32
Total batch reconstruction loss: 0.06196066364645958
Training batch 17 / 32
Total batch reconstruction loss: 0.060957297682762146
Training batch 18 / 32
Total batch reconstruction loss: 0.05250496789813042
Training batch 19 / 32
Total batch reconstruction loss: 0.05695644021034241
Training batch 20 / 32
Total batch reconstruction loss: 0.060531605035066605
Training batch 21 / 32
Total batch reconstruction loss: 0.06155449151992798
Training batch 22 / 32
Total batch reconstruction loss: 0.06035112589597702
Training batch 23 / 32
Total batch reconstruction loss: 0.06384699791669846
Training batch 24 / 32
Total batch reconstruction loss: 0.06071623042225838
Training batch 25 / 32
Total batch reconstruction loss: 0.0553547739982605
Training batch 26 / 32
Total batch reconstruction loss: 0.06141269952058792
Training batch 27 / 32
Total batch reconstruction loss: 0.055679257959127426
Training batch 28 / 32
Total batch reconstruction loss: 0.05608709901571274
Training batch 29 / 32
Total batch reconstruction loss: 0.05376070737838745
Training batch 30 / 32
Total batch reconstruction loss: 0.06369481980800629
Training batch 31 / 32
Total batch reconstruction loss: 0.056796908378601074
Training batch 32 / 32
Total batch reconstruction loss: 0.04937734454870224
Epoch [407/500], Train Loss: 0.0564, Validation Loss: 0.0564, Generator Loss: 11.7872, Discriminator Loss: 0.3102
Training epoch 408 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.05856069549918175
Training batch 2 / 32
Total batch reconstruction loss: 0.05487060546875
Training batch 3 / 32
Total batch reconstruction loss: 0.06599584221839905
Training batch 4 / 32
Total batch reconstruction loss: 0.05803757533431053
Training batch 5 / 32
Total batch reconstruction loss: 0.060304395854473114
Training batch 6 / 32
Total batch reconstruction loss: 0.056616611778736115
Training batch 7 / 32
Total batch reconstruction loss: 0.055646225810050964
Training batch 8 / 32
Total batch reconstruction loss: 0.06093916669487953
Training batch 9 / 32
Total batch reconstruction loss: 0.05778859928250313
Training batch 10 / 32
Total batch reconstruction loss: 0.060987092554569244
Training batch 11 / 32
Total batch reconstruction loss: 0.0563766285777092
Training batch 12 / 32
Total batch reconstruction loss: 0.05764350667595863
Training batch 13 / 32
Total batch reconstruction loss: 0.05850241333246231
Training batch 14 / 32
Total batch reconstruction loss: 0.05900757759809494
Training batch 15 / 32
Total batch reconstruction loss: 0.05856423079967499
Training batch 16 / 32
Total batch reconstruction loss: 0.06322783976793289
Training batch 17 / 32
Total batch reconstruction loss: 0.057886090129613876
Training batch 18 / 32
Total batch reconstruction loss: 0.05760594829916954
Training batch 19 / 32
Total batch reconstruction loss: 0.052386097609996796
Training batch 20 / 32
Total batch reconstruction loss: 0.0579410195350647
Training batch 21 / 32
Total batch reconstruction loss: 0.05573410913348198
Training batch 22 / 32
Total batch reconstruction loss: 0.06362300366163254
Training batch 23 / 32
Total batch reconstruction loss: 0.05955009162425995
Training batch 24 / 32
Total batch reconstruction loss: 0.05909144878387451
Training batch 25 / 32
Total batch reconstruction loss: 0.05585676059126854
Training batch 26 / 32
Total batch reconstruction loss: 0.05812368914484978
Training batch 27 / 32
Total batch reconstruction loss: 0.06330859661102295
Training batch 28 / 32
Total batch reconstruction loss: 0.06037840247154236
Training batch 29 / 32
Total batch reconstruction loss: 0.057215239852666855
Training batch 30 / 32
Total batch reconstruction loss: 0.058566879481077194
Training batch 31 / 32
Total batch reconstruction loss: 0.057764697819948196
Training batch 32 / 32
Total batch reconstruction loss: 0.06360046565532684
Epoch [408/500], Train Loss: 0.0570, Validation Loss: 0.0600, Generator Loss: 11.8425, Discriminator Loss: 0.3010
Training epoch 409 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.05724528059363365
Training batch 2 / 32
Total batch reconstruction loss: 0.05747763812541962
Training batch 3 / 32
Total batch reconstruction loss: 0.05886950343847275
Training batch 4 / 32
Total batch reconstruction loss: 0.057027604430913925
Training batch 5 / 32
Total batch reconstruction loss: 0.05936505272984505
Training batch 6 / 32
Total batch reconstruction loss: 0.05904562026262283
Training batch 7 / 32
Total batch reconstruction loss: 0.06448420882225037
Training batch 8 / 32
Total batch reconstruction loss: 0.059235796332359314
Training batch 9 / 32
Total batch reconstruction loss: 0.05878881737589836
Training batch 10 / 32
Total batch reconstruction loss: 0.062048859894275665
Training batch 11 / 32
Total batch reconstruction loss: 0.06548711657524109
Training batch 12 / 32
Total batch reconstruction loss: 0.05559855327010155
Training batch 13 / 32
Total batch reconstruction loss: 0.05699451267719269
Training batch 14 / 32
Total batch reconstruction loss: 0.05909336358308792
Training batch 15 / 32
Total batch reconstruction loss: 0.05739634484052658
Training batch 16 / 32
Total batch reconstruction loss: 0.06128014624118805
Training batch 17 / 32
Total batch reconstruction loss: 0.057322416454553604
Training batch 18 / 32
Total batch reconstruction loss: 0.06250735372304916
Training batch 19 / 32
Total batch reconstruction loss: 0.06277091056108475
Training batch 20 / 32
Total batch reconstruction loss: 0.06015215069055557
Training batch 21 / 32
Total batch reconstruction loss: 0.0591239333152771
Training batch 22 / 32
Total batch reconstruction loss: 0.0557686910033226
Training batch 23 / 32
Total batch reconstruction loss: 0.05857880413532257
Training batch 24 / 32
Total batch reconstruction loss: 0.05758257210254669
Training batch 25 / 32
Total batch reconstruction loss: 0.05666451156139374
Training batch 26 / 32
Total batch reconstruction loss: 0.061585210263729095
Training batch 27 / 32
Total batch reconstruction loss: 0.05824904143810272
Training batch 28 / 32
Total batch reconstruction loss: 0.058227356523275375
Training batch 29 / 32
Total batch reconstruction loss: 0.05795346572995186
Training batch 30 / 32
Total batch reconstruction loss: 0.06297065317630768
Training batch 31 / 32
Total batch reconstruction loss: 0.06308893114328384
Training batch 32 / 32
Total batch reconstruction loss: 0.049037832766771317
Epoch [409/500], Train Loss: 0.0573, Validation Loss: 0.0567, Generator Loss: 11.8910, Discriminator Loss: 0.3081
Training epoch 410 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.057983431965112686
Training batch 2 / 32
Total batch reconstruction loss: 0.05959607660770416
Training batch 3 / 32
Total batch reconstruction loss: 0.06048055365681648
Training batch 4 / 32
Total batch reconstruction loss: 0.059741802513599396
Training batch 5 / 32
Total batch reconstruction loss: 0.057881876826286316
Training batch 6 / 32
Total batch reconstruction loss: 0.056154657155275345
Training batch 7 / 32
Total batch reconstruction loss: 0.05955660715699196
Training batch 8 / 32
Total batch reconstruction loss: 0.06557733565568924
Training batch 9 / 32
Total batch reconstruction loss: 0.059260472655296326
Training batch 10 / 32
Total batch reconstruction loss: 0.058973975479602814
Training batch 11 / 32
Total batch reconstruction loss: 0.06372596323490143
Training batch 12 / 32
Total batch reconstruction loss: 0.057962920516729355
Training batch 13 / 32
Total batch reconstruction loss: 0.06131264194846153
Training batch 14 / 32
Total batch reconstruction loss: 0.05607657507061958
Training batch 15 / 32
Total batch reconstruction loss: 0.05682593584060669
Training batch 16 / 32
Total batch reconstruction loss: 0.06225069984793663
Training batch 17 / 32
Total batch reconstruction loss: 0.05923641473054886
Training batch 18 / 32
Total batch reconstruction loss: 0.055780671536922455
Training batch 19 / 32
Total batch reconstruction loss: 0.0570632740855217
Training batch 20 / 32
Total batch reconstruction loss: 0.05413772165775299
Training batch 21 / 32
Total batch reconstruction loss: 0.05997795984148979
Training batch 22 / 32
Total batch reconstruction loss: 0.05901158228516579
Training batch 23 / 32
Total batch reconstruction loss: 0.060470350086688995
Training batch 24 / 32
Total batch reconstruction loss: 0.05827682465314865
Training batch 25 / 32
Total batch reconstruction loss: 0.05696669965982437
Training batch 26 / 32
Total batch reconstruction loss: 0.058967702090740204
Training batch 27 / 32
Total batch reconstruction loss: 0.05800609290599823
Training batch 28 / 32
Total batch reconstruction loss: 0.05701051652431488
Training batch 29 / 32
Total batch reconstruction loss: 0.06093723326921463
Training batch 30 / 32
Total batch reconstruction loss: 0.05676516145467758
Training batch 31 / 32
Total batch reconstruction loss: 0.05804000049829483
Training batch 32 / 32
Total batch reconstruction loss: 0.04383091628551483
Epoch [410/500], Train Loss: 0.0566, Validation Loss: 0.0564, Generator Loss: 11.7455, Discriminator Loss: 0.3132
Training epoch 411 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.05682763457298279
Training batch 2 / 32
Total batch reconstruction loss: 0.05713994801044464
Training batch 3 / 32
Total batch reconstruction loss: 0.055409692227840424
Training batch 4 / 32
Total batch reconstruction loss: 0.058558475226163864
Training batch 5 / 32
Total batch reconstruction loss: 0.06488268077373505
Training batch 6 / 32
Total batch reconstruction loss: 0.0543479397892952
Training batch 7 / 32
Total batch reconstruction loss: 0.056732263416051865
Training batch 8 / 32
Total batch reconstruction loss: 0.05894516408443451
Training batch 9 / 32
Total batch reconstruction loss: 0.059595637023448944
Training batch 10 / 32
Total batch reconstruction loss: 0.05518098548054695
Training batch 11 / 32
Total batch reconstruction loss: 0.06136780232191086
Training batch 12 / 32
Total batch reconstruction loss: 0.058056335896253586
Training batch 13 / 32
Total batch reconstruction loss: 0.056215010583400726
Training batch 14 / 32
Total batch reconstruction loss: 0.05786377936601639
Training batch 15 / 32
Total batch reconstruction loss: 0.06494538486003876
Training batch 16 / 32
Total batch reconstruction loss: 0.058328475803136826
Training batch 17 / 32
Total batch reconstruction loss: 0.056442029774188995
Training batch 18 / 32
Total batch reconstruction loss: 0.05871487408876419
Training batch 19 / 32
Total batch reconstruction loss: 0.05825485289096832
Training batch 20 / 32
Total batch reconstruction loss: 0.0569392554461956
Training batch 21 / 32
Total batch reconstruction loss: 0.05644816905260086
Training batch 22 / 32
Total batch reconstruction loss: 0.059709981083869934
Training batch 23 / 32
Total batch reconstruction loss: 0.059838779270648956
Training batch 24 / 32
Total batch reconstruction loss: 0.05816638469696045
Training batch 25 / 32
Total batch reconstruction loss: 0.05485930293798447
Training batch 26 / 32
Total batch reconstruction loss: 0.06333285570144653
Training batch 27 / 32
Total batch reconstruction loss: 0.0587288960814476
Training batch 28 / 32
Total batch reconstruction loss: 0.054668039083480835
Training batch 29 / 32
Total batch reconstruction loss: 0.06375782936811447
Training batch 30 / 32
Total batch reconstruction loss: 0.05990023910999298
Training batch 31 / 32
Total batch reconstruction loss: 0.057343944907188416
Training batch 32 / 32
Total batch reconstruction loss: 0.05304155498743057
Epoch [411/500], Train Loss: 0.0565, Validation Loss: 0.0575, Generator Loss: 11.7200, Discriminator Loss: 0.3197
Training epoch 412 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.05707798898220062
Training batch 2 / 32
Total batch reconstruction loss: 0.058888424187898636
Training batch 3 / 32
Total batch reconstruction loss: 0.056403324007987976
Training batch 4 / 32
Total batch reconstruction loss: 0.061372216790914536
Training batch 5 / 32
Total batch reconstruction loss: 0.05772537738084793
Training batch 6 / 32
Total batch reconstruction loss: 0.05782033130526543
Training batch 7 / 32
Total batch reconstruction loss: 0.05583355948328972
Training batch 8 / 32
Total batch reconstruction loss: 0.0611402690410614
Training batch 9 / 32
Total batch reconstruction loss: 0.056655578315258026
Training batch 10 / 32
Total batch reconstruction loss: 0.05491162836551666
Training batch 11 / 32
Total batch reconstruction loss: 0.058488041162490845
Training batch 12 / 32
Total batch reconstruction loss: 0.05663454532623291
Training batch 13 / 32
Total batch reconstruction loss: 0.05796559527516365
Training batch 14 / 32
Total batch reconstruction loss: 0.056936174631118774
Training batch 15 / 32
Total batch reconstruction loss: 0.05774099379777908
Training batch 16 / 32
Total batch reconstruction loss: 0.05939042195677757
Training batch 17 / 32
Total batch reconstruction loss: 0.0587318129837513
Training batch 18 / 32
Total batch reconstruction loss: 0.05847863480448723
Training batch 19 / 32
Total batch reconstruction loss: 0.06309574097394943
Training batch 20 / 32
Total batch reconstruction loss: 0.06258571147918701
Training batch 21 / 32
Total batch reconstruction loss: 0.06022166460752487
Training batch 22 / 32
Total batch reconstruction loss: 0.05970221012830734
Training batch 23 / 32
Total batch reconstruction loss: 0.05960459262132645
Training batch 24 / 32
Total batch reconstruction loss: 0.059003740549087524
Training batch 25 / 32
Total batch reconstruction loss: 0.05720888078212738
Training batch 26 / 32
Total batch reconstruction loss: 0.05804544687271118
Training batch 27 / 32
Total batch reconstruction loss: 0.05865338444709778
Training batch 28 / 32
Total batch reconstruction loss: 0.060075096786022186
Training batch 29 / 32
Total batch reconstruction loss: 0.05438697338104248
Training batch 30 / 32
Total batch reconstruction loss: 0.05706014111638069
Training batch 31 / 32
Total batch reconstruction loss: 0.05786997824907303
Training batch 32 / 32
Total batch reconstruction loss: 0.048330094665288925
Epoch [412/500], Train Loss: 0.0561, Validation Loss: 0.0573, Generator Loss: 11.6835, Discriminator Loss: 0.3094
Training epoch 413 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.053102605044841766
Training batch 2 / 32
Total batch reconstruction loss: 0.059904322028160095
Training batch 3 / 32
Total batch reconstruction loss: 0.059752196073532104
Training batch 4 / 32
Total batch reconstruction loss: 0.05481832101941109
Training batch 5 / 32
Total batch reconstruction loss: 0.056421004235744476
Training batch 6 / 32
Total batch reconstruction loss: 0.06021478772163391
Training batch 7 / 32
Total batch reconstruction loss: 0.059877604246139526
Training batch 8 / 32
Total batch reconstruction loss: 0.053406596183776855
Training batch 9 / 32
Total batch reconstruction loss: 0.057212945073843
Training batch 10 / 32
Total batch reconstruction loss: 0.057237278670072556
Training batch 11 / 32
Total batch reconstruction loss: 0.05792883783578873
Training batch 12 / 32
Total batch reconstruction loss: 0.06000182777643204
Training batch 13 / 32
Total batch reconstruction loss: 0.05977485701441765
Training batch 14 / 32
Total batch reconstruction loss: 0.05989058315753937
Training batch 15 / 32
Total batch reconstruction loss: 0.06006726622581482
Training batch 16 / 32
Total batch reconstruction loss: 0.055015698075294495
Training batch 17 / 32
Total batch reconstruction loss: 0.06174032390117645
Training batch 18 / 32
Total batch reconstruction loss: 0.06379984319210052
Training batch 19 / 32
Total batch reconstruction loss: 0.0562891811132431
Training batch 20 / 32
Total batch reconstruction loss: 0.05833856761455536
Training batch 21 / 32
Total batch reconstruction loss: 0.0584443137049675
Training batch 22 / 32
Total batch reconstruction loss: 0.06137126684188843
Training batch 23 / 32
Total batch reconstruction loss: 0.05910879746079445
Training batch 24 / 32
Total batch reconstruction loss: 0.060043446719646454
Training batch 25 / 32
Total batch reconstruction loss: 0.05537301301956177
Training batch 26 / 32
Total batch reconstruction loss: 0.06316842883825302
Training batch 27 / 32
Total batch reconstruction loss: 0.05697254091501236
Training batch 28 / 32
Total batch reconstruction loss: 0.05356740206480026
Training batch 29 / 32
Total batch reconstruction loss: 0.05822350084781647
Training batch 30 / 32
Total batch reconstruction loss: 0.05766473710536957
Training batch 31 / 32
Total batch reconstruction loss: 0.058033715933561325
Training batch 32 / 32
Total batch reconstruction loss: 0.059535905718803406
Epoch [413/500], Train Loss: 0.0563, Validation Loss: 0.0560, Generator Loss: 11.7291, Discriminator Loss: 0.3221
Training epoch 414 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.060125429183244705
Training batch 2 / 32
Total batch reconstruction loss: 0.05798869580030441
Training batch 3 / 32
Total batch reconstruction loss: 0.058364443480968475
Training batch 4 / 32
Total batch reconstruction loss: 0.06724816560745239
Training batch 5 / 32
Total batch reconstruction loss: 0.057759858667850494
Training batch 6 / 32
Total batch reconstruction loss: 0.051842592656612396
Training batch 7 / 32
Total batch reconstruction loss: 0.061189599335193634
Training batch 8 / 32
Total batch reconstruction loss: 0.05513511225581169
Training batch 9 / 32
Total batch reconstruction loss: 0.05726534128189087
Training batch 10 / 32
Total batch reconstruction loss: 0.058280665427446365
Training batch 11 / 32
Total batch reconstruction loss: 0.057338640093803406
Training batch 12 / 32
Total batch reconstruction loss: 0.06000899150967598
Training batch 13 / 32
Total batch reconstruction loss: 0.05812510848045349
Training batch 14 / 32
Total batch reconstruction loss: 0.05688033625483513
Training batch 15 / 32
Total batch reconstruction loss: 0.061423979699611664
Training batch 16 / 32
Total batch reconstruction loss: 0.06037817895412445
Training batch 17 / 32
Total batch reconstruction loss: 0.060158777981996536
Training batch 18 / 32
Total batch reconstruction loss: 0.05833163857460022
Training batch 19 / 32
Total batch reconstruction loss: 0.055084310472011566
Training batch 20 / 32
Total batch reconstruction loss: 0.05772747844457626
Training batch 21 / 32
Total batch reconstruction loss: 0.06444413959980011
Training batch 22 / 32
Total batch reconstruction loss: 0.05855119973421097
Training batch 23 / 32
Total batch reconstruction loss: 0.05559290200471878
Training batch 24 / 32
Total batch reconstruction loss: 0.06072704866528511
Training batch 25 / 32
Total batch reconstruction loss: 0.05929785221815109
Training batch 26 / 32
Total batch reconstruction loss: 0.05790172889828682
Training batch 27 / 32
Total batch reconstruction loss: 0.06069038808345795
Training batch 28 / 32
Total batch reconstruction loss: 0.058386702090501785
Training batch 29 / 32
Total batch reconstruction loss: 0.05790600925683975
Training batch 30 / 32
Total batch reconstruction loss: 0.06075940281152725
Training batch 31 / 32
Total batch reconstruction loss: 0.05706716328859329
Training batch 32 / 32
Total batch reconstruction loss: 0.056656889617443085
Epoch [414/500], Train Loss: 0.0569, Validation Loss: 0.0568, Generator Loss: 11.8123, Discriminator Loss: 0.3152
Training epoch 415 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.058831267058849335
Training batch 2 / 32
Total batch reconstruction loss: 0.05896887183189392
Training batch 3 / 32
Total batch reconstruction loss: 0.060175586491823196
Training batch 4 / 32
Total batch reconstruction loss: 0.054470084607601166
Training batch 5 / 32
Total batch reconstruction loss: 0.05803561210632324
Training batch 6 / 32
Total batch reconstruction loss: 0.05801374465227127
Training batch 7 / 32
Total batch reconstruction loss: 0.05816838517785072
Training batch 8 / 32
Total batch reconstruction loss: 0.05918389931321144
Training batch 9 / 32
Total batch reconstruction loss: 0.061270661652088165
Training batch 10 / 32
Total batch reconstruction loss: 0.055842865258455276
Training batch 11 / 32
Total batch reconstruction loss: 0.055066294968128204
Training batch 12 / 32
Total batch reconstruction loss: 0.06087210774421692
Training batch 13 / 32
Total batch reconstruction loss: 0.05418951436877251
Training batch 14 / 32
Total batch reconstruction loss: 0.05679585784673691
Training batch 15 / 32
Total batch reconstruction loss: 0.0589049756526947
Training batch 16 / 32
Total batch reconstruction loss: 0.05722920969128609
Training batch 17 / 32
Total batch reconstruction loss: 0.06017739325761795
Training batch 18 / 32
Total batch reconstruction loss: 0.05471302568912506
Training batch 19 / 32
Total batch reconstruction loss: 0.055567510426044464
Training batch 20 / 32
Total batch reconstruction loss: 0.057383984327316284
Training batch 21 / 32
Total batch reconstruction loss: 0.05591233819723129
Training batch 22 / 32
Total batch reconstruction loss: 0.05512436106801033
Training batch 23 / 32
Total batch reconstruction loss: 0.06337045878171921
Training batch 24 / 32
Total batch reconstruction loss: 0.061522603034973145
Training batch 25 / 32
Total batch reconstruction loss: 0.07233769446611404
Training batch 26 / 32
Total batch reconstruction loss: 0.06031660735607147
Training batch 27 / 32
Total batch reconstruction loss: 0.0569046288728714
Training batch 28 / 32
Total batch reconstruction loss: 0.0591740608215332
Training batch 29 / 32
Total batch reconstruction loss: 0.058611832559108734
Training batch 30 / 32
Total batch reconstruction loss: 0.05725768208503723
Training batch 31 / 32
Total batch reconstruction loss: 0.06042598932981491
Training batch 32 / 32
Total batch reconstruction loss: 0.06414328515529633
Epoch [415/500], Train Loss: 0.0567, Validation Loss: 0.0571, Generator Loss: 11.8143, Discriminator Loss: 0.3158
Training epoch 416 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.06272420287132263
Training batch 2 / 32
Total batch reconstruction loss: 0.058482639491558075
Training batch 3 / 32
Total batch reconstruction loss: 0.05927480757236481
Training batch 4 / 32
Total batch reconstruction loss: 0.06442396342754364
Training batch 5 / 32
Total batch reconstruction loss: 0.056659914553165436
Training batch 6 / 32
Total batch reconstruction loss: 0.0548928938806057
Training batch 7 / 32
Total batch reconstruction loss: 0.06027284264564514
Training batch 8 / 32
Total batch reconstruction loss: 0.05693177133798599
Training batch 9 / 32
Total batch reconstruction loss: 0.061526842415332794
Training batch 10 / 32
Total batch reconstruction loss: 0.0549197793006897
Training batch 11 / 32
Total batch reconstruction loss: 0.0548025518655777
Training batch 12 / 32
Total batch reconstruction loss: 0.0602249950170517
Training batch 13 / 32
Total batch reconstruction loss: 0.06196640431880951
Training batch 14 / 32
Total batch reconstruction loss: 0.054158613085746765
Training batch 15 / 32
Total batch reconstruction loss: 0.061656828969717026
Training batch 16 / 32
Total batch reconstruction loss: 0.05782713741064072
Training batch 17 / 32
Total batch reconstruction loss: 0.055009134113788605
Training batch 18 / 32
Total batch reconstruction loss: 0.058024536818265915
Training batch 19 / 32
Total batch reconstruction loss: 0.056756358593702316
Training batch 20 / 32
Total batch reconstruction loss: 0.05868052691221237
Training batch 21 / 32
Total batch reconstruction loss: 0.05515015497803688
Training batch 22 / 32
Total batch reconstruction loss: 0.05528324097394943
Training batch 23 / 32
Total batch reconstruction loss: 0.061598725616931915
Training batch 24 / 32
Total batch reconstruction loss: 0.058273643255233765
Training batch 25 / 32
Total batch reconstruction loss: 0.059823177754879
Training batch 26 / 32
Total batch reconstruction loss: 0.05435420200228691
Training batch 27 / 32
Total batch reconstruction loss: 0.053120531141757965
Training batch 28 / 32
Total batch reconstruction loss: 0.05695894733071327
Training batch 29 / 32
Total batch reconstruction loss: 0.06544923037290573
Training batch 30 / 32
Total batch reconstruction loss: 0.06165929138660431
Training batch 31 / 32
Total batch reconstruction loss: 0.05571836233139038
Training batch 32 / 32
Total batch reconstruction loss: 0.049780845642089844
Epoch [416/500], Train Loss: 0.0561, Validation Loss: 0.0574, Generator Loss: 11.6686, Discriminator Loss: 0.3206
Training epoch 417 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.05946432799100876
Training batch 2 / 32
Total batch reconstruction loss: 0.059573862701654434
Training batch 3 / 32
Total batch reconstruction loss: 0.06124288961291313
Training batch 4 / 32
Total batch reconstruction loss: 0.05780748277902603
Training batch 5 / 32
Total batch reconstruction loss: 0.05153569579124451
Training batch 6 / 32
Total batch reconstruction loss: 0.057956695556640625
Training batch 7 / 32
Total batch reconstruction loss: 0.05487709492444992
Training batch 8 / 32
Total batch reconstruction loss: 0.059403326362371445
Training batch 9 / 32
Total batch reconstruction loss: 0.05828879028558731
Training batch 10 / 32
Total batch reconstruction loss: 0.05689691752195358
Training batch 11 / 32
Total batch reconstruction loss: 0.057205814868211746
Training batch 12 / 32
Total batch reconstruction loss: 0.06328420341014862
Training batch 13 / 32
Total batch reconstruction loss: 0.05220463126897812
Training batch 14 / 32
Total batch reconstruction loss: 0.05916830524802208
Training batch 15 / 32
Total batch reconstruction loss: 0.06380455195903778
Training batch 16 / 32
Total batch reconstruction loss: 0.057281509041786194
Training batch 17 / 32
Total batch reconstruction loss: 0.05827049911022186
Training batch 18 / 32
Total batch reconstruction loss: 0.06147302687168121
Training batch 19 / 32
Total batch reconstruction loss: 0.058269038796424866
Training batch 20 / 32
Total batch reconstruction loss: 0.055452682077884674
Training batch 21 / 32
Total batch reconstruction loss: 0.05884629487991333
Training batch 22 / 32
Total batch reconstruction loss: 0.056021276861429214
Training batch 23 / 32
Total batch reconstruction loss: 0.05413195490837097
Training batch 24 / 32
Total batch reconstruction loss: 0.06665299087762833
Training batch 25 / 32
Total batch reconstruction loss: 0.053858909755945206
Training batch 26 / 32
Total batch reconstruction loss: 0.06014395132660866
Training batch 27 / 32
Total batch reconstruction loss: 0.056564733386039734
Training batch 28 / 32
Total batch reconstruction loss: 0.06422074139118195
Training batch 29 / 32
Total batch reconstruction loss: 0.05632290616631508
Training batch 30 / 32
Total batch reconstruction loss: 0.06084899604320526
Training batch 31 / 32
Total batch reconstruction loss: 0.058142393827438354
Training batch 32 / 32
Total batch reconstruction loss: 0.0766410380601883
Epoch [417/500], Train Loss: 0.0568, Validation Loss: 0.0573, Generator Loss: 11.8598, Discriminator Loss: 0.3049
Training epoch 418 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.059111665934324265
Training batch 2 / 32
Total batch reconstruction loss: 0.06175766885280609
Training batch 3 / 32
Total batch reconstruction loss: 0.06340047717094421
Training batch 4 / 32
Total batch reconstruction loss: 0.05892375856637955
Training batch 5 / 32
Total batch reconstruction loss: 0.0601239874958992
Training batch 6 / 32
Total batch reconstruction loss: 0.059361718595027924
Training batch 7 / 32
Total batch reconstruction loss: 0.057708047330379486
Training batch 8 / 32
Total batch reconstruction loss: 0.054275475442409515
Training batch 9 / 32
Total batch reconstruction loss: 0.05887089669704437
Training batch 10 / 32
Total batch reconstruction loss: 0.057542555034160614
Training batch 11 / 32
Total batch reconstruction loss: 0.06468360126018524
Training batch 12 / 32
Total batch reconstruction loss: 0.053020767867565155
Training batch 13 / 32
Total batch reconstruction loss: 0.05849231034517288
Training batch 14 / 32
Total batch reconstruction loss: 0.057031527161598206
Training batch 15 / 32
Total batch reconstruction loss: 0.05523498356342316
Training batch 16 / 32
Total batch reconstruction loss: 0.059835683554410934
Training batch 17 / 32
Total batch reconstruction loss: 0.06037747859954834
Training batch 18 / 32
Total batch reconstruction loss: 0.060842130333185196
Training batch 19 / 32
Total batch reconstruction loss: 0.06176863610744476
Training batch 20 / 32
Total batch reconstruction loss: 0.05353495851159096
Training batch 21 / 32
Total batch reconstruction loss: 0.05895106494426727
Training batch 22 / 32
Total batch reconstruction loss: 0.059075914323329926
Training batch 23 / 32
Total batch reconstruction loss: 0.05943655967712402
Training batch 24 / 32
Total batch reconstruction loss: 0.058193761855363846
Training batch 25 / 32
Total batch reconstruction loss: 0.0575505830347538
Training batch 26 / 32
Total batch reconstruction loss: 0.05740877613425255
Training batch 27 / 32
Total batch reconstruction loss: 0.05485745146870613
Training batch 28 / 32
Total batch reconstruction loss: 0.060643576085567474
Training batch 29 / 32
Total batch reconstruction loss: 0.055366672575473785
Training batch 30 / 32
Total batch reconstruction loss: 0.06397076696157455
Training batch 31 / 32
Total batch reconstruction loss: 0.05511898547410965
Training batch 32 / 32
Total batch reconstruction loss: 0.04819295555353165
Epoch [418/500], Train Loss: 0.0564, Validation Loss: 0.0569, Generator Loss: 11.7192, Discriminator Loss: 0.3192
Training epoch 419 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.05811076611280441
Training batch 2 / 32
Total batch reconstruction loss: 0.05870538204908371
Training batch 3 / 32
Total batch reconstruction loss: 0.05707194656133652
Training batch 4 / 32
Total batch reconstruction loss: 0.05834230035543442
Training batch 5 / 32
Total batch reconstruction loss: 0.05768962204456329
Training batch 6 / 32
Total batch reconstruction loss: 0.05516662448644638
Training batch 7 / 32
Total batch reconstruction loss: 0.05800771713256836
Training batch 8 / 32
Total batch reconstruction loss: 0.056329626590013504
Training batch 9 / 32
Total batch reconstruction loss: 0.05823391675949097
Training batch 10 / 32
Total batch reconstruction loss: 0.059741221368312836
Training batch 11 / 32
Total batch reconstruction loss: 0.06351403146982193
Training batch 12 / 32
Total batch reconstruction loss: 0.06121016666293144
Training batch 13 / 32
Total batch reconstruction loss: 0.059630125761032104
Training batch 14 / 32
Total batch reconstruction loss: 0.06216088682413101
Training batch 15 / 32
Total batch reconstruction loss: 0.05630132555961609
Training batch 16 / 32
Total batch reconstruction loss: 0.056201860308647156
Training batch 17 / 32
Total batch reconstruction loss: 0.06359150260686874
Training batch 18 / 32
Total batch reconstruction loss: 0.0577629879117012
Training batch 19 / 32
Total batch reconstruction loss: 0.0594775453209877
Training batch 20 / 32
Total batch reconstruction loss: 0.059420906007289886
Training batch 21 / 32
Total batch reconstruction loss: 0.05604330822825432
Training batch 22 / 32
Total batch reconstruction loss: 0.057203806936740875
Training batch 23 / 32
Total batch reconstruction loss: 0.054573558270931244
Training batch 24 / 32
Total batch reconstruction loss: 0.05399573966860771
Training batch 25 / 32
Total batch reconstruction loss: 0.06158147752285004
Training batch 26 / 32
Total batch reconstruction loss: 0.05624176561832428
Training batch 27 / 32
Total batch reconstruction loss: 0.05787765234708786
Training batch 28 / 32
Total batch reconstruction loss: 0.06144026294350624
Training batch 29 / 32
Total batch reconstruction loss: 0.06387791037559509
Training batch 30 / 32
Total batch reconstruction loss: 0.056936755776405334
Training batch 31 / 32
Total batch reconstruction loss: 0.05674491077661514
Training batch 32 / 32
Total batch reconstruction loss: 0.05174323543906212
Epoch [419/500], Train Loss: 0.0563, Validation Loss: 0.0587, Generator Loss: 11.7223, Discriminator Loss: 0.3179
Training epoch 420 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.05775792896747589
Training batch 2 / 32
Total batch reconstruction loss: 0.05637871474027634
Training batch 3 / 32
Total batch reconstruction loss: 0.05469128116965294
Training batch 4 / 32
Total batch reconstruction loss: 0.058529313653707504
Training batch 5 / 32
Total batch reconstruction loss: 0.06359982490539551
Training batch 6 / 32
Total batch reconstruction loss: 0.057593025267124176
Training batch 7 / 32
Total batch reconstruction loss: 0.056220490485429764
Training batch 8 / 32
Total batch reconstruction loss: 0.06129689887166023
Training batch 9 / 32
Total batch reconstruction loss: 0.05474841594696045
Training batch 10 / 32
Total batch reconstruction loss: 0.06421832740306854
Training batch 11 / 32
Total batch reconstruction loss: 0.058010127395391464
Training batch 12 / 32
Total batch reconstruction loss: 0.05833004415035248
Training batch 13 / 32
Total batch reconstruction loss: 0.061324894428253174
Training batch 14 / 32
Total batch reconstruction loss: 0.06192798167467117
Training batch 15 / 32
Total batch reconstruction loss: 0.05754627287387848
Training batch 16 / 32
Total batch reconstruction loss: 0.05956397205591202
Training batch 17 / 32
Total batch reconstruction loss: 0.05481715872883797
Training batch 18 / 32
Total batch reconstruction loss: 0.05561554804444313
Training batch 19 / 32
Total batch reconstruction loss: 0.061254341155290604
Training batch 20 / 32
Total batch reconstruction loss: 0.055289968848228455
Training batch 21 / 32
Total batch reconstruction loss: 0.05861733853816986
Training batch 22 / 32
Total batch reconstruction loss: 0.05942294001579285
Training batch 23 / 32
Total batch reconstruction loss: 0.06642947345972061
Training batch 24 / 32
Total batch reconstruction loss: 0.05536942183971405
Training batch 25 / 32
Total batch reconstruction loss: 0.05927058309316635
Training batch 26 / 32
Total batch reconstruction loss: 0.056840620934963226
Training batch 27 / 32
Total batch reconstruction loss: 0.05758967623114586
Training batch 28 / 32
Total batch reconstruction loss: 0.05510006472468376
Training batch 29 / 32
Total batch reconstruction loss: 0.05656260624527931
Training batch 30 / 32
Total batch reconstruction loss: 0.06070279702544212
Training batch 31 / 32
Total batch reconstruction loss: 0.05747630447149277
Training batch 32 / 32
Total batch reconstruction loss: 0.06309536844491959
Epoch [420/500], Train Loss: 0.0566, Validation Loss: 0.0602, Generator Loss: 11.7888, Discriminator Loss: 0.3222
Training epoch 421 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.058847248554229736
Training batch 2 / 32
Total batch reconstruction loss: 0.05920597165822983
Training batch 3 / 32
Total batch reconstruction loss: 0.058258056640625
Training batch 4 / 32
Total batch reconstruction loss: 0.05602393299341202
Training batch 5 / 32
Total batch reconstruction loss: 0.061960265040397644
Training batch 6 / 32
Total batch reconstruction loss: 0.05875241383910179
Training batch 7 / 32
Total batch reconstruction loss: 0.059144552797079086
Training batch 8 / 32
Total batch reconstruction loss: 0.05734248459339142
Training batch 9 / 32
Total batch reconstruction loss: 0.057327400892972946
Training batch 10 / 32
Total batch reconstruction loss: 0.06108498573303223
Training batch 11 / 32
Total batch reconstruction loss: 0.05716340243816376
Training batch 12 / 32
Total batch reconstruction loss: 0.06149823218584061
Training batch 13 / 32
Total batch reconstruction loss: 0.058819424360990524
Training batch 14 / 32
Total batch reconstruction loss: 0.05741184577345848
Training batch 15 / 32
Total batch reconstruction loss: 0.057618435472249985
Training batch 16 / 32
Total batch reconstruction loss: 0.05823651701211929
Training batch 17 / 32
Total batch reconstruction loss: 0.06102755665779114
Training batch 18 / 32
Total batch reconstruction loss: 0.05921521037817001
Training batch 19 / 32
Total batch reconstruction loss: 0.056402988731861115
Training batch 20 / 32
Total batch reconstruction loss: 0.058212194591760635
Training batch 21 / 32
Total batch reconstruction loss: 0.061096496880054474
Training batch 22 / 32
Total batch reconstruction loss: 0.05524688586592674
Training batch 23 / 32
Total batch reconstruction loss: 0.061087630689144135
Training batch 24 / 32
Total batch reconstruction loss: 0.05834587663412094
Training batch 25 / 32
Total batch reconstruction loss: 0.052409492433071136
Training batch 26 / 32
Total batch reconstruction loss: 0.05865076929330826
Training batch 27 / 32
Total batch reconstruction loss: 0.057386986911296844
Training batch 28 / 32
Total batch reconstruction loss: 0.05717136710882187
Training batch 29 / 32
Total batch reconstruction loss: 0.05927036702632904
Training batch 30 / 32
Total batch reconstruction loss: 0.05815886706113815
Training batch 31 / 32
Total batch reconstruction loss: 0.06409865617752075
Training batch 32 / 32
Total batch reconstruction loss: 0.06033208221197128
Epoch [421/500], Train Loss: 0.0567, Validation Loss: 0.0563, Generator Loss: 11.7987, Discriminator Loss: 0.3105
Training epoch 422 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.05869879573583603
Training batch 2 / 32
Total batch reconstruction loss: 0.05927717685699463
Training batch 3 / 32
Total batch reconstruction loss: 0.057849347591400146
Training batch 4 / 32
Total batch reconstruction loss: 0.05559879541397095
Training batch 5 / 32
Total batch reconstruction loss: 0.05520518869161606
Training batch 6 / 32
Total batch reconstruction loss: 0.05669720470905304
Training batch 7 / 32
Total batch reconstruction loss: 0.06019970774650574
Training batch 8 / 32
Total batch reconstruction loss: 0.05879738926887512
Training batch 9 / 32
Total batch reconstruction loss: 0.05550500750541687
Training batch 10 / 32
Total batch reconstruction loss: 0.05807049572467804
Training batch 11 / 32
Total batch reconstruction loss: 0.06151866167783737
Training batch 12 / 32
Total batch reconstruction loss: 0.06519587337970734
Training batch 13 / 32
Total batch reconstruction loss: 0.05564742907881737
Training batch 14 / 32
Total batch reconstruction loss: 0.0594877228140831
Training batch 15 / 32
Total batch reconstruction loss: 0.06051187589764595
Training batch 16 / 32
Total batch reconstruction loss: 0.053468137979507446
Training batch 17 / 32
Total batch reconstruction loss: 0.05832870304584503
Training batch 18 / 32
Total batch reconstruction loss: 0.061523180454969406
Training batch 19 / 32
Total batch reconstruction loss: 0.060891829431056976
Training batch 20 / 32
Total batch reconstruction loss: 0.056700266897678375
Training batch 21 / 32
Total batch reconstruction loss: 0.05709490925073624
Training batch 22 / 32
Total batch reconstruction loss: 0.05739656090736389
Training batch 23 / 32
Total batch reconstruction loss: 0.06148599088191986
Training batch 24 / 32
Total batch reconstruction loss: 0.05970805138349533
Training batch 25 / 32
Total batch reconstruction loss: 0.05796783044934273
Training batch 26 / 32
Total batch reconstruction loss: 0.058736689388751984
Training batch 27 / 32
Total batch reconstruction loss: 0.05862150341272354
Training batch 28 / 32
Total batch reconstruction loss: 0.05941339582204819
Training batch 29 / 32
Total batch reconstruction loss: 0.06104305386543274
Training batch 30 / 32
Total batch reconstruction loss: 0.05677718296647072
Training batch 31 / 32
Total batch reconstruction loss: 0.06061802804470062
Training batch 32 / 32
Total batch reconstruction loss: 0.0679142102599144
Epoch [422/500], Train Loss: 0.0573, Validation Loss: 0.0572, Generator Loss: 11.8502, Discriminator Loss: 0.3251
Training epoch 423 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.05644627660512924
Training batch 2 / 32
Total batch reconstruction loss: 0.06267547607421875
Training batch 3 / 32
Total batch reconstruction loss: 0.05766003578901291
Training batch 4 / 32
Total batch reconstruction loss: 0.05664857104420662
Training batch 5 / 32
Total batch reconstruction loss: 0.05971914529800415
Training batch 6 / 32
Total batch reconstruction loss: 0.0609133280813694
Training batch 7 / 32
Total batch reconstruction loss: 0.055945925414562225
Training batch 8 / 32
Total batch reconstruction loss: 0.059953831136226654
Training batch 9 / 32
Total batch reconstruction loss: 0.05603864789009094
Training batch 10 / 32
Total batch reconstruction loss: 0.05695430934429169
Training batch 11 / 32
Total batch reconstruction loss: 0.057407915592193604
Training batch 12 / 32
Total batch reconstruction loss: 0.05699385330080986
Training batch 13 / 32
Total batch reconstruction loss: 0.059544771909713745
Training batch 14 / 32
Total batch reconstruction loss: 0.05462481826543808
Training batch 15 / 32
Total batch reconstruction loss: 0.057274654507637024
Training batch 16 / 32
Total batch reconstruction loss: 0.06125708669424057
Training batch 17 / 32
Total batch reconstruction loss: 0.0601055733859539
Training batch 18 / 32
Total batch reconstruction loss: 0.058355025947093964
Training batch 19 / 32
Total batch reconstruction loss: 0.056874826550483704
Training batch 20 / 32
Total batch reconstruction loss: 0.057474102824926376
Training batch 21 / 32
Total batch reconstruction loss: 0.05705980956554413
Training batch 22 / 32
Total batch reconstruction loss: 0.057741791009902954
Training batch 23 / 32
Total batch reconstruction loss: 0.05676020309329033
Training batch 24 / 32
Total batch reconstruction loss: 0.06337649375200272
Training batch 25 / 32
Total batch reconstruction loss: 0.0557534322142601
Training batch 26 / 32
Total batch reconstruction loss: 0.06267987191677094
Training batch 27 / 32
Total batch reconstruction loss: 0.06358526647090912
Training batch 28 / 32
Total batch reconstruction loss: 0.054932691156864166
Training batch 29 / 32
Total batch reconstruction loss: 0.06053769588470459
Training batch 30 / 32
Total batch reconstruction loss: 0.053822800517082214
Training batch 31 / 32
Total batch reconstruction loss: 0.05755503475666046
Training batch 32 / 32
Total batch reconstruction loss: 0.05657781660556793
Epoch [423/500], Train Loss: 0.0565, Validation Loss: 0.0570, Generator Loss: 11.7137, Discriminator Loss: 0.3131
Training epoch 424 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.05944235995411873
Training batch 2 / 32
Total batch reconstruction loss: 0.06869965046644211
Training batch 3 / 32
Total batch reconstruction loss: 0.05697692930698395
Training batch 4 / 32
Total batch reconstruction loss: 0.06747443974018097
Training batch 5 / 32
Total batch reconstruction loss: 0.05911720544099808
Training batch 6 / 32
Total batch reconstruction loss: 0.056523971259593964
Training batch 7 / 32
Total batch reconstruction loss: 0.05661899596452713
Training batch 8 / 32
Total batch reconstruction loss: 0.05742580071091652
Training batch 9 / 32
Total batch reconstruction loss: 0.05563663691282272
Training batch 10 / 32
Total batch reconstruction loss: 0.058039892464876175
Training batch 11 / 32
Total batch reconstruction loss: 0.058899395167827606
Training batch 12 / 32
Total batch reconstruction loss: 0.05934424698352814
Training batch 13 / 32
Total batch reconstruction loss: 0.05824870243668556
Training batch 14 / 32
Total batch reconstruction loss: 0.06016283109784126
Training batch 15 / 32
Total batch reconstruction loss: 0.0583931989967823
Training batch 16 / 32
Total batch reconstruction loss: 0.05768220126628876
Training batch 17 / 32
Total batch reconstruction loss: 0.056832205504179
Training batch 18 / 32
Total batch reconstruction loss: 0.06564342975616455
Training batch 19 / 32
Total batch reconstruction loss: 0.05616176128387451
Training batch 20 / 32
Total batch reconstruction loss: 0.05659862607717514
Training batch 21 / 32
Total batch reconstruction loss: 0.05731646716594696
Training batch 22 / 32
Total batch reconstruction loss: 0.057141587138175964
Training batch 23 / 32
Total batch reconstruction loss: 0.056774526834487915
Training batch 24 / 32
Total batch reconstruction loss: 0.05803582817316055
Training batch 25 / 32
Total batch reconstruction loss: 0.05466772988438606
Training batch 26 / 32
Total batch reconstruction loss: 0.05698520690202713
Training batch 27 / 32
Total batch reconstruction loss: 0.05840142443776131
Training batch 28 / 32
Total batch reconstruction loss: 0.058505941182374954
Training batch 29 / 32
Total batch reconstruction loss: 0.05876448005437851
Training batch 30 / 32
Total batch reconstruction loss: 0.059281013906002045
Training batch 31 / 32
Total batch reconstruction loss: 0.055321503430604935
Training batch 32 / 32
Total batch reconstruction loss: 0.0473284013569355
Epoch [424/500], Train Loss: 0.0562, Validation Loss: 0.0572, Generator Loss: 11.7120, Discriminator Loss: 0.3133
Training epoch 425 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.057325854897499084
Training batch 2 / 32
Total batch reconstruction loss: 0.05735606700181961
Training batch 3 / 32
Total batch reconstruction loss: 0.056851956993341446
Training batch 4 / 32
Total batch reconstruction loss: 0.05709332600235939
Training batch 5 / 32
Total batch reconstruction loss: 0.05616507679224014
Training batch 6 / 32
Total batch reconstruction loss: 0.06292599439620972
Training batch 7 / 32
Total batch reconstruction loss: 0.058315642178058624
Training batch 8 / 32
Total batch reconstruction loss: 0.05221521854400635
Training batch 9 / 32
Total batch reconstruction loss: 0.060015738010406494
Training batch 10 / 32
Total batch reconstruction loss: 0.061545222997665405
Training batch 11 / 32
Total batch reconstruction loss: 0.05548705905675888
Training batch 12 / 32
Total batch reconstruction loss: 0.05377746373414993
Training batch 13 / 32
Total batch reconstruction loss: 0.05699066445231438
Training batch 14 / 32
Total batch reconstruction loss: 0.05915737897157669
Training batch 15 / 32
Total batch reconstruction loss: 0.06053011864423752
Training batch 16 / 32
Total batch reconstruction loss: 0.06089599430561066
Training batch 17 / 32
Total batch reconstruction loss: 0.05565621703863144
Training batch 18 / 32
Total batch reconstruction loss: 0.05747135356068611
Training batch 19 / 32
Total batch reconstruction loss: 0.06030386686325073
Training batch 20 / 32
Total batch reconstruction loss: 0.05934913828969002
Training batch 21 / 32
Total batch reconstruction loss: 0.06007048487663269
Training batch 22 / 32
Total batch reconstruction loss: 0.06446319818496704
Training batch 23 / 32
Total batch reconstruction loss: 0.05421213433146477
Training batch 24 / 32
Total batch reconstruction loss: 0.05642325431108475
Training batch 25 / 32
Total batch reconstruction loss: 0.059271346777677536
Training batch 26 / 32
Total batch reconstruction loss: 0.05758831650018692
Training batch 27 / 32
Total batch reconstruction loss: 0.058235980570316315
Training batch 28 / 32
Total batch reconstruction loss: 0.06176181882619858
Training batch 29 / 32
Total batch reconstruction loss: 0.05797223001718521
Training batch 30 / 32
Total batch reconstruction loss: 0.06056755408644676
Training batch 31 / 32
Total batch reconstruction loss: 0.06343275308609009
Training batch 32 / 32
Total batch reconstruction loss: 0.05371833220124245
Epoch [425/500], Train Loss: 0.0566, Validation Loss: 0.0566, Generator Loss: 11.7415, Discriminator Loss: 0.3097
Training epoch 426 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.054666172713041306
Training batch 2 / 32
Total batch reconstruction loss: 0.05681265890598297
Training batch 3 / 32
Total batch reconstruction loss: 0.05597253143787384
Training batch 4 / 32
Total batch reconstruction loss: 0.06252242624759674
Training batch 5 / 32
Total batch reconstruction loss: 0.05552256107330322
Training batch 6 / 32
Total batch reconstruction loss: 0.05464239791035652
Training batch 7 / 32
Total batch reconstruction loss: 0.059539660811424255
Training batch 8 / 32
Total batch reconstruction loss: 0.059877704828977585
Training batch 9 / 32
Total batch reconstruction loss: 0.060097094625234604
Training batch 10 / 32
Total batch reconstruction loss: 0.056757424026727676
Training batch 11 / 32
Total batch reconstruction loss: 0.05595151335000992
Training batch 12 / 32
Total batch reconstruction loss: 0.058534637093544006
Training batch 13 / 32
Total batch reconstruction loss: 0.057347338646650314
Training batch 14 / 32
Total batch reconstruction loss: 0.06500894576311111
Training batch 15 / 32
Total batch reconstruction loss: 0.06301235407590866
Training batch 16 / 32
Total batch reconstruction loss: 0.058955542743206024
Training batch 17 / 32
Total batch reconstruction loss: 0.057445794343948364
Training batch 18 / 32
Total batch reconstruction loss: 0.06046319752931595
Training batch 19 / 32
Total batch reconstruction loss: 0.05626196414232254
Training batch 20 / 32
Total batch reconstruction loss: 0.060256801545619965
Training batch 21 / 32
Total batch reconstruction loss: 0.05777902901172638
Training batch 22 / 32
Total batch reconstruction loss: 0.05846152454614639
Training batch 23 / 32
Total batch reconstruction loss: 0.05690218508243561
Training batch 24 / 32
Total batch reconstruction loss: 0.05597364157438278
Training batch 25 / 32
Total batch reconstruction loss: 0.05591754987835884
Training batch 26 / 32
Total batch reconstruction loss: 0.056157246232032776
Training batch 27 / 32
Total batch reconstruction loss: 0.057129859924316406
Training batch 28 / 32
Total batch reconstruction loss: 0.06225436180830002
Training batch 29 / 32
Total batch reconstruction loss: 0.06047787517309189
Training batch 30 / 32
Total batch reconstruction loss: 0.05657574534416199
Training batch 31 / 32
Total batch reconstruction loss: 0.06229056045413017
Training batch 32 / 32
Total batch reconstruction loss: 0.11566460132598877
Epoch [426/500], Train Loss: 0.0580, Validation Loss: 0.0564, Generator Loss: 12.0963, Discriminator Loss: 0.3233
Training epoch 427 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.05645929276943207
Training batch 2 / 32
Total batch reconstruction loss: 0.05824686214327812
Training batch 3 / 32
Total batch reconstruction loss: 0.06070049852132797
Training batch 4 / 32
Total batch reconstruction loss: 0.06647433340549469
Training batch 5 / 32
Total batch reconstruction loss: 0.060822002589702606
Training batch 6 / 32
Total batch reconstruction loss: 0.05974210798740387
Training batch 7 / 32
Total batch reconstruction loss: 0.056663546711206436
Training batch 8 / 32
Total batch reconstruction loss: 0.05868655443191528
Training batch 9 / 32
Total batch reconstruction loss: 0.06285488605499268
Training batch 10 / 32
Total batch reconstruction loss: 0.06056607514619827
Training batch 11 / 32
Total batch reconstruction loss: 0.06309590488672256
Training batch 12 / 32
Total batch reconstruction loss: 0.06893281638622284
Training batch 13 / 32
Total batch reconstruction loss: 0.05692611634731293
Training batch 14 / 32
Total batch reconstruction loss: 0.06110982596874237
Training batch 15 / 32
Total batch reconstruction loss: 0.05927230417728424
Training batch 16 / 32
Total batch reconstruction loss: 0.05997149273753166
Training batch 17 / 32
Total batch reconstruction loss: 0.05996217206120491
Training batch 18 / 32
Total batch reconstruction loss: 0.058520711958408356
Training batch 19 / 32
Total batch reconstruction loss: 0.05708294361829758
Training batch 20 / 32
Total batch reconstruction loss: 0.05920817703008652
Training batch 21 / 32
Total batch reconstruction loss: 0.056562647223472595
Training batch 22 / 32
Total batch reconstruction loss: 0.06043286994099617
Training batch 23 / 32
Total batch reconstruction loss: 0.06271135061979294
Training batch 24 / 32
Total batch reconstruction loss: 0.05708892643451691
Training batch 25 / 32
Total batch reconstruction loss: 0.05580585449934006
Training batch 26 / 32
Total batch reconstruction loss: 0.06055411696434021
Training batch 27 / 32
Total batch reconstruction loss: 0.06026599556207657
Training batch 28 / 32
Total batch reconstruction loss: 0.05866885930299759
Training batch 29 / 32
Total batch reconstruction loss: 0.05969100445508957
Training batch 30 / 32
Total batch reconstruction loss: 0.05688340961933136
Training batch 31 / 32
Total batch reconstruction loss: 0.06075869873166084
Training batch 32 / 32
Total batch reconstruction loss: 0.06607268750667572
Epoch [427/500], Train Loss: 0.0577, Validation Loss: 0.0576, Generator Loss: 12.0737, Discriminator Loss: 0.3193
Training epoch 428 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.0653204694390297
Training batch 2 / 32
Total batch reconstruction loss: 0.05989416688680649
Training batch 3 / 32
Total batch reconstruction loss: 0.056139323860406876
Training batch 4 / 32
Total batch reconstruction loss: 0.05381505936384201
Training batch 5 / 32
Total batch reconstruction loss: 0.06091240793466568
Training batch 6 / 32
Total batch reconstruction loss: 0.06221165135502815
Training batch 7 / 32
Total batch reconstruction loss: 0.05852063000202179
Training batch 8 / 32
Total batch reconstruction loss: 0.061049699783325195
Training batch 9 / 32
Total batch reconstruction loss: 0.05755453184247017
Training batch 10 / 32
Total batch reconstruction loss: 0.061568424105644226
Training batch 11 / 32
Total batch reconstruction loss: 0.05861558020114899
Training batch 12 / 32
Total batch reconstruction loss: 0.058871518820524216
Training batch 13 / 32
Total batch reconstruction loss: 0.055780645459890366
Training batch 14 / 32
Total batch reconstruction loss: 0.05286812037229538
Training batch 15 / 32
Total batch reconstruction loss: 0.05882798880338669
Training batch 16 / 32
Total batch reconstruction loss: 0.06114800274372101
Training batch 17 / 32
Total batch reconstruction loss: 0.060357652604579926
Training batch 18 / 32
Total batch reconstruction loss: 0.05673598125576973
Training batch 19 / 32
Total batch reconstruction loss: 0.06074395030736923
Training batch 20 / 32
Total batch reconstruction loss: 0.06372368335723877
Training batch 21 / 32
Total batch reconstruction loss: 0.055638983845710754
Training batch 22 / 32
Total batch reconstruction loss: 0.055508024990558624
Training batch 23 / 32
Total batch reconstruction loss: 0.05901174992322922
Training batch 24 / 32
Total batch reconstruction loss: 0.057897359132766724
Training batch 25 / 32
Total batch reconstruction loss: 0.05779337137937546
Training batch 26 / 32
Total batch reconstruction loss: 0.055684059858322144
Training batch 27 / 32
Total batch reconstruction loss: 0.06058765575289726
Training batch 28 / 32
Total batch reconstruction loss: 0.057558685541152954
Training batch 29 / 32
Total batch reconstruction loss: 0.06039692088961601
Training batch 30 / 32
Total batch reconstruction loss: 0.05877472460269928
Training batch 31 / 32
Total batch reconstruction loss: 0.05975496023893356
Training batch 32 / 32
Total batch reconstruction loss: 0.058203473687171936
Epoch [428/500], Train Loss: 0.0570, Validation Loss: 0.0574, Generator Loss: 11.8213, Discriminator Loss: 0.3277
Training epoch 429 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.052906133234500885
Training batch 2 / 32
Total batch reconstruction loss: 0.05859728157520294
Training batch 3 / 32
Total batch reconstruction loss: 0.06060335785150528
Training batch 4 / 32
Total batch reconstruction loss: 0.06055999547243118
Training batch 5 / 32
Total batch reconstruction loss: 0.05666882172226906
Training batch 6 / 32
Total batch reconstruction loss: 0.055693089962005615
Training batch 7 / 32
Total batch reconstruction loss: 0.05615459382534027
Training batch 8 / 32
Total batch reconstruction loss: 0.05759590119123459
Training batch 9 / 32
Total batch reconstruction loss: 0.05892510339617729
Training batch 10 / 32
Total batch reconstruction loss: 0.05833013355731964
Training batch 11 / 32
Total batch reconstruction loss: 0.05524836480617523
Training batch 12 / 32
Total batch reconstruction loss: 0.056464117020368576
Training batch 13 / 32
Total batch reconstruction loss: 0.05990871414542198
Training batch 14 / 32
Total batch reconstruction loss: 0.06364399194717407
Training batch 15 / 32
Total batch reconstruction loss: 0.05560582876205444
Training batch 16 / 32
Total batch reconstruction loss: 0.05691246688365936
Training batch 17 / 32
Total batch reconstruction loss: 0.05706134811043739
Training batch 18 / 32
Total batch reconstruction loss: 0.05986668914556503
Training batch 19 / 32
Total batch reconstruction loss: 0.05524802207946777
Training batch 20 / 32
Total batch reconstruction loss: 0.05737926810979843
Training batch 21 / 32
Total batch reconstruction loss: 0.05868475139141083
Training batch 22 / 32
Total batch reconstruction loss: 0.06002259999513626
Training batch 23 / 32
Total batch reconstruction loss: 0.06237611174583435
Training batch 24 / 32
Total batch reconstruction loss: 0.061857856810092926
Training batch 25 / 32
Total batch reconstruction loss: 0.06013482064008713
Training batch 26 / 32
Total batch reconstruction loss: 0.06103726103901863
Training batch 27 / 32
Total batch reconstruction loss: 0.06301632523536682
Training batch 28 / 32
Total batch reconstruction loss: 0.05704139173030853
Training batch 29 / 32
Total batch reconstruction loss: 0.05676029622554779
Training batch 30 / 32
Total batch reconstruction loss: 0.0584731325507164
Training batch 31 / 32
Total batch reconstruction loss: 0.06053406000137329
Training batch 32 / 32
Total batch reconstruction loss: 0.05810295417904854
Epoch [429/500], Train Loss: 0.0563, Validation Loss: 0.0570, Generator Loss: 11.7682, Discriminator Loss: 0.3103
Training epoch 430 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.060936905443668365
Training batch 2 / 32
Total batch reconstruction loss: 0.05689559131860733
Training batch 3 / 32
Total batch reconstruction loss: 0.058901119977235794
Training batch 4 / 32
Total batch reconstruction loss: 0.0633002370595932
Training batch 5 / 32
Total batch reconstruction loss: 0.06167716532945633
Training batch 6 / 32
Total batch reconstruction loss: 0.05685385316610336
Training batch 7 / 32
Total batch reconstruction loss: 0.05704539641737938
Training batch 8 / 32
Total batch reconstruction loss: 0.05945974588394165
Training batch 9 / 32
Total batch reconstruction loss: 0.05662732198834419
Training batch 10 / 32
Total batch reconstruction loss: 0.05703844875097275
Training batch 11 / 32
Total batch reconstruction loss: 0.06300193816423416
Training batch 12 / 32
Total batch reconstruction loss: 0.058774448931217194
Training batch 13 / 32
Total batch reconstruction loss: 0.055140309035778046
Training batch 14 / 32
Total batch reconstruction loss: 0.05818646028637886
Training batch 15 / 32
Total batch reconstruction loss: 0.05860816687345505
Training batch 16 / 32
Total batch reconstruction loss: 0.05548818036913872
Training batch 17 / 32
Total batch reconstruction loss: 0.056640613824129105
Training batch 18 / 32
Total batch reconstruction loss: 0.05938588082790375
Training batch 19 / 32
Total batch reconstruction loss: 0.06126228719949722
Training batch 20 / 32
Total batch reconstruction loss: 0.05574262887239456
Training batch 21 / 32
Total batch reconstruction loss: 0.06098398566246033
Training batch 22 / 32
Total batch reconstruction loss: 0.06026478856801987
Training batch 23 / 32
Total batch reconstruction loss: 0.05953523889183998
Training batch 24 / 32
Total batch reconstruction loss: 0.05678778886795044
Training batch 25 / 32
Total batch reconstruction loss: 0.055155444890260696
Training batch 26 / 32
Total batch reconstruction loss: 0.05541197955608368
Training batch 27 / 32
Total batch reconstruction loss: 0.05932231247425079
Training batch 28 / 32
Total batch reconstruction loss: 0.05616375058889389
Training batch 29 / 32
Total batch reconstruction loss: 0.06058131903409958
Training batch 30 / 32
Total batch reconstruction loss: 0.06135709583759308
Training batch 31 / 32
Total batch reconstruction loss: 0.06109239161014557
Training batch 32 / 32
Total batch reconstruction loss: 0.049961384385824203
Epoch [430/500], Train Loss: 0.0567, Validation Loss: 0.0576, Generator Loss: 11.7362, Discriminator Loss: 0.3197
Training epoch 431 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.058405324816703796
Training batch 2 / 32
Total batch reconstruction loss: 0.05288203805685043
Training batch 3 / 32
Total batch reconstruction loss: 0.05711875483393669
Training batch 4 / 32
Total batch reconstruction loss: 0.06146958842873573
Training batch 5 / 32
Total batch reconstruction loss: 0.055056922137737274
Training batch 6 / 32
Total batch reconstruction loss: 0.06009541451931
Training batch 7 / 32
Total batch reconstruction loss: 0.05499931424856186
Training batch 8 / 32
Total batch reconstruction loss: 0.05891937017440796
Training batch 9 / 32
Total batch reconstruction loss: 0.05674979090690613
Training batch 10 / 32
Total batch reconstruction loss: 0.05374252051115036
Training batch 11 / 32
Total batch reconstruction loss: 0.06267008930444717
Training batch 12 / 32
Total batch reconstruction loss: 0.05612506344914436
Training batch 13 / 32
Total batch reconstruction loss: 0.060890451073646545
Training batch 14 / 32
Total batch reconstruction loss: 0.06018462032079697
Training batch 15 / 32
Total batch reconstruction loss: 0.05763363838195801
Training batch 16 / 32
Total batch reconstruction loss: 0.06232135370373726
Training batch 17 / 32
Total batch reconstruction loss: 0.05938400328159332
Training batch 18 / 32
Total batch reconstruction loss: 0.05699397623538971
Training batch 19 / 32
Total batch reconstruction loss: 0.060556262731552124
Training batch 20 / 32
Total batch reconstruction loss: 0.058624785393476486
Training batch 21 / 32
Total batch reconstruction loss: 0.0647197812795639
Training batch 22 / 32
Total batch reconstruction loss: 0.05724234879016876
Training batch 23 / 32
Total batch reconstruction loss: 0.05821877717971802
Training batch 24 / 32
Total batch reconstruction loss: 0.05784669890999794
Training batch 25 / 32
Total batch reconstruction loss: 0.05571144074201584
Training batch 26 / 32
Total batch reconstruction loss: 0.06352690607309341
Training batch 27 / 32
Total batch reconstruction loss: 0.060274139046669006
Training batch 28 / 32
Total batch reconstruction loss: 0.05542057007551193
Training batch 29 / 32
Total batch reconstruction loss: 0.05990659072995186
Training batch 30 / 32
Total batch reconstruction loss: 0.060144130140542984
Training batch 31 / 32
Total batch reconstruction loss: 0.05653844028711319
Training batch 32 / 32
Total batch reconstruction loss: 0.0542711541056633
Epoch [431/500], Train Loss: 0.0564, Validation Loss: 0.0565, Generator Loss: 11.7522, Discriminator Loss: 0.3137
Training epoch 432 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.0546419695019722
Training batch 2 / 32
Total batch reconstruction loss: 0.05844317004084587
Training batch 3 / 32
Total batch reconstruction loss: 0.06236720085144043
Training batch 4 / 32
Total batch reconstruction loss: 0.06008286029100418
Training batch 5 / 32
Total batch reconstruction loss: 0.060916800051927567
Training batch 6 / 32
Total batch reconstruction loss: 0.05595683306455612
Training batch 7 / 32
Total batch reconstruction loss: 0.05769491195678711
Training batch 8 / 32
Total batch reconstruction loss: 0.05861157551407814
Training batch 9 / 32
Total batch reconstruction loss: 0.06155657023191452
Training batch 10 / 32
Total batch reconstruction loss: 0.05635901540517807
Training batch 11 / 32
Total batch reconstruction loss: 0.060370612889528275
Training batch 12 / 32
Total batch reconstruction loss: 0.05486811697483063
Training batch 13 / 32
Total batch reconstruction loss: 0.05879566818475723
Training batch 14 / 32
Total batch reconstruction loss: 0.06218670308589935
Training batch 15 / 32
Total batch reconstruction loss: 0.05673650652170181
Training batch 16 / 32
Total batch reconstruction loss: 0.05727774649858475
Training batch 17 / 32
Total batch reconstruction loss: 0.05804592743515968
Training batch 18 / 32
Total batch reconstruction loss: 0.05436014011502266
Training batch 19 / 32
Total batch reconstruction loss: 0.05840238183736801
Training batch 20 / 32
Total batch reconstruction loss: 0.05905938893556595
Training batch 21 / 32
Total batch reconstruction loss: 0.05947203189134598
Training batch 22 / 32
Total batch reconstruction loss: 0.05885164812207222
Training batch 23 / 32
Total batch reconstruction loss: 0.05721032992005348
Training batch 24 / 32
Total batch reconstruction loss: 0.05885733664035797
Training batch 25 / 32
Total batch reconstruction loss: 0.05696255713701248
Training batch 26 / 32
Total batch reconstruction loss: 0.06054234504699707
Training batch 27 / 32
Total batch reconstruction loss: 0.061965081840753555
Training batch 28 / 32
Total batch reconstruction loss: 0.0544351264834404
Training batch 29 / 32
Total batch reconstruction loss: 0.058568403124809265
Training batch 30 / 32
Total batch reconstruction loss: 0.06354741007089615
Training batch 31 / 32
Total batch reconstruction loss: 0.057415127754211426
Training batch 32 / 32
Total batch reconstruction loss: 0.05869162082672119
Epoch [432/500], Train Loss: 0.0565, Validation Loss: 0.0576, Generator Loss: 11.7679, Discriminator Loss: 0.3260
Training epoch 433 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.05731295421719551
Training batch 2 / 32
Total batch reconstruction loss: 0.0582222118973732
Training batch 3 / 32
Total batch reconstruction loss: 0.06387538462877274
Training batch 4 / 32
Total batch reconstruction loss: 0.05814017355442047
Training batch 5 / 32
Total batch reconstruction loss: 0.05789272114634514
Training batch 6 / 32
Total batch reconstruction loss: 0.05863920599222183
Training batch 7 / 32
Total batch reconstruction loss: 0.060014426708221436
Training batch 8 / 32
Total batch reconstruction loss: 0.059760235249996185
Training batch 9 / 32
Total batch reconstruction loss: 0.05618630722165108
Training batch 10 / 32
Total batch reconstruction loss: 0.05562521144747734
Training batch 11 / 32
Total batch reconstruction loss: 0.058697689324617386
Training batch 12 / 32
Total batch reconstruction loss: 0.05548950284719467
Training batch 13 / 32
Total batch reconstruction loss: 0.055861979722976685
Training batch 14 / 32
Total batch reconstruction loss: 0.05498240143060684
Training batch 15 / 32
Total batch reconstruction loss: 0.061234574764966965
Training batch 16 / 32
Total batch reconstruction loss: 0.057377561926841736
Training batch 17 / 32
Total batch reconstruction loss: 0.05669962987303734
Training batch 18 / 32
Total batch reconstruction loss: 0.059155117720365524
Training batch 19 / 32
Total batch reconstruction loss: 0.06311796605587006
Training batch 20 / 32
Total batch reconstruction loss: 0.06563141196966171
Training batch 21 / 32
Total batch reconstruction loss: 0.06214642524719238
Training batch 22 / 32
Total batch reconstruction loss: 0.059707581996917725
Training batch 23 / 32
Total batch reconstruction loss: 0.05472579598426819
Training batch 24 / 32
Total batch reconstruction loss: 0.0568639412522316
Training batch 25 / 32
Total batch reconstruction loss: 0.055339932441711426
Training batch 26 / 32
Total batch reconstruction loss: 0.05603126436471939
Training batch 27 / 32
Total batch reconstruction loss: 0.05815733224153519
Training batch 28 / 32
Total batch reconstruction loss: 0.05902984365820885
Training batch 29 / 32
Total batch reconstruction loss: 0.061921797692775726
Training batch 30 / 32
Total batch reconstruction loss: 0.0564974769949913
Training batch 31 / 32
Total batch reconstruction loss: 0.05853956937789917
Training batch 32 / 32
Total batch reconstruction loss: 0.05063557252287865
Epoch [433/500], Train Loss: 0.0563, Validation Loss: 0.0571, Generator Loss: 11.7179, Discriminator Loss: 0.3085
Training epoch 434 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.06076539680361748
Training batch 2 / 32
Total batch reconstruction loss: 0.056099783629179
Training batch 3 / 32
Total batch reconstruction loss: 0.05764719843864441
Training batch 4 / 32
Total batch reconstruction loss: 0.05703146755695343
Training batch 5 / 32
Total batch reconstruction loss: 0.05683142691850662
Training batch 6 / 32
Total batch reconstruction loss: 0.05409036949276924
Training batch 7 / 32
Total batch reconstruction loss: 0.06580063700675964
Training batch 8 / 32
Total batch reconstruction loss: 0.06030784547328949
Training batch 9 / 32
Total batch reconstruction loss: 0.05603310465812683
Training batch 10 / 32
Total batch reconstruction loss: 0.05661844462156296
Training batch 11 / 32
Total batch reconstruction loss: 0.058422356843948364
Training batch 12 / 32
Total batch reconstruction loss: 0.06079695373773575
Training batch 13 / 32
Total batch reconstruction loss: 0.062078967690467834
Training batch 14 / 32
Total batch reconstruction loss: 0.05223006382584572
Training batch 15 / 32
Total batch reconstruction loss: 0.05624566972255707
Training batch 16 / 32
Total batch reconstruction loss: 0.05784526467323303
Training batch 17 / 32
Total batch reconstruction loss: 0.05673861503601074
Training batch 18 / 32
Total batch reconstruction loss: 0.06114235892891884
Training batch 19 / 32
Total batch reconstruction loss: 0.06568674743175507
Training batch 20 / 32
Total batch reconstruction loss: 0.055304013192653656
Training batch 21 / 32
Total batch reconstruction loss: 0.061278462409973145
Training batch 22 / 32
Total batch reconstruction loss: 0.058564431965351105
Training batch 23 / 32
Total batch reconstruction loss: 0.0635639950633049
Training batch 24 / 32
Total batch reconstruction loss: 0.059555429965257645
Training batch 25 / 32
Total batch reconstruction loss: 0.0572647750377655
Training batch 26 / 32
Total batch reconstruction loss: 0.05970407649874687
Training batch 27 / 32
Total batch reconstruction loss: 0.06449572741985321
Training batch 28 / 32
Total batch reconstruction loss: 0.05465693026781082
Training batch 29 / 32
Total batch reconstruction loss: 0.05492173880338669
Training batch 30 / 32
Total batch reconstruction loss: 0.06111627072095871
Training batch 31 / 32
Total batch reconstruction loss: 0.05964994430541992
Training batch 32 / 32
Total batch reconstruction loss: 0.05173764377832413
Epoch [434/500], Train Loss: 0.0568, Validation Loss: 0.0565, Generator Loss: 11.7806, Discriminator Loss: 0.3147
Training epoch 435 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.05670754611492157
Training batch 2 / 32
Total batch reconstruction loss: 0.057946041226387024
Training batch 3 / 32
Total batch reconstruction loss: 0.05887711048126221
Training batch 4 / 32
Total batch reconstruction loss: 0.05671079829335213
Training batch 5 / 32
Total batch reconstruction loss: 0.057834185659885406
Training batch 6 / 32
Total batch reconstruction loss: 0.06300238519906998
Training batch 7 / 32
Total batch reconstruction loss: 0.052956461906433105
Training batch 8 / 32
Total batch reconstruction loss: 0.05515853688120842
Training batch 9 / 32
Total batch reconstruction loss: 0.05647267401218414
Training batch 10 / 32
Total batch reconstruction loss: 0.062140993773937225
Training batch 11 / 32
Total batch reconstruction loss: 0.05577581003308296
Training batch 12 / 32
Total batch reconstruction loss: 0.05719366669654846
Training batch 13 / 32
Total batch reconstruction loss: 0.059055425226688385
Training batch 14 / 32
Total batch reconstruction loss: 0.060302965342998505
Training batch 15 / 32
Total batch reconstruction loss: 0.05735265836119652
Training batch 16 / 32
Total batch reconstruction loss: 0.0604364275932312
Training batch 17 / 32
Total batch reconstruction loss: 0.06245972588658333
Training batch 18 / 32
Total batch reconstruction loss: 0.0579025037586689
Training batch 19 / 32
Total batch reconstruction loss: 0.06275822222232819
Training batch 20 / 32
Total batch reconstruction loss: 0.05526600778102875
Training batch 21 / 32
Total batch reconstruction loss: 0.06106004863977432
Training batch 22 / 32
Total batch reconstruction loss: 0.061984386295080185
Training batch 23 / 32
Total batch reconstruction loss: 0.05451182276010513
Training batch 24 / 32
Total batch reconstruction loss: 0.057488009333610535
Training batch 25 / 32
Total batch reconstruction loss: 0.058207787573337555
Training batch 26 / 32
Total batch reconstruction loss: 0.05707176774740219
Training batch 27 / 32
Total batch reconstruction loss: 0.058423902839422226
Training batch 28 / 32
Total batch reconstruction loss: 0.06585350632667542
Training batch 29 / 32
Total batch reconstruction loss: 0.055466003715991974
Training batch 30 / 32
Total batch reconstruction loss: 0.055742181837558746
Training batch 31 / 32
Total batch reconstruction loss: 0.059488147497177124
Training batch 32 / 32
Total batch reconstruction loss: 0.08416551351547241
Epoch [435/500], Train Loss: 0.0575, Validation Loss: 0.0572, Generator Loss: 11.9201, Discriminator Loss: 0.3116
Training epoch 436 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.05808817222714424
Training batch 2 / 32
Total batch reconstruction loss: 0.05694027245044708
Training batch 3 / 32
Total batch reconstruction loss: 0.0627802386879921
Training batch 4 / 32
Total batch reconstruction loss: 0.05988800898194313
Training batch 5 / 32
Total batch reconstruction loss: 0.060828447341918945
Training batch 6 / 32
Total batch reconstruction loss: 0.05644325911998749
Training batch 7 / 32
Total batch reconstruction loss: 0.058187514543533325
Training batch 8 / 32
Total batch reconstruction loss: 0.05839597433805466
Training batch 9 / 32
Total batch reconstruction loss: 0.0582810714840889
Training batch 10 / 32
Total batch reconstruction loss: 0.054802123457193375
Training batch 11 / 32
Total batch reconstruction loss: 0.05799306929111481
Training batch 12 / 32
Total batch reconstruction loss: 0.05919594317674637
Training batch 13 / 32
Total batch reconstruction loss: 0.05599033460021019
Training batch 14 / 32
Total batch reconstruction loss: 0.05381178483366966
Training batch 15 / 32
Total batch reconstruction loss: 0.05359071120619774
Training batch 16 / 32
Total batch reconstruction loss: 0.06216508150100708
Training batch 17 / 32
Total batch reconstruction loss: 0.06382568180561066
Training batch 18 / 32
Total batch reconstruction loss: 0.053941234946250916
Training batch 19 / 32
Total batch reconstruction loss: 0.059452831745147705
Training batch 20 / 32
Total batch reconstruction loss: 0.056739211082458496
Training batch 21 / 32
Total batch reconstruction loss: 0.05561582371592522
Training batch 22 / 32
Total batch reconstruction loss: 0.057620659470558167
Training batch 23 / 32
Total batch reconstruction loss: 0.06120964139699936
Training batch 24 / 32
Total batch reconstruction loss: 0.058368850499391556
Training batch 25 / 32
Total batch reconstruction loss: 0.06711450219154358
Training batch 26 / 32
Total batch reconstruction loss: 0.06478404998779297
Training batch 27 / 32
Total batch reconstruction loss: 0.05790908634662628
Training batch 28 / 32
Total batch reconstruction loss: 0.05999340862035751
Training batch 29 / 32
Total batch reconstruction loss: 0.059241488575935364
Training batch 30 / 32
Total batch reconstruction loss: 0.054272692650556564
Training batch 31 / 32
Total batch reconstruction loss: 0.05895283445715904
Training batch 32 / 32
Total batch reconstruction loss: 0.049382515251636505
Epoch [436/500], Train Loss: 0.0565, Validation Loss: 0.0564, Generator Loss: 11.7278, Discriminator Loss: 0.3212
Training epoch 437 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.05739115923643112
Training batch 2 / 32
Total batch reconstruction loss: 0.057845182716846466
Training batch 3 / 32
Total batch reconstruction loss: 0.054554350674152374
Training batch 4 / 32
Total batch reconstruction loss: 0.05665881931781769
Training batch 5 / 32
Total batch reconstruction loss: 0.06429620832204819
Training batch 6 / 32
Total batch reconstruction loss: 0.06203113868832588
Training batch 7 / 32
Total batch reconstruction loss: 0.057028330862522125
Training batch 8 / 32
Total batch reconstruction loss: 0.058618590235710144
Training batch 9 / 32
Total batch reconstruction loss: 0.05938679724931717
Training batch 10 / 32
Total batch reconstruction loss: 0.06393413245677948
Training batch 11 / 32
Total batch reconstruction loss: 0.058778874576091766
Training batch 12 / 32
Total batch reconstruction loss: 0.0629863515496254
Training batch 13 / 32
Total batch reconstruction loss: 0.056261513382196426
Training batch 14 / 32
Total batch reconstruction loss: 0.053885240107774734
Training batch 15 / 32
Total batch reconstruction loss: 0.05462002381682396
Training batch 16 / 32
Total batch reconstruction loss: 0.05547749623656273
Training batch 17 / 32
Total batch reconstruction loss: 0.056819893419742584
Training batch 18 / 32
Total batch reconstruction loss: 0.06415421515703201
Training batch 19 / 32
Total batch reconstruction loss: 0.06212413311004639
Training batch 20 / 32
Total batch reconstruction loss: 0.05753724277019501
Training batch 21 / 32
Total batch reconstruction loss: 0.06239638477563858
Training batch 22 / 32
Total batch reconstruction loss: 0.05838093161582947
Training batch 23 / 32
Total batch reconstruction loss: 0.058671656996011734
Training batch 24 / 32
Total batch reconstruction loss: 0.05805014818906784
Training batch 25 / 32
Total batch reconstruction loss: 0.0577571839094162
Training batch 26 / 32
Total batch reconstruction loss: 0.05674261972308159
Training batch 27 / 32
Total batch reconstruction loss: 0.054680235683918
Training batch 28 / 32
Total batch reconstruction loss: 0.059638891369104385
Training batch 29 / 32
Total batch reconstruction loss: 0.05796290561556816
Training batch 30 / 32
Total batch reconstruction loss: 0.0613931268453598
Training batch 31 / 32
Total batch reconstruction loss: 0.05512183904647827
Training batch 32 / 32
Total batch reconstruction loss: 0.05379132553935051
Epoch [437/500], Train Loss: 0.0566, Validation Loss: 0.0584, Generator Loss: 11.7486, Discriminator Loss: 0.3193
Training epoch 438 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.0533008798956871
Training batch 2 / 32
Total batch reconstruction loss: 0.061302222311496735
Training batch 3 / 32
Total batch reconstruction loss: 0.0637502521276474
Training batch 4 / 32
Total batch reconstruction loss: 0.06132681667804718
Training batch 5 / 32
Total batch reconstruction loss: 0.05853100121021271
Training batch 6 / 32
Total batch reconstruction loss: 0.0589253269135952
Training batch 7 / 32
Total batch reconstruction loss: 0.05705671012401581
Training batch 8 / 32
Total batch reconstruction loss: 0.05643204599618912
Training batch 9 / 32
Total batch reconstruction loss: 0.058944039046764374
Training batch 10 / 32
Total batch reconstruction loss: 0.05975917726755142
Training batch 11 / 32
Total batch reconstruction loss: 0.05669145658612251
Training batch 12 / 32
Total batch reconstruction loss: 0.061103589832782745
Training batch 13 / 32
Total batch reconstruction loss: 0.055034905672073364
Training batch 14 / 32
Total batch reconstruction loss: 0.058554790914058685
Training batch 15 / 32
Total batch reconstruction loss: 0.05721501260995865
Training batch 16 / 32
Total batch reconstruction loss: 0.059257421642541885
Training batch 17 / 32
Total batch reconstruction loss: 0.05253423750400543
Training batch 18 / 32
Total batch reconstruction loss: 0.05740572512149811
Training batch 19 / 32
Total batch reconstruction loss: 0.06036858633160591
Training batch 20 / 32
Total batch reconstruction loss: 0.0641779825091362
Training batch 21 / 32
Total batch reconstruction loss: 0.06164226680994034
Training batch 22 / 32
Total batch reconstruction loss: 0.05929034203290939
Training batch 23 / 32
Total batch reconstruction loss: 0.05750183016061783
Training batch 24 / 32
Total batch reconstruction loss: 0.05509881675243378
Training batch 25 / 32
Total batch reconstruction loss: 0.06117299944162369
Training batch 26 / 32
Total batch reconstruction loss: 0.05479935556650162
Training batch 27 / 32
Total batch reconstruction loss: 0.06275209784507751
Training batch 28 / 32
Total batch reconstruction loss: 0.056398823857307434
Training batch 29 / 32
Total batch reconstruction loss: 0.05838799849152565
Training batch 30 / 32
Total batch reconstruction loss: 0.053570471704006195
Training batch 31 / 32
Total batch reconstruction loss: 0.05934961885213852
Training batch 32 / 32
Total batch reconstruction loss: 0.06156669557094574
Epoch [438/500], Train Loss: 0.0564, Validation Loss: 0.0570, Generator Loss: 11.7674, Discriminator Loss: 0.3273
Training epoch 439 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.05732225254178047
Training batch 2 / 32
Total batch reconstruction loss: 0.055949702858924866
Training batch 3 / 32
Total batch reconstruction loss: 0.05675515532493591
Training batch 4 / 32
Total batch reconstruction loss: 0.057449087500572205
Training batch 5 / 32
Total batch reconstruction loss: 0.058216869831085205
Training batch 6 / 32
Total batch reconstruction loss: 0.05432889610528946
Training batch 7 / 32
Total batch reconstruction loss: 0.060788996517658234
Training batch 8 / 32
Total batch reconstruction loss: 0.05765213072299957
Training batch 9 / 32
Total batch reconstruction loss: 0.05858039855957031
Training batch 10 / 32
Total batch reconstruction loss: 0.05500330030918121
Training batch 11 / 32
Total batch reconstruction loss: 0.06000629439949989
Training batch 12 / 32
Total batch reconstruction loss: 0.05865058675408363
Training batch 13 / 32
Total batch reconstruction loss: 0.05637573450803757
Training batch 14 / 32
Total batch reconstruction loss: 0.05911054089665413
Training batch 15 / 32
Total batch reconstruction loss: 0.06204259768128395
Training batch 16 / 32
Total batch reconstruction loss: 0.056466713547706604
Training batch 17 / 32
Total batch reconstruction loss: 0.054945509880781174
Training batch 18 / 32
Total batch reconstruction loss: 0.057885654270648956
Training batch 19 / 32
Total batch reconstruction loss: 0.05589921027421951
Training batch 20 / 32
Total batch reconstruction loss: 0.06513480097055435
Training batch 21 / 32
Total batch reconstruction loss: 0.06068629026412964
Training batch 22 / 32
Total batch reconstruction loss: 0.06032975763082504
Training batch 23 / 32
Total batch reconstruction loss: 0.06123581901192665
Training batch 24 / 32
Total batch reconstruction loss: 0.056762661784887314
Training batch 25 / 32
Total batch reconstruction loss: 0.057006798684597015
Training batch 26 / 32
Total batch reconstruction loss: 0.05841287970542908
Training batch 27 / 32
Total batch reconstruction loss: 0.057919375598430634
Training batch 28 / 32
Total batch reconstruction loss: 0.05892288684844971
Training batch 29 / 32
Total batch reconstruction loss: 0.05726638436317444
Training batch 30 / 32
Total batch reconstruction loss: 0.059666819870471954
Training batch 31 / 32
Total batch reconstruction loss: 0.06198178976774216
Training batch 32 / 32
Total batch reconstruction loss: 0.06504129618406296
Epoch [439/500], Train Loss: 0.0564, Validation Loss: 0.0570, Generator Loss: 11.7822, Discriminator Loss: 0.3140
Training epoch 440 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.05516713112592697
Training batch 2 / 32
Total batch reconstruction loss: 0.05970805138349533
Training batch 3 / 32
Total batch reconstruction loss: 0.06298668682575226
Training batch 4 / 32
Total batch reconstruction loss: 0.05825025588274002
Training batch 5 / 32
Total batch reconstruction loss: 0.05744713544845581
Training batch 6 / 32
Total batch reconstruction loss: 0.06286751478910446
Training batch 7 / 32
Total batch reconstruction loss: 0.05959615483880043
Training batch 8 / 32
Total batch reconstruction loss: 0.06331606954336166
Training batch 9 / 32
Total batch reconstruction loss: 0.0597798228263855
Training batch 10 / 32
Total batch reconstruction loss: 0.06261276453733444
Training batch 11 / 32
Total batch reconstruction loss: 0.05618736147880554
Training batch 12 / 32
Total batch reconstruction loss: 0.0591784231364727
Training batch 13 / 32
Total batch reconstruction loss: 0.05761132016777992
Training batch 14 / 32
Total batch reconstruction loss: 0.06200479716062546
Training batch 15 / 32
Total batch reconstruction loss: 0.05973542109131813
Training batch 16 / 32
Total batch reconstruction loss: 0.05856416001915932
Training batch 17 / 32
Total batch reconstruction loss: 0.05680350959300995
Training batch 18 / 32
Total batch reconstruction loss: 0.05641542375087738
Training batch 19 / 32
Total batch reconstruction loss: 0.05522501468658447
Training batch 20 / 32
Total batch reconstruction loss: 0.0571928471326828
Training batch 21 / 32
Total batch reconstruction loss: 0.06234602630138397
Training batch 22 / 32
Total batch reconstruction loss: 0.056401897221803665
Training batch 23 / 32
Total batch reconstruction loss: 0.061951201409101486
Training batch 24 / 32
Total batch reconstruction loss: 0.06198877841234207
Training batch 25 / 32
Total batch reconstruction loss: 0.05911620706319809
Training batch 26 / 32
Total batch reconstruction loss: 0.059468239545822144
Training batch 27 / 32
Total batch reconstruction loss: 0.05565708130598068
Training batch 28 / 32
Total batch reconstruction loss: 0.0604056715965271
Training batch 29 / 32
Total batch reconstruction loss: 0.05603655055165291
Training batch 30 / 32
Total batch reconstruction loss: 0.05537904426455498
Training batch 31 / 32
Total batch reconstruction loss: 0.05720306932926178
Training batch 32 / 32
Total batch reconstruction loss: 0.062000006437301636
Epoch [440/500], Train Loss: 0.0575, Validation Loss: 0.0569, Generator Loss: 11.8699, Discriminator Loss: 0.3215
Training epoch 441 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.06375541538000107
Training batch 2 / 32
Total batch reconstruction loss: 0.06617651134729385
Training batch 3 / 32
Total batch reconstruction loss: 0.0610484853386879
Training batch 4 / 32
Total batch reconstruction loss: 0.0582248717546463
Training batch 5 / 32
Total batch reconstruction loss: 0.058065496385097504
Training batch 6 / 32
Total batch reconstruction loss: 0.06080888211727142
Training batch 7 / 32
Total batch reconstruction loss: 0.06133938953280449
Training batch 8 / 32
Total batch reconstruction loss: 0.05842917785048485
Training batch 9 / 32
Total batch reconstruction loss: 0.059550173580646515
Training batch 10 / 32
Total batch reconstruction loss: 0.06410388648509979
Training batch 11 / 32
Total batch reconstruction loss: 0.05965198576450348
Training batch 12 / 32
Total batch reconstruction loss: 0.05724278837442398
Training batch 13 / 32
Total batch reconstruction loss: 0.054335981607437134
Training batch 14 / 32
Total batch reconstruction loss: 0.06163625791668892
Training batch 15 / 32
Total batch reconstruction loss: 0.05652247369289398
Training batch 16 / 32
Total batch reconstruction loss: 0.056241586804389954
Training batch 17 / 32
Total batch reconstruction loss: 0.05894586443901062
Training batch 18 / 32
Total batch reconstruction loss: 0.059345461428165436
Training batch 19 / 32
Total batch reconstruction loss: 0.05567379295825958
Training batch 20 / 32
Total batch reconstruction loss: 0.05798598378896713
Training batch 21 / 32
Total batch reconstruction loss: 0.05706235021352768
Training batch 22 / 32
Total batch reconstruction loss: 0.05987142026424408
Training batch 23 / 32
Total batch reconstruction loss: 0.05482281371951103
Training batch 24 / 32
Total batch reconstruction loss: 0.057470209896564484
Training batch 25 / 32
Total batch reconstruction loss: 0.05953201651573181
Training batch 26 / 32
Total batch reconstruction loss: 0.05877041071653366
Training batch 27 / 32
Total batch reconstruction loss: 0.05050951987504959
Training batch 28 / 32
Total batch reconstruction loss: 0.06316075474023819
Training batch 29 / 32
Total batch reconstruction loss: 0.05733323469758034
Training batch 30 / 32
Total batch reconstruction loss: 0.05699636787176132
Training batch 31 / 32
Total batch reconstruction loss: 0.05767182260751724
Training batch 32 / 32
Total batch reconstruction loss: 0.05900602787733078
Epoch [441/500], Train Loss: 0.0569, Validation Loss: 0.0571, Generator Loss: 11.8313, Discriminator Loss: 0.3074
Training epoch 442 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.05950365215539932
Training batch 2 / 32
Total batch reconstruction loss: 0.05745986849069595
Training batch 3 / 32
Total batch reconstruction loss: 0.06249876692891121
Training batch 4 / 32
Total batch reconstruction loss: 0.05769791454076767
Training batch 5 / 32
Total batch reconstruction loss: 0.06293132901191711
Training batch 6 / 32
Total batch reconstruction loss: 0.05864570662379265
Training batch 7 / 32
Total batch reconstruction loss: 0.05298687145113945
Training batch 8 / 32
Total batch reconstruction loss: 0.05483665317296982
Training batch 9 / 32
Total batch reconstruction loss: 0.057731181383132935
Training batch 10 / 32
Total batch reconstruction loss: 0.06281580030918121
Training batch 11 / 32
Total batch reconstruction loss: 0.055056214332580566
Training batch 12 / 32
Total batch reconstruction loss: 0.057327985763549805
Training batch 13 / 32
Total batch reconstruction loss: 0.058217570185661316
Training batch 14 / 32
Total batch reconstruction loss: 0.05533178895711899
Training batch 15 / 32
Total batch reconstruction loss: 0.06035013869404793
Training batch 16 / 32
Total batch reconstruction loss: 0.05627010390162468
Training batch 17 / 32
Total batch reconstruction loss: 0.05597729980945587
Training batch 18 / 32
Total batch reconstruction loss: 0.0633682981133461
Training batch 19 / 32
Total batch reconstruction loss: 0.055763401091098785
Training batch 20 / 32
Total batch reconstruction loss: 0.0561772957444191
Training batch 21 / 32
Total batch reconstruction loss: 0.05846233293414116
Training batch 22 / 32
Total batch reconstruction loss: 0.05700746178627014
Training batch 23 / 32
Total batch reconstruction loss: 0.06299321353435516
Training batch 24 / 32
Total batch reconstruction loss: 0.05922535061836243
Training batch 25 / 32
Total batch reconstruction loss: 0.0582292340695858
Training batch 26 / 32
Total batch reconstruction loss: 0.057982124388217926
Training batch 27 / 32
Total batch reconstruction loss: 0.059702787548303604
Training batch 28 / 32
Total batch reconstruction loss: 0.06042410433292389
Training batch 29 / 32
Total batch reconstruction loss: 0.05893737077713013
Training batch 30 / 32
Total batch reconstruction loss: 0.060173340141773224
Training batch 31 / 32
Total batch reconstruction loss: 0.061357878148555756
Training batch 32 / 32
Total batch reconstruction loss: 0.04960033670067787
Epoch [442/500], Train Loss: 0.0563, Validation Loss: 0.0575, Generator Loss: 11.7228, Discriminator Loss: 0.3251
Training epoch 443 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.055836863815784454
Training batch 2 / 32
Total batch reconstruction loss: 0.06338124722242355
Training batch 3 / 32
Total batch reconstruction loss: 0.0602937787771225
Training batch 4 / 32
Total batch reconstruction loss: 0.05379175394773483
Training batch 5 / 32
Total batch reconstruction loss: 0.05673293396830559
Training batch 6 / 32
Total batch reconstruction loss: 0.05534418672323227
Training batch 7 / 32
Total batch reconstruction loss: 0.06099884212017059
Training batch 8 / 32
Total batch reconstruction loss: 0.05878844112157822
Training batch 9 / 32
Total batch reconstruction loss: 0.05865379422903061
Training batch 10 / 32
Total batch reconstruction loss: 0.05583800747990608
Training batch 11 / 32
Total batch reconstruction loss: 0.05802880972623825
Training batch 12 / 32
Total batch reconstruction loss: 0.05748602747917175
Training batch 13 / 32
Total batch reconstruction loss: 0.059479039162397385
Training batch 14 / 32
Total batch reconstruction loss: 0.062092289328575134
Training batch 15 / 32
Total batch reconstruction loss: 0.05574026331305504
Training batch 16 / 32
Total batch reconstruction loss: 0.05570736154913902
Training batch 17 / 32
Total batch reconstruction loss: 0.054668113589286804
Training batch 18 / 32
Total batch reconstruction loss: 0.06012430787086487
Training batch 19 / 32
Total batch reconstruction loss: 0.06455525010824203
Training batch 20 / 32
Total batch reconstruction loss: 0.05500974506139755
Training batch 21 / 32
Total batch reconstruction loss: 0.056313157081604004
Training batch 22 / 32
Total batch reconstruction loss: 0.06147834658622742
Training batch 23 / 32
Total batch reconstruction loss: 0.05696117877960205
Training batch 24 / 32
Total batch reconstruction loss: 0.05514558404684067
Training batch 25 / 32
Total batch reconstruction loss: 0.0616559162735939
Training batch 26 / 32
Total batch reconstruction loss: 0.060003817081451416
Training batch 27 / 32
Total batch reconstruction loss: 0.060936570167541504
Training batch 28 / 32
Total batch reconstruction loss: 0.058067940175533295
Training batch 29 / 32
Total batch reconstruction loss: 0.05970261991024017
Training batch 30 / 32
Total batch reconstruction loss: 0.06069466471672058
Training batch 31 / 32
Total batch reconstruction loss: 0.055693820118904114
Training batch 32 / 32
Total batch reconstruction loss: 0.053010113537311554
Epoch [443/500], Train Loss: 0.0563, Validation Loss: 0.0576, Generator Loss: 11.7049, Discriminator Loss: 0.3208
Training epoch 444 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.0576743483543396
Training batch 2 / 32
Total batch reconstruction loss: 0.06000702083110809
Training batch 3 / 32
Total batch reconstruction loss: 0.060753677040338516
Training batch 4 / 32
Total batch reconstruction loss: 0.05939045175909996
Training batch 5 / 32
Total batch reconstruction loss: 0.06416434049606323
Training batch 6 / 32
Total batch reconstruction loss: 0.0597003735601902
Training batch 7 / 32
Total batch reconstruction loss: 0.06249267980456352
Training batch 8 / 32
Total batch reconstruction loss: 0.06062956154346466
Training batch 9 / 32
Total batch reconstruction loss: 0.057848334312438965
Training batch 10 / 32
Total batch reconstruction loss: 0.05986097455024719
Training batch 11 / 32
Total batch reconstruction loss: 0.060442011803388596
Training batch 12 / 32
Total batch reconstruction loss: 0.05913231521844864
Training batch 13 / 32
Total batch reconstruction loss: 0.05606003478169441
Training batch 14 / 32
Total batch reconstruction loss: 0.054518699645996094
Training batch 15 / 32
Total batch reconstruction loss: 0.06129424646496773
Training batch 16 / 32
Total batch reconstruction loss: 0.058127325028181076
Training batch 17 / 32
Total batch reconstruction loss: 0.05812203139066696
Training batch 18 / 32
Total batch reconstruction loss: 0.0554845817387104
Training batch 19 / 32
Total batch reconstruction loss: 0.060755886137485504
Training batch 20 / 32
Total batch reconstruction loss: 0.0580502487719059
Training batch 21 / 32
Total batch reconstruction loss: 0.05879300460219383
Training batch 22 / 32
Total batch reconstruction loss: 0.05687820166349411
Training batch 23 / 32
Total batch reconstruction loss: 0.057985611259937286
Training batch 24 / 32
Total batch reconstruction loss: 0.05950087308883667
Training batch 25 / 32
Total batch reconstruction loss: 0.0621173270046711
Training batch 26 / 32
Total batch reconstruction loss: 0.05517611652612686
Training batch 27 / 32
Total batch reconstruction loss: 0.05761420726776123
Training batch 28 / 32
Total batch reconstruction loss: 0.06171005964279175
Training batch 29 / 32
Total batch reconstruction loss: 0.05330340564250946
Training batch 30 / 32
Total batch reconstruction loss: 0.06065058708190918
Training batch 31 / 32
Total batch reconstruction loss: 0.05856272578239441
Training batch 32 / 32
Total batch reconstruction loss: 0.08461780846118927
Epoch [444/500], Train Loss: 0.0580, Validation Loss: 0.0593, Generator Loss: 12.0117, Discriminator Loss: 0.3217
Training epoch 445 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.061787091195583344
Training batch 2 / 32
Total batch reconstruction loss: 0.059355057775974274
Training batch 3 / 32
Total batch reconstruction loss: 0.060029685497283936
Training batch 4 / 32
Total batch reconstruction loss: 0.05846588686108589
Training batch 5 / 32
Total batch reconstruction loss: 0.060656629502773285
Training batch 6 / 32
Total batch reconstruction loss: 0.05695081502199173
Training batch 7 / 32
Total batch reconstruction loss: 0.06019482761621475
Training batch 8 / 32
Total batch reconstruction loss: 0.05579948425292969
Training batch 9 / 32
Total batch reconstruction loss: 0.06037119776010513
Training batch 10 / 32
Total batch reconstruction loss: 0.05713564157485962
Training batch 11 / 32
Total batch reconstruction loss: 0.053987838327884674
Training batch 12 / 32
Total batch reconstruction loss: 0.060126401484012604
Training batch 13 / 32
Total batch reconstruction loss: 0.0577738881111145
Training batch 14 / 32
Total batch reconstruction loss: 0.05636681616306305
Training batch 15 / 32
Total batch reconstruction loss: 0.06153668090701103
Training batch 16 / 32
Total batch reconstruction loss: 0.05844957381486893
Training batch 17 / 32
Total batch reconstruction loss: 0.05731363967061043
Training batch 18 / 32
Total batch reconstruction loss: 0.05882185697555542
Training batch 19 / 32
Total batch reconstruction loss: 0.0585503987967968
Training batch 20 / 32
Total batch reconstruction loss: 0.06356470286846161
Training batch 21 / 32
Total batch reconstruction loss: 0.06120404228568077
Training batch 22 / 32
Total batch reconstruction loss: 0.056003134697675705
Training batch 23 / 32
Total batch reconstruction loss: 0.05288172885775566
Training batch 24 / 32
Total batch reconstruction loss: 0.060202233493328094
Training batch 25 / 32
Total batch reconstruction loss: 0.058089740574359894
Training batch 26 / 32
Total batch reconstruction loss: 0.05629411339759827
Training batch 27 / 32
Total batch reconstruction loss: 0.05782867968082428
Training batch 28 / 32
Total batch reconstruction loss: 0.05532564967870712
Training batch 29 / 32
Total batch reconstruction loss: 0.05999036133289337
Training batch 30 / 32
Total batch reconstruction loss: 0.05817411467432976
Training batch 31 / 32
Total batch reconstruction loss: 0.06118439882993698
Training batch 32 / 32
Total batch reconstruction loss: 0.05228474736213684
Epoch [445/500], Train Loss: 0.0565, Validation Loss: 0.0566, Generator Loss: 11.7423, Discriminator Loss: 0.3025
Training epoch 446 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.05757175758481026
Training batch 2 / 32
Total batch reconstruction loss: 0.06038038432598114
Training batch 3 / 32
Total batch reconstruction loss: 0.05927709862589836
Training batch 4 / 32
Total batch reconstruction loss: 0.06302952766418457
Training batch 5 / 32
Total batch reconstruction loss: 0.05696266144514084
Training batch 6 / 32
Total batch reconstruction loss: 0.05744418129324913
Training batch 7 / 32
Total batch reconstruction loss: 0.05874825268983841
Training batch 8 / 32
Total batch reconstruction loss: 0.05938469618558884
Training batch 9 / 32
Total batch reconstruction loss: 0.06328190863132477
Training batch 10 / 32
Total batch reconstruction loss: 0.0568985790014267
Training batch 11 / 32
Total batch reconstruction loss: 0.061962977051734924
Training batch 12 / 32
Total batch reconstruction loss: 0.05995919182896614
Training batch 13 / 32
Total batch reconstruction loss: 0.05808611214160919
Training batch 14 / 32
Total batch reconstruction loss: 0.055006757378578186
Training batch 15 / 32
Total batch reconstruction loss: 0.06449009478092194
Training batch 16 / 32
Total batch reconstruction loss: 0.055437441915273666
Training batch 17 / 32
Total batch reconstruction loss: 0.057329267263412476
Training batch 18 / 32
Total batch reconstruction loss: 0.05975602567195892
Training batch 19 / 32
Total batch reconstruction loss: 0.05274472385644913
Training batch 20 / 32
Total batch reconstruction loss: 0.05664588510990143
Training batch 21 / 32
Total batch reconstruction loss: 0.06079212948679924
Training batch 22 / 32
Total batch reconstruction loss: 0.05842472240328789
Training batch 23 / 32
Total batch reconstruction loss: 0.06051233410835266
Training batch 24 / 32
Total batch reconstruction loss: 0.057286858558654785
Training batch 25 / 32
Total batch reconstruction loss: 0.05990564823150635
Training batch 26 / 32
Total batch reconstruction loss: 0.05706959217786789
Training batch 27 / 32
Total batch reconstruction loss: 0.05568033084273338
Training batch 28 / 32
Total batch reconstruction loss: 0.055932022631168365
Training batch 29 / 32
Total batch reconstruction loss: 0.05312158539891243
Training batch 30 / 32
Total batch reconstruction loss: 0.055511724203825
Training batch 31 / 32
Total batch reconstruction loss: 0.06232193857431412
Training batch 32 / 32
Total batch reconstruction loss: 0.05943727865815163
Epoch [446/500], Train Loss: 0.0565, Validation Loss: 0.0601, Generator Loss: 11.7620, Discriminator Loss: 0.3106
Training epoch 447 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.05540642887353897
Training batch 2 / 32
Total batch reconstruction loss: 0.058758191764354706
Training batch 3 / 32
Total batch reconstruction loss: 0.056170061230659485
Training batch 4 / 32
Total batch reconstruction loss: 0.06225356459617615
Training batch 5 / 32
Total batch reconstruction loss: 0.06004096567630768
Training batch 6 / 32
Total batch reconstruction loss: 0.05637457221746445
Training batch 7 / 32
Total batch reconstruction loss: 0.059524014592170715
Training batch 8 / 32
Total batch reconstruction loss: 0.05885308235883713
Training batch 9 / 32
Total batch reconstruction loss: 0.055931009352207184
Training batch 10 / 32
Total batch reconstruction loss: 0.06213221698999405
Training batch 11 / 32
Total batch reconstruction loss: 0.05861715227365494
Training batch 12 / 32
Total batch reconstruction loss: 0.059457115828990936
Training batch 13 / 32
Total batch reconstruction loss: 0.057740308344364166
Training batch 14 / 32
Total batch reconstruction loss: 0.057550862431526184
Training batch 15 / 32
Total batch reconstruction loss: 0.056326963007450104
Training batch 16 / 32
Total batch reconstruction loss: 0.05830816552042961
Training batch 17 / 32
Total batch reconstruction loss: 0.05657476931810379
Training batch 18 / 32
Total batch reconstruction loss: 0.05742619186639786
Training batch 19 / 32
Total batch reconstruction loss: 0.05787358433008194
Training batch 20 / 32
Total batch reconstruction loss: 0.05687279254198074
Training batch 21 / 32
Total batch reconstruction loss: 0.060919396579265594
Training batch 22 / 32
Total batch reconstruction loss: 0.05575597286224365
Training batch 23 / 32
Total batch reconstruction loss: 0.0629073828458786
Training batch 24 / 32
Total batch reconstruction loss: 0.055496856570243835
Training batch 25 / 32
Total batch reconstruction loss: 0.06109330430626869
Training batch 26 / 32
Total batch reconstruction loss: 0.05888296663761139
Training batch 27 / 32
Total batch reconstruction loss: 0.058643028140068054
Training batch 28 / 32
Total batch reconstruction loss: 0.05799246206879616
Training batch 29 / 32
Total batch reconstruction loss: 0.06022820621728897
Training batch 30 / 32
Total batch reconstruction loss: 0.05571369081735611
Training batch 31 / 32
Total batch reconstruction loss: 0.056330762803554535
Training batch 32 / 32
Total batch reconstruction loss: 0.0615827813744545
Epoch [447/500], Train Loss: 0.0563, Validation Loss: 0.0577, Generator Loss: 11.7501, Discriminator Loss: 0.2976
Training epoch 448 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.05701744556427002
Training batch 2 / 32
Total batch reconstruction loss: 0.0626130923628807
Training batch 3 / 32
Total batch reconstruction loss: 0.060630328953266144
Training batch 4 / 32
Total batch reconstruction loss: 0.05679572746157646
Training batch 5 / 32
Total batch reconstruction loss: 0.06384018808603287
Training batch 6 / 32
Total batch reconstruction loss: 0.055737145245075226
Training batch 7 / 32
Total batch reconstruction loss: 0.054771989583969116
Training batch 8 / 32
Total batch reconstruction loss: 0.06903710961341858
Training batch 9 / 32
Total batch reconstruction loss: 0.05395577847957611
Training batch 10 / 32
Total batch reconstruction loss: 0.06109140068292618
Training batch 11 / 32
Total batch reconstruction loss: 0.05348900705575943
Training batch 12 / 32
Total batch reconstruction loss: 0.05656104534864426
Training batch 13 / 32
Total batch reconstruction loss: 0.05982433259487152
Training batch 14 / 32
Total batch reconstruction loss: 0.060809627175331116
Training batch 15 / 32
Total batch reconstruction loss: 0.05720280855894089
Training batch 16 / 32
Total batch reconstruction loss: 0.05682628974318504
Training batch 17 / 32
Total batch reconstruction loss: 0.05695892125368118
Training batch 18 / 32
Total batch reconstruction loss: 0.05603613704442978
Training batch 19 / 32
Total batch reconstruction loss: 0.05617739260196686
Training batch 20 / 32
Total batch reconstruction loss: 0.060330696403980255
Training batch 21 / 32
Total batch reconstruction loss: 0.05675091594457626
Training batch 22 / 32
Total batch reconstruction loss: 0.05277041345834732
Training batch 23 / 32
Total batch reconstruction loss: 0.06158856302499771
Training batch 24 / 32
Total batch reconstruction loss: 0.05754915624856949
Training batch 25 / 32
Total batch reconstruction loss: 0.060581788420677185
Training batch 26 / 32
Total batch reconstruction loss: 0.0581781342625618
Training batch 27 / 32
Total batch reconstruction loss: 0.051545143127441406
Training batch 28 / 32
Total batch reconstruction loss: 0.05802794545888901
Training batch 29 / 32
Total batch reconstruction loss: 0.05633809044957161
Training batch 30 / 32
Total batch reconstruction loss: 0.058691807091236115
Training batch 31 / 32
Total batch reconstruction loss: 0.05940675735473633
Training batch 32 / 32
Total batch reconstruction loss: 0.05642343685030937
Epoch [448/500], Train Loss: 0.0561, Validation Loss: 0.0559, Generator Loss: 11.6797, Discriminator Loss: 0.3071
Training epoch 449 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.05396347492933273
Training batch 2 / 32
Total batch reconstruction loss: 0.056750666350126266
Training batch 3 / 32
Total batch reconstruction loss: 0.05967570096254349
Training batch 4 / 32
Total batch reconstruction loss: 0.05941317230463028
Training batch 5 / 32
Total batch reconstruction loss: 0.05894794315099716
Training batch 6 / 32
Total batch reconstruction loss: 0.05503755062818527
Training batch 7 / 32
Total batch reconstruction loss: 0.06261706352233887
Training batch 8 / 32
Total batch reconstruction loss: 0.057679299265146255
Training batch 9 / 32
Total batch reconstruction loss: 0.055496975779533386
Training batch 10 / 32
Total batch reconstruction loss: 0.056787073612213135
Training batch 11 / 32
Total batch reconstruction loss: 0.05745522677898407
Training batch 12 / 32
Total batch reconstruction loss: 0.05767735093832016
Training batch 13 / 32
Total batch reconstruction loss: 0.05695310980081558
Training batch 14 / 32
Total batch reconstruction loss: 0.0614987313747406
Training batch 15 / 32
Total batch reconstruction loss: 0.05767311900854111
Training batch 16 / 32
Total batch reconstruction loss: 0.056816913187503815
Training batch 17 / 32
Total batch reconstruction loss: 0.058108165860176086
Training batch 18 / 32
Total batch reconstruction loss: 0.05974852293729782
Training batch 19 / 32
Total batch reconstruction loss: 0.05400030314922333
Training batch 20 / 32
Total batch reconstruction loss: 0.05863208696246147
Training batch 21 / 32
Total batch reconstruction loss: 0.055167488753795624
Training batch 22 / 32
Total batch reconstruction loss: 0.058632947504520416
Training batch 23 / 32
Total batch reconstruction loss: 0.06552665680646896
Training batch 24 / 32
Total batch reconstruction loss: 0.05682520195841789
Training batch 25 / 32
Total batch reconstruction loss: 0.058878861367702484
Training batch 26 / 32
Total batch reconstruction loss: 0.05700986087322235
Training batch 27 / 32
Total batch reconstruction loss: 0.05940224975347519
Training batch 28 / 32
Total batch reconstruction loss: 0.05987215414643288
Training batch 29 / 32
Total batch reconstruction loss: 0.05374380946159363
Training batch 30 / 32
Total batch reconstruction loss: 0.062444061040878296
Training batch 31 / 32
Total batch reconstruction loss: 0.05438457056879997
Training batch 32 / 32
Total batch reconstruction loss: 0.05098775774240494
Epoch [449/500], Train Loss: 0.0558, Validation Loss: 0.0564, Generator Loss: 11.6151, Discriminator Loss: 0.3169
Training epoch 450 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.057692475616931915
Training batch 2 / 32
Total batch reconstruction loss: 0.055767159909009933
Training batch 3 / 32
Total batch reconstruction loss: 0.06697583943605423
Training batch 4 / 32
Total batch reconstruction loss: 0.0633496344089508
Training batch 5 / 32
Total batch reconstruction loss: 0.05807892605662346
Training batch 6 / 32
Total batch reconstruction loss: 0.05745909363031387
Training batch 7 / 32
Total batch reconstruction loss: 0.0562848299741745
Training batch 8 / 32
Total batch reconstruction loss: 0.05837037414312363
Training batch 9 / 32
Total batch reconstruction loss: 0.0585080161690712
Training batch 10 / 32
Total batch reconstruction loss: 0.056070856750011444
Training batch 11 / 32
Total batch reconstruction loss: 0.05594130605459213
Training batch 12 / 32
Total batch reconstruction loss: 0.06076352670788765
Training batch 13 / 32
Total batch reconstruction loss: 0.06558727473020554
Training batch 14 / 32
Total batch reconstruction loss: 0.054979681968688965
Training batch 15 / 32
Total batch reconstruction loss: 0.06268667429685593
Training batch 16 / 32
Total batch reconstruction loss: 0.06501127779483795
Training batch 17 / 32
Total batch reconstruction loss: 0.05303565412759781
Training batch 18 / 32
Total batch reconstruction loss: 0.05734161287546158
Training batch 19 / 32
Total batch reconstruction loss: 0.056908294558525085
Training batch 20 / 32
Total batch reconstruction loss: 0.05552893504500389
Training batch 21 / 32
Total batch reconstruction loss: 0.056185029447078705
Training batch 22 / 32
Total batch reconstruction loss: 0.0618148148059845
Training batch 23 / 32
Total batch reconstruction loss: 0.06190175563097
Training batch 24 / 32
Total batch reconstruction loss: 0.05603022873401642
Training batch 25 / 32
Total batch reconstruction loss: 0.05657745525240898
Training batch 26 / 32
Total batch reconstruction loss: 0.05505148693919182
Training batch 27 / 32
Total batch reconstruction loss: 0.056848347187042236
Training batch 28 / 32
Total batch reconstruction loss: 0.05871998146176338
Training batch 29 / 32
Total batch reconstruction loss: 0.05497129261493683
Training batch 30 / 32
Total batch reconstruction loss: 0.05785191431641579
Training batch 31 / 32
Total batch reconstruction loss: 0.058342017233371735
Training batch 32 / 32
Total batch reconstruction loss: 0.06584064662456512
Epoch [450/500], Train Loss: 0.0565, Validation Loss: 0.0563, Generator Loss: 11.7942, Discriminator Loss: 0.3231
Training epoch 451 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.05762428417801857
Training batch 2 / 32
Total batch reconstruction loss: 0.061150770634412766
Training batch 3 / 32
Total batch reconstruction loss: 0.05719239264726639
Training batch 4 / 32
Total batch reconstruction loss: 0.06093309074640274
Training batch 5 / 32
Total batch reconstruction loss: 0.056841593235731125
Training batch 6 / 32
Total batch reconstruction loss: 0.06250517815351486
Training batch 7 / 32
Total batch reconstruction loss: 0.05961896479129791
Training batch 8 / 32
Total batch reconstruction loss: 0.05866808444261551
Training batch 9 / 32
Total batch reconstruction loss: 0.06412115693092346
Training batch 10 / 32
Total batch reconstruction loss: 0.05961333215236664
Training batch 11 / 32
Total batch reconstruction loss: 0.055827539414167404
Training batch 12 / 32
Total batch reconstruction loss: 0.05603146553039551
Training batch 13 / 32
Total batch reconstruction loss: 0.0604010634124279
Training batch 14 / 32
Total batch reconstruction loss: 0.056962303817272186
Training batch 15 / 32
Total batch reconstruction loss: 0.056397803127765656
Training batch 16 / 32
Total batch reconstruction loss: 0.05662574991583824
Training batch 17 / 32
Total batch reconstruction loss: 0.0554002970457077
Training batch 18 / 32
Total batch reconstruction loss: 0.05746447294950485
Training batch 19 / 32
Total batch reconstruction loss: 0.05861897021532059
Training batch 20 / 32
Total batch reconstruction loss: 0.058142319321632385
Training batch 21 / 32
Total batch reconstruction loss: 0.05416441708803177
Training batch 22 / 32
Total batch reconstruction loss: 0.06153874471783638
Training batch 23 / 32
Total batch reconstruction loss: 0.057293519377708435
Training batch 24 / 32
Total batch reconstruction loss: 0.05496761575341225
Training batch 25 / 32
Total batch reconstruction loss: 0.060545776039361954
Training batch 26 / 32
Total batch reconstruction loss: 0.060282934457063675
Training batch 27 / 32
Total batch reconstruction loss: 0.05846428498625755
Training batch 28 / 32
Total batch reconstruction loss: 0.054372429847717285
Training batch 29 / 32
Total batch reconstruction loss: 0.06180303543806076
Training batch 30 / 32
Total batch reconstruction loss: 0.060326144099235535
Training batch 31 / 32
Total batch reconstruction loss: 0.05301061272621155
Training batch 32 / 32
Total batch reconstruction loss: 0.059131793677806854
Epoch [451/500], Train Loss: 0.0562, Validation Loss: 0.0574, Generator Loss: 11.7401, Discriminator Loss: 0.3050
Training epoch 452 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.056654781103134155
Training batch 2 / 32
Total batch reconstruction loss: 0.05345989763736725
Training batch 3 / 32
Total batch reconstruction loss: 0.06068969517946243
Training batch 4 / 32
Total batch reconstruction loss: 0.05703255534172058
Training batch 5 / 32
Total batch reconstruction loss: 0.05730324238538742
Training batch 6 / 32
Total batch reconstruction loss: 0.06026701629161835
Training batch 7 / 32
Total batch reconstruction loss: 0.058675069361925125
Training batch 8 / 32
Total batch reconstruction loss: 0.06167106702923775
Training batch 9 / 32
Total batch reconstruction loss: 0.059230200946331024
Training batch 10 / 32
Total batch reconstruction loss: 0.06472564488649368
Training batch 11 / 32
Total batch reconstruction loss: 0.05388754606246948
Training batch 12 / 32
Total batch reconstruction loss: 0.056493788957595825
Training batch 13 / 32
Total batch reconstruction loss: 0.05659085512161255
Training batch 14 / 32
Total batch reconstruction loss: 0.056549206376075745
Training batch 15 / 32
Total batch reconstruction loss: 0.05636997148394585
Training batch 16 / 32
Total batch reconstruction loss: 0.06271272897720337
Training batch 17 / 32
Total batch reconstruction loss: 0.05792814493179321
Training batch 18 / 32
Total batch reconstruction loss: 0.06102752685546875
Training batch 19 / 32
Total batch reconstruction loss: 0.05530475825071335
Training batch 20 / 32
Total batch reconstruction loss: 0.06359221041202545
Training batch 21 / 32
Total batch reconstruction loss: 0.05704515427350998
Training batch 22 / 32
Total batch reconstruction loss: 0.056077346205711365
Training batch 23 / 32
Total batch reconstruction loss: 0.05889327824115753
Training batch 24 / 32
Total batch reconstruction loss: 0.05998464673757553
Training batch 25 / 32
Total batch reconstruction loss: 0.05768687278032303
Training batch 26 / 32
Total batch reconstruction loss: 0.05877811834216118
Training batch 27 / 32
Total batch reconstruction loss: 0.058838605880737305
Training batch 28 / 32
Total batch reconstruction loss: 0.054727330803871155
Training batch 29 / 32
Total batch reconstruction loss: 0.0634591355919838
Training batch 30 / 32
Total batch reconstruction loss: 0.05937957763671875
Training batch 31 / 32
Total batch reconstruction loss: 0.057449884712696075
Training batch 32 / 32
Total batch reconstruction loss: 0.06023358926177025
Epoch [452/500], Train Loss: 0.0567, Validation Loss: 0.0564, Generator Loss: 11.7716, Discriminator Loss: 0.3174
Training epoch 453 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.0593150332570076
Training batch 2 / 32
Total batch reconstruction loss: 0.056072212755680084
Training batch 3 / 32
Total batch reconstruction loss: 0.05839523673057556
Training batch 4 / 32
Total batch reconstruction loss: 0.05948162078857422
Training batch 5 / 32
Total batch reconstruction loss: 0.06513285636901855
Training batch 6 / 32
Total batch reconstruction loss: 0.05851828306913376
Training batch 7 / 32
Total batch reconstruction loss: 0.05878932401537895
Training batch 8 / 32
Total batch reconstruction loss: 0.057275496423244476
Training batch 9 / 32
Total batch reconstruction loss: 0.060584984719753265
Training batch 10 / 32
Total batch reconstruction loss: 0.06303326785564423
Training batch 11 / 32
Total batch reconstruction loss: 0.0602521151304245
Training batch 12 / 32
Total batch reconstruction loss: 0.060928404331207275
Training batch 13 / 32
Total batch reconstruction loss: 0.060920316725969315
Training batch 14 / 32
Total batch reconstruction loss: 0.05885493755340576
Training batch 15 / 32
Total batch reconstruction loss: 0.05991435796022415
Training batch 16 / 32
Total batch reconstruction loss: 0.05251793563365936
Training batch 17 / 32
Total batch reconstruction loss: 0.058363839983940125
Training batch 18 / 32
Total batch reconstruction loss: 0.057371363043785095
Training batch 19 / 32
Total batch reconstruction loss: 0.05707462131977081
Training batch 20 / 32
Total batch reconstruction loss: 0.05697799474000931
Training batch 21 / 32
Total batch reconstruction loss: 0.05609738826751709
Training batch 22 / 32
Total batch reconstruction loss: 0.05519410967826843
Training batch 23 / 32
Total batch reconstruction loss: 0.05541810020804405
Training batch 24 / 32
Total batch reconstruction loss: 0.05486026033759117
Training batch 25 / 32
Total batch reconstruction loss: 0.057276178151369095
Training batch 26 / 32
Total batch reconstruction loss: 0.059147026389837265
Training batch 27 / 32
Total batch reconstruction loss: 0.05873584374785423
Training batch 28 / 32
Total batch reconstruction loss: 0.0632367879152298
Training batch 29 / 32
Total batch reconstruction loss: 0.06049279868602753
Training batch 30 / 32
Total batch reconstruction loss: 0.05436791479587555
Training batch 31 / 32
Total batch reconstruction loss: 0.0578688383102417
Training batch 32 / 32
Total batch reconstruction loss: 0.07037530839443207
Epoch [453/500], Train Loss: 0.0570, Validation Loss: 0.0574, Generator Loss: 11.8361, Discriminator Loss: 0.3170
Training epoch 454 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.0593801774084568
Training batch 2 / 32
Total batch reconstruction loss: 0.06050017848610878
Training batch 3 / 32
Total batch reconstruction loss: 0.060823727399110794
Training batch 4 / 32
Total batch reconstruction loss: 0.05966278910636902
Training batch 5 / 32
Total batch reconstruction loss: 0.05581619590520859
Training batch 6 / 32
Total batch reconstruction loss: 0.060935311019420624
Training batch 7 / 32
Total batch reconstruction loss: 0.05672789365053177
Training batch 8 / 32
Total batch reconstruction loss: 0.05498122051358223
Training batch 9 / 32
Total batch reconstruction loss: 0.0651121586561203
Training batch 10 / 32
Total batch reconstruction loss: 0.06148117780685425
Training batch 11 / 32
Total batch reconstruction loss: 0.05731507018208504
Training batch 12 / 32
Total batch reconstruction loss: 0.05899704247713089
Training batch 13 / 32
Total batch reconstruction loss: 0.06044001877307892
Training batch 14 / 32
Total batch reconstruction loss: 0.05454324185848236
Training batch 15 / 32
Total batch reconstruction loss: 0.058438174426555634
Training batch 16 / 32
Total batch reconstruction loss: 0.05719675123691559
Training batch 17 / 32
Total batch reconstruction loss: 0.06124795228242874
Training batch 18 / 32
Total batch reconstruction loss: 0.06197600066661835
Training batch 19 / 32
Total batch reconstruction loss: 0.052408747375011444
Training batch 20 / 32
Total batch reconstruction loss: 0.06411022692918777
Training batch 21 / 32
Total batch reconstruction loss: 0.059373389929533005
Training batch 22 / 32
Total batch reconstruction loss: 0.05520347133278847
Training batch 23 / 32
Total batch reconstruction loss: 0.05573327839374542
Training batch 24 / 32
Total batch reconstruction loss: 0.060336772352457047
Training batch 25 / 32
Total batch reconstruction loss: 0.05699112266302109
Training batch 26 / 32
Total batch reconstruction loss: 0.05720420926809311
Training batch 27 / 32
Total batch reconstruction loss: 0.05748898163437843
Training batch 28 / 32
Total batch reconstruction loss: 0.057284481823444366
Training batch 29 / 32
Total batch reconstruction loss: 0.05985917150974274
Training batch 30 / 32
Total batch reconstruction loss: 0.05568664148449898
Training batch 31 / 32
Total batch reconstruction loss: 0.059577636420726776
Training batch 32 / 32
Total batch reconstruction loss: 0.04942198097705841
Epoch [454/500], Train Loss: 0.0563, Validation Loss: 0.0566, Generator Loss: 11.7331, Discriminator Loss: 0.3095
Training epoch 455 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.06155882775783539
Training batch 2 / 32
Total batch reconstruction loss: 0.06199440732598305
Training batch 3 / 32
Total batch reconstruction loss: 0.06159059703350067
Training batch 4 / 32
Total batch reconstruction loss: 0.06318499147891998
Training batch 5 / 32
Total batch reconstruction loss: 0.05979366600513458
Training batch 6 / 32
Total batch reconstruction loss: 0.06320901215076447
Training batch 7 / 32
Total batch reconstruction loss: 0.05895385891199112
Training batch 8 / 32
Total batch reconstruction loss: 0.05468801409006119
Training batch 9 / 32
Total batch reconstruction loss: 0.05965610593557358
Training batch 10 / 32
Total batch reconstruction loss: 0.054945968091487885
Training batch 11 / 32
Total batch reconstruction loss: 0.053624141961336136
Training batch 12 / 32
Total batch reconstruction loss: 0.05873614549636841
Training batch 13 / 32
Total batch reconstruction loss: 0.05961119011044502
Training batch 14 / 32
Total batch reconstruction loss: 0.05860048905014992
Training batch 15 / 32
Total batch reconstruction loss: 0.05776715278625488
Training batch 16 / 32
Total batch reconstruction loss: 0.05696455016732216
Training batch 17 / 32
Total batch reconstruction loss: 0.060330674052238464
Training batch 18 / 32
Total batch reconstruction loss: 0.05875919759273529
Training batch 19 / 32
Total batch reconstruction loss: 0.05632321536540985
Training batch 20 / 32
Total batch reconstruction loss: 0.05952493101358414
Training batch 21 / 32
Total batch reconstruction loss: 0.057332463562488556
Training batch 22 / 32
Total batch reconstruction loss: 0.05673578754067421
Training batch 23 / 32
Total batch reconstruction loss: 0.05818122252821922
Training batch 24 / 32
Total batch reconstruction loss: 0.054241858422756195
Training batch 25 / 32
Total batch reconstruction loss: 0.05795295536518097
Training batch 26 / 32
Total batch reconstruction loss: 0.05559783801436424
Training batch 27 / 32
Total batch reconstruction loss: 0.057323068380355835
Training batch 28 / 32
Total batch reconstruction loss: 0.05817577242851257
Training batch 29 / 32
Total batch reconstruction loss: 0.05871548503637314
Training batch 30 / 32
Total batch reconstruction loss: 0.05559655278921127
Training batch 31 / 32
Total batch reconstruction loss: 0.057696763426065445
Training batch 32 / 32
Total batch reconstruction loss: 0.05904107540845871
Epoch [455/500], Train Loss: 0.0565, Validation Loss: 0.0567, Generator Loss: 11.7282, Discriminator Loss: 0.3229
Training epoch 456 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.05765961855649948
Training batch 2 / 32
Total batch reconstruction loss: 0.05904082953929901
Training batch 3 / 32
Total batch reconstruction loss: 0.059564702212810516
Training batch 4 / 32
Total batch reconstruction loss: 0.057777732610702515
Training batch 5 / 32
Total batch reconstruction loss: 0.05845596641302109
Training batch 6 / 32
Total batch reconstruction loss: 0.05497216433286667
Training batch 7 / 32
Total batch reconstruction loss: 0.054068390280008316
Training batch 8 / 32
Total batch reconstruction loss: 0.057299546897411346
Training batch 9 / 32
Total batch reconstruction loss: 0.059082452207803726
Training batch 10 / 32
Total batch reconstruction loss: 0.0579649917781353
Training batch 11 / 32
Total batch reconstruction loss: 0.05768510699272156
Training batch 12 / 32
Total batch reconstruction loss: 0.062241896986961365
Training batch 13 / 32
Total batch reconstruction loss: 0.05641831457614899
Training batch 14 / 32
Total batch reconstruction loss: 0.06012676656246185
Training batch 15 / 32
Total batch reconstruction loss: 0.05558351054787636
Training batch 16 / 32
Total batch reconstruction loss: 0.06259238719940186
Training batch 17 / 32
Total batch reconstruction loss: 0.05813029035925865
Training batch 18 / 32
Total batch reconstruction loss: 0.06189984828233719
Training batch 19 / 32
Total batch reconstruction loss: 0.058173857629299164
Training batch 20 / 32
Total batch reconstruction loss: 0.05344642698764801
Training batch 21 / 32
Total batch reconstruction loss: 0.056469470262527466
Training batch 22 / 32
Total batch reconstruction loss: 0.059894390404224396
Training batch 23 / 32
Total batch reconstruction loss: 0.056384652853012085
Training batch 24 / 32
Total batch reconstruction loss: 0.05901474133133888
Training batch 25 / 32
Total batch reconstruction loss: 0.05638901889324188
Training batch 26 / 32
Total batch reconstruction loss: 0.05872751772403717
Training batch 27 / 32
Total batch reconstruction loss: 0.05745379254221916
Training batch 28 / 32
Total batch reconstruction loss: 0.060139868408441544
Training batch 29 / 32
Total batch reconstruction loss: 0.0531599298119545
Training batch 30 / 32
Total batch reconstruction loss: 0.05928244814276695
Training batch 31 / 32
Total batch reconstruction loss: 0.059146903455257416
Training batch 32 / 32
Total batch reconstruction loss: 0.0634508803486824
Epoch [456/500], Train Loss: 0.0562, Validation Loss: 0.0569, Generator Loss: 11.7011, Discriminator Loss: 0.3234
Training epoch 457 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.05596955865621567
Training batch 2 / 32
Total batch reconstruction loss: 0.05749814957380295
Training batch 3 / 32
Total batch reconstruction loss: 0.05795661732554436
Training batch 4 / 32
Total batch reconstruction loss: 0.055413469672203064
Training batch 5 / 32
Total batch reconstruction loss: 0.06174084171652794
Training batch 6 / 32
Total batch reconstruction loss: 0.06206454336643219
Training batch 7 / 32
Total batch reconstruction loss: 0.060913149267435074
Training batch 8 / 32
Total batch reconstruction loss: 0.06005031615495682
Training batch 9 / 32
Total batch reconstruction loss: 0.05967871844768524
Training batch 10 / 32
Total batch reconstruction loss: 0.059060513973236084
Training batch 11 / 32
Total batch reconstruction loss: 0.05829457566142082
Training batch 12 / 32
Total batch reconstruction loss: 0.058027200400829315
Training batch 13 / 32
Total batch reconstruction loss: 0.056284256279468536
Training batch 14 / 32
Total batch reconstruction loss: 0.05851277709007263
Training batch 15 / 32
Total batch reconstruction loss: 0.05899330973625183
Training batch 16 / 32
Total batch reconstruction loss: 0.06025322526693344
Training batch 17 / 32
Total batch reconstruction loss: 0.05440271645784378
Training batch 18 / 32
Total batch reconstruction loss: 0.05776717886328697
Training batch 19 / 32
Total batch reconstruction loss: 0.05775342136621475
Training batch 20 / 32
Total batch reconstruction loss: 0.057223156094551086
Training batch 21 / 32
Total batch reconstruction loss: 0.056683603674173355
Training batch 22 / 32
Total batch reconstruction loss: 0.05359013378620148
Training batch 23 / 32
Total batch reconstruction loss: 0.06057290732860565
Training batch 24 / 32
Total batch reconstruction loss: 0.0573350265622139
Training batch 25 / 32
Total batch reconstruction loss: 0.05679800733923912
Training batch 26 / 32
Total batch reconstruction loss: 0.06197106093168259
Training batch 27 / 32
Total batch reconstruction loss: 0.0586298443377018
Training batch 28 / 32
Total batch reconstruction loss: 0.058163367211818695
Training batch 29 / 32
Total batch reconstruction loss: 0.055370815098285675
Training batch 30 / 32
Total batch reconstruction loss: 0.057273976504802704
Training batch 31 / 32
Total batch reconstruction loss: 0.05793614313006401
Training batch 32 / 32
Total batch reconstruction loss: 0.057452987879514694
Epoch [457/500], Train Loss: 0.0562, Validation Loss: 0.0579, Generator Loss: 11.6890, Discriminator Loss: 0.3150
Training epoch 458 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.06221064180135727
Training batch 2 / 32
Total batch reconstruction loss: 0.055661220103502274
Training batch 3 / 32
Total batch reconstruction loss: 0.05600482597947121
Training batch 4 / 32
Total batch reconstruction loss: 0.059663694351911545
Training batch 5 / 32
Total batch reconstruction loss: 0.05574547499418259
Training batch 6 / 32
Total batch reconstruction loss: 0.05825376510620117
Training batch 7 / 32
Total batch reconstruction loss: 0.054895177483558655
Training batch 8 / 32
Total batch reconstruction loss: 0.060141392052173615
Training batch 9 / 32
Total batch reconstruction loss: 0.057851001620292664
Training batch 10 / 32
Total batch reconstruction loss: 0.05724988132715225
Training batch 11 / 32
Total batch reconstruction loss: 0.058204472064971924
Training batch 12 / 32
Total batch reconstruction loss: 0.05790122598409653
Training batch 13 / 32
Total batch reconstruction loss: 0.0546879842877388
Training batch 14 / 32
Total batch reconstruction loss: 0.05672503635287285
Training batch 15 / 32
Total batch reconstruction loss: 0.059757299721241
Training batch 16 / 32
Total batch reconstruction loss: 0.06398162245750427
Training batch 17 / 32
Total batch reconstruction loss: 0.05633552744984627
Training batch 18 / 32
Total batch reconstruction loss: 0.05759651958942413
Training batch 19 / 32
Total batch reconstruction loss: 0.05877896025776863
Training batch 20 / 32
Total batch reconstruction loss: 0.0582665279507637
Training batch 21 / 32
Total batch reconstruction loss: 0.05728263780474663
Training batch 22 / 32
Total batch reconstruction loss: 0.06027214229106903
Training batch 23 / 32
Total batch reconstruction loss: 0.06149791181087494
Training batch 24 / 32
Total batch reconstruction loss: 0.05571771040558815
Training batch 25 / 32
Total batch reconstruction loss: 0.05748852342367172
Training batch 26 / 32
Total batch reconstruction loss: 0.05419103056192398
Training batch 27 / 32
Total batch reconstruction loss: 0.05760914087295532
Training batch 28 / 32
Total batch reconstruction loss: 0.05859242007136345
Training batch 29 / 32
Total batch reconstruction loss: 0.05954282730817795
Training batch 30 / 32
Total batch reconstruction loss: 0.05974368378520012
Training batch 31 / 32
Total batch reconstruction loss: 0.05346420407295227
Training batch 32 / 32
Total batch reconstruction loss: 0.058444876223802567
Epoch [458/500], Train Loss: 0.0560, Validation Loss: 0.0569, Generator Loss: 11.6604, Discriminator Loss: 0.3100
Training epoch 459 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.055077895522117615
Training batch 2 / 32
Total batch reconstruction loss: 0.05971468240022659
Training batch 3 / 32
Total batch reconstruction loss: 0.05832550674676895
Training batch 4 / 32
Total batch reconstruction loss: 0.06013360619544983
Training batch 5 / 32
Total batch reconstruction loss: 0.05543813854455948
Training batch 6 / 32
Total batch reconstruction loss: 0.05901171639561653
Training batch 7 / 32
Total batch reconstruction loss: 0.05792776867747307
Training batch 8 / 32
Total batch reconstruction loss: 0.06079573184251785
Training batch 9 / 32
Total batch reconstruction loss: 0.06058276444673538
Training batch 10 / 32
Total batch reconstruction loss: 0.057100385427474976
Training batch 11 / 32
Total batch reconstruction loss: 0.05679637938737869
Training batch 12 / 32
Total batch reconstruction loss: 0.06254850327968597
Training batch 13 / 32
Total batch reconstruction loss: 0.058930039405822754
Training batch 14 / 32
Total batch reconstruction loss: 0.05795585364103317
Training batch 15 / 32
Total batch reconstruction loss: 0.0645311251282692
Training batch 16 / 32
Total batch reconstruction loss: 0.05504555627703667
Training batch 17 / 32
Total batch reconstruction loss: 0.061086080968379974
Training batch 18 / 32
Total batch reconstruction loss: 0.05579783022403717
Training batch 19 / 32
Total batch reconstruction loss: 0.05872310698032379
Training batch 20 / 32
Total batch reconstruction loss: 0.056779153645038605
Training batch 21 / 32
Total batch reconstruction loss: 0.058456048369407654
Training batch 22 / 32
Total batch reconstruction loss: 0.05620148777961731
Training batch 23 / 32
Total batch reconstruction loss: 0.054825421422719955
Training batch 24 / 32
Total batch reconstruction loss: 0.05795344337821007
Training batch 25 / 32
Total batch reconstruction loss: 0.05474954843521118
Training batch 26 / 32
Total batch reconstruction loss: 0.05858466029167175
Training batch 27 / 32
Total batch reconstruction loss: 0.05798843130469322
Training batch 28 / 32
Total batch reconstruction loss: 0.05667318403720856
Training batch 29 / 32
Total batch reconstruction loss: 0.05324225872755051
Training batch 30 / 32
Total batch reconstruction loss: 0.05610412359237671
Training batch 31 / 32
Total batch reconstruction loss: 0.059512510895729065
Training batch 32 / 32
Total batch reconstruction loss: 0.059206001460552216
Epoch [459/500], Train Loss: 0.0558, Validation Loss: 0.0568, Generator Loss: 11.6690, Discriminator Loss: 0.3152
Training epoch 460 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.05492255836725235
Training batch 2 / 32
Total batch reconstruction loss: 0.05931072309613228
Training batch 3 / 32
Total batch reconstruction loss: 0.05459418520331383
Training batch 4 / 32
Total batch reconstruction loss: 0.05565459281206131
Training batch 5 / 32
Total batch reconstruction loss: 0.06502150744199753
Training batch 6 / 32
Total batch reconstruction loss: 0.056378766894340515
Training batch 7 / 32
Total batch reconstruction loss: 0.05529167130589485
Training batch 8 / 32
Total batch reconstruction loss: 0.058337777853012085
Training batch 9 / 32
Total batch reconstruction loss: 0.05758307874202728
Training batch 10 / 32
Total batch reconstruction loss: 0.060611702501773834
Training batch 11 / 32
Total batch reconstruction loss: 0.05978342145681381
Training batch 12 / 32
Total batch reconstruction loss: 0.05578934773802757
Training batch 13 / 32
Total batch reconstruction loss: 0.06011807173490524
Training batch 14 / 32
Total batch reconstruction loss: 0.059754595160484314
Training batch 15 / 32
Total batch reconstruction loss: 0.05698457360267639
Training batch 16 / 32
Total batch reconstruction loss: 0.05483294278383255
Training batch 17 / 32
Total batch reconstruction loss: 0.060025282204151154
Training batch 18 / 32
Total batch reconstruction loss: 0.06019052490592003
Training batch 19 / 32
Total batch reconstruction loss: 0.059335991740226746
Training batch 20 / 32
Total batch reconstruction loss: 0.06045302376151085
Training batch 21 / 32
Total batch reconstruction loss: 0.05658671259880066
Training batch 22 / 32
Total batch reconstruction loss: 0.05991934612393379
Training batch 23 / 32
Total batch reconstruction loss: 0.05645400285720825
Training batch 24 / 32
Total batch reconstruction loss: 0.06267622858285904
Training batch 25 / 32
Total batch reconstruction loss: 0.058038510382175446
Training batch 26 / 32
Total batch reconstruction loss: 0.05569097772240639
Training batch 27 / 32
Total batch reconstruction loss: 0.0539373904466629
Training batch 28 / 32
Total batch reconstruction loss: 0.05791904777288437
Training batch 29 / 32
Total batch reconstruction loss: 0.06021025776863098
Training batch 30 / 32
Total batch reconstruction loss: 0.057041607797145844
Training batch 31 / 32
Total batch reconstruction loss: 0.05911901593208313
Training batch 32 / 32
Total batch reconstruction loss: 0.05337899550795555
Epoch [460/500], Train Loss: 0.0561, Validation Loss: 0.0570, Generator Loss: 11.6648, Discriminator Loss: 0.3213
Training epoch 461 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.055159907788038254
Training batch 2 / 32
Total batch reconstruction loss: 0.05586832016706467
Training batch 3 / 32
Total batch reconstruction loss: 0.05780598521232605
Training batch 4 / 32
Total batch reconstruction loss: 0.06273174285888672
Training batch 5 / 32
Total batch reconstruction loss: 0.05680897831916809
Training batch 6 / 32
Total batch reconstruction loss: 0.05727238953113556
Training batch 7 / 32
Total batch reconstruction loss: 0.05551038682460785
Training batch 8 / 32
Total batch reconstruction loss: 0.05840854346752167
Training batch 9 / 32
Total batch reconstruction loss: 0.057395730167627335
Training batch 10 / 32
Total batch reconstruction loss: 0.05637737363576889
Training batch 11 / 32
Total batch reconstruction loss: 0.059339605271816254
Training batch 12 / 32
Total batch reconstruction loss: 0.055430494248867035
Training batch 13 / 32
Total batch reconstruction loss: 0.061894625425338745
Training batch 14 / 32
Total batch reconstruction loss: 0.05990888550877571
Training batch 15 / 32
Total batch reconstruction loss: 0.05740704387426376
Training batch 16 / 32
Total batch reconstruction loss: 0.058763712644577026
Training batch 17 / 32
Total batch reconstruction loss: 0.05751406028866768
Training batch 18 / 32
Total batch reconstruction loss: 0.05944894626736641
Training batch 19 / 32
Total batch reconstruction loss: 0.05605227127671242
Training batch 20 / 32
Total batch reconstruction loss: 0.053485557436943054
Training batch 21 / 32
Total batch reconstruction loss: 0.056561462581157684
Training batch 22 / 32
Total batch reconstruction loss: 0.06155424937605858
Training batch 23 / 32
Total batch reconstruction loss: 0.058958157896995544
Training batch 24 / 32
Total batch reconstruction loss: 0.061736464500427246
Training batch 25 / 32
Total batch reconstruction loss: 0.0566139817237854
Training batch 26 / 32
Total batch reconstruction loss: 0.060271091759204865
Training batch 27 / 32
Total batch reconstruction loss: 0.054867662489414215
Training batch 28 / 32
Total batch reconstruction loss: 0.058022599667310715
Training batch 29 / 32
Total batch reconstruction loss: 0.05661214143037796
Training batch 30 / 32
Total batch reconstruction loss: 0.0596998855471611
Training batch 31 / 32
Total batch reconstruction loss: 0.0568220280110836
Training batch 32 / 32
Total batch reconstruction loss: 0.04957301542162895
Epoch [461/500], Train Loss: 0.0556, Validation Loss: 0.0563, Generator Loss: 11.5877, Discriminator Loss: 0.3221
Training epoch 462 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.05973352864384651
Training batch 2 / 32
Total batch reconstruction loss: 0.05738450586795807
Training batch 3 / 32
Total batch reconstruction loss: 0.05809595435857773
Training batch 4 / 32
Total batch reconstruction loss: 0.06078319996595383
Training batch 5 / 32
Total batch reconstruction loss: 0.05532795935869217
Training batch 6 / 32
Total batch reconstruction loss: 0.05679246038198471
Training batch 7 / 32
Total batch reconstruction loss: 0.060239277780056
Training batch 8 / 32
Total batch reconstruction loss: 0.05735306814312935
Training batch 9 / 32
Total batch reconstruction loss: 0.05871798098087311
Training batch 10 / 32
Total batch reconstruction loss: 0.06508355587720871
Training batch 11 / 32
Total batch reconstruction loss: 0.056663576513528824
Training batch 12 / 32
Total batch reconstruction loss: 0.06070654094219208
Training batch 13 / 32
Total batch reconstruction loss: 0.06295157968997955
Training batch 14 / 32
Total batch reconstruction loss: 0.058149304240942
Training batch 15 / 32
Total batch reconstruction loss: 0.05841962248086929
Training batch 16 / 32
Total batch reconstruction loss: 0.06465709209442139
Training batch 17 / 32
Total batch reconstruction loss: 0.061999525874853134
Training batch 18 / 32
Total batch reconstruction loss: 0.055874038487672806
Training batch 19 / 32
Total batch reconstruction loss: 0.05799476057291031
Training batch 20 / 32
Total batch reconstruction loss: 0.055605292320251465
Training batch 21 / 32
Total batch reconstruction loss: 0.05290607362985611
Training batch 22 / 32
Total batch reconstruction loss: 0.05178208649158478
Training batch 23 / 32
Total batch reconstruction loss: 0.05957454442977905
Training batch 24 / 32
Total batch reconstruction loss: 0.05520957335829735
Training batch 25 / 32
Total batch reconstruction loss: 0.058045364916324615
Training batch 26 / 32
Total batch reconstruction loss: 0.05659385025501251
Training batch 27 / 32
Total batch reconstruction loss: 0.05190041661262512
Training batch 28 / 32
Total batch reconstruction loss: 0.057799071073532104
Training batch 29 / 32
Total batch reconstruction loss: 0.056366611272096634
Training batch 30 / 32
Total batch reconstruction loss: 0.05779923498630524
Training batch 31 / 32
Total batch reconstruction loss: 0.05806241184473038
Training batch 32 / 32
Total batch reconstruction loss: 0.051875658333301544
Epoch [462/500], Train Loss: 0.0558, Validation Loss: 0.0575, Generator Loss: 11.6295, Discriminator Loss: 0.3277
Training epoch 463 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.0576690174639225
Training batch 2 / 32
Total batch reconstruction loss: 0.05704724043607712
Training batch 3 / 32
Total batch reconstruction loss: 0.056638747453689575
Training batch 4 / 32
Total batch reconstruction loss: 0.05763183534145355
Training batch 5 / 32
Total batch reconstruction loss: 0.05915318429470062
Training batch 6 / 32
Total batch reconstruction loss: 0.05882596969604492
Training batch 7 / 32
Total batch reconstruction loss: 0.06450963020324707
Training batch 8 / 32
Total batch reconstruction loss: 0.05710504204034805
Training batch 9 / 32
Total batch reconstruction loss: 0.05928080901503563
Training batch 10 / 32
Total batch reconstruction loss: 0.057751890271902084
Training batch 11 / 32
Total batch reconstruction loss: 0.059433069080114365
Training batch 12 / 32
Total batch reconstruction loss: 0.05758853256702423
Training batch 13 / 32
Total batch reconstruction loss: 0.05856858566403389
Training batch 14 / 32
Total batch reconstruction loss: 0.06053128466010094
Training batch 15 / 32
Total batch reconstruction loss: 0.056280240416526794
Training batch 16 / 32
Total batch reconstruction loss: 0.06103186681866646
Training batch 17 / 32
Total batch reconstruction loss: 0.060696132481098175
Training batch 18 / 32
Total batch reconstruction loss: 0.059757083654403687
Training batch 19 / 32
Total batch reconstruction loss: 0.055066291242837906
Training batch 20 / 32
Total batch reconstruction loss: 0.059639912098646164
Training batch 21 / 32
Total batch reconstruction loss: 0.055205728858709335
Training batch 22 / 32
Total batch reconstruction loss: 0.0549008883535862
Training batch 23 / 32
Total batch reconstruction loss: 0.05916082486510277
Training batch 24 / 32
Total batch reconstruction loss: 0.059068143367767334
Training batch 25 / 32
Total batch reconstruction loss: 0.0647534504532814
Training batch 26 / 32
Total batch reconstruction loss: 0.05298586189746857
Training batch 27 / 32
Total batch reconstruction loss: 0.05770079791545868
Training batch 28 / 32
Total batch reconstruction loss: 0.05671054497361183
Training batch 29 / 32
Total batch reconstruction loss: 0.05482437461614609
Training batch 30 / 32
Total batch reconstruction loss: 0.055303845554590225
Training batch 31 / 32
Total batch reconstruction loss: 0.06185218691825867
Training batch 32 / 32
Total batch reconstruction loss: 0.05449635162949562
Epoch [463/500], Train Loss: 0.0562, Validation Loss: 0.0567, Generator Loss: 11.6995, Discriminator Loss: 0.3202
Training epoch 464 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.055721379816532135
Training batch 2 / 32
Total batch reconstruction loss: 0.06079915165901184
Training batch 3 / 32
Total batch reconstruction loss: 0.06171279773116112
Training batch 4 / 32
Total batch reconstruction loss: 0.05557208135724068
Training batch 5 / 32
Total batch reconstruction loss: 0.06255175173282623
Training batch 6 / 32
Total batch reconstruction loss: 0.058159179985523224
Training batch 7 / 32
Total batch reconstruction loss: 0.05904792994260788
Training batch 8 / 32
Total batch reconstruction loss: 0.0563410259783268
Training batch 9 / 32
Total batch reconstruction loss: 0.055713996291160583
Training batch 10 / 32
Total batch reconstruction loss: 0.0617702379822731
Training batch 11 / 32
Total batch reconstruction loss: 0.06380598247051239
Training batch 12 / 32
Total batch reconstruction loss: 0.058337628841400146
Training batch 13 / 32
Total batch reconstruction loss: 0.056630559265613556
Training batch 14 / 32
Total batch reconstruction loss: 0.05935089290142059
Training batch 15 / 32
Total batch reconstruction loss: 0.05689052492380142
Training batch 16 / 32
Total batch reconstruction loss: 0.060301803052425385
Training batch 17 / 32
Total batch reconstruction loss: 0.05418822169303894
Training batch 18 / 32
Total batch reconstruction loss: 0.05604885146021843
Training batch 19 / 32
Total batch reconstruction loss: 0.06506013870239258
Training batch 20 / 32
Total batch reconstruction loss: 0.05404895171523094
Training batch 21 / 32
Total batch reconstruction loss: 0.05982130393385887
Training batch 22 / 32
Total batch reconstruction loss: 0.05967344343662262
Training batch 23 / 32
Total batch reconstruction loss: 0.05836476758122444
Training batch 24 / 32
Total batch reconstruction loss: 0.057133108377456665
Training batch 25 / 32
Total batch reconstruction loss: 0.06108129024505615
Training batch 26 / 32
Total batch reconstruction loss: 0.05792934447526932
Training batch 27 / 32
Total batch reconstruction loss: 0.05643712729215622
Training batch 28 / 32
Total batch reconstruction loss: 0.05882559344172478
Training batch 29 / 32
Total batch reconstruction loss: 0.061409033834934235
Training batch 30 / 32
Total batch reconstruction loss: 0.058142200112342834
Training batch 31 / 32
Total batch reconstruction loss: 0.056397873908281326
Training batch 32 / 32
Total batch reconstruction loss: 0.048775557428598404
Epoch [464/500], Train Loss: 0.0564, Validation Loss: 0.0574, Generator Loss: 11.7319, Discriminator Loss: 0.3045
Training epoch 465 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.06115856021642685
Training batch 2 / 32
Total batch reconstruction loss: 0.05632156878709793
Training batch 3 / 32
Total batch reconstruction loss: 0.059810951352119446
Training batch 4 / 32
Total batch reconstruction loss: 0.05917252600193024
Training batch 5 / 32
Total batch reconstruction loss: 0.052823975682258606
Training batch 6 / 32
Total batch reconstruction loss: 0.0587061271071434
Training batch 7 / 32
Total batch reconstruction loss: 0.06034744530916214
Training batch 8 / 32
Total batch reconstruction loss: 0.058810096234083176
Training batch 9 / 32
Total batch reconstruction loss: 0.057244427502155304
Training batch 10 / 32
Total batch reconstruction loss: 0.05579546093940735
Training batch 11 / 32
Total batch reconstruction loss: 0.058627642691135406
Training batch 12 / 32
Total batch reconstruction loss: 0.05516878515481949
Training batch 13 / 32
Total batch reconstruction loss: 0.05651606246829033
Training batch 14 / 32
Total batch reconstruction loss: 0.06394664943218231
Training batch 15 / 32
Total batch reconstruction loss: 0.06345885246992111
Training batch 16 / 32
Total batch reconstruction loss: 0.05936107784509659
Training batch 17 / 32
Total batch reconstruction loss: 0.06080533564090729
Training batch 18 / 32
Total batch reconstruction loss: 0.05793797969818115
Training batch 19 / 32
Total batch reconstruction loss: 0.06041020527482033
Training batch 20 / 32
Total batch reconstruction loss: 0.055935539305210114
Training batch 21 / 32
Total batch reconstruction loss: 0.05631694942712784
Training batch 22 / 32
Total batch reconstruction loss: 0.05891982465982437
Training batch 23 / 32
Total batch reconstruction loss: 0.05662011355161667
Training batch 24 / 32
Total batch reconstruction loss: 0.060160037130117416
Training batch 25 / 32
Total batch reconstruction loss: 0.05530750751495361
Training batch 26 / 32
Total batch reconstruction loss: 0.0596344880759716
Training batch 27 / 32
Total batch reconstruction loss: 0.05880146846175194
Training batch 28 / 32
Total batch reconstruction loss: 0.06242334842681885
Training batch 29 / 32
Total batch reconstruction loss: 0.06269039213657379
Training batch 30 / 32
Total batch reconstruction loss: 0.057236917316913605
Training batch 31 / 32
Total batch reconstruction loss: 0.055770933628082275
Training batch 32 / 32
Total batch reconstruction loss: 0.11165088415145874
Epoch [465/500], Train Loss: 0.0585, Validation Loss: 0.0569, Generator Loss: 12.1136, Discriminator Loss: 0.3203
Training epoch 466 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.0626506358385086
Training batch 2 / 32
Total batch reconstruction loss: 0.057861052453517914
Training batch 3 / 32
Total batch reconstruction loss: 0.05937027558684349
Training batch 4 / 32
Total batch reconstruction loss: 0.0619850717484951
Training batch 5 / 32
Total batch reconstruction loss: 0.059409111738204956
Training batch 6 / 32
Total batch reconstruction loss: 0.06030190736055374
Training batch 7 / 32
Total batch reconstruction loss: 0.05965261161327362
Training batch 8 / 32
Total batch reconstruction loss: 0.0556403212249279
Training batch 9 / 32
Total batch reconstruction loss: 0.060039885342121124
Training batch 10 / 32
Total batch reconstruction loss: 0.054915718734264374
Training batch 11 / 32
Total batch reconstruction loss: 0.053310882300138474
Training batch 12 / 32
Total batch reconstruction loss: 0.06295748054981232
Training batch 13 / 32
Total batch reconstruction loss: 0.05780245363712311
Training batch 14 / 32
Total batch reconstruction loss: 0.060340337455272675
Training batch 15 / 32
Total batch reconstruction loss: 0.05863082781434059
Training batch 16 / 32
Total batch reconstruction loss: 0.05885595455765724
Training batch 17 / 32
Total batch reconstruction loss: 0.05945470184087753
Training batch 18 / 32
Total batch reconstruction loss: 0.058061324059963226
Training batch 19 / 32
Total batch reconstruction loss: 0.05671273544430733
Training batch 20 / 32
Total batch reconstruction loss: 0.055713266134262085
Training batch 21 / 32
Total batch reconstruction loss: 0.05642981082201004
Training batch 22 / 32
Total batch reconstruction loss: 0.058356188237667084
Training batch 23 / 32
Total batch reconstruction loss: 0.05586845055222511
Training batch 24 / 32
Total batch reconstruction loss: 0.05583428218960762
Training batch 25 / 32
Total batch reconstruction loss: 0.060232408344745636
Training batch 26 / 32
Total batch reconstruction loss: 0.0635249987244606
Training batch 27 / 32
Total batch reconstruction loss: 0.05771051347255707
Training batch 28 / 32
Total batch reconstruction loss: 0.06566029042005539
Training batch 29 / 32
Total batch reconstruction loss: 0.059916310012340546
Training batch 30 / 32
Total batch reconstruction loss: 0.05831322446465492
Training batch 31 / 32
Total batch reconstruction loss: 0.055734992027282715
Training batch 32 / 32
Total batch reconstruction loss: 0.05608151853084564
Epoch [466/500], Train Loss: 0.0565, Validation Loss: 0.0571, Generator Loss: 11.8055, Discriminator Loss: 0.3106
Training epoch 467 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.056110091507434845
Training batch 2 / 32
Total batch reconstruction loss: 0.05501743406057358
Training batch 3 / 32
Total batch reconstruction loss: 0.058273494243621826
Training batch 4 / 32
Total batch reconstruction loss: 0.059097059071063995
Training batch 5 / 32
Total batch reconstruction loss: 0.06317950040102005
Training batch 6 / 32
Total batch reconstruction loss: 0.05914665013551712
Training batch 7 / 32
Total batch reconstruction loss: 0.061053793877363205
Training batch 8 / 32
Total batch reconstruction loss: 0.059899140149354935
Training batch 9 / 32
Total batch reconstruction loss: 0.05548427626490593
Training batch 10 / 32
Total batch reconstruction loss: 0.061819177120923996
Training batch 11 / 32
Total batch reconstruction loss: 0.05696715787053108
Training batch 12 / 32
Total batch reconstruction loss: 0.05827169492840767
Training batch 13 / 32
Total batch reconstruction loss: 0.060801126062870026
Training batch 14 / 32
Total batch reconstruction loss: 0.05793792009353638
Training batch 15 / 32
Total batch reconstruction loss: 0.06043156981468201
Training batch 16 / 32
Total batch reconstruction loss: 0.05761662870645523
Training batch 17 / 32
Total batch reconstruction loss: 0.05673660337924957
Training batch 18 / 32
Total batch reconstruction loss: 0.06066890060901642
Training batch 19 / 32
Total batch reconstruction loss: 0.05512958765029907
Training batch 20 / 32
Total batch reconstruction loss: 0.05976599454879761
Training batch 21 / 32
Total batch reconstruction loss: 0.0570296011865139
Training batch 22 / 32
Total batch reconstruction loss: 0.05724051967263222
Training batch 23 / 32
Total batch reconstruction loss: 0.05842368304729462
Training batch 24 / 32
Total batch reconstruction loss: 0.06015701964497566
Training batch 25 / 32
Total batch reconstruction loss: 0.05928097665309906
Training batch 26 / 32
Total batch reconstruction loss: 0.05497349053621292
Training batch 27 / 32
Total batch reconstruction loss: 0.05281753093004227
Training batch 28 / 32
Total batch reconstruction loss: 0.062282826751470566
Training batch 29 / 32
Total batch reconstruction loss: 0.05888466536998749
Training batch 30 / 32
Total batch reconstruction loss: 0.06259346008300781
Training batch 31 / 32
Total batch reconstruction loss: 0.06135929003357887
Training batch 32 / 32
Total batch reconstruction loss: 0.05371018499135971
Epoch [467/500], Train Loss: 0.0567, Validation Loss: 0.0565, Generator Loss: 11.7710, Discriminator Loss: 0.3125
Training epoch 468 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.055556006729602814
Training batch 2 / 32
Total batch reconstruction loss: 0.055094920098781586
Training batch 3 / 32
Total batch reconstruction loss: 0.061138708144426346
Training batch 4 / 32
Total batch reconstruction loss: 0.05650481581687927
Training batch 5 / 32
Total batch reconstruction loss: 0.055200207978487015
Training batch 6 / 32
Total batch reconstruction loss: 0.05876613035798073
Training batch 7 / 32
Total batch reconstruction loss: 0.0629340261220932
Training batch 8 / 32
Total batch reconstruction loss: 0.05832216888666153
Training batch 9 / 32
Total batch reconstruction loss: 0.05557672306895256
Training batch 10 / 32
Total batch reconstruction loss: 0.055525168776512146
Training batch 11 / 32
Total batch reconstruction loss: 0.060036107897758484
Training batch 12 / 32
Total batch reconstruction loss: 0.059087179601192474
Training batch 13 / 32
Total batch reconstruction loss: 0.057918988168239594
Training batch 14 / 32
Total batch reconstruction loss: 0.06021956354379654
Training batch 15 / 32
Total batch reconstruction loss: 0.05621488764882088
Training batch 16 / 32
Total batch reconstruction loss: 0.055844392627477646
Training batch 17 / 32
Total batch reconstruction loss: 0.06180911138653755
Training batch 18 / 32
Total batch reconstruction loss: 0.05692672356963158
Training batch 19 / 32
Total batch reconstruction loss: 0.0589025542140007
Training batch 20 / 32
Total batch reconstruction loss: 0.05968612805008888
Training batch 21 / 32
Total batch reconstruction loss: 0.056737374514341354
Training batch 22 / 32
Total batch reconstruction loss: 0.05756182223558426
Training batch 23 / 32
Total batch reconstruction loss: 0.059762515127658844
Training batch 24 / 32
Total batch reconstruction loss: 0.055775851011276245
Training batch 25 / 32
Total batch reconstruction loss: 0.0582934245467186
Training batch 26 / 32
Total batch reconstruction loss: 0.05942044034600258
Training batch 27 / 32
Total batch reconstruction loss: 0.06153059005737305
Training batch 28 / 32
Total batch reconstruction loss: 0.05723046511411667
Training batch 29 / 32
Total batch reconstruction loss: 0.05929890275001526
Training batch 30 / 32
Total batch reconstruction loss: 0.06104610487818718
Training batch 31 / 32
Total batch reconstruction loss: 0.05576123297214508
Training batch 32 / 32
Total batch reconstruction loss: 0.046354930847883224
Epoch [468/500], Train Loss: 0.0556, Validation Loss: 0.0558, Generator Loss: 11.6321, Discriminator Loss: 0.3146
Training epoch 469 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.06239519268274307
Training batch 2 / 32
Total batch reconstruction loss: 0.0562562495470047
Training batch 3 / 32
Total batch reconstruction loss: 0.05614333599805832
Training batch 4 / 32
Total batch reconstruction loss: 0.058668673038482666
Training batch 5 / 32
Total batch reconstruction loss: 0.057942748069763184
Training batch 6 / 32
Total batch reconstruction loss: 0.05882123112678528
Training batch 7 / 32
Total batch reconstruction loss: 0.05766607075929642
Training batch 8 / 32
Total batch reconstruction loss: 0.060302503407001495
Training batch 9 / 32
Total batch reconstruction loss: 0.06757107377052307
Training batch 10 / 32
Total batch reconstruction loss: 0.059136852622032166
Training batch 11 / 32
Total batch reconstruction loss: 0.05635702982544899
Training batch 12 / 32
Total batch reconstruction loss: 0.057100340723991394
Training batch 13 / 32
Total batch reconstruction loss: 0.05488800257444382
Training batch 14 / 32
Total batch reconstruction loss: 0.0575731135904789
Training batch 15 / 32
Total batch reconstruction loss: 0.05991248041391373
Training batch 16 / 32
Total batch reconstruction loss: 0.06050010398030281
Training batch 17 / 32
Total batch reconstruction loss: 0.05854382738471031
Training batch 18 / 32
Total batch reconstruction loss: 0.05653306841850281
Training batch 19 / 32
Total batch reconstruction loss: 0.05673760175704956
Training batch 20 / 32
Total batch reconstruction loss: 0.05805933475494385
Training batch 21 / 32
Total batch reconstruction loss: 0.057874470949172974
Training batch 22 / 32
Total batch reconstruction loss: 0.05462926626205444
Training batch 23 / 32
Total batch reconstruction loss: 0.05424806475639343
Training batch 24 / 32
Total batch reconstruction loss: 0.05908886343240738
Training batch 25 / 32
Total batch reconstruction loss: 0.0570819191634655
Training batch 26 / 32
Total batch reconstruction loss: 0.05874445661902428
Training batch 27 / 32
Total batch reconstruction loss: 0.0589408203959465
Training batch 28 / 32
Total batch reconstruction loss: 0.05278157442808151
Training batch 29 / 32
Total batch reconstruction loss: 0.05848969146609306
Training batch 30 / 32
Total batch reconstruction loss: 0.060366131365299225
Training batch 31 / 32
Total batch reconstruction loss: 0.05816061794757843
Training batch 32 / 32
Total batch reconstruction loss: 0.059160493314266205
Epoch [469/500], Train Loss: 0.0561, Validation Loss: 0.0562, Generator Loss: 11.7017, Discriminator Loss: 0.3085
Training epoch 470 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.059910498559474945
Training batch 2 / 32
Total batch reconstruction loss: 0.06030285730957985
Training batch 3 / 32
Total batch reconstruction loss: 0.05743436515331268
Training batch 4 / 32
Total batch reconstruction loss: 0.05727797746658325
Training batch 5 / 32
Total batch reconstruction loss: 0.06407591700553894
Training batch 6 / 32
Total batch reconstruction loss: 0.05415928363800049
Training batch 7 / 32
Total batch reconstruction loss: 0.053815096616744995
Training batch 8 / 32
Total batch reconstruction loss: 0.05875014141201973
Training batch 9 / 32
Total batch reconstruction loss: 0.05475554242730141
Training batch 10 / 32
Total batch reconstruction loss: 0.05407896637916565
Training batch 11 / 32
Total batch reconstruction loss: 0.06239969655871391
Training batch 12 / 32
Total batch reconstruction loss: 0.05743011459708214
Training batch 13 / 32
Total batch reconstruction loss: 0.05562913790345192
Training batch 14 / 32
Total batch reconstruction loss: 0.05696023255586624
Training batch 15 / 32
Total batch reconstruction loss: 0.06037839874625206
Training batch 16 / 32
Total batch reconstruction loss: 0.05925474315881729
Training batch 17 / 32
Total batch reconstruction loss: 0.05685751140117645
Training batch 18 / 32
Total batch reconstruction loss: 0.05788656324148178
Training batch 19 / 32
Total batch reconstruction loss: 0.06142308562994003
Training batch 20 / 32
Total batch reconstruction loss: 0.06142818555235863
Training batch 21 / 32
Total batch reconstruction loss: 0.053563959896564484
Training batch 22 / 32
Total batch reconstruction loss: 0.059467099606990814
Training batch 23 / 32
Total batch reconstruction loss: 0.05912864953279495
Training batch 24 / 32
Total batch reconstruction loss: 0.061582230031490326
Training batch 25 / 32
Total batch reconstruction loss: 0.06117270886898041
Training batch 26 / 32
Total batch reconstruction loss: 0.05562552809715271
Training batch 27 / 32
Total batch reconstruction loss: 0.059222206473350525
Training batch 28 / 32
Total batch reconstruction loss: 0.05638202279806137
Training batch 29 / 32
Total batch reconstruction loss: 0.05769423395395279
Training batch 30 / 32
Total batch reconstruction loss: 0.06024503707885742
Training batch 31 / 32
Total batch reconstruction loss: 0.05630195885896683
Training batch 32 / 32
Total batch reconstruction loss: 0.05757910758256912
Epoch [470/500], Train Loss: 0.0563, Validation Loss: 0.0573, Generator Loss: 11.6996, Discriminator Loss: 0.3289
Training epoch 471 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.060738563537597656
Training batch 2 / 32
Total batch reconstruction loss: 0.053845249116420746
Training batch 3 / 32
Total batch reconstruction loss: 0.06368502229452133
Training batch 4 / 32
Total batch reconstruction loss: 0.05565592646598816
Training batch 5 / 32
Total batch reconstruction loss: 0.056864865124225616
Training batch 6 / 32
Total batch reconstruction loss: 0.05783989652991295
Training batch 7 / 32
Total batch reconstruction loss: 0.06165722757577896
Training batch 8 / 32
Total batch reconstruction loss: 0.06136571243405342
Training batch 9 / 32
Total batch reconstruction loss: 0.058729786425828934
Training batch 10 / 32
Total batch reconstruction loss: 0.061301834881305695
Training batch 11 / 32
Total batch reconstruction loss: 0.055615779012441635
Training batch 12 / 32
Total batch reconstruction loss: 0.05707463249564171
Training batch 13 / 32
Total batch reconstruction loss: 0.05747907608747482
Training batch 14 / 32
Total batch reconstruction loss: 0.05636201053857803
Training batch 15 / 32
Total batch reconstruction loss: 0.05653103440999985
Training batch 16 / 32
Total batch reconstruction loss: 0.0614980012178421
Training batch 17 / 32
Total batch reconstruction loss: 0.0599987730383873
Training batch 18 / 32
Total batch reconstruction loss: 0.05761149525642395
Training batch 19 / 32
Total batch reconstruction loss: 0.05888637900352478
Training batch 20 / 32
Total batch reconstruction loss: 0.06092926859855652
Training batch 21 / 32
Total batch reconstruction loss: 0.06206926703453064
Training batch 22 / 32
Total batch reconstruction loss: 0.058017730712890625
Training batch 23 / 32
Total batch reconstruction loss: 0.05773407965898514
Training batch 24 / 32
Total batch reconstruction loss: 0.06156344711780548
Training batch 25 / 32
Total batch reconstruction loss: 0.0558275431394577
Training batch 26 / 32
Total batch reconstruction loss: 0.057954318821430206
Training batch 27 / 32
Total batch reconstruction loss: 0.0586608424782753
Training batch 28 / 32
Total batch reconstruction loss: 0.055237188935279846
Training batch 29 / 32
Total batch reconstruction loss: 0.05636977776885033
Training batch 30 / 32
Total batch reconstruction loss: 0.0614016093313694
Training batch 31 / 32
Total batch reconstruction loss: 0.05900207906961441
Training batch 32 / 32
Total batch reconstruction loss: 0.06496010720729828
Epoch [471/500], Train Loss: 0.0572, Validation Loss: 0.0568, Generator Loss: 11.8341, Discriminator Loss: 0.3133
Training epoch 472 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.055048808455467224
Training batch 2 / 32
Total batch reconstruction loss: 0.05809720978140831
Training batch 3 / 32
Total batch reconstruction loss: 0.05853283405303955
Training batch 4 / 32
Total batch reconstruction loss: 0.0648898035287857
Training batch 5 / 32
Total batch reconstruction loss: 0.05331158638000488
Training batch 6 / 32
Total batch reconstruction loss: 0.05769922584295273
Training batch 7 / 32
Total batch reconstruction loss: 0.059070900082588196
Training batch 8 / 32
Total batch reconstruction loss: 0.05747251212596893
Training batch 9 / 32
Total batch reconstruction loss: 0.05645523965358734
Training batch 10 / 32
Total batch reconstruction loss: 0.05910259485244751
Training batch 11 / 32
Total batch reconstruction loss: 0.06668804585933685
Training batch 12 / 32
Total batch reconstruction loss: 0.05683611333370209
Training batch 13 / 32
Total batch reconstruction loss: 0.05539363622665405
Training batch 14 / 32
Total batch reconstruction loss: 0.05753346532583237
Training batch 15 / 32
Total batch reconstruction loss: 0.05686682090163231
Training batch 16 / 32
Total batch reconstruction loss: 0.06270686537027359
Training batch 17 / 32
Total batch reconstruction loss: 0.05650155618786812
Training batch 18 / 32
Total batch reconstruction loss: 0.057108186185359955
Training batch 19 / 32
Total batch reconstruction loss: 0.05759594216942787
Training batch 20 / 32
Total batch reconstruction loss: 0.0616597943007946
Training batch 21 / 32
Total batch reconstruction loss: 0.05351872742176056
Training batch 22 / 32
Total batch reconstruction loss: 0.05362248793244362
Training batch 23 / 32
Total batch reconstruction loss: 0.058850131928920746
Training batch 24 / 32
Total batch reconstruction loss: 0.057591039687395096
Training batch 25 / 32
Total batch reconstruction loss: 0.05913308635354042
Training batch 26 / 32
Total batch reconstruction loss: 0.058519843965768814
Training batch 27 / 32
Total batch reconstruction loss: 0.05853928625583649
Training batch 28 / 32
Total batch reconstruction loss: 0.05735856294631958
Training batch 29 / 32
Total batch reconstruction loss: 0.06621968746185303
Training batch 30 / 32
Total batch reconstruction loss: 0.05716719850897789
Training batch 31 / 32
Total batch reconstruction loss: 0.05676855146884918
Training batch 32 / 32
Total batch reconstruction loss: 0.09271639585494995
Epoch [472/500], Train Loss: 0.0572, Validation Loss: 0.0586, Generator Loss: 11.9377, Discriminator Loss: 0.3089
Training epoch 473 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.06151179224252701
Training batch 2 / 32
Total batch reconstruction loss: 0.06260339915752411
Training batch 3 / 32
Total batch reconstruction loss: 0.05852959305047989
Training batch 4 / 32
Total batch reconstruction loss: 0.057424984872341156
Training batch 5 / 32
Total batch reconstruction loss: 0.06323609501123428
Training batch 6 / 32
Total batch reconstruction loss: 0.057998474687337875
Training batch 7 / 32
Total batch reconstruction loss: 0.06353463232517242
Training batch 8 / 32
Total batch reconstruction loss: 0.06453409790992737
Training batch 9 / 32
Total batch reconstruction loss: 0.055905818939208984
Training batch 10 / 32
Total batch reconstruction loss: 0.060432493686676025
Training batch 11 / 32
Total batch reconstruction loss: 0.058374740183353424
Training batch 12 / 32
Total batch reconstruction loss: 0.05685477703809738
Training batch 13 / 32
Total batch reconstruction loss: 0.05696547031402588
Training batch 14 / 32
Total batch reconstruction loss: 0.0617852546274662
Training batch 15 / 32
Total batch reconstruction loss: 0.05928995460271835
Training batch 16 / 32
Total batch reconstruction loss: 0.05635155364871025
Training batch 17 / 32
Total batch reconstruction loss: 0.0551685206592083
Training batch 18 / 32
Total batch reconstruction loss: 0.06051675230264664
Training batch 19 / 32
Total batch reconstruction loss: 0.06268680095672607
Training batch 20 / 32
Total batch reconstruction loss: 0.05628163367509842
Training batch 21 / 32
Total batch reconstruction loss: 0.055778954178094864
Training batch 22 / 32
Total batch reconstruction loss: 0.06312490999698639
Training batch 23 / 32
Total batch reconstruction loss: 0.05761539936065674
Training batch 24 / 32
Total batch reconstruction loss: 0.059661537408828735
Training batch 25 / 32
Total batch reconstruction loss: 0.05922287702560425
Training batch 26 / 32
Total batch reconstruction loss: 0.0602964349091053
Training batch 27 / 32
Total batch reconstruction loss: 0.05945290997624397
Training batch 28 / 32
Total batch reconstruction loss: 0.05832459777593613
Training batch 29 / 32
Total batch reconstruction loss: 0.05533970147371292
Training batch 30 / 32
Total batch reconstruction loss: 0.054448097944259644
Training batch 31 / 32
Total batch reconstruction loss: 0.055755093693733215
Training batch 32 / 32
Total batch reconstruction loss: 0.056854307651519775
Epoch [473/500], Train Loss: 0.0568, Validation Loss: 0.0568, Generator Loss: 11.8583, Discriminator Loss: 0.3145
Training epoch 474 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.05534116178750992
Training batch 2 / 32
Total batch reconstruction loss: 0.06156899034976959
Training batch 3 / 32
Total batch reconstruction loss: 0.05618296563625336
Training batch 4 / 32
Total batch reconstruction loss: 0.057076018303632736
Training batch 5 / 32
Total batch reconstruction loss: 0.058826372027397156
Training batch 6 / 32
Total batch reconstruction loss: 0.057599518448114395
Training batch 7 / 32
Total batch reconstruction loss: 0.06496165692806244
Training batch 8 / 32
Total batch reconstruction loss: 0.05858194828033447
Training batch 9 / 32
Total batch reconstruction loss: 0.05368344113230705
Training batch 10 / 32
Total batch reconstruction loss: 0.060925696045160294
Training batch 11 / 32
Total batch reconstruction loss: 0.055280901491642
Training batch 12 / 32
Total batch reconstruction loss: 0.057337842881679535
Training batch 13 / 32
Total batch reconstruction loss: 0.05526236444711685
Training batch 14 / 32
Total batch reconstruction loss: 0.056790366768836975
Training batch 15 / 32
Total batch reconstruction loss: 0.05507386103272438
Training batch 16 / 32
Total batch reconstruction loss: 0.05402415245771408
Training batch 17 / 32
Total batch reconstruction loss: 0.05811144411563873
Training batch 18 / 32
Total batch reconstruction loss: 0.06144123151898384
Training batch 19 / 32
Total batch reconstruction loss: 0.061679549515247345
Training batch 20 / 32
Total batch reconstruction loss: 0.05775783956050873
Training batch 21 / 32
Total batch reconstruction loss: 0.05765276402235031
Training batch 22 / 32
Total batch reconstruction loss: 0.060212839394807816
Training batch 23 / 32
Total batch reconstruction loss: 0.05902975797653198
Training batch 24 / 32
Total batch reconstruction loss: 0.0550839900970459
Training batch 25 / 32
Total batch reconstruction loss: 0.0589558407664299
Training batch 26 / 32
Total batch reconstruction loss: 0.06159387528896332
Training batch 27 / 32
Total batch reconstruction loss: 0.0582275316119194
Training batch 28 / 32
Total batch reconstruction loss: 0.06127769872546196
Training batch 29 / 32
Total batch reconstruction loss: 0.05852508544921875
Training batch 30 / 32
Total batch reconstruction loss: 0.057569846510887146
Training batch 31 / 32
Total batch reconstruction loss: 0.054295726120471954
Training batch 32 / 32
Total batch reconstruction loss: 0.06458057463169098
Epoch [474/500], Train Loss: 0.0564, Validation Loss: 0.0570, Generator Loss: 11.7298, Discriminator Loss: 0.3000
Training epoch 475 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.05722818523645401
Training batch 2 / 32
Total batch reconstruction loss: 0.0539667084813118
Training batch 3 / 32
Total batch reconstruction loss: 0.06175151467323303
Training batch 4 / 32
Total batch reconstruction loss: 0.0544935017824173
Training batch 5 / 32
Total batch reconstruction loss: 0.05857620760798454
Training batch 6 / 32
Total batch reconstruction loss: 0.06408989429473877
Training batch 7 / 32
Total batch reconstruction loss: 0.058715514838695526
Training batch 8 / 32
Total batch reconstruction loss: 0.05655056983232498
Training batch 9 / 32
Total batch reconstruction loss: 0.05639749765396118
Training batch 10 / 32
Total batch reconstruction loss: 0.05982819199562073
Training batch 11 / 32
Total batch reconstruction loss: 0.0600753054022789
Training batch 12 / 32
Total batch reconstruction loss: 0.05783954635262489
Training batch 13 / 32
Total batch reconstruction loss: 0.05679823458194733
Training batch 14 / 32
Total batch reconstruction loss: 0.056678421795368195
Training batch 15 / 32
Total batch reconstruction loss: 0.06102737784385681
Training batch 16 / 32
Total batch reconstruction loss: 0.0576188862323761
Training batch 17 / 32
Total batch reconstruction loss: 0.057326771318912506
Training batch 18 / 32
Total batch reconstruction loss: 0.058642275631427765
Training batch 19 / 32
Total batch reconstruction loss: 0.06025638431310654
Training batch 20 / 32
Total batch reconstruction loss: 0.05423737317323685
Training batch 21 / 32
Total batch reconstruction loss: 0.054565928876399994
Training batch 22 / 32
Total batch reconstruction loss: 0.054567743092775345
Training batch 23 / 32
Total batch reconstruction loss: 0.058927591890096664
Training batch 24 / 32
Total batch reconstruction loss: 0.05767000466585159
Training batch 25 / 32
Total batch reconstruction loss: 0.05542230233550072
Training batch 26 / 32
Total batch reconstruction loss: 0.058999139815568924
Training batch 27 / 32
Total batch reconstruction loss: 0.06054458022117615
Training batch 28 / 32
Total batch reconstruction loss: 0.05827092006802559
Training batch 29 / 32
Total batch reconstruction loss: 0.05874878913164139
Training batch 30 / 32
Total batch reconstruction loss: 0.05467165261507034
Training batch 31 / 32
Total batch reconstruction loss: 0.05904962494969368
Training batch 32 / 32
Total batch reconstruction loss: 0.04976598173379898
Epoch [475/500], Train Loss: 0.0555, Validation Loss: 0.0565, Generator Loss: 11.5846, Discriminator Loss: 0.3208
Training epoch 476 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.06286003440618515
Training batch 2 / 32
Total batch reconstruction loss: 0.06298806518316269
Training batch 3 / 32
Total batch reconstruction loss: 0.05621645599603653
Training batch 4 / 32
Total batch reconstruction loss: 0.059181712567806244
Training batch 5 / 32
Total batch reconstruction loss: 0.05927661806344986
Training batch 6 / 32
Total batch reconstruction loss: 0.05559089779853821
Training batch 7 / 32
Total batch reconstruction loss: 0.06023651361465454
Training batch 8 / 32
Total batch reconstruction loss: 0.059251703321933746
Training batch 9 / 32
Total batch reconstruction loss: 0.056003980338573456
Training batch 10 / 32
Total batch reconstruction loss: 0.054965198040008545
Training batch 11 / 32
Total batch reconstruction loss: 0.05660872161388397
Training batch 12 / 32
Total batch reconstruction loss: 0.059656839817762375
Training batch 13 / 32
Total batch reconstruction loss: 0.05874919891357422
Training batch 14 / 32
Total batch reconstruction loss: 0.05568967014551163
Training batch 15 / 32
Total batch reconstruction loss: 0.06336352229118347
Training batch 16 / 32
Total batch reconstruction loss: 0.05468608811497688
Training batch 17 / 32
Total batch reconstruction loss: 0.0584881454706192
Training batch 18 / 32
Total batch reconstruction loss: 0.05770789831876755
Training batch 19 / 32
Total batch reconstruction loss: 0.06299736350774765
Training batch 20 / 32
Total batch reconstruction loss: 0.06035913899540901
Training batch 21 / 32
Total batch reconstruction loss: 0.05478688329458237
Training batch 22 / 32
Total batch reconstruction loss: 0.05805422365665436
Training batch 23 / 32
Total batch reconstruction loss: 0.057250238955020905
Training batch 24 / 32
Total batch reconstruction loss: 0.05758129060268402
Training batch 25 / 32
Total batch reconstruction loss: 0.057695433497428894
Training batch 26 / 32
Total batch reconstruction loss: 0.05811281502246857
Training batch 27 / 32
Total batch reconstruction loss: 0.06331752240657806
Training batch 28 / 32
Total batch reconstruction loss: 0.052940502762794495
Training batch 29 / 32
Total batch reconstruction loss: 0.05546613037586212
Training batch 30 / 32
Total batch reconstruction loss: 0.054702576249837875
Training batch 31 / 32
Total batch reconstruction loss: 0.057629384100437164
Training batch 32 / 32
Total batch reconstruction loss: 0.0814913734793663
Epoch [476/500], Train Loss: 0.0570, Validation Loss: 0.0567, Generator Loss: 11.8434, Discriminator Loss: 0.3139
Training epoch 477 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.0546332523226738
Training batch 2 / 32
Total batch reconstruction loss: 0.057247549295425415
Training batch 3 / 32
Total batch reconstruction loss: 0.06106026470661163
Training batch 4 / 32
Total batch reconstruction loss: 0.054706305265426636
Training batch 5 / 32
Total batch reconstruction loss: 0.062444981187582016
Training batch 6 / 32
Total batch reconstruction loss: 0.060371093451976776
Training batch 7 / 32
Total batch reconstruction loss: 0.06134825199842453
Training batch 8 / 32
Total batch reconstruction loss: 0.060060352087020874
Training batch 9 / 32
Total batch reconstruction loss: 0.05957931652665138
Training batch 10 / 32
Total batch reconstruction loss: 0.057245634496212006
Training batch 11 / 32
Total batch reconstruction loss: 0.05439329892396927
Training batch 12 / 32
Total batch reconstruction loss: 0.05960710346698761
Training batch 13 / 32
Total batch reconstruction loss: 0.055906396359205246
Training batch 14 / 32
Total batch reconstruction loss: 0.05949380248785019
Training batch 15 / 32
Total batch reconstruction loss: 0.055276066064834595
Training batch 16 / 32
Total batch reconstruction loss: 0.05889583006501198
Training batch 17 / 32
Total batch reconstruction loss: 0.06012309342622757
Training batch 18 / 32
Total batch reconstruction loss: 0.05916644632816315
Training batch 19 / 32
Total batch reconstruction loss: 0.05842190980911255
Training batch 20 / 32
Total batch reconstruction loss: 0.060059644281864166
Training batch 21 / 32
Total batch reconstruction loss: 0.054893914610147476
Training batch 22 / 32
Total batch reconstruction loss: 0.05871180444955826
Training batch 23 / 32
Total batch reconstruction loss: 0.05980324745178223
Training batch 24 / 32
Total batch reconstruction loss: 0.057350438088178635
Training batch 25 / 32
Total batch reconstruction loss: 0.06411125510931015
Training batch 26 / 32
Total batch reconstruction loss: 0.05877363681793213
Training batch 27 / 32
Total batch reconstruction loss: 0.058592379093170166
Training batch 28 / 32
Total batch reconstruction loss: 0.05817790701985359
Training batch 29 / 32
Total batch reconstruction loss: 0.06072884425520897
Training batch 30 / 32
Total batch reconstruction loss: 0.05427989736199379
Training batch 31 / 32
Total batch reconstruction loss: 0.05803552642464638
Training batch 32 / 32
Total batch reconstruction loss: 0.05358561873435974
Epoch [477/500], Train Loss: 0.0565, Validation Loss: 0.0565, Generator Loss: 11.7422, Discriminator Loss: 0.3049
Training epoch 478 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.05705803260207176
Training batch 2 / 32
Total batch reconstruction loss: 0.05875985324382782
Training batch 3 / 32
Total batch reconstruction loss: 0.057302702218294144
Training batch 4 / 32
Total batch reconstruction loss: 0.05777228623628616
Training batch 5 / 32
Total batch reconstruction loss: 0.060951367020606995
Training batch 6 / 32
Total batch reconstruction loss: 0.05683363974094391
Training batch 7 / 32
Total batch reconstruction loss: 0.0554266981780529
Training batch 8 / 32
Total batch reconstruction loss: 0.057789139449596405
Training batch 9 / 32
Total batch reconstruction loss: 0.05971810221672058
Training batch 10 / 32
Total batch reconstruction loss: 0.0561751127243042
Training batch 11 / 32
Total batch reconstruction loss: 0.0580517053604126
Training batch 12 / 32
Total batch reconstruction loss: 0.05787033587694168
Training batch 13 / 32
Total batch reconstruction loss: 0.057939790189266205
Training batch 14 / 32
Total batch reconstruction loss: 0.06336994469165802
Training batch 15 / 32
Total batch reconstruction loss: 0.05938319116830826
Training batch 16 / 32
Total batch reconstruction loss: 0.05799403786659241
Training batch 17 / 32
Total batch reconstruction loss: 0.060717321932315826
Training batch 18 / 32
Total batch reconstruction loss: 0.057326026260852814
Training batch 19 / 32
Total batch reconstruction loss: 0.055727358907461166
Training batch 20 / 32
Total batch reconstruction loss: 0.055500004440546036
Training batch 21 / 32
Total batch reconstruction loss: 0.054737597703933716
Training batch 22 / 32
Total batch reconstruction loss: 0.060600392520427704
Training batch 23 / 32
Total batch reconstruction loss: 0.05916173756122589
Training batch 24 / 32
Total batch reconstruction loss: 0.05835025757551193
Training batch 25 / 32
Total batch reconstruction loss: 0.05760757625102997
Training batch 26 / 32
Total batch reconstruction loss: 0.05886949226260185
Training batch 27 / 32
Total batch reconstruction loss: 0.05959168076515198
Training batch 28 / 32
Total batch reconstruction loss: 0.05472933501005173
Training batch 29 / 32
Total batch reconstruction loss: 0.0557999312877655
Training batch 30 / 32
Total batch reconstruction loss: 0.05885717272758484
Training batch 31 / 32
Total batch reconstruction loss: 0.05535268411040306
Training batch 32 / 32
Total batch reconstruction loss: 0.06026529520750046
Epoch [478/500], Train Loss: 0.0560, Validation Loss: 0.0573, Generator Loss: 11.6610, Discriminator Loss: 0.3230
Training epoch 479 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.05707197263836861
Training batch 2 / 32
Total batch reconstruction loss: 0.06359922140836716
Training batch 3 / 32
Total batch reconstruction loss: 0.058651428669691086
Training batch 4 / 32
Total batch reconstruction loss: 0.060235247015953064
Training batch 5 / 32
Total batch reconstruction loss: 0.0600045844912529
Training batch 6 / 32
Total batch reconstruction loss: 0.05584043264389038
Training batch 7 / 32
Total batch reconstruction loss: 0.05771512910723686
Training batch 8 / 32
Total batch reconstruction loss: 0.05378811061382294
Training batch 9 / 32
Total batch reconstruction loss: 0.05800997093319893
Training batch 10 / 32
Total batch reconstruction loss: 0.05922875553369522
Training batch 11 / 32
Total batch reconstruction loss: 0.055418677628040314
Training batch 12 / 32
Total batch reconstruction loss: 0.06039240583777428
Training batch 13 / 32
Total batch reconstruction loss: 0.05902887135744095
Training batch 14 / 32
Total batch reconstruction loss: 0.05902012065052986
Training batch 15 / 32
Total batch reconstruction loss: 0.058488521724939346
Training batch 16 / 32
Total batch reconstruction loss: 0.05422188341617584
Training batch 17 / 32
Total batch reconstruction loss: 0.058665208518505096
Training batch 18 / 32
Total batch reconstruction loss: 0.058426037430763245
Training batch 19 / 32
Total batch reconstruction loss: 0.061103083193302155
Training batch 20 / 32
Total batch reconstruction loss: 0.059691280126571655
Training batch 21 / 32
Total batch reconstruction loss: 0.055582642555236816
Training batch 22 / 32
Total batch reconstruction loss: 0.060596585273742676
Training batch 23 / 32
Total batch reconstruction loss: 0.061776839196681976
Training batch 24 / 32
Total batch reconstruction loss: 0.05839969962835312
Training batch 25 / 32
Total batch reconstruction loss: 0.05934453010559082
Training batch 26 / 32
Total batch reconstruction loss: 0.054612547159194946
Training batch 27 / 32
Total batch reconstruction loss: 0.06067706272006035
Training batch 28 / 32
Total batch reconstruction loss: 0.05591079592704773
Training batch 29 / 32
Total batch reconstruction loss: 0.059540461748838425
Training batch 30 / 32
Total batch reconstruction loss: 0.062414128333330154
Training batch 31 / 32
Total batch reconstruction loss: 0.056831423193216324
Training batch 32 / 32
Total batch reconstruction loss: 0.05284924805164337
Epoch [479/500], Train Loss: 0.0562, Validation Loss: 0.0564, Generator Loss: 11.7394, Discriminator Loss: 0.3125
Training epoch 480 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.05795235186815262
Training batch 2 / 32
Total batch reconstruction loss: 0.058946676552295685
Training batch 3 / 32
Total batch reconstruction loss: 0.057919856160879135
Training batch 4 / 32
Total batch reconstruction loss: 0.060765814036130905
Training batch 5 / 32
Total batch reconstruction loss: 0.05817880854010582
Training batch 6 / 32
Total batch reconstruction loss: 0.06035517901182175
Training batch 7 / 32
Total batch reconstruction loss: 0.059220172464847565
Training batch 8 / 32
Total batch reconstruction loss: 0.060414381325244904
Training batch 9 / 32
Total batch reconstruction loss: 0.05697299540042877
Training batch 10 / 32
Total batch reconstruction loss: 0.061649344861507416
Training batch 11 / 32
Total batch reconstruction loss: 0.05561864376068115
Training batch 12 / 32
Total batch reconstruction loss: 0.05798564851284027
Training batch 13 / 32
Total batch reconstruction loss: 0.056754548102617264
Training batch 14 / 32
Total batch reconstruction loss: 0.06085188686847687
Training batch 15 / 32
Total batch reconstruction loss: 0.05677245557308197
Training batch 16 / 32
Total batch reconstruction loss: 0.054797619581222534
Training batch 17 / 32
Total batch reconstruction loss: 0.05798786133527756
Training batch 18 / 32
Total batch reconstruction loss: 0.059547893702983856
Training batch 19 / 32
Total batch reconstruction loss: 0.06018999218940735
Training batch 20 / 32
Total batch reconstruction loss: 0.0609608069062233
Training batch 21 / 32
Total batch reconstruction loss: 0.05777311697602272
Training batch 22 / 32
Total batch reconstruction loss: 0.05495292320847511
Training batch 23 / 32
Total batch reconstruction loss: 0.05953231453895569
Training batch 24 / 32
Total batch reconstruction loss: 0.05921153724193573
Training batch 25 / 32
Total batch reconstruction loss: 0.05971750617027283
Training batch 26 / 32
Total batch reconstruction loss: 0.056102603673934937
Training batch 27 / 32
Total batch reconstruction loss: 0.0572688952088356
Training batch 28 / 32
Total batch reconstruction loss: 0.0564645379781723
Training batch 29 / 32
Total batch reconstruction loss: 0.05928657948970795
Training batch 30 / 32
Total batch reconstruction loss: 0.05414048954844475
Training batch 31 / 32
Total batch reconstruction loss: 0.05671332776546478
Training batch 32 / 32
Total batch reconstruction loss: 0.05946530029177666
Epoch [480/500], Train Loss: 0.0562, Validation Loss: 0.0560, Generator Loss: 11.7226, Discriminator Loss: 0.3121
Training epoch 481 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.059996459633111954
Training batch 2 / 32
Total batch reconstruction loss: 0.05631038919091225
Training batch 3 / 32
Total batch reconstruction loss: 0.05350828170776367
Training batch 4 / 32
Total batch reconstruction loss: 0.057777486741542816
Training batch 5 / 32
Total batch reconstruction loss: 0.053098686039447784
Training batch 6 / 32
Total batch reconstruction loss: 0.058649688959121704
Training batch 7 / 32
Total batch reconstruction loss: 0.05634019896388054
Training batch 8 / 32
Total batch reconstruction loss: 0.05854786932468414
Training batch 9 / 32
Total batch reconstruction loss: 0.06382960826158524
Training batch 10 / 32
Total batch reconstruction loss: 0.0599215030670166
Training batch 11 / 32
Total batch reconstruction loss: 0.0555286630988121
Training batch 12 / 32
Total batch reconstruction loss: 0.059361644089221954
Training batch 13 / 32
Total batch reconstruction loss: 0.05638250336050987
Training batch 14 / 32
Total batch reconstruction loss: 0.059771277010440826
Training batch 15 / 32
Total batch reconstruction loss: 0.058179132640361786
Training batch 16 / 32
Total batch reconstruction loss: 0.05779295414686203
Training batch 17 / 32
Total batch reconstruction loss: 0.058713994920253754
Training batch 18 / 32
Total batch reconstruction loss: 0.06029190123081207
Training batch 19 / 32
Total batch reconstruction loss: 0.06188439950346947
Training batch 20 / 32
Total batch reconstruction loss: 0.05252330005168915
Training batch 21 / 32
Total batch reconstruction loss: 0.0603514164686203
Training batch 22 / 32
Total batch reconstruction loss: 0.05651220306754112
Training batch 23 / 32
Total batch reconstruction loss: 0.060547925531864166
Training batch 24 / 32
Total batch reconstruction loss: 0.05917512625455856
Training batch 25 / 32
Total batch reconstruction loss: 0.054023317992687225
Training batch 26 / 32
Total batch reconstruction loss: 0.06344981491565704
Training batch 27 / 32
Total batch reconstruction loss: 0.06119609996676445
Training batch 28 / 32
Total batch reconstruction loss: 0.058432549238204956
Training batch 29 / 32
Total batch reconstruction loss: 0.059674084186553955
Training batch 30 / 32
Total batch reconstruction loss: 0.054261840879917145
Training batch 31 / 32
Total batch reconstruction loss: 0.05795108526945114
Training batch 32 / 32
Total batch reconstruction loss: 0.05579839646816254
Epoch [481/500], Train Loss: 0.0561, Validation Loss: 0.0578, Generator Loss: 11.6975, Discriminator Loss: 0.3079
Training epoch 482 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.058614738285541534
Training batch 2 / 32
Total batch reconstruction loss: 0.06418919563293457
Training batch 3 / 32
Total batch reconstruction loss: 0.05515528842806816
Training batch 4 / 32
Total batch reconstruction loss: 0.06381144374608994
Training batch 5 / 32
Total batch reconstruction loss: 0.06099911034107208
Training batch 6 / 32
Total batch reconstruction loss: 0.055533699691295624
Training batch 7 / 32
Total batch reconstruction loss: 0.057620786130428314
Training batch 8 / 32
Total batch reconstruction loss: 0.055578410625457764
Training batch 9 / 32
Total batch reconstruction loss: 0.057960331439971924
Training batch 10 / 32
Total batch reconstruction loss: 0.05528431385755539
Training batch 11 / 32
Total batch reconstruction loss: 0.0569818951189518
Training batch 12 / 32
Total batch reconstruction loss: 0.059842631220817566
Training batch 13 / 32
Total batch reconstruction loss: 0.053735144436359406
Training batch 14 / 32
Total batch reconstruction loss: 0.05447262153029442
Training batch 15 / 32
Total batch reconstruction loss: 0.056715480983257294
Training batch 16 / 32
Total batch reconstruction loss: 0.05548040568828583
Training batch 17 / 32
Total batch reconstruction loss: 0.058687735348939896
Training batch 18 / 32
Total batch reconstruction loss: 0.053138770163059235
Training batch 19 / 32
Total batch reconstruction loss: 0.05445893853902817
Training batch 20 / 32
Total batch reconstruction loss: 0.05508062243461609
Training batch 21 / 32
Total batch reconstruction loss: 0.059617284685373306
Training batch 22 / 32
Total batch reconstruction loss: 0.05781158059835434
Training batch 23 / 32
Total batch reconstruction loss: 0.06255541741847992
Training batch 24 / 32
Total batch reconstruction loss: 0.05628199130296707
Training batch 25 / 32
Total batch reconstruction loss: 0.0653345063328743
Training batch 26 / 32
Total batch reconstruction loss: 0.056676093488931656
Training batch 27 / 32
Total batch reconstruction loss: 0.05856628343462944
Training batch 28 / 32
Total batch reconstruction loss: 0.05928166210651398
Training batch 29 / 32
Total batch reconstruction loss: 0.06217075139284134
Training batch 30 / 32
Total batch reconstruction loss: 0.06154628098011017
Training batch 31 / 32
Total batch reconstruction loss: 0.06328210234642029
Training batch 32 / 32
Total batch reconstruction loss: 0.06780846416950226
Epoch [482/500], Train Loss: 0.0568, Validation Loss: 0.0567, Generator Loss: 11.7831, Discriminator Loss: 0.3112
Training epoch 483 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.05405975878238678
Training batch 2 / 32
Total batch reconstruction loss: 0.05690668895840645
Training batch 3 / 32
Total batch reconstruction loss: 0.05809520557522774
Training batch 4 / 32
Total batch reconstruction loss: 0.05735473707318306
Training batch 5 / 32
Total batch reconstruction loss: 0.05794942006468773
Training batch 6 / 32
Total batch reconstruction loss: 0.054491568356752396
Training batch 7 / 32
Total batch reconstruction loss: 0.05583426356315613
Training batch 8 / 32
Total batch reconstruction loss: 0.06638246029615402
Training batch 9 / 32
Total batch reconstruction loss: 0.05679652839899063
Training batch 10 / 32
Total batch reconstruction loss: 0.056525178253650665
Training batch 11 / 32
Total batch reconstruction loss: 0.05723269283771515
Training batch 12 / 32
Total batch reconstruction loss: 0.058875225484371185
Training batch 13 / 32
Total batch reconstruction loss: 0.06392557173967361
Training batch 14 / 32
Total batch reconstruction loss: 0.059883758425712585
Training batch 15 / 32
Total batch reconstruction loss: 0.0589032880961895
Training batch 16 / 32
Total batch reconstruction loss: 0.06426998227834702
Training batch 17 / 32
Total batch reconstruction loss: 0.056827813386917114
Training batch 18 / 32
Total batch reconstruction loss: 0.05441302806138992
Training batch 19 / 32
Total batch reconstruction loss: 0.05822174996137619
Training batch 20 / 32
Total batch reconstruction loss: 0.058032434433698654
Training batch 21 / 32
Total batch reconstruction loss: 0.05548679828643799
Training batch 22 / 32
Total batch reconstruction loss: 0.05887513980269432
Training batch 23 / 32
Total batch reconstruction loss: 0.06085680425167084
Training batch 24 / 32
Total batch reconstruction loss: 0.05816928297281265
Training batch 25 / 32
Total batch reconstruction loss: 0.05770853906869888
Training batch 26 / 32
Total batch reconstruction loss: 0.05777277052402496
Training batch 27 / 32
Total batch reconstruction loss: 0.06007324904203415
Training batch 28 / 32
Total batch reconstruction loss: 0.057184167206287384
Training batch 29 / 32
Total batch reconstruction loss: 0.057343535125255585
Training batch 30 / 32
Total batch reconstruction loss: 0.05357074365019798
Training batch 31 / 32
Total batch reconstruction loss: 0.05578786879777908
Training batch 32 / 32
Total batch reconstruction loss: 0.05244118720293045
Epoch [483/500], Train Loss: 0.0558, Validation Loss: 0.0565, Generator Loss: 11.6214, Discriminator Loss: 0.3320
Training epoch 484 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.057713583111763
Training batch 2 / 32
Total batch reconstruction loss: 0.057655371725559235
Training batch 3 / 32
Total batch reconstruction loss: 0.06250935792922974
Training batch 4 / 32
Total batch reconstruction loss: 0.059595655649900436
Training batch 5 / 32
Total batch reconstruction loss: 0.054248783737421036
Training batch 6 / 32
Total batch reconstruction loss: 0.058189764618873596
Training batch 7 / 32
Total batch reconstruction loss: 0.05776958912611008
Training batch 8 / 32
Total batch reconstruction loss: 0.05711955577135086
Training batch 9 / 32
Total batch reconstruction loss: 0.055865105241537094
Training batch 10 / 32
Total batch reconstruction loss: 0.05606751888990402
Training batch 11 / 32
Total batch reconstruction loss: 0.06304730474948883
Training batch 12 / 32
Total batch reconstruction loss: 0.0604284405708313
Training batch 13 / 32
Total batch reconstruction loss: 0.05519033223390579
Training batch 14 / 32
Total batch reconstruction loss: 0.05704086273908615
Training batch 15 / 32
Total batch reconstruction loss: 0.06036348268389702
Training batch 16 / 32
Total batch reconstruction loss: 0.05920160561800003
Training batch 17 / 32
Total batch reconstruction loss: 0.05631757527589798
Training batch 18 / 32
Total batch reconstruction loss: 0.0564027801156044
Training batch 19 / 32
Total batch reconstruction loss: 0.05520831048488617
Training batch 20 / 32
Total batch reconstruction loss: 0.05827140808105469
Training batch 21 / 32
Total batch reconstruction loss: 0.057859696447849274
Training batch 22 / 32
Total batch reconstruction loss: 0.06289584934711456
Training batch 23 / 32
Total batch reconstruction loss: 0.06151125952601433
Training batch 24 / 32
Total batch reconstruction loss: 0.059326913207769394
Training batch 25 / 32
Total batch reconstruction loss: 0.05945706367492676
Training batch 26 / 32
Total batch reconstruction loss: 0.05354677885770798
Training batch 27 / 32
Total batch reconstruction loss: 0.06137433648109436
Training batch 28 / 32
Total batch reconstruction loss: 0.057363297790288925
Training batch 29 / 32
Total batch reconstruction loss: 0.05673239752650261
Training batch 30 / 32
Total batch reconstruction loss: 0.055203549563884735
Training batch 31 / 32
Total batch reconstruction loss: 0.058925896883010864
Training batch 32 / 32
Total batch reconstruction loss: 0.06551270931959152
Epoch [484/500], Train Loss: 0.0564, Validation Loss: 0.0575, Generator Loss: 11.7351, Discriminator Loss: 0.3211
Training epoch 485 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.056681059300899506
Training batch 2 / 32
Total batch reconstruction loss: 0.06010449677705765
Training batch 3 / 32
Total batch reconstruction loss: 0.05787065625190735
Training batch 4 / 32
Total batch reconstruction loss: 0.05787717178463936
Training batch 5 / 32
Total batch reconstruction loss: 0.05693875998258591
Training batch 6 / 32
Total batch reconstruction loss: 0.05475636199116707
Training batch 7 / 32
Total batch reconstruction loss: 0.055727407336235046
Training batch 8 / 32
Total batch reconstruction loss: 0.055411264300346375
Training batch 9 / 32
Total batch reconstruction loss: 0.06136774271726608
Training batch 10 / 32
Total batch reconstruction loss: 0.060851775109767914
Training batch 11 / 32
Total batch reconstruction loss: 0.05978208780288696
Training batch 12 / 32
Total batch reconstruction loss: 0.06477627158164978
Training batch 13 / 32
Total batch reconstruction loss: 0.054198987782001495
Training batch 14 / 32
Total batch reconstruction loss: 0.05761510133743286
Training batch 15 / 32
Total batch reconstruction loss: 0.05512217432260513
Training batch 16 / 32
Total batch reconstruction loss: 0.05998452752828598
Training batch 17 / 32
Total batch reconstruction loss: 0.057732924818992615
Training batch 18 / 32
Total batch reconstruction loss: 0.05485331267118454
Training batch 19 / 32
Total batch reconstruction loss: 0.061778031289577484
Training batch 20 / 32
Total batch reconstruction loss: 0.0609036386013031
Training batch 21 / 32
Total batch reconstruction loss: 0.05610846355557442
Training batch 22 / 32
Total batch reconstruction loss: 0.056306853890419006
Training batch 23 / 32
Total batch reconstruction loss: 0.06021251156926155
Training batch 24 / 32
Total batch reconstruction loss: 0.05915180593729019
Training batch 25 / 32
Total batch reconstruction loss: 0.06128300353884697
Training batch 26 / 32
Total batch reconstruction loss: 0.057866163551807404
Training batch 27 / 32
Total batch reconstruction loss: 0.05914907157421112
Training batch 28 / 32
Total batch reconstruction loss: 0.061870068311691284
Training batch 29 / 32
Total batch reconstruction loss: 0.05375386029481888
Training batch 30 / 32
Total batch reconstruction loss: 0.05706515163183212
Training batch 31 / 32
Total batch reconstruction loss: 0.06110221520066261
Training batch 32 / 32
Total batch reconstruction loss: 0.05338065326213837
Epoch [485/500], Train Loss: 0.0563, Validation Loss: 0.0568, Generator Loss: 11.7056, Discriminator Loss: 0.3113
Training epoch 486 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.05974298343062401
Training batch 2 / 32
Total batch reconstruction loss: 0.05579517036676407
Training batch 3 / 32
Total batch reconstruction loss: 0.05712179094552994
Training batch 4 / 32
Total batch reconstruction loss: 0.0537845678627491
Training batch 5 / 32
Total batch reconstruction loss: 0.05649029463529587
Training batch 6 / 32
Total batch reconstruction loss: 0.060666874051094055
Training batch 7 / 32
Total batch reconstruction loss: 0.056351661682128906
Training batch 8 / 32
Total batch reconstruction loss: 0.057101570069789886
Training batch 9 / 32
Total batch reconstruction loss: 0.06293970346450806
Training batch 10 / 32
Total batch reconstruction loss: 0.058056995272636414
Training batch 11 / 32
Total batch reconstruction loss: 0.0636044442653656
Training batch 12 / 32
Total batch reconstruction loss: 0.05897640064358711
Training batch 13 / 32
Total batch reconstruction loss: 0.056699804961681366
Training batch 14 / 32
Total batch reconstruction loss: 0.058530211448669434
Training batch 15 / 32
Total batch reconstruction loss: 0.06129983440041542
Training batch 16 / 32
Total batch reconstruction loss: 0.057639263570308685
Training batch 17 / 32
Total batch reconstruction loss: 0.05510930344462395
Training batch 18 / 32
Total batch reconstruction loss: 0.05939357727766037
Training batch 19 / 32
Total batch reconstruction loss: 0.05857393890619278
Training batch 20 / 32
Total batch reconstruction loss: 0.057674288749694824
Training batch 21 / 32
Total batch reconstruction loss: 0.05485484004020691
Training batch 22 / 32
Total batch reconstruction loss: 0.06241534277796745
Training batch 23 / 32
Total batch reconstruction loss: 0.05681785196065903
Training batch 24 / 32
Total batch reconstruction loss: 0.057398680597543716
Training batch 25 / 32
Total batch reconstruction loss: 0.05846034735441208
Training batch 26 / 32
Total batch reconstruction loss: 0.054524779319763184
Training batch 27 / 32
Total batch reconstruction loss: 0.058874696493148804
Training batch 28 / 32
Total batch reconstruction loss: 0.057074908167123795
Training batch 29 / 32
Total batch reconstruction loss: 0.057053543627262115
Training batch 30 / 32
Total batch reconstruction loss: 0.05601610243320465
Training batch 31 / 32
Total batch reconstruction loss: 0.06228702887892723
Training batch 32 / 32
Total batch reconstruction loss: 0.05029454827308655
Epoch [486/500], Train Loss: 0.0559, Validation Loss: 0.0561, Generator Loss: 11.6453, Discriminator Loss: 0.3118
Training epoch 487 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.060988545417785645
Training batch 2 / 32
Total batch reconstruction loss: 0.05791865661740303
Training batch 3 / 32
Total batch reconstruction loss: 0.054654378443956375
Training batch 4 / 32
Total batch reconstruction loss: 0.06116345152258873
Training batch 5 / 32
Total batch reconstruction loss: 0.055342838168144226
Training batch 6 / 32
Total batch reconstruction loss: 0.060273099690675735
Training batch 7 / 32
Total batch reconstruction loss: 0.06129022687673569
Training batch 8 / 32
Total batch reconstruction loss: 0.05423595756292343
Training batch 9 / 32
Total batch reconstruction loss: 0.05424152687191963
Training batch 10 / 32
Total batch reconstruction loss: 0.054426148533821106
Training batch 11 / 32
Total batch reconstruction loss: 0.05734652653336525
Training batch 12 / 32
Total batch reconstruction loss: 0.05397384613752365
Training batch 13 / 32
Total batch reconstruction loss: 0.0585608035326004
Training batch 14 / 32
Total batch reconstruction loss: 0.05815267562866211
Training batch 15 / 32
Total batch reconstruction loss: 0.060101889073848724
Training batch 16 / 32
Total batch reconstruction loss: 0.06435272097587585
Training batch 17 / 32
Total batch reconstruction loss: 0.05411726236343384
Training batch 18 / 32
Total batch reconstruction loss: 0.051731906831264496
Training batch 19 / 32
Total batch reconstruction loss: 0.05986534804105759
Training batch 20 / 32
Total batch reconstruction loss: 0.053271979093551636
Training batch 21 / 32
Total batch reconstruction loss: 0.06495200097560883
Training batch 22 / 32
Total batch reconstruction loss: 0.05674049258232117
Training batch 23 / 32
Total batch reconstruction loss: 0.05507859215140343
Training batch 24 / 32
Total batch reconstruction loss: 0.06393914669752121
Training batch 25 / 32
Total batch reconstruction loss: 0.06097688898444176
Training batch 26 / 32
Total batch reconstruction loss: 0.05968055874109268
Training batch 27 / 32
Total batch reconstruction loss: 0.057829853147268295
Training batch 28 / 32
Total batch reconstruction loss: 0.05555399879813194
Training batch 29 / 32
Total batch reconstruction loss: 0.05939503386616707
Training batch 30 / 32
Total batch reconstruction loss: 0.056330181658267975
Training batch 31 / 32
Total batch reconstruction loss: 0.05693676322698593
Training batch 32 / 32
Total batch reconstruction loss: 0.0507664829492569
Epoch [487/500], Train Loss: 0.0555, Validation Loss: 0.0560, Generator Loss: 11.5961, Discriminator Loss: 0.3131
Training epoch 488 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.053946353495121
Training batch 2 / 32
Total batch reconstruction loss: 0.06502474844455719
Training batch 3 / 32
Total batch reconstruction loss: 0.06102690100669861
Training batch 4 / 32
Total batch reconstruction loss: 0.059762559831142426
Training batch 5 / 32
Total batch reconstruction loss: 0.059057071805000305
Training batch 6 / 32
Total batch reconstruction loss: 0.062109120190143585
Training batch 7 / 32
Total batch reconstruction loss: 0.05620326101779938
Training batch 8 / 32
Total batch reconstruction loss: 0.06017942354083061
Training batch 9 / 32
Total batch reconstruction loss: 0.05776338279247284
Training batch 10 / 32
Total batch reconstruction loss: 0.057320788502693176
Training batch 11 / 32
Total batch reconstruction loss: 0.055789969861507416
Training batch 12 / 32
Total batch reconstruction loss: 0.06113216280937195
Training batch 13 / 32
Total batch reconstruction loss: 0.05900765582919121
Training batch 14 / 32
Total batch reconstruction loss: 0.05480802804231644
Training batch 15 / 32
Total batch reconstruction loss: 0.05936431512236595
Training batch 16 / 32
Total batch reconstruction loss: 0.060357410460710526
Training batch 17 / 32
Total batch reconstruction loss: 0.05859425663948059
Training batch 18 / 32
Total batch reconstruction loss: 0.05988561362028122
Training batch 19 / 32
Total batch reconstruction loss: 0.05781322717666626
Training batch 20 / 32
Total batch reconstruction loss: 0.06079418212175369
Training batch 21 / 32
Total batch reconstruction loss: 0.059427108615636826
Training batch 22 / 32
Total batch reconstruction loss: 0.05660444498062134
Training batch 23 / 32
Total batch reconstruction loss: 0.05552329123020172
Training batch 24 / 32
Total batch reconstruction loss: 0.05930041894316673
Training batch 25 / 32
Total batch reconstruction loss: 0.05890714377164841
Training batch 26 / 32
Total batch reconstruction loss: 0.060674410313367844
Training batch 27 / 32
Total batch reconstruction loss: 0.059022605419158936
Training batch 28 / 32
Total batch reconstruction loss: 0.05457450449466705
Training batch 29 / 32
Total batch reconstruction loss: 0.0553118996322155
Training batch 30 / 32
Total batch reconstruction loss: 0.05792120844125748
Training batch 31 / 32
Total batch reconstruction loss: 0.05799171328544617
Training batch 32 / 32
Total batch reconstruction loss: 0.05464382469654083
Epoch [488/500], Train Loss: 0.0567, Validation Loss: 0.0573, Generator Loss: 11.7497, Discriminator Loss: 0.3268
Training epoch 489 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.054901354014873505
Training batch 2 / 32
Total batch reconstruction loss: 0.0647391825914383
Training batch 3 / 32
Total batch reconstruction loss: 0.0584535077214241
Training batch 4 / 32
Total batch reconstruction loss: 0.06383930146694183
Training batch 5 / 32
Total batch reconstruction loss: 0.05641477555036545
Training batch 6 / 32
Total batch reconstruction loss: 0.05489088594913483
Training batch 7 / 32
Total batch reconstruction loss: 0.05619211122393608
Training batch 8 / 32
Total batch reconstruction loss: 0.06276442110538483
Training batch 9 / 32
Total batch reconstruction loss: 0.055672213435173035
Training batch 10 / 32
Total batch reconstruction loss: 0.05663587152957916
Training batch 11 / 32
Total batch reconstruction loss: 0.06305141746997833
Training batch 12 / 32
Total batch reconstruction loss: 0.05787602812051773
Training batch 13 / 32
Total batch reconstruction loss: 0.061691198498010635
Training batch 14 / 32
Total batch reconstruction loss: 0.05599961057305336
Training batch 15 / 32
Total batch reconstruction loss: 0.0638735443353653
Training batch 16 / 32
Total batch reconstruction loss: 0.060233838856220245
Training batch 17 / 32
Total batch reconstruction loss: 0.05487879365682602
Training batch 18 / 32
Total batch reconstruction loss: 0.06111015006899834
Training batch 19 / 32
Total batch reconstruction loss: 0.054482363164424896
Training batch 20 / 32
Total batch reconstruction loss: 0.06149788200855255
Training batch 21 / 32
Total batch reconstruction loss: 0.058855440467596054
Training batch 22 / 32
Total batch reconstruction loss: 0.057814113795757294
Training batch 23 / 32
Total batch reconstruction loss: 0.05570555105805397
Training batch 24 / 32
Total batch reconstruction loss: 0.055379822850227356
Training batch 25 / 32
Total batch reconstruction loss: 0.05363106727600098
Training batch 26 / 32
Total batch reconstruction loss: 0.05901288241147995
Training batch 27 / 32
Total batch reconstruction loss: 0.054849643260240555
Training batch 28 / 32
Total batch reconstruction loss: 0.057326726615428925
Training batch 29 / 32
Total batch reconstruction loss: 0.05654006078839302
Training batch 30 / 32
Total batch reconstruction loss: 0.057634398341178894
Training batch 31 / 32
Total batch reconstruction loss: 0.06012074649333954
Training batch 32 / 32
Total batch reconstruction loss: 0.05011897534132004
Epoch [489/500], Train Loss: 0.0561, Validation Loss: 0.0567, Generator Loss: 11.6740, Discriminator Loss: 0.3063
Training epoch 490 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.05686115846037865
Training batch 2 / 32
Total batch reconstruction loss: 0.058350831270217896
Training batch 3 / 32
Total batch reconstruction loss: 0.054982684552669525
Training batch 4 / 32
Total batch reconstruction loss: 0.056319545954465866
Training batch 5 / 32
Total batch reconstruction loss: 0.05953935161232948
Training batch 6 / 32
Total batch reconstruction loss: 0.059086985886096954
Training batch 7 / 32
Total batch reconstruction loss: 0.05971109867095947
Training batch 8 / 32
Total batch reconstruction loss: 0.057487890124320984
Training batch 9 / 32
Total batch reconstruction loss: 0.06033280864357948
Training batch 10 / 32
Total batch reconstruction loss: 0.06354884058237076
Training batch 11 / 32
Total batch reconstruction loss: 0.05876637250185013
Training batch 12 / 32
Total batch reconstruction loss: 0.05742537975311279
Training batch 13 / 32
Total batch reconstruction loss: 0.06464312970638275
Training batch 14 / 32
Total batch reconstruction loss: 0.057576604187488556
Training batch 15 / 32
Total batch reconstruction loss: 0.05946478247642517
Training batch 16 / 32
Total batch reconstruction loss: 0.05688200891017914
Training batch 17 / 32
Total batch reconstruction loss: 0.05746486783027649
Training batch 18 / 32
Total batch reconstruction loss: 0.061363887041807175
Training batch 19 / 32
Total batch reconstruction loss: 0.05595934018492699
Training batch 20 / 32
Total batch reconstruction loss: 0.055724136531353
Training batch 21 / 32
Total batch reconstruction loss: 0.059888407588005066
Training batch 22 / 32
Total batch reconstruction loss: 0.05603296309709549
Training batch 23 / 32
Total batch reconstruction loss: 0.05950091779232025
Training batch 24 / 32
Total batch reconstruction loss: 0.05703018978238106
Training batch 25 / 32
Total batch reconstruction loss: 0.05917106568813324
Training batch 26 / 32
Total batch reconstruction loss: 0.056293681263923645
Training batch 27 / 32
Total batch reconstruction loss: 0.05569157004356384
Training batch 28 / 32
Total batch reconstruction loss: 0.055835649371147156
Training batch 29 / 32
Total batch reconstruction loss: 0.058440618216991425
Training batch 30 / 32
Total batch reconstruction loss: 0.05799535661935806
Training batch 31 / 32
Total batch reconstruction loss: 0.056628383696079254
Training batch 32 / 32
Total batch reconstruction loss: 0.058848850429058075
Epoch [490/500], Train Loss: 0.0563, Validation Loss: 0.0563, Generator Loss: 11.7073, Discriminator Loss: 0.3145
Training epoch 491 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.05782989412546158
Training batch 2 / 32
Total batch reconstruction loss: 0.0598292276263237
Training batch 3 / 32
Total batch reconstruction loss: 0.06182974576950073
Training batch 4 / 32
Total batch reconstruction loss: 0.05890023335814476
Training batch 5 / 32
Total batch reconstruction loss: 0.058185771107673645
Training batch 6 / 32
Total batch reconstruction loss: 0.053791873157024384
Training batch 7 / 32
Total batch reconstruction loss: 0.06194647401571274
Training batch 8 / 32
Total batch reconstruction loss: 0.06011652573943138
Training batch 9 / 32
Total batch reconstruction loss: 0.0619874931871891
Training batch 10 / 32
Total batch reconstruction loss: 0.0591665580868721
Training batch 11 / 32
Total batch reconstruction loss: 0.05531106889247894
Training batch 12 / 32
Total batch reconstruction loss: 0.057337723672389984
Training batch 13 / 32
Total batch reconstruction loss: 0.056242119520902634
Training batch 14 / 32
Total batch reconstruction loss: 0.05601309984922409
Training batch 15 / 32
Total batch reconstruction loss: 0.05585146322846413
Training batch 16 / 32
Total batch reconstruction loss: 0.05861683934926987
Training batch 17 / 32
Total batch reconstruction loss: 0.05960908159613609
Training batch 18 / 32
Total batch reconstruction loss: 0.05751975625753403
Training batch 19 / 32
Total batch reconstruction loss: 0.05523808300495148
Training batch 20 / 32
Total batch reconstruction loss: 0.05773095041513443
Training batch 21 / 32
Total batch reconstruction loss: 0.05795028805732727
Training batch 22 / 32
Total batch reconstruction loss: 0.05836755037307739
Training batch 23 / 32
Total batch reconstruction loss: 0.05703030526638031
Training batch 24 / 32
Total batch reconstruction loss: 0.06249489635229111
Training batch 25 / 32
Total batch reconstruction loss: 0.06024710834026337
Training batch 26 / 32
Total batch reconstruction loss: 0.05820325389504433
Training batch 27 / 32
Total batch reconstruction loss: 0.06105852127075195
Training batch 28 / 32
Total batch reconstruction loss: 0.05920672044157982
Training batch 29 / 32
Total batch reconstruction loss: 0.0545722097158432
Training batch 30 / 32
Total batch reconstruction loss: 0.057074375450611115
Training batch 31 / 32
Total batch reconstruction loss: 0.057094771414995193
Training batch 32 / 32
Total batch reconstruction loss: 0.05784183740615845
Epoch [491/500], Train Loss: 0.0564, Validation Loss: 0.0562, Generator Loss: 11.7221, Discriminator Loss: 0.3068
Training epoch 492 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.053991205990314484
Training batch 2 / 32
Total batch reconstruction loss: 0.06058696657419205
Training batch 3 / 32
Total batch reconstruction loss: 0.062013883143663406
Training batch 4 / 32
Total batch reconstruction loss: 0.05463402345776558
Training batch 5 / 32
Total batch reconstruction loss: 0.05725981295108795
Training batch 6 / 32
Total batch reconstruction loss: 0.057101257145404816
Training batch 7 / 32
Total batch reconstruction loss: 0.056159697473049164
Training batch 8 / 32
Total batch reconstruction loss: 0.0605962872505188
Training batch 9 / 32
Total batch reconstruction loss: 0.06279392540454865
Training batch 10 / 32
Total batch reconstruction loss: 0.06009945273399353
Training batch 11 / 32
Total batch reconstruction loss: 0.06048741191625595
Training batch 12 / 32
Total batch reconstruction loss: 0.05769721418619156
Training batch 13 / 32
Total batch reconstruction loss: 0.0545450821518898
Training batch 14 / 32
Total batch reconstruction loss: 0.05961967259645462
Training batch 15 / 32
Total batch reconstruction loss: 0.05643991380929947
Training batch 16 / 32
Total batch reconstruction loss: 0.0577668771147728
Training batch 17 / 32
Total batch reconstruction loss: 0.052629657089710236
Training batch 18 / 32
Total batch reconstruction loss: 0.056302256882190704
Training batch 19 / 32
Total batch reconstruction loss: 0.06236524134874344
Training batch 20 / 32
Total batch reconstruction loss: 0.06111159175634384
Training batch 21 / 32
Total batch reconstruction loss: 0.05715128406882286
Training batch 22 / 32
Total batch reconstruction loss: 0.05820069834589958
Training batch 23 / 32
Total batch reconstruction loss: 0.0538303479552269
Training batch 24 / 32
Total batch reconstruction loss: 0.0650741308927536
Training batch 25 / 32
Total batch reconstruction loss: 0.056628063321113586
Training batch 26 / 32
Total batch reconstruction loss: 0.05802087485790253
Training batch 27 / 32
Total batch reconstruction loss: 0.0590367466211319
Training batch 28 / 32
Total batch reconstruction loss: 0.05672141909599304
Training batch 29 / 32
Total batch reconstruction loss: 0.05679772049188614
Training batch 30 / 32
Total batch reconstruction loss: 0.05517835170030594
Training batch 31 / 32
Total batch reconstruction loss: 0.058895543217659
Training batch 32 / 32
Total batch reconstruction loss: 0.05109331011772156
Epoch [492/500], Train Loss: 0.0557, Validation Loss: 0.0563, Generator Loss: 11.6346, Discriminator Loss: 0.3161
Training epoch 493 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.054967332631349564
Training batch 2 / 32
Total batch reconstruction loss: 0.056898556649684906
Training batch 3 / 32
Total batch reconstruction loss: 0.05318116396665573
Training batch 4 / 32
Total batch reconstruction loss: 0.054932765662670135
Training batch 5 / 32
Total batch reconstruction loss: 0.0547076016664505
Training batch 6 / 32
Total batch reconstruction loss: 0.05895955115556717
Training batch 7 / 32
Total batch reconstruction loss: 0.06155795603990555
Training batch 8 / 32
Total batch reconstruction loss: 0.05735708773136139
Training batch 9 / 32
Total batch reconstruction loss: 0.06578313559293747
Training batch 10 / 32
Total batch reconstruction loss: 0.06114514544606209
Training batch 11 / 32
Total batch reconstruction loss: 0.05780111253261566
Training batch 12 / 32
Total batch reconstruction loss: 0.05762447416782379
Training batch 13 / 32
Total batch reconstruction loss: 0.05338709056377411
Training batch 14 / 32
Total batch reconstruction loss: 0.05789508298039436
Training batch 15 / 32
Total batch reconstruction loss: 0.055758386850357056
Training batch 16 / 32
Total batch reconstruction loss: 0.059099629521369934
Training batch 17 / 32
Total batch reconstruction loss: 0.05465111881494522
Training batch 18 / 32
Total batch reconstruction loss: 0.06613386422395706
Training batch 19 / 32
Total batch reconstruction loss: 0.05778539553284645
Training batch 20 / 32
Total batch reconstruction loss: 0.06084534898400307
Training batch 21 / 32
Total batch reconstruction loss: 0.060050033032894135
Training batch 22 / 32
Total batch reconstruction loss: 0.0550474151968956
Training batch 23 / 32
Total batch reconstruction loss: 0.05374398082494736
Training batch 24 / 32
Total batch reconstruction loss: 0.061788756400346756
Training batch 25 / 32
Total batch reconstruction loss: 0.059030838310718536
Training batch 26 / 32
Total batch reconstruction loss: 0.052780941128730774
Training batch 27 / 32
Total batch reconstruction loss: 0.05920480191707611
Training batch 28 / 32
Total batch reconstruction loss: 0.0584571436047554
Training batch 29 / 32
Total batch reconstruction loss: 0.056264471262693405
Training batch 30 / 32
Total batch reconstruction loss: 0.0563167929649353
Training batch 31 / 32
Total batch reconstruction loss: 0.06307891011238098
Training batch 32 / 32
Total batch reconstruction loss: 0.05462295562028885
Epoch [493/500], Train Loss: 0.0559, Validation Loss: 0.0570, Generator Loss: 11.6360, Discriminator Loss: 0.3160
Training epoch 494 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.05775216966867447
Training batch 2 / 32
Total batch reconstruction loss: 0.05778167396783829
Training batch 3 / 32
Total batch reconstruction loss: 0.05737285315990448
Training batch 4 / 32
Total batch reconstruction loss: 0.055271320044994354
Training batch 5 / 32
Total batch reconstruction loss: 0.062381256371736526
Training batch 6 / 32
Total batch reconstruction loss: 0.0637730062007904
Training batch 7 / 32
Total batch reconstruction loss: 0.05957822874188423
Training batch 8 / 32
Total batch reconstruction loss: 0.058000896126031876
Training batch 9 / 32
Total batch reconstruction loss: 0.06020069122314453
Training batch 10 / 32
Total batch reconstruction loss: 0.05608760565519333
Training batch 11 / 32
Total batch reconstruction loss: 0.05932319536805153
Training batch 12 / 32
Total batch reconstruction loss: 0.058256931602954865
Training batch 13 / 32
Total batch reconstruction loss: 0.05755220353603363
Training batch 14 / 32
Total batch reconstruction loss: 0.05543321371078491
Training batch 15 / 32
Total batch reconstruction loss: 0.05629216879606247
Training batch 16 / 32
Total batch reconstruction loss: 0.05433172360062599
Training batch 17 / 32
Total batch reconstruction loss: 0.06305596232414246
Training batch 18 / 32
Total batch reconstruction loss: 0.05723727494478226
Training batch 19 / 32
Total batch reconstruction loss: 0.054946351796388626
Training batch 20 / 32
Total batch reconstruction loss: 0.06100282445549965
Training batch 21 / 32
Total batch reconstruction loss: 0.05724571645259857
Training batch 22 / 32
Total batch reconstruction loss: 0.05832841247320175
Training batch 23 / 32
Total batch reconstruction loss: 0.05640818923711777
Training batch 24 / 32
Total batch reconstruction loss: 0.05844344198703766
Training batch 25 / 32
Total batch reconstruction loss: 0.05851680785417557
Training batch 26 / 32
Total batch reconstruction loss: 0.06012215465307236
Training batch 27 / 32
Total batch reconstruction loss: 0.052567705512046814
Training batch 28 / 32
Total batch reconstruction loss: 0.05776023119688034
Training batch 29 / 32
Total batch reconstruction loss: 0.05654754489660263
Training batch 30 / 32
Total batch reconstruction loss: 0.05518385022878647
Training batch 31 / 32
Total batch reconstruction loss: 0.05553813278675079
Training batch 32 / 32
Total batch reconstruction loss: 0.05255230516195297
Epoch [494/500], Train Loss: 0.0555, Validation Loss: 0.0562, Generator Loss: 11.5931, Discriminator Loss: 0.3235
Training epoch 495 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.05385975539684296
Training batch 2 / 32
Total batch reconstruction loss: 0.05777731537818909
Training batch 3 / 32
Total batch reconstruction loss: 0.05857495218515396
Training batch 4 / 32
Total batch reconstruction loss: 0.05789532512426376
Training batch 5 / 32
Total batch reconstruction loss: 0.05434967577457428
Training batch 6 / 32
Total batch reconstruction loss: 0.054586127400398254
Training batch 7 / 32
Total batch reconstruction loss: 0.055370595306158066
Training batch 8 / 32
Total batch reconstruction loss: 0.0594794824719429
Training batch 9 / 32
Total batch reconstruction loss: 0.056452926248311996
Training batch 10 / 32
Total batch reconstruction loss: 0.0533762164413929
Training batch 11 / 32
Total batch reconstruction loss: 0.055961646139621735
Training batch 12 / 32
Total batch reconstruction loss: 0.059300944209098816
Training batch 13 / 32
Total batch reconstruction loss: 0.059756021946668625
Training batch 14 / 32
Total batch reconstruction loss: 0.058887820690870285
Training batch 15 / 32
Total batch reconstruction loss: 0.05846918374300003
Training batch 16 / 32
Total batch reconstruction loss: 0.06022687628865242
Training batch 17 / 32
Total batch reconstruction loss: 0.05916436016559601
Training batch 18 / 32
Total batch reconstruction loss: 0.05492623895406723
Training batch 19 / 32
Total batch reconstruction loss: 0.06371039152145386
Training batch 20 / 32
Total batch reconstruction loss: 0.06081454083323479
Training batch 21 / 32
Total batch reconstruction loss: 0.06294094026088715
Training batch 22 / 32
Total batch reconstruction loss: 0.05613183230161667
Training batch 23 / 32
Total batch reconstruction loss: 0.0564897283911705
Training batch 24 / 32
Total batch reconstruction loss: 0.05607577785849571
Training batch 25 / 32
Total batch reconstruction loss: 0.06324706971645355
Training batch 26 / 32
Total batch reconstruction loss: 0.058580927550792694
Training batch 27 / 32
Total batch reconstruction loss: 0.05811560899019241
Training batch 28 / 32
Total batch reconstruction loss: 0.0573844350874424
Training batch 29 / 32
Total batch reconstruction loss: 0.06095673143863678
Training batch 30 / 32
Total batch reconstruction loss: 0.05946514755487442
Training batch 31 / 32
Total batch reconstruction loss: 0.056030724197626114
Training batch 32 / 32
Total batch reconstruction loss: 0.050800684839487076
Epoch [495/500], Train Loss: 0.0559, Validation Loss: 0.0571, Generator Loss: 11.6197, Discriminator Loss: 0.3228
Training epoch 496 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.055420245975255966
Training batch 2 / 32
Total batch reconstruction loss: 0.057811811566352844
Training batch 3 / 32
Total batch reconstruction loss: 0.05860062688589096
Training batch 4 / 32
Total batch reconstruction loss: 0.061743877828121185
Training batch 5 / 32
Total batch reconstruction loss: 0.06036372110247612
Training batch 6 / 32
Total batch reconstruction loss: 0.06671591848134995
Training batch 7 / 32
Total batch reconstruction loss: 0.05670144408941269
Training batch 8 / 32
Total batch reconstruction loss: 0.05745755136013031
Training batch 9 / 32
Total batch reconstruction loss: 0.05816392973065376
Training batch 10 / 32
Total batch reconstruction loss: 0.05409449711441994
Training batch 11 / 32
Total batch reconstruction loss: 0.06010916456580162
Training batch 12 / 32
Total batch reconstruction loss: 0.05737236887216568
Training batch 13 / 32
Total batch reconstruction loss: 0.05656201392412186
Training batch 14 / 32
Total batch reconstruction loss: 0.05758986622095108
Training batch 15 / 32
Total batch reconstruction loss: 0.05605879798531532
Training batch 16 / 32
Total batch reconstruction loss: 0.05583452805876732
Training batch 17 / 32
Total batch reconstruction loss: 0.06033766642212868
Training batch 18 / 32
Total batch reconstruction loss: 0.05413113161921501
Training batch 19 / 32
Total batch reconstruction loss: 0.05767327547073364
Training batch 20 / 32
Total batch reconstruction loss: 0.05824694037437439
Training batch 21 / 32
Total batch reconstruction loss: 0.06342613697052002
Training batch 22 / 32
Total batch reconstruction loss: 0.057205989956855774
Training batch 23 / 32
Total batch reconstruction loss: 0.05745770037174225
Training batch 24 / 32
Total batch reconstruction loss: 0.05829084664583206
Training batch 25 / 32
Total batch reconstruction loss: 0.06094120442867279
Training batch 26 / 32
Total batch reconstruction loss: 0.0561700314283371
Training batch 27 / 32
Total batch reconstruction loss: 0.05815524235367775
Training batch 28 / 32
Total batch reconstruction loss: 0.06235995143651962
Training batch 29 / 32
Total batch reconstruction loss: 0.056544482707977295
Training batch 30 / 32
Total batch reconstruction loss: 0.055396683514118195
Training batch 31 / 32
Total batch reconstruction loss: 0.05846978724002838
Training batch 32 / 32
Total batch reconstruction loss: 0.05424513667821884
Epoch [496/500], Train Loss: 0.0560, Validation Loss: 0.0565, Generator Loss: 11.6893, Discriminator Loss: 0.3190
Training epoch 497 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.06024927645921707
Training batch 2 / 32
Total batch reconstruction loss: 0.05949319526553154
Training batch 3 / 32
Total batch reconstruction loss: 0.06136946752667427
Training batch 4 / 32
Total batch reconstruction loss: 0.05555332452058792
Training batch 5 / 32
Total batch reconstruction loss: 0.05877966806292534
Training batch 6 / 32
Total batch reconstruction loss: 0.061438966542482376
Training batch 7 / 32
Total batch reconstruction loss: 0.05594421923160553
Training batch 8 / 32
Total batch reconstruction loss: 0.05366191640496254
Training batch 9 / 32
Total batch reconstruction loss: 0.05676595866680145
Training batch 10 / 32
Total batch reconstruction loss: 0.056804366409778595
Training batch 11 / 32
Total batch reconstruction loss: 0.05765081197023392
Training batch 12 / 32
Total batch reconstruction loss: 0.05892438068985939
Training batch 13 / 32
Total batch reconstruction loss: 0.057924725115299225
Training batch 14 / 32
Total batch reconstruction loss: 0.059846267104148865
Training batch 15 / 32
Total batch reconstruction loss: 0.06118127331137657
Training batch 16 / 32
Total batch reconstruction loss: 0.05528305470943451
Training batch 17 / 32
Total batch reconstruction loss: 0.059253498911857605
Training batch 18 / 32
Total batch reconstruction loss: 0.05867394804954529
Training batch 19 / 32
Total batch reconstruction loss: 0.06159301847219467
Training batch 20 / 32
Total batch reconstruction loss: 0.05648862570524216
Training batch 21 / 32
Total batch reconstruction loss: 0.0588444247841835
Training batch 22 / 32
Total batch reconstruction loss: 0.057816773653030396
Training batch 23 / 32
Total batch reconstruction loss: 0.05764664337038994
Training batch 24 / 32
Total batch reconstruction loss: 0.057756636291742325
Training batch 25 / 32
Total batch reconstruction loss: 0.05810587853193283
Training batch 26 / 32
Total batch reconstruction loss: 0.05459047481417656
Training batch 27 / 32
Total batch reconstruction loss: 0.059419117867946625
Training batch 28 / 32
Total batch reconstruction loss: 0.05807197839021683
Training batch 29 / 32
Total batch reconstruction loss: 0.05480760335922241
Training batch 30 / 32
Total batch reconstruction loss: 0.06001890450716019
Training batch 31 / 32
Total batch reconstruction loss: 0.059880271553993225
Training batch 32 / 32
Total batch reconstruction loss: 0.06512400507926941
Epoch [497/500], Train Loss: 0.0564, Validation Loss: 0.0573, Generator Loss: 11.7598, Discriminator Loss: 0.3062
Training epoch 498 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.05975405126810074
Training batch 2 / 32
Total batch reconstruction loss: 0.05755888670682907
Training batch 3 / 32
Total batch reconstruction loss: 0.058804191648960114
Training batch 4 / 32
Total batch reconstruction loss: 0.05685247480869293
Training batch 5 / 32
Total batch reconstruction loss: 0.058218587189912796
Training batch 6 / 32
Total batch reconstruction loss: 0.059237949550151825
Training batch 7 / 32
Total batch reconstruction loss: 0.05665253847837448
Training batch 8 / 32
Total batch reconstruction loss: 0.05559709668159485
Training batch 9 / 32
Total batch reconstruction loss: 0.05562809854745865
Training batch 10 / 32
Total batch reconstruction loss: 0.05684790015220642
Training batch 11 / 32
Total batch reconstruction loss: 0.059933047741651535
Training batch 12 / 32
Total batch reconstruction loss: 0.054560549557209015
Training batch 13 / 32
Total batch reconstruction loss: 0.06085097789764404
Training batch 14 / 32
Total batch reconstruction loss: 0.055782753974199295
Training batch 15 / 32
Total batch reconstruction loss: 0.058254748582839966
Training batch 16 / 32
Total batch reconstruction loss: 0.05785130336880684
Training batch 17 / 32
Total batch reconstruction loss: 0.058079760521650314
Training batch 18 / 32
Total batch reconstruction loss: 0.0599028542637825
Training batch 19 / 32
Total batch reconstruction loss: 0.05570288002490997
Training batch 20 / 32
Total batch reconstruction loss: 0.052717700600624084
Training batch 21 / 32
Total batch reconstruction loss: 0.05752687528729439
Training batch 22 / 32
Total batch reconstruction loss: 0.05482081323862076
Training batch 23 / 32
Total batch reconstruction loss: 0.05906740576028824
Training batch 24 / 32
Total batch reconstruction loss: 0.0652426928281784
Training batch 25 / 32
Total batch reconstruction loss: 0.05691360682249069
Training batch 26 / 32
Total batch reconstruction loss: 0.05895398557186127
Training batch 27 / 32
Total batch reconstruction loss: 0.0588231235742569
Training batch 28 / 32
Total batch reconstruction loss: 0.05270141735672951
Training batch 29 / 32
Total batch reconstruction loss: 0.05523591488599777
Training batch 30 / 32
Total batch reconstruction loss: 0.05931585282087326
Training batch 31 / 32
Total batch reconstruction loss: 0.06064746528863907
Training batch 32 / 32
Total batch reconstruction loss: 0.0704362764954567
Epoch [498/500], Train Loss: 0.0559, Validation Loss: 0.0564, Generator Loss: 11.6871, Discriminator Loss: 0.3122
Training epoch 499 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.06086244434118271
Training batch 2 / 32
Total batch reconstruction loss: 0.05453720688819885
Training batch 3 / 32
Total batch reconstruction loss: 0.05500254034996033
Training batch 4 / 32
Total batch reconstruction loss: 0.05844893306493759
Training batch 5 / 32
Total batch reconstruction loss: 0.05759561061859131
Training batch 6 / 32
Total batch reconstruction loss: 0.058549024164676666
Training batch 7 / 32
Total batch reconstruction loss: 0.05908520519733429
Training batch 8 / 32
Total batch reconstruction loss: 0.05528455227613449
Training batch 9 / 32
Total batch reconstruction loss: 0.06473098695278168
Training batch 10 / 32
Total batch reconstruction loss: 0.058591850101947784
Training batch 11 / 32
Total batch reconstruction loss: 0.05646717920899391
Training batch 12 / 32
Total batch reconstruction loss: 0.05746254697442055
Training batch 13 / 32
Total batch reconstruction loss: 0.059488553553819656
Training batch 14 / 32
Total batch reconstruction loss: 0.05946577340364456
Training batch 15 / 32
Total batch reconstruction loss: 0.054855525493621826
Training batch 16 / 32
Total batch reconstruction loss: 0.05644582211971283
Training batch 17 / 32
Total batch reconstruction loss: 0.05849974602460861
Training batch 18 / 32
Total batch reconstruction loss: 0.06009863317012787
Training batch 19 / 32
Total batch reconstruction loss: 0.06288105994462967
Training batch 20 / 32
Total batch reconstruction loss: 0.057550154626369476
Training batch 21 / 32
Total batch reconstruction loss: 0.05359506607055664
Training batch 22 / 32
Total batch reconstruction loss: 0.05508630722761154
Training batch 23 / 32
Total batch reconstruction loss: 0.060874514281749725
Training batch 24 / 32
Total batch reconstruction loss: 0.06108561158180237
Training batch 25 / 32
Total batch reconstruction loss: 0.06210613250732422
Training batch 26 / 32
Total batch reconstruction loss: 0.057488322257995605
Training batch 27 / 32
Total batch reconstruction loss: 0.05907377973198891
Training batch 28 / 32
Total batch reconstruction loss: 0.056659892201423645
Training batch 29 / 32
Total batch reconstruction loss: 0.06136475130915642
Training batch 30 / 32
Total batch reconstruction loss: 0.05695028230547905
Training batch 31 / 32
Total batch reconstruction loss: 0.05712822079658508
Training batch 32 / 32
Total batch reconstruction loss: 0.04898121580481529
Epoch [499/500], Train Loss: 0.0562, Validation Loss: 0.0562, Generator Loss: 11.6659, Discriminator Loss: 0.3194
Training epoch 500 / 500
Training batch 1 / 32
Total batch reconstruction loss: 0.05572228506207466
Training batch 2 / 32
Total batch reconstruction loss: 0.05603497847914696
Training batch 3 / 32
Total batch reconstruction loss: 0.057734400033950806
Training batch 4 / 32
Total batch reconstruction loss: 0.05455918237566948
Training batch 5 / 32
Total batch reconstruction loss: 0.061128899455070496
Training batch 6 / 32
Total batch reconstruction loss: 0.05607825145125389
Training batch 7 / 32
Total batch reconstruction loss: 0.054444700479507446
Training batch 8 / 32
Total batch reconstruction loss: 0.06249609589576721
Training batch 9 / 32
Total batch reconstruction loss: 0.05463435500860214
Training batch 10 / 32
Total batch reconstruction loss: 0.055363766849040985
Training batch 11 / 32
Total batch reconstruction loss: 0.05673635005950928
Training batch 12 / 32
Total batch reconstruction loss: 0.0595538355410099
Training batch 13 / 32
Total batch reconstruction loss: 0.057100895792245865
Training batch 14 / 32
Total batch reconstruction loss: 0.05844110995531082
Training batch 15 / 32
Total batch reconstruction loss: 0.061128176748752594
Training batch 16 / 32
Total batch reconstruction loss: 0.056024692952632904
Training batch 17 / 32
Total batch reconstruction loss: 0.05901747941970825
Training batch 18 / 32
Total batch reconstruction loss: 0.0603790357708931
Training batch 19 / 32
Total batch reconstruction loss: 0.05893043428659439
Training batch 20 / 32
Total batch reconstruction loss: 0.05537496879696846
Training batch 21 / 32
Total batch reconstruction loss: 0.05838901549577713
Training batch 22 / 32
Total batch reconstruction loss: 0.05947062000632286
Training batch 23 / 32
Total batch reconstruction loss: 0.05700894817709923
Training batch 24 / 32
Total batch reconstruction loss: 0.06293784081935883
Training batch 25 / 32
Total batch reconstruction loss: 0.06299927085638046
Training batch 26 / 32
Total batch reconstruction loss: 0.05346427112817764
Training batch 27 / 32
Total batch reconstruction loss: 0.056772246956825256
Training batch 28 / 32
Total batch reconstruction loss: 0.058983709663152695
Training batch 29 / 32
Total batch reconstruction loss: 0.05892913043498993
Training batch 30 / 32
Total batch reconstruction loss: 0.05691838636994362
Training batch 31 / 32
Total batch reconstruction loss: 0.059785909950733185
Training batch 32 / 32
Total batch reconstruction loss: 0.05681603401899338
Epoch [500/500], Train Loss: 0.0558, Validation Loss: 0.0577, Generator Loss: 11.6625, Discriminator Loss: 0.3030
